{
    "2025-03-21": {
        "0": {
            "title": "Nvidia Quantum Day: Jensen Huang, D-Wave, Rigetti, IonQ, on Future of Quantum Computing",
            "link": "https://www.barrons.com/livecoverage/nvidia-quantum-day-d-wave-rigetti-ionq",
            "snippet": "Quantum industry leaders and Nvidia CEO Jensen Huang are set to discuss the future of quantum computing from Nvidia's GTC conference.",
            "score": 0.9129596948623657,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "GTC 2025 \u2013 Announcements and Live Updates",
            "link": "https://blogs.nvidia.com/blog/nvidia-keynote-at-gtc-2025-ai-news-live-updates/",
            "snippet": "What's next in AI is at GTC 2025. Explore the news and live updates from the show, from the keynote to the final session.",
            "score": 0.9221877455711365,
            "sentiment": null,
            "probability": null,
            "content": "All the news from NVIDIA\u2019s biggest gathering of the year, including new services and hardware, tech demos and what\u2019s next in AI.\n\nWhat\u2019s next in AI is at GTC 2025. Not only the technology, but the people and ideas that are pushing AI forward \u2014 creating new opportunities, novel solutions and whole new ways of thinking. For all of that, this is the place.\n\nHere\u2019s where to find the news, hear the discussions, see the robots and ponder the just-plain mind-blowing. From the keynote to the final session, check back for live coverage from San Jose, California.\n\nQuantum Day: NVIDIA CEO Announces Quantum Computing Lab, and NVIDIA Tech Key to Advancing Quantum Innovations, Industry Leaders Say \ud83d\udd17\n\nNVIDIA is diving deeper into quantum computing with plans to open a dedicated research lab in Boston, founder and CEO Jensen Huang announced Thursday as he kicked off a series of panels at GTC featuring industry leaders.\n\n\u201cIt will likely be the most advanced accelerated computing, hybrid quantum computing research lab in the world,\u201d Huang said.\n\nThe panels at GTC\u2019s inaugural Quantum Day highlighted the critical role accelerated computing will play in advancing the nascent technology.\n\nWhile technologies and approaches to quantum development vary, panelists agreed that GPUs and quantum systems will work hand in hand to unlock new breakthroughs.\n\nNVIDIA\u2019s new lab will serve as a hub for collaboration with leading researchers from institutions such as Harvard University and the Massachusetts Institute of Technology, among others.\n\nThe facility aims to accelerate innovation in quantum computing, a field with vast potential for revolutionizing industries from cryptography to materials science.\n\nThe event, moderated by Huang, included three panels and a long list of companies building quantum systems. Guests included:\n\nAlan Baratz, CEO of D-Wave\n\nBen Bloom, founder and CEO of Atom Computing\n\nJohn Levy, cofounder and CEO of SEEQC\n\nKrysta Svore, technical fellow at Microsoft\n\nLo\u00efc Henriet, CEO of Pasqal\n\nMatthew Kinsella, CEO of Infleqtion\n\nMikhail Lukin, Joshua and Beth Friedman University Professor at Harvard University and cofounder of QuEra Computing\n\nPete Shadbolt, cofounder and chief scientific officer of PsiQuantum\n\nPeter Chapman, executive chair at IonQ\n\nRajeeb Hazra, president and CEO of Quantinuum\n\nRob Schoelkopf, cofounder and chief scientist at Quantum Circuits\n\nSimone Severini, general manager of quantum technologies at AWS\n\nSubodh Kulkarni, CEO of Rigetti\n\nTh\u00e9au Peronnin, cofounder and CEO of Alice & Bob\n\nThey discussed quantum computing progress on reaching double-digit numbers of logical qubits, as well as challenges, such as the needs to reach triple-digit qubits for practical applications and to scale to millions of qubits in the years ahead.\n\nLearn more about quantum computing.\n\nAI Is Changing the Game \u2014 Literally \ud83d\udd17\n\nAI is revolutionizing live sports, unlocking new ways to engage fans, optimize operations and enhance player performance, said sports industry leaders on a panel at GTC.\n\n\u201cAI has been in sports for many years, but the [industry] continue[s] to be early adopters of the latest innovations,\u201d said Sepi Motamedi, senior product marketing manager at NVIDIA.\n\nFor the National Hockey League and La Liga, Spain\u2019s premier football league, AI isn\u2019t just a tool \u2014 it\u2019s a game changer. Le Liga has cameras in all its stadiums, retrieving over 3 million data points per match, according to Javier Gil Fernandez, head of AI implementation and development at the league.\n\nMeanwhile, the NHL is piloting optical tracking to capture players\u2019 skeletal movements. \u201cThe amount of data is incredible, and it\u2019s just going to keep growing,\u201d said Dave Lehanski, executive vice president of business development and innovation at the NHL.\n\nBeyond data, AI is reshaping fan experiences. \u201cWouldn\u2019t it be great if AI automatically served up game storylines in real time?\u201d Lehanski asked.\n\nAI-powered video analysis will let fans instantly search, summarize and relive key moments.\n\nAs AI integration deepens, the sports industry faces challenges \u2014 from legal hurdles to regulation. But one thing is clear: \u201cIf the industry understands AI and can leverage it fully, we will be good to go,\u201d Fernandez said. The future of sports is fast, smart and AI-driven.\n\nBritish Member of Parliament Shares UK\u2019s Sovereign AI Vision \ud83d\udd17\n\nSovereign AI \u2014 a nation\u2019s capacity to develop and deploy AI using domestic infrastructure, datasets and workforces \u2014 was a key theme at GTC, highlighted in a half-day summit headlined by government and industry leaders from countries including Brazil, Denmark, India, Japan, Thailand and the U.K.\n\nIn decades and centuries past, the availability of resources including coal, oil and electricity \u201cdefined which governments and countries grew and which did not,\u201d said Peter Kyle, a U.K. member of parliament and secretary of state for science, innovation and technology, at the summit. \u201cToday, we find ourselves in the midst of another epochal shift. Who swims and who sinks will depend on compute.\u201d\n\nKyle outlined the U.K.\u2019s AI Opportunities Action Plan, which aims to expand compute capacity for AI research at least 30x by 2030 and will transform currently unused facilities and land to develop data centers for building AI models trained on the country\u2019s rich datasets in healthcare and other domains.\n\nThe U.K. government\u2019s Isambard-AI supercomputer, which features 5,448 NVIDIA GH200 Grace Hopper Superchips to deliver a whopping 21 exaflops of AI performance, represents the progress already made toward the nation\u2019s sovereign AI goals. With the emergence of the new test-time scaling law of AI development, Kyle noted, there\u2019s much more ahead.\n\n\u201cThe computer is only as good as the people who are using it and the data that you put into it,\u201d Kyle said. To \u201ctake on the great challenges that will define the century to come will require more than just brute capacity. Building bigger or faster is simply not enough \u2014 in the age of compute, states must build smarter, too.\u201d\n\nNVIDIA Omniverse-Powered Moon Machines to Explore Lunar Surface \ud83d\udd17\n\nAcross the rugged, cratered landscape of the moon, a coordinated fleet of autonomous machines \u2014 drones, excavators and haulers \u2014 seamlessly navigates the harsh vacuum of space, working in sync to map, extract and transport valuable resources.\n\nThis concept for the future of lunar exploration was laid out by Lutz Richter, a space robotics expert, in a captivating presentation at GTC.\n\nRichter, representing premier IT consulting and digital services provider SoftServe, described how his team has helped simulate and model this multi-robot system using the NVIDIA Omniverse platform for physical AI. \u201cWe think space will benefit a lot from the advanced simulation and modeling tools offered by Omniverse,\u201d Richter said.\n\nThe drones would scour the lunar surface, using high-resolution sensors to identify regions of interest for ice extraction. The ground-based excavator would then meticulously dig into the regolith, with SoftServe\u2019s advanced soil mechanics simulation optimizing the process of unearthing the icy deposits. Once loaded, the hauler vehicle would transport the cargo back to the lunar outpost.\n\nRichter also showcased SoftServe\u2019s work simulating the drones\u2019 ability to explore lunar caves. The drones use real-time visual simultaneous localization and mapping to navigate the complex terrain and chart potential ice reserves hidden within.\n\nBy integrating robotics with Omniverse\u2019s powerful simulation and collaboration tools, SoftServe drives a groundbreaking approach to lunar resource exploration.\n\nDemocratizing Immersive Experiences With AI \ud83d\udd17\n\nAt the XR Pavilion, hosted at the Tech Interactive science and technology center, GTC attendees explored immersive demos of digital twins, 3D rendering and virtual reality experiences. If the frequent smiles and exclamations of \u201cwow\u201d are an indication of where things are headed with these visual AI technologies, there\u2019s a lot to look forward to.\n\nRobotics Experts Discuss Outlook for Humanoids \ud83d\udd17\n\nSpeaking to a crowd at San Jose Civic Center, a panel of robotics industry executives weighed in humanoid robot development and what the future holds for making them a reality with consumers.\n\nPanelists discussed the role of large language models, simulation and improvements in robotics hardware in helping develop specialized humanoid robots.\n\n\u201cWe have some of the most extraordinary founders on some of the best robot hardware I\u2019ve ever seen,\u201d said Jim Fan, principal research scientist and senior research manager at NVIDIA. \u201cI think hardware has become a lot better and a lot cheaper.\u201d\n\nAaron Saunders, chief technology officer of Boston Dynamics, said that physics-based simulations have been groundbreaking for the field of robotics. \u201cThe closure of the sim-to-real gap is a big deal,\u201d he said.\n\nLearning by experience is the main change that has happened in robotics, noted Deepak Pathak, cofounder and CEO of Skild AI. \u201cWhat has changed is completely how we approach robotics,\u201d he said.\n\nPanelists also talked about the importance of data sources in training models. \u201cNow we get this global data the way we get an internet, and you can solve all the problems we have today, and all we need is a lot more GPUs,\u201d said Bernt B\u00f8rnich, founder and CEO of 1X.\n\nRobots Galore on the Show Floor \ud83d\udd17\n\nFrom helpful humanoids to robotic arms supporting surgeries, deliveries and industrial processes, physical AI advancements are on full display with interactive demos at GTC.\n\nHow Low-Cost, Self-Driving Microscopes Can Transform Public Health \ud83d\udd17\n\nManu Prakash is on a mission to empower community health workers and citizen scientists around the world with AI-powered microscopes that help detect malaria and other diseases.\n\nPrakash is cofounder of microscopy company Cephla and an associate professor at the Woods Institute for the Environment at Stanford University. He\u2019s integrating AI into low-cost microscopes that can be deployed in grassroots healthcare environments for real-time disease monitoring at scale.\n\n\u201cThis is the first machine that can do something like this in real time,\u201d he said, demonstrating live how Octopi, Cephla\u2019s modular robotic microscopy platform, can rapidly analyze red blood cells to detect malaria parasites. \u201cOne of the phenomenal aspects of this is we can train it not just for malaria \u2014 we can train it for any disease.\u201d\n\nPrakash and his team found that deploying their pipeline on the NVIDIA Jetson Nano module accelerated image processing by 5x compared with using desktop computers \u2014 enabling them to examine about 3 million red blood cells per minute.\n\nBy enabling rapid, low-cost disease detection that can be used by community health workers, Prakash hopes to advance the capabilities of global public health organizations to better tackle challenges such as the emergence of antibiotic-resistant disease strains.\n\nBeyond malaria, his team has collaborated with researchers to adapt their AI pipeline for other disease areas. One organization was able to train an AI pipeline to detect four types of sickle cell disease within just six months. Another trained a pipeline to detect if individuals are actively spreading tuberculosis.\n\n\u201cSometimes, instruments with a little bit of intelligence can go an astronomical distance,\u201d Prakash said.\n\nHow AI Is Unlocking a New Era of Creativity in Filmmaking \ud83d\udd17\n\nAI is making filmmaking more accessible and changing how filmmakers bring their visions to life.\n\nSpeaking at GTC, Haohong Wang, general manager at TCL, shared how AI-powered tools like Runway, Sora and MineStudio are unlocking faster workflows, new artistic possibilities and a more accessible future for content creation.\n\nAI Meets Creativity: Traditional filmmaking can be slow and expensive. Wang predicts that AI-generated content budgets could make it possible to create filmed content at less than 10% of today\u2019s costs, making high-quality storytelling available to more filmmakers.\n\nBut it\u2019s not just about savings. AI allows artists to experiment with new visual styles in real time, letting them generate entire scenes instantly, explore new storytelling techniques and break traditional production barriers.\n\nDirecting AI: AI filmmaking still struggles with consistency, camera control and human movement, Wang noted. Many directors feel they lack the same control as traditional filmmaking. The solution: a structured 3D workflow. By digitizing AI-generated assets \u2014 characters, environments and animations \u2014 filmmakers can blend the best of AI and traditional techniques, ensuring precise control over cinematography and visual consistency.\n\nContent Flywheel: Wang described AI filmmaking as a \u201cflywheel model,\u201d where easier content creation fuels audience growth, attracts advertisers and drives demand for more creative work.\n\n\u201cSo you can see there\u2019s a loop [so] that the advertising money will eventually go back to the creative community,\u201d Wang explained. \u201cOnce this opportunity is there, more and more creatives will join to use AI to produce more original content and get more audiences, get more advertisers.\u201d\n\nWhile some in Hollywood remain skeptical, Wang believes growing adoption will make AI a standard tool for filmmakers \u2014 just as CGI now is.\n\nA Sneak Peek: To show that potential, Wang showed clips from \u201cNext Stop Paris,\u201d a hybrid live-action and AI short film. The story follows two strangers on an adventure \u2014 and a twist reveals AI has been shaping the narrative all along.\n\nHealthcare VP Kimberly Powell Highlights Life Sciences Leaps \ud83d\udd17\n\nAddressing a packed crowd at the Montgomery Theater yesterday, Kimberly Powell, vice president of healthcare and life sciences at NVIDIA, underscored no less than nine GTC announcements delivering industry advances.\n\nThe news included the new NVIDIA DGX Spark AI supercomputer, NVIDIA AgentIQ, the MONAI multimodal and agent framework, NVIDIA Holoscan 3.0, NVIDIA Isaac for Healthcare and partner moves from Sapio Sciences, Cadence and Epic, which is accelerating AI with NVIDIA NIM on Microsoft Azure.\n\nAnd those were only just some of the highlights.\n\n\u201cMany of you have chosen to release some of your most important breakthroughs here at GTC, with several dozen new product announcements and critical breakthroughs,\u201d said Powell.\n\nShe discussed the role of accelerated computing in medical imaging and genomics, as well as in the introduction of the world\u2019s largest biology foundation model, Evo 2, trained on 9 trillion nucleotides, the building blocks for RNA and DNA.\n\nPowell said that biology represents an unsolved problem that\u2019s now positioned to benefit from leaps in progress from AI.\n\n\u201cWe are starting to experience some exponential levels of biology intelligence by being able to represent biology in a computer,\u201d she said.\n\nPowell also discussed the role of AI in drug discovery, the development of AI agents for use in clinical trials and the introduction of physical AI for medical devices.\n\n\u201cWe\u2019re at this next frontier where not only do we have digital agents but we\u2019re going to have physical agents where we\u2019re going to embody AI into physical things,\u201d said Powell.\n\nRead more about how AI is advancing healthcare and watch the talk replay \u2014 virtual registration is free.\n\nUnlock Peak AI Performance With NVIDIA DGX Cloud Benchmarking \ud83d\udd17\n\nAchieving peak AI performance requires more than just powerful hardware. Understanding how hardware and software impact AI workload performance is crucial for technical validation and business planning.\n\nTo help organizations make data-driven decisions and maximize GPU utilization, NVIDIA launched NVIDIA DGX Cloud Benchmarking \u2014 a suite of tools that empowers users to optimize their AI workload and infrastructure investments effectively.\n\nDGX Cloud Benchmarking includes three key components:\n\nPerformance Recipes: Standardized benchmark scripts and baseline results that enable AI platform performance validation across a diverse set of AI workloads.\n\nStandardized benchmark scripts and baseline results that enable AI platform performance validation across a diverse set of AI workloads. Performance Explorer: An interactive tool that provides performance data across AI workloads.\n\nAn interactive tool that provides performance data across AI workloads. Expert guidance and best practices: Work with NVIDIA to simplify the optimization process.\n\nBy providing standardized benchmarks and actionable insights, DGX Cloud Benchmarking helps AI teams optimize their workloads, reduce costs and accelerate innovation.\n\nNVIDIA collaborates with cloud service providers to co-engineer and optimize performance across the entire stack and deliver highly performant AI platforms. This ensures that optimizations are based on real-world scenarios, leading to significant performance improvements over time.\n\nDGX Cloud Benchmarking is designed to evolve with the AI industry, incorporating new models, hardware platforms and software optimizations to ensure customers have access to the latest performance insights.\n\nWhether you\u2019re an AI development team, or an IT team or a cloud service provider validating infrastructure performance, DGX Cloud Benchmarking provides the tools needed to unlock peak AI performance.\n\nLearn more about DGX Cloud Benchmarking.\n\nNVIDIA Expands Benefits for Startups \ud83d\udd17\n\nAs part of a significant expansion of benefits for members of the NVIDIA Inception program for cutting-edge startups, NVIDIA is offering qualified members up to $100,000 in NVIDIA DGX Cloud credits for dedicated NVIDIA H100 GPU capacity.\n\nMembers approved for this benefit will also have access to expert office hours that provide technical guidance and support. With DGX Cloud, a unified AI platform, Inception members can use NVIDIA software, services and AI expertise to build, customize and deploy applications on their preferred cloud.\n\nOther new and enhanced benefits \u2014 encompassing nearly $300,000 in credits and discounts \u2014 offer unparalleled access to cutting-edge technology from NVIDIA Cloud Partners (NCPs), NVIDIA Partner Network independent software vendors (ISVs) and select Inception members. These offers can help Inception startups and members of the NVIDIA Connect program for ISVs build and scale their businesses, fostering innovation and growth across diverse industries.\n\nNCP Offers and Additional Benefits\n\nNCPs such as Nebius, Scaleway and Lintasarta are offering free cloud credits to eligible Inception members, helping them reduce costs and foster innovation. Yotta and Lambda are providing Inception and Connect members with cloud credits to boost scalability and speed time to market.\n\nCompanies adopting NCP services will benefit from enhanced local support, expanded GPU allocation and access, advanced technical capabilities and hands-on assistance, access to local data centers, and flexible pricing and billing to meet the distinct needs of small and growing companies.\n\nNew and expanded NCP collaborations will include onboarding and technical support through training and webinars, as well as access to regional events and opportunities for go-to-market support.\n\nInception members will also gain access to top tools and software from companies like AI21, Beamr, Bria, DeepChecks, Lightning AI, RGo Robotics, Tabnine and Weights & Biases.\n\nMembers of the NVIDIA Connect program for ISVs will also gain access to select benefits, available only through the Inception and Connect member benefits catalog.\n\nJoin the NVIDIA Inception and NVIDIA Connect programs. Additional partners and exclusive offers will be added regularly.\n\nCompanies using and showcasing NVIDIA technology to build compelling tools and services for startup and developer audiences can learn more about the programs and fill out a partner benefit proposal.\n\nResearch Giants Bill Dally, Yann LeCun Share the Stage at GTC \ud83d\udd17\n\nTwo luminaries in AI research \u2014 Bill Dally, chief scientist at NVIDIA, and Yann LeCun, chief AI scientist at Meta and a professor at New York University \u2014 took the stage today at the San Jose Civic auditorium to share their vision for the field\u2019s future.\n\nDally asked LeCun to share his thoughts on artificial general intelligence, the impact of AI on scientific research, his current work on world models and the qualities needed to foster innovation.\n\nLeCun predicted that artificial general intelligence, or AGI \u2014 which he prefers to call advanced machine intelligence, because \u201chuman intelligence is superspecialized, so calling it general is a misnomer\u201d \u2014 will be viable in three to five years.\n\nHe emphasized the importance of open-source projects to support the development of diverse AI assistants.\n\n\u201cWe need assistants that are extremely diverse,\u201d he said. \u201cWe need to speak all languages, understand all the cultures, all the value systems, all the sectors of interest. So we need a platform that anybody can use to build those assistants, a diverse population of assistants \u2014 and right now that can only be done through open-source platforms.\u201d\n\nLeCun also spoke about his work at Meta to develop world models that can understand, reason and plan around physical environments.\n\n\u201cWhat you need is a predictor that, given the state of the world and an action you imagine, can predict the next state of the world,\u201d he said. \u201cAnd if you have such a system, then you can plan a sequence of actions to arrive at a particular outcome.\u201d\n\nDally noted that building world models like these will require significant AI infrastructure powered by NVIDIA GPUs.\n\n\u201cKeep them coming,\u201d LeCun quipped. \u201cWe\u2019re going to need all the computation we can get our hands on.\u201d\n\nWrap-Up: Key Points From the GTC Keynote \ud83d\udd17\n\nHere are the major points NVIDIA founder and CEO Jensen Huang covered in his keynote:\n\nWe\u2019re at a $1 trillion computing inflection point. AI computing demand is accelerating rapidly, driven by the rise of reasoning AI and agentic AI. The scale and complexity of AI workloads are transforming data center investments worldwide.\n\nAI computing demand is accelerating rapidly, driven by the rise of reasoning AI and agentic AI. The scale and complexity of AI workloads are transforming data center investments worldwide. NVIDIA Blackwell is in full production, delivering 40x the performance of Hopper. The Blackwell architecture significantly enhances AI model training and inference, enabling more efficient and scalable AI applications. And the next evolution of the NVIDIA Blackwell AI factory platform, Blackwell Ultra, will be coming to systems in the second half of this year.\n\nThe Blackwell architecture significantly enhances AI model training and inference, enabling more efficient and scalable AI applications. And the next evolution of the NVIDIA Blackwell AI factory platform, Blackwell Ultra, will be coming to systems in the second half of this year. NVIDIA will follow an annual rhythm for the buildout of AI infrastructure. Each year will bring new GPUs, CPUs and accelerated computing advancements, including the upcoming NVIDIA Vera Rubin architecture, designed to drive performance gains and efficiency improvements in AI data centers.\n\nEach year will bring new GPUs, CPUs and accelerated computing advancements, including the upcoming NVIDIA Vera Rubin architecture, designed to drive performance gains and efficiency improvements in AI data centers. AI infrastructure, including photonics and AI-optimized storage, is set to revolutionize the industry. Advanced networking and storage solutions will improve AI scalability, efficiency and energy consumption in large-scale data centers.\n\nAdvanced networking and storage solutions will improve AI scalability, efficiency and energy consumption in large-scale data centers. Physical AI for industrial and robotics is a $50 trillion opportunity. AI-powered robotics and automation are set to transform manufacturing, logistics, healthcare and other industries, with the NVIDIA Isaac and Cosmos platforms leading the way.\n\nWatch the replay:\n\nRead more details about the keynote below and stay tuned for more insights and sessions throughout the conference, running through Friday, March 21.\n\nDive In: NVIDIA Founder and CEO Jensen Huang\u2019s Keynote at GTC \ud83d\udd17\n\nWelcome to NVIDIA\n\nHuang kicked off the keynote by bringing the audience \u201cinto NVIDIA\u2019s headquarters,\u201d with a stunning visual of its lobby that seemed to embrace the audience. He began by talking about where NVIDIA started 25 years ago, with GPUs. He described the growth of AI over the past decade, including the emergence of agentic AI \u2014 which can reason how to solve a problem, plan and take action.\n\nAI at an Inflection Point\n\nHuang then outlined the development of AI able to reason \u201cstep by step by step\u201d and discussed how demand for inference and reinforcement learning is driving demand for AI computing. Demand for GPUs from the top four cloud service providers is surging, as AI is going through an \u201cinflection point.\u201d Huang said he expects the value of data center buildout to reach $1 trillion.\n\nNVIDIA CUDA Ecosystem\n\nNVIDIA CUDA-X GPU accelerated libraries and microservices are now serving every industry, Huang explained. In the future, Huang said, every company will have two factories: one for what they build and another for AI. Ticking through a sampling of NVIDIA\u2019s role in a variety of endeavors, he announced that NVIDIA is going to open-source its cuOpt decision optimization platform. The installed base of CUDA is now \u201ceverywhere,\u201d he said. \u201cWe have reached the tipping point of accelerated computing \u2014 CUDA has made it possible.\u201d\n\nGeneral Motors, NVIDIA Collaborate on AI\n\nAI needs infrastructure, Huang explained. AI is now going out \u201cto the rest of the world,\u201d in robotics and self-driving cars, factories and wireless networks. One of the earliest industries AI went into was autonomous vehicles, Huang said. \u201cWe build technology that almost every single self-driving car company uses,\u201d he added, whether in the data center or the car. Huang announced the next step of that journey: GM, the largest U.S. automaker, is adopting NVIDIA AI, simulation and accelerated computing to develop next-generation vehicles, factories and robots. He also announced NVIDIA Halos, a comprehensive safety system bringing together NVIDIA\u2019s lineup of automotive hardware and software safety solutions with its cutting-edge AI research in AV safety.\n\nData Centers and Inference\n\nNext up: Huang spoke about data centers. He reported that the NVIDIA Blackwell platform is in full production, sharing a shot of systems from a broad range of industry partners. \u201cHow is this not beautiful,\u201d Huang said.\n\nHe described how Blackwell supports extreme scale-up. \u201cThe reason why we wanted to do this is to solve an extreme problem,\u201d Huang said, \u201cand it\u2019s called inference.\u201d\n\nInference, Huang explained, is token generation, which will be critical to businesses. AI factories that generate these tokens have to be built with extreme efficiency and performance. And demand for tokens will only grow, with the latest generation of reasoning models able to think through and solve increasingly complex problems.\n\nTo further accelerate inference on a large scale, Huang announced NVIDIA Dynamo, open-source software for accelerating and scaling AI reasoning models in AI factories. \u201cIt is essentially the operating system of an AI factory,\u201d Huang said.\n\nNVIDIA Blackwell Ultra\n\n\n\nHuang described how Blackwell delivers a \u201cgiant leap\u201d in inference performance. \u201cYou want to make sure you have the most energy-efficient architecture you can possibly get,\u201d Huang said, showing how Blackwell does more work, using less power than the previous generation. \u201cThe more you buy, the more you save,\u201d Huang said, drawing laughter from the audience. \u201cIt\u2019s even better than that \u2014 the more you buy, the more you make.\u201d\n\nHuang then rolled a video showing how a new NVIDIA Omniverse Blueprint can help plan a 1 gigawatt AI factory, letting engineers design, test and optimize a new generation of intelligence manufacturing data centers using digital twins.\n\nThen, he announced the next evolution of the NVIDIA Blackwell AI factory platform, NVIDIA Blackwell Ultra, coming in systems the second half of this year. NVIDIA Blackwell Ultra boosts training and test-time scaling inference \u2014 the art of applying more compute during inference to improve accuracy \u2014 to enable organizations everywhere to accelerate applications such as AI reasoning, agentic AI and physical AI.\n\nNVIDIA Vera Rubin\n\nPaying tribute to astronomer Vera Rubin, Huang outlined a roadmap that will deliver data center performance gains for years to come, Huang offered new details on the next-generation NVIDIA Rubin Ultra GPU and NVIDIA Vera CPU architectures, packed with innovations. \u201cBasically everything is brand new except for the chassis,\u201d Huang said. Systems built on Rubin Ultra, including the Vera Rubin NVL 144, will arrive in the second half of next year. And due for the second half of 2027: systems built on Rubin Ultra. \u201cYou can see that Rubin is going to drive the cost down tremendously,\u201d Huang said.\n\nNVIDIA Photonics\n\nHuang shifted to discussing how NVIDIA will help customers scale out to ever larger systems. Key to that is tightly integrating photonics \u2014 networking technologies that rely on transmitting data using light rather than electrical signals \u2014 into accelerated computing infrastructure. Enabling AI factories to connect millions of GPUs across sites while reducing energy consumption and operational costs, NVIDIA Spectrum-X and NVIDIA Quantum-X silicon photonics networking switches fuse electronic circuits and optical communications. \u201cThis is really crazy technology,\u201d Huang said. NVIDIA photonics switches integrate optics innovations with 4x fewer lasers to deliver 3.5x more power efficiency, 63x greater signal integrity, 10x better network resiliency at scale and 1.3x faster deployment compared with traditional methods.\n\nDGX Spark and DGX Station\n\nEnabling AI developers, researchers, data scientists and students to prototype, fine-tune and inference large models on desktops, NVIDIA unveiled DGX personal AI supercomputers powered by the NVIDIA Grace Blackwell platform. Describing it as the perfect Christmas present, Huang announced DGX Spark \u2014 formerly Project DIGITS \u2014 and DGX Station, a new high-performance NVIDIA Grace Blackwell desktop supercomputer powered by the NVIDIA Blackwell Ultra platform, bringing the power of the Grace Blackwell architecture to the desktop. Users can run these models locally or deploy them on NVIDIA DGX Cloud or any other accelerated cloud or data center infrastructure. \u201cThis is the computer of the age of AI,\u201d Huang said.\n\nAgentic AI\n\n\n\nTransitioning into a discussion of the future of agentic AI, Huang announced the open Llama Nemotron family of models with reasoning capabilities, designed to provide developers and enterprises a business-ready foundation for creating advanced AI agents that can work independently or as connected teams to solve complex tasks. Built on Llama models, the NVIDIA Llama Nemotron reasoning family delivers on-demand AI reasoning capabilities. NVIDIA enhanced the new reasoning model family during post-training to improve multistep math, coding, reasoning and complex decision-making.\n\nPhysical AI and Robotics\n\nDescribing robots as the next $10 trillion industry, Huang said that by the end of this decade, the world is going to be at least 50 million workers short. NVIDIA offers a complete suite of technologies for training, deploying, simulating and testing next-generation robotics.\n\nIn a video, Huang announced the availability of NVIDIA Isaac GR00T N1, the world\u2019s first open, fully customizable foundation model for generalized humanoid reasoning and skills.\n\nNVIDIA also announced a major release of new NVIDIA Cosmos world foundation models, introducing an open and fully customizable reasoning model for physical AI development and giving developers unprecedented control over world generation.\n\n\u201cUsing Omniverse to condition Cosmos, and Cosmos to generate an infinite number of environments, allows us to create data that is grounded, controlled by us and yet systematically infinite at the same time,\u201d Huang said.\n\nHe also introduced the Newton open-source physics engine for robotics simulation, under development with Google DeepMind and Disney Research, before he was joined on stage by a diminutive robot, \u201cBlue,\u201d that emerged from a hatch on the floor, beeping and booping at Huang.\n\n\u2018Let\u2019s Wrap Up\u2019\n\nHuang wound up his talk by emphasizing several major themes.\n\nFirst, Blackwell is in full production \u2014 \u201cand the ramp is incredible, customer demand is incredible,\u201d Huang reported. \u201cAnd for good reason, because there is an inflection point in AI, the amount of computation we have to do in AI is so much greater as a result of reasoning AI, and the training of reasoning AI systems and agentic systems.\u201d\n\nSecond, Blackwell NVL72 with Dynamo offers 40x the AI factory performance of NVIDIA Hopper, and \u201cinference is going to be one of the most important workloads in the next decade as we scale out AI.\u201d\n\nThird, NVIDIA has an \u201cannual rhythm\u201d of roadmaps, so the world can plan its AI infrastructure. NVIDIA is building three AI infrastructures: one for the cloud, a second for the enterprise and a third for robots.\n\nCountdown to Keynote at GTC \ud83d\udd17\n\nThe SAP Center is waking up, bathed in a soothing purple glow. A sea of attendees flows in \u2014 developers, researchers, business leaders \u2014 filling the space with a charge of anticipation.\n\nUp on the big screen: the pre-show with Acquired podcast hosts Ben Gilbert and David Rosenthal, who are interviewing industry figures from the GTC show floor. Earlier today they even got a drop-in from NVIDIA founder and CEO Jensen Huang.\n\nIt\u2019s almost time.\n\nAt 10:00 a.m. PT, Huang takes the stage, setting the tone for everything unfolding at GTC. Crowds in the arena will feel it \u2014 the shuffle of last-minute arrivals, the glow of smartphone screens.\n\nThose tuning in remotely won\u2019t miss a moment. Watch the keynote live. The future of AI, accelerated computing and everything in between is about to be unveiled.\n\nStay with us \u2014 we\u2019re just getting started.\n\nKeynote Today: Tune In Early for the Preshow With Acquired \ud83d\udd17\n\nThe excitement starts long before NVIDIA founder and CEO Jensen Huang takes the stage. Catch the \u201cLive at NVIDIA GTC With Acquired\u201d broadcast now, featuring luminary speakers who\u2019ll offer fascinating insights into NVIDIA\u2019s remarkable journey over the past three decades.\n\nState of AI Art: How Artists Bring AI, Robots Into Creative Process \ud83d\udd17\n\n\u201cConsumption,\u201d a piece by BREAKFAST, on display at GTC.\n\nArtists over millennia have created with every medium available \u2014 fruit and vegetable dyes, blocks of marble, acrylic paints, cameras, rendering software, 3D printers. They\u2019re now harnessing AI and robotics to build immersive experiences that reflect on the ways people respond to and interact with technology.\n\nIn a GTC panel discussion hosted today by Heather Schoell, creative director of AI strategy at NVIDIA, four artists shared how they use AI and robotics in their creative processes.\n\nThe integration of robotics in art \u201cis a very logical continuation in what is a very long spectrum of humans and our relationship to our tools,\u201d said panelist Catie Cuan, founder of consumer AI company Zenie and a postdoc at Stanford University, where she leads art and robotics efforts at the Stanford Robotics Center.\n\nA former professional dancer, Cuan\u2019s works include an eight-hour duet with a robot arm and a symphony of dancing robots that played sounds as they moved.\n\nAnother panelist, Alexander Reben, was OpenAI\u2019s first artist in residence. He\u2019s used large language models, visual generative AI models and NeRFs to create 3D sculptures.\n\nInteractive art by two of the panelists \u2014 Zolty, a kinetic and robotics artist known as BREAKFAST, and Emanuel Gollob, a doctoral researcher at the University of Arts Linz, Austria \u2014 is on display at the main entrance of GTC.\n\nBREAKFAST works with real-time data to create large kinetic art sculptures that are on display aboard a Royal Caribbean cruise ship and the Fontainebleau hotel in Las Vegas. The studio\u2019s piece at GTC, titled \u201cConsumption,\u201d runs on an NVIDIA RTX GPU and features robotic arches that move in response to real-time data about water usage, including rainfall, reservoirs, groundwater levels and municipal supplies.\n\nGollob\u2019s robotic installation, called \u201cDoing Nothing With AI,\u201d uses generative robot control, brainwave measurements and reinforcement learning to move in a way that guides a participant wearing an EEG headband to do nothing and enjoy a moment of inaction.\n\nPanelists Discuss Energy Efficiency in the Age of Generative AI \ud83d\udd17\n\nTo a packed room of attendees, panelists held an illuminating discussion on AI, energy and climate issues today at GTC. Josh Parker, senior director of corporate sustainability at NVIDIA, was joined by Lauren Risi from the Wilson Center, David Sandalow from Columbia University and Bernhard Lorentz of Deloitte.\n\n\u201cIn the world of climate change, there\u2019s an enormous amount of things that AI can do,\u201d said Sandalow. \u201cHow can you mobilize AI tools to mitigate climate change?\u201d\n\nThe industry experts addressed concerns about energy efficiency with the rise of generative AI and the uptick in data centers.\n\n\u201cNVIDIA GPUs are delivering 100,000x better energy efficiency over the past decade,\u201d said Parker, summing up the energy efficiency gains from NVIDIA GPUs.\n\nBMW Group Shows Off 3D Collaboration for Battery Assembly Factories \ud83d\udd17\n\nBMW Group presented the latest updates to its 3D AppStore today at GTC.\n\nThe 3D AppStore, BMW\u2019s cloud streaming service for high-end visualization, is now making it possible to support the automaker\u2019s battery assembly process across various factories worldwide. It allows engineers, designers and others to work in the digital environment collaboratively.\n\n\u201cWith the avatars, they see the factory scene and can be in there together, and then you can basically just ask what is happening here, and the AI gives you a detailed description of what the station here is doing,\u201d said Xaver Freiherr Loeffelholz von Colberg, product owner of 3D AppStore at BMW Group.\n\nOffering a glimpse into the future of automotive design and manufacturing, BMW has two more speaker sessions on Thursday, March 20.\n\nExplaining Tokens \u2014 the Language and Currency of AI \ud83d\udd17\n\nGTC attendees are likely to hear a lot about tokens \u2014 the language and currency of AI.\n\nTokens are units of data processed by AI models during training and inference, enabling prediction, generation and reasoning. The faster tokens can be processed, the faster AI models can learn and respond.\n\nGet up to speed on tokens, tokenization and the ways enterprises can boost revenue by lowering cost per token in our explainer article.\n\nGTC 2025: Real AI, Real Problems, Real Solutions \ud83d\udd17\n\nAI is confronting humanity\u2019s toughest challenges head on. See it unfold next week at the NVIDIA GTC conference in San Jose, California.\n\nFrom transformative healthcare sessions like \u201cRevolutionizing Cardiac MRI Analysis and Diagnosis With AI\u201d and \u201cDesigning the Future: Protein Engineering, AI and Responsible Innovation\u201d to environmental breakthroughs in \u201cAutonomous Systems and Remote Sensing for Better Earth Data,\u201d \u201cThe Role of AI and Accelerated Computing in Understanding and Mitigating Urban Climate Change\u201d and \u201cEnhancing Photovoltaic Power Prediction With High-Resolution Weather Forecasting From NVIDIA Earth-2,\u201d the impact is tangible and global.\n\nThe Future Rolls Into San Jose \ud83d\udd17\n\n\n\nAnyone who\u2019s been in downtown San Jose lately has seen it happening. The banners are up. The streets are shifting. The whole city is getting a fresh coat of NVIDIA green.\n\nFrom March 17-21, San Jose will become a crossroads for the thinkers, tinkerers and true enthusiasts of AI, robotics and accelerated computing. The conversations will be sharp, fast-moving and sometimes improbable \u2014 but that\u2019s the point.\n\nAt the center of it all? NVIDIA founder and CEO Jensen Huang\u2019s keynote, offering a glimpse into the future. It\u2019ll take place at the SAP Center on Tuesday, March 18, at 10 a.m. PT. Expect big ideas, a few surprises, some roars of laughter and the occasional moment that leaves the room silent.\n\nBut GTC isn\u2019t just what happens on stage. It\u2019s a conference that refuses to stay inside its walls. It spills out into sessions at McEnery Convention Center, hands-on demos at the Tech Interactive Museum, late-night conversations at the Plaza de C\u00e9sar Ch\u00e1vez night market and more. San Jose isn\u2019t just hosting GTC. It\u2019s becoming it.\n\nThe speakers are a mix of visionaries and builders \u2014 the kind of people who make you rethink what\u2019s possible:\n\n\ud83e\udde0 Yann LeCun \u2013 chief AI scientist at Meta, professor, New York University\n\n\ud83c\udfc6 Frances Arnold \u2013 Nobel Laureate, Caltech\n\n\ud83d\ude97 RJ Scaringe \u2013 founder and CEO of Rivian\n\n\ud83e\udd16 Pieter Abbeel \u2013 robotics pioneer, UC Berkeley\n\n\ud83c\udf0d Arthur Mensch \u2013 CEO of Mistral AI\n\n\ud83c\udf2e Joe Park \u2013 chief digital and technology officer of Yum! Brands\n\n\u265f\ufe0f Noam Brown \u2013 research scientist at OpenAI\n\nSome are pushing the limits of AI itself; others are weaving it into the world around us.\n\n\ud83d\udce2 Want in? Register now.\n\nCheck back here for what to watch, read and play \u2014 and what it all means. Tune in to all the big moments, the small surprises and the ideas that\u2019ll stick for years to come.\n\nSee you in San Jose. #GTC25",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia CEO Huang: Tariffs shouldn't 'significantly' hurt financials. Hopes to increase onshore manufacturing.",
            "link": "https://finance.yahoo.com/news/nvidia-ceo-huang-tariffs-shouldnt-significantly-hurt-financials-hopes-to-increase-onshore-manufacturing-131510678.html",
            "snippet": "Nvidia CEO Jensen Huang said he isn't expecting tariffs to hurt the company's financial outlook.",
            "score": 0.9455126523971558,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) CEO Jensen Huang said he isn't expecting tariffs to hurt the company's financial outlook. During a press briefing following his keynote at Nvidia's GTC 2025 event, the executive explained that he believes his company's supply chain is agile enough to withstand the threat of a duty on chips.\n\n\"I think in the near term, based on what we know, we're not expecting significant impact to our outlook and our financials,\" Huang said.\n\nCEO Jensen Huang talks during the keynote address of Nvidia GTC on March 18 in San Jose, Calif. (AP Photo/Nic Coury) \u00b7 ASSOCIATED PRESS\n\nPresident Trump has threatened to levy upwards of 25% tariffs on chips manufactured outside the US, putting companies like Nvidia, AMD, Apple, and others at risk of having to either eat some or all of the increased costs of bringing their chips to America or pass it on to their customers.\n\nBut Huang said Nvidia's network of suppliers is nimble and distributed among a number of countries, which could help the company deal with tariffs.\n\nRead more: The latest news and updates on Trump's tariffs\n\n\"They're kind of distributed in a lot of places, and it depends on what gets built, what gets purchased in the United States, and where [is] the final destination of the product,\" Huang explained. \"And so, there's just a whole lot of equations involved and it depends on which country gets tariffed.\"\n\nOver the long term, Huang said he wants to increase Nvidia's manufacturing capacity in the US.\n\n\"The simplest way to think about that is our agility is terrific today, missing onshore manufacturing,\" the CEO said. \"If we add onshore manufacturing by the end of this year, we should be quite good.\"\n\nNvidia, like many chip giants, relies heavily on Taiwan-based TSMC (TSMC34.SA) to build its hardware. And while the semiconductor manufacturer has a massive presence in its home market, it's building new facilities in the US and abroad.\n\nIn early March, the company announced plans to increase its investment in the US from an already announced $65 billion to $165 billion. The additional $100 billion, the firm said, will go toward the construction of three new fabrication plants, two packaging facilities, and an R&D center.\n\nStill, TSMC has to provide capacity to a number of customers, meaning it won't be able to allocate all that new manufacturing capability to Nvidia's needs.\n\nIn addition to tariffs, Nvidia is dealing with the threat of increased US export controls on chips headed for the Chinese market. The moves come after China's DeepSeek revealed that it developed its V3 and R1 models using Nvidia's below-top-of-the-line chips, raising fears that the country could outpace the US in AI advancements.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia launches research center to accelerate quantum computing breakthrough",
            "link": "https://www.networkworld.com/article/3851393/nvidia-launches-research-center-to-accelerate-quantum-computing-breakthrough.html",
            "snippet": "The tech giant is collaborating with prominent quantum computing companies and academia to solve some of quantum computing's most complex challenges at the...",
            "score": 0.7123223543167114,
            "sentiment": null,
            "probability": null,
            "content": "In a strategic move that could reshape the future of computing, Nvidia is establishing the Nvidia Accelerated Quantum Research Center (NVAQC) in Boston, where the worlds of quantum hardware and AI supercomputing will converge to tackle what many consider computing\u2019s final frontier.\n\nThe initiative represents one of the most significant corporate investments in quantum computing to date, potentially accelerating the timeline to practical, real-world quantum applications.\n\n\u201cQuantum computing will augment AI supercomputers to tackle some of the world\u2019s most important problems, from drug discovery to materials development,\u201d Nvidia CEO Jensen Huang said in a statement. \u201cWorking with the wider quantum research community to advance CUDA-quantum hybrid computing, the Nvidia Accelerated Quantum Research Center is where breakthroughs will be made to create large-scale, useful, accelerated quantum supercomputers.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "NVIDIA Adds Magna to ADAS Roster",
            "link": "https://www.wardsauto.com/industry/nvidia-adds-magna-to-adas-roster",
            "snippet": "NVIDIA is adding Magna International, the 4th largest Tier 1 auto supplier in the world by revenue, to its roster of partners to develop ADAS technology the...",
            "score": 0.9204798340797424,
            "sentiment": null,
            "probability": null,
            "content": "It\u2019s a big week for NVIDIA, adding Magna International to its growing list of auto industry partners using its AI and driver-assistance systems (ADAS) technology the same week it announced a major partnership with General Motors.\n\nThe deal with Magna, the 4th largest Tier 1 auto supplier in the world by revenue, calls for integrating the NVIDIA DRIVE AGX Thor system-on-a-chip within Magna\u2019s next generation AI applications for software-defined vehicles (SDVs), various levels of autonomous driving and vehicle advanced interior cabin technology solutions. NVIDIA\u2019s chips run the safety-certified DriveOS operating system and is built on the Blackwell GPU architecture.\n\nMagna says it will develop and test the latest advancements in L2+ through L4 active safety solutions on DRIVE Thor.\n\n\"Combining NVIDIA accelerated compute and AI capabilities with Magna\u2019s extensive automotive expertise and innovation, we aim to explore new standards for next-generation SDV intelligence and autonomy,\u201d said Steven Jenkins, Vice President of Technology Strategy at Magna Electronics. \u201cOur collaboration allows us to develop market applications for AI-powered solutions that could redefine the driving experience and address the evolving demands of the automotive industry.\u201d\n\nRelated:Autoline Daily 2025: Top Industry News for March 20\n\nMagna International has been actively expanding its capabilities in ADAS and SDVs as part of its long-term strategy to align with the automotive industry\u2019s shift toward electrification, automation, and connected technologies.\n\nThe company views ADAS and software as core growth areas, and it has invested heavily in both organic R&D and strategic acquisitions to build a comprehensive technology stack.\n\nIn the ADAS domain, Magna develops a broad range of products including cameras, radar, LiDAR, domain controllers, and perception software. These systems support functions such as adaptive cruise control, automated emergency braking, lane keeping, and highway assist.\n\nOne of Magna\u2019s key differentiators is its ability to integrate these components into complete systems, backed by its experience in vehicle systems integration. The company also collaborates with tech firms and OEMs to accelerate the rollout of scalable ADAS platforms\u2014offering solutions that range from Level 1 driver assistance to Level 3 conditional automation.\n\nOn the software-defined vehicle front, Magna is positioning itself to be a systems integrator and software provider for next-generation E/E architectures. As vehicles increasingly rely on centralized computing and over-the-air (OTA) software updates, Magna has developed embedded software and middleware that support real-time operating systems, functional safety, and cybersecurity. It has also partnered with companies like LG and Aptiv to co-develop key technologies for digital cockpit systems and advanced electronics.\n\nRelated:BYD Unveils Super e-Platform with \u2018Flash-Charge Battery\u2019\n\n\u201cAs the automotive industry transitions to safer, more intelligent vehicles with autonomous driving capabilities, our collaboration with Magna is the latest in our endeavors to bring our safety-certified in-vehicle accelerated compute and AI to the transportation industry,\u201d said Ali Kani, Vice President of Automotive at NVIDIA. \u201cBy combining core technologies and Magna's integration expertise, we aim to shape the future of mobility.\u201d\n\nAccording to NVIDIA, DRIVE AGX Thor delivers up to 1,000 trillion operations per second of AI compute power, featuring 8-bit floating point support optimized for transformer models, large language models and generative AI workloads.\n\nCanada-based Magna plans to unveil a working demonstration platform expected in Q4 2025. As part of this, Magna will tap NVIDIA Drive-OS for development, integration, validation and production workflows.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Here are Friday's biggest analyst calls: Nvidia, Nike, Apple, Tesla, Starbucks, Netflix, Micron, FedEx & more",
            "link": "https://www.cnbc.com/2025/03/21/wall-street-top-stock-calls-by-analysts-friday-like-nvidia.html",
            "snippet": "2025 CNBC LLC. All Rights Reserved. A Division of NBCUniversal. Data is a real-time snapshot *Data is delayed at least 15 minutes.",
            "score": 0.9438015222549438,
            "sentiment": null,
            "probability": null,
            "content": "Here are the biggest calls on Wall Street on Friday: Goldman Sachs reiterates Nike as buy Goldman says Nike's earnings report on Thursday provided \"fodder\" for bulls and bears but that it's ultimately sticking with the stock. \"We remain constructive on the stock but acknowledge the company is early in its turnaround journey.\" JPMorgan reiterates Apple as overweight JPMorgan called Apple \"defensive\" and says it has \"cyclical drivers.\" \"We expect product cycle drivers in the form of iPhone 16E as well as iPad and Mac refreshes to support upside and/or resilience to revenue and earnings, in contrast to the negative sentiment recently relative to delays in AI Siri launches, which are less impactful to near- to medium-term estimates.\" Goldman Sachs reiterates FedEx as buy Goldman says it's sticking with the stock following earnings on Thursday. \"While near term economic and policy uncertainty could continue to impact FDX and the transports at large with respect to their ability to gain traction in the market \u2013 we continue to look to FDX to drive idiosyncratic cost takeout and operational improvement, as well as eventually deliver on an LTL [less than truckload] spin-off as previously announced.\" JPMorgan upgrades Super Micro to neutral from underweight JPMorgan upgraded the AI chip company and says it's starting to move past recent challenges. \"We are upgrading shares of Super Micro to Neutral from Underweight prior as the company has cycled past the uncertainty in relation to pending SEC filings and is on the cusp of benefitting from ramp in Blackwell based server shipments which are already seeing materially higher demand than prior generation, with additional benefit to revenue growth from higher ASPs [average selling prices]. Read more about this call here . Goldman Sachs upgrades Adaptive Biotechnologies to buy from neutral Goldman says the biotech company is \"well positioned to benefit from its attractive economics in the company's niche in hematologic cancers.\" \"Additionally, we have become increasingly positive on the MRD [minimal residual disease] space and believe ADPT is well positioned to benefit from its attractive economics in the company's niche in hematologic cancers.\" Morgan Stanley reiterates Tesla as overweight The firm lowered its price target on the stock on Thursday night to $410 per share from $430 and says Tesla still remains a top idea. \"We reiterate our view that while Tesla's YTD auto deliveries have been mostly below expectations, it is not particularly narrative changing for our investment thesis.\" Read more about this call here. UBS reiterates Nvidia as buy UBS said Nvidia is well positioned for AI exciting its Global AI Conference earlier this week. \"We have long viewed sovereign AI as an underappreciated driver of demand for AI infrastructure, and our sense coming out of this summit is that there is a long runway of AI infrastructure development ahead.\" KeyBanc upgrades Celanese to overweight from sector weight Key says the specialty materials company is a beneficiary of peace in Ukraine. \"Lastly, CE would be a major beneficiary from a potential peace in Ukraine, and better demand in China due to recent government stimulus. We admit the likelihood of these two events remains uncertain.\" Goldman Sachs upgrades Danaher to buy from neutral Goldman upgraded the life sciences and diagnostics conglomerate and says the stock's valuation is too attractive to ignore. \"We are upgrading DHR to Buy from Neutral, with a refreshed view following our Dec 7th '23 DHR downgrade on the back of rich valuation and '24 and '25 consensus earnings expectations which we believed were too high at the time.\" Read more about this call here . Morgan Stanley upgrades Norwegian to equal weight from underweight Morgan Stanley said in its upgrade of Norwegian Cruise Line that it sees a more balanced risk/reward for the cruise company. \"We are upgrading the stock to EW as our main thesis around not being able to narrow its net cruise costs (relative to Net Yields) has played out and driven the relative underperformance albeit amidst a market correction.\" Deutsche Bank reiterates Starbucks as buy The firm says its analysis indicates Starbucks' prices are high but not unreasonably so and shares should continue to grind higher. \"While we recognize concerns that Starbucks is expensive, specialty coffee overall is expensive, and despite that, the market has been growing at an ~8% CAGR over the last decade, supporting our view that there is strong underlying demand for away-from-home coffee.\" Jefferies reiterates Netflix as buy Jefferies says the stock remains well positioned as \"engagements\" are rising. \"NFLX grew web visits ~17% y/y in Jan, this dropped to a decline of ~4% in Feb '25. In our view, this was to be expected coming off a strong content slate (Squid Game, The Night Agent, NFL). Despite the lower volume of visits, NFLX ranked 3rd in growth of duration on site.\" Loop downgrades FedEx to sell from hold Loop downgraded the stock following earnings on Thursday citing tariff risk. \"Firstly, on April 2nd the Trump Administration is scheduled to unveil its comprehensive tariff strategy for global trade and, like it or not, FedEx's brand is synonymous with global trade. Secondly, with economists ratcheting up US recession risk, be aware that FedEx is a really bad recession stock because thin Express margins amplify the earnings hit whenever there's pressure on the top line.\" Read more about this call here . DA Davidson upgrades AutoZone to buy from neutral DA Davidson says AutoZone is defensive. \"We are upgrading to BUY for four reasons including (1) investor flight to quality, (2) defensive positioning, (3) inflation / tariff beneficiary, and (4) commercial market share gains.\" Wells Fargo reiterates Micron as overweight The firm says Micron remains a top pick with strong demand following earnings on Thursday. \"We think MU should trade higher as we see investor sentiment shift to focus on path to $11/sh+ EPS.\"",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia, Micron, Nike, Nio, Carnival, FedEx: Stocks to watch",
            "link": "https://qz.com/stock-watch-nvidia-tesla-nike-micron-fedex-nio-carnival-1851771446",
            "snippet": "Futures point to a lower open after some earnings disappointed investors.",
            "score": 0.9371042847633362,
            "sentiment": null,
            "probability": null,
            "content": "Stocks fell in premarket trading today as the shares of Nike (NKE0.00% ) and FedEx (FDX0.00% ) declined after the companies\u2019 earnings disappointed investors. Futures tied to the S&P 500, Nasdaq 100 and Dow Jones Industrial Average pointed to a lower open as of about 7:45 a.m. EDT. The benchmarks declined yesterday.\n\nCostco demands price cuts from Chinese suppliers as trade tensions escalate and import costs rise CC Share Subtitles Off\n\nEnglish view video Costco demands price cuts from Chinese suppliers as trade tensions escalate and import costs rise\n\nCostco demands price cuts from Chinese suppliers as trade tensions escalate and import costs rise CC Share Subtitles Off\n\nEnglish Costco demands price cuts from Chinese suppliers as trade tensions escalate and import costs rise\n\nThe shares of Chinese electric-vehicle makers will be in focus again today after Nio\u2019s (NIO0.00% ) net loss unexpectedly widened. Yesterday, Geely Auto\u2019s (GELYF0.00% ) American depositary receipts gained 3% in U.S. trading after its earnings beat projections, but shares of its Zeekr EV unit declined 6.9% despite outpacing fourth-quarter expectations. Tesla (TSLA0.00% ) edged up about 0.5% in premarket trading today.\n\nAdvertisement\n\nHere are some stocks to watch today:\n\nCarnival\n\nCarnival\u2019s (CCL0.00% ) stock edged down about 0.9% ahead of the cruise operators earnings, scheduled at 9 a.m. EDT today.\n\nAdvertisement\n\nFedEx\n\nFedEx stock dropped 7.3% in premarket trading after it cut its full-year revenue and profit forecasts due to economic softening and uncertainty in the U.S., as well as severe weather.\n\nAdvertisement\n\nMicron\n\nMicron\u2019s (MU0.00% ) shares fell 3.1% premarket, reversing postmarket gains. The memory-chip maker\u2019s earnings, released after the close yesterday, exceeded expectations for fiscal second-quarter revenue and profit on AI-related demand. The company also boosted its forecasts for the current period.\n\nAdvertisement\n\nNike\n\nNike\u2019s stock tumbled 6.4% before the open after the shoemaker reported a tough quarter as it sought to pivot back toward its athletic origins while navigating increased tariffs on China-made goods and a softening U.S. consumer market.\n\nAdvertisement\n\nNio\n\nNio\u2019s ADRs fell 4% in premarket trading after the Chinese EV-maker\u2019s net loss unexpectedly widened. The shares plunged 8.9% yesterday ahead of the results, and fell 8.8% in Hong Kong on Friday. The shares of Xpeng (XPEV0.00% ), Li Auto (LI0.00% ) and Zeekr also declined in premarket trading.\n\nAdvertisement\n\nNvidia\n\nNvidia\u2019s (NVDA0.00% ) shares slid 0.8% in premarket trading as its AI conference wraps up today. CEO Jensen Huang said the chipmaker is partnering with quantum computing firms including Quantinuum and QuEra Computing to build a Boston-based research center that will integrate quantum computing with AI supercomputers.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NVIDIA GTC 2025 Highlights Foundation Models and AI Drug Discovery",
            "link": "https://www.genengnews.com/topics/artificial-intelligence/nvidia-gtc-2025-highlights-foundation-models-and-ai-drug-discovery/",
            "snippet": "Researchers, executives, and thought leaders across healthcare gathered at NVIDIA GTC to discuss today's pressing AI life science applications.",
            "score": 0.9191568493843079,
            "sentiment": null,
            "probability": null,
            "content": "San Jose \u2014 \u201cThe only way to hold more people at GTC is to grow San Jose!\u201d exclaimed Jensen Huang, CEO of Nvidia, as he took the stage to address over 25,000 in-person (and roughly 300,000 virtual) attendees during his keynote at San Jose\u2019s SAP Center at the annual NVIDIA GTC conference.\n\nNvidia, the Silicon Valley-based microprocessing giant, delivered a packed program to capture the impact of AI across a range of industries, including life sciences, at what Huang described as the \u201cSuper Bowl of AI\u2026 where everyone wins.\u201d\n\nOver 700 healthcare companies from more than 40 countries were represented at GTC to discuss today\u2019s pressing AI applications, from the promise of protein design and foundation models to digitizing healthcare records and fully autonomous labs. Among the line-up of speakers were Nobel Laureate Francis Arnold, PhD, Vijay Pande, PhD, general partner at Andreesen Horowitz, Patrick Hsu, PhD, co-founder and a core investigator of the Arc Institute, and more.\n\nKimberly Powell, vp of healthcare at Nvidia, highlighted that we\u2019ve come a long way in a short period of time in applying large language models and generative AI approaches for the field of drug discovery.\n\n\u201cWe\u2019re taking all the right models and packaging them up so people can access them. They\u2019re getting built into software and R&D platforms of the pharmaceutical industry. So far in 2025, we\u2019re seeing rapid adoption of these capabilities, and we know we need to keep pushing the capabilities higher,\u201d Powell told GEN.\n\nAmong the wealth of Nvidia announcements was the unveiling of DGX Spark powered by the NVIDIA Grace Blackwell platform, which delivers the utility of an AI supercomputer in a desktop-friendly size ideal for researcher and data scientist workloads.\n\nNVIDIA BioNeMo, which provides science-specific AI frameworks, pre-trained models, and generative AI tools to support drug discovery, also received a round of updates.\n\nSapio Sciences, a software development company focusing on drug research, announced the integration of NVIDIA BioNeMo into the Sapio Lab Informatics Platform, which brings computational drug discovery directly into Sapio ELN (Electronic Lab Notebook). Within Sapio ELN, researchers can access BioNeMo NIM microservices to identify and optimize drug candidates using molecular modeling, including AlphaFold2 NIM for predicting protein structures, MoIMIM NIM to design and optimize small molecules, and DiffDock NIM, a docking model developed by MIT.\n\nAdditionally, Cadence announced the expansion of its multi-year collaboration with Nvidia, focusing on driving accelerated computing and agentic AI. Notably, the integration of NVIDIA BioNeMo NIM microservices with Orion, Cadence\u2019s cloud-native molecular design platform, will accelerate tools for drug discovery by combining AI and cloud GPUs. NIM microservices expand Orion\u2019s capabilities in the areas of de novo protein structure prediction, small molecule generative AI, and foundational AI models for antibody property prediction.\n\nFoundational partnerships\n\nNvidia\u2019s commercial announcements were also paired with partnership highlights that emphasized the rise of foundation models that generalize across biological tasks.\n\nIn February, Nvidia, in collaboration with the Arc Institute, announced the release of Evo 2, the \u201clargest publicly accessible AI model for biology to date.\u201d The genome foundation model was trained on more than 9.3 trillion nucleotides from the genomes of more than 128,000 species across the tree of life to provide both predictive and generative capabilities, including identifying disease-associated gene variants and synthesizing new genomes computationally.\n\nEvo 2 is available for public use on the NVIDIA BioNeMo platform and as an interactive user-friendly interface called Evo Designer. In addition, the authors have made its training data, training, inference code, and model weights open source.\n\nNvidia has described an ecosystem approach to partnering to ensure that what\u2019s being built is co-developed with industry experts, such as researchers at Arc, to ensure utility in the field.\n\n\u201cWe partner with people who are pushing the envelope. Arc is clearly doing that at the very bleeding edge of what\u2019s in biological understanding. The consequence of the collaboration is a platform for everyone,\u201d Anthony Costa, PhD, director of digital biology at Nvidia, told GEN.\n\nHsu, one of the lead researchers of Evo 2, explained what makes these technologies powerful is when there are users providing feedback on areas of research that \u201cwe don\u2019t do at Arc.\u201d Example Evo 2 user applications span from basic research to the patient setting, including understanding biosynthetic gene clusters for synthesizing small molecules and analyzing labeled patient data for patient stratification.\n\n\u201cWhen people get more sophisticated on these models, we learn more about the science and technology to get ready to build the next thing,\u201d weighed in Costa.\n\nTaken together, the theme of GTC remained collaboration and wide utility.\n\n\u201cAs a first-time GTC attendee, I\u2019m blown away by the horizontality of the attendees. AI touches every part of society and it\u2019s really remarkable to see the breadth of industries at GTC,\u201d said Hsu.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia\u2019s Huang Pivots to the Next Stage of AI \u2014 Making It Useful",
            "link": "https://www.bloomberg.com/news/newsletters/2025-03-21/nvidia-s-huang-pivots-to-the-next-stage-of-ai-making-it-useful",
            "snippet": "Welcome to Tech In Depth, our revamped daily newsletter with reporting and analysis about the business of tech from Bloomberg's journalists around the world...",
            "score": 0.9015340209007263,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia Continues Torrid AI Startup Investment Pace, Outstripping Microsoft And Google",
            "link": "https://news.crunchbase.com/venture/nvda-goog-msft-ai-startup-investment/",
            "snippet": "Just last month Lambda, which offers computing services and hardware for training artificial intelligence software, raised a $480 million Series D that...",
            "score": 0.8802596926689148,
            "sentiment": null,
            "probability": null,
            "content": "0 Shares Email\n\nFacebook\n\nTwitter\n\nLinkedIn\n\nJust last month Lambda, which offers computing services and hardware for training artificial intelligence software, raised a $480 million Series D that reportedly valued the company at $2.5 billion.\n\nThat same day, Together AI, a developer of a cloud platform to allow developers to build on open and custom artificial intelligence models, raised a $305 million funding round at a $3.3 billion valuation.\n\nWhat the rounds had in common was one investor: chip giant Nvidia.\n\nThat should come as little surprise to anyone who has watched AI funding in the last two-plus years since Microsoft agreed to a multiyear, multibillion-dollar investment into OpenAI \u2014 the first and loudest shot in the artificial intelligence arms Big Tech would begin to play.\n\nNvidia continues to outpace other huge AI players such as Microsoft and Google both in terms of funding rounds it participates in for AI startups, and the total value of those rounds. (It is impossible to say how much Nvidia invested in most rounds as a specific stake in a round is not usually divulged.)\n\nLet\u2019s take a look at what these three tech titans \u2014 who have been the most curious when it comes to VC-backed startups in AI \u2014 have done in the past year-plus.\n\nNvidia\n\nJust last year, Nvidia took part in 26 rounds that totaled $18.8 billion, per Crunchbase data. That far outpaces its two dozen deals in 2023, which totaled $4.5 billion.\n\nHowever, to be fair, that dollar total is bloated since the chipmaker took part in xAI\u2019s $6 billion round in November and OpenAI\u2019s massive $6.6 billion deal in October.\n\nAside from those deals, Nvidia also took part in London-based self-driving car startup Wayve\u2019s $1.1 billion round, and data labeling and evaluation startup Scale AI\u2019s $1 billion round that valued it at a stunning $13.8 billion, both in May.\n\nWhat may be most impressive about Nvidia\u2019s AI investing is how it is driving its overall corporate investment strategy. In 2022, Nvidia invested in nine funding rounds, per Crunchbase. In each of the past two years, it has participated in 36 a year.\n\nThe company\u2019s venture arm, NVentures, also has been active \u2014 although on a much smaller scale. Last year it took part in 14 investments, with those rounds totaling $89 million, per Crunchbase data. That\u2019s slightly ahead of 2023, when the investment arm participated in 11 deals that totaled $824 million.\n\nMicrosoft\n\nPerhaps surprisingly, the same cannot be said for Microsoft.\n\nLast year, the Redmond, Washington-based tech giant took part in nine investments in AI startups, with those rounds totaling nearly $10 billion, per Crunchbase data. That was slightly behind 2023, when it participated in a dozen deals that totaled $12.2 billion.\n\nSome of Microsoft\u2019s biggest deals actually overlap with Nvidia, as it also took part in both the previously mentioned rounds for OpenAI and Wayve, as well as massive fundings for Figure and Bright Machines.\n\nHowever, the Windows maker also made headlines when it poured a $1.5 billion strategic investment into United Arab Emirates-based artificial intelligence firm G42.\n\nThe company\u2019s venture arm, M12, has continued to keep up a pretty consistent pace. Last year, the firm took part in 10 funding rounds that totaled $268 million, per Crunchbase data. Similarly, in 2023 it took part in 9 deals totaling $442 million.\n\nMicrosoft has not shifted back to heavily investing in VC-backed startups this year yet, only taking part in a small pre-seed round for GuidenAI.\n\nGoogle\n\nWhile Microsoft\u2019s slight dip in AI funding might be surprising, Google\u2019s drop is extremely eye-catching.\n\nThe search giant saw its corporate investing drop by two-thirds, being part of only four deals that totaled $3.2 million, per Crunchbase data. Its biggest round was co-leading CharacterX\u2019s $2.8 million seed round.\n\nIn 2023, Google took part in a dozen rounds totaling nearly $3.6 billion.\n\nOn the flip side, its venture arm, GV, was quite active last year, taking part in 22 deals that totaled a whopping $1.5 billion, per Crunchbase data. That includes being part of a $250 million Series E for Vercel, a platform that allows companies to develop web applications in the cloud. The new round valued the startup at $3.25 billion.\n\nGV also was part of a $400 million Series D for Lightmatter, a startup that uses light to link chips together and to do calculations for the deep learning necessary for AI. The round valued the company at $4.4 billion.\n\nShifting strategies?\n\nUnlike Microsoft, Google seems to be ramping back up. In January, the search giant invested another $1 billion into Anthropic. Previously, Google had invested $2 billion into the OpenAI rival.\n\nThen last month, Google participated in AI-powered humanoid robotics company Apptronik\u2019s massive $350 million Series A co-led by B Capital and Capital Factory.\n\nWhile Google and Nvidia still seem keen on investing in the startup world, Microsoft\u2019s complex relationship with OpenAI \u2014 both a partner and competitor that seems to be cozying up to the likes of SoftBank and Oracle \u2014 and its retreat in 2024 seem to indicate the tech giant is more focused on internal development.\n\nIt also seems clear Nvidia will continue to invest in and push this current AI ecosystem \u2014 one which it feeds and dominates with its high-performing chips.\n\nRegardless of changing strategies, these three AI players still are bringing a lot of cash to the table for startups looking to make a name for themselves in the AI space.\n\nRelated Crunchbase Pro queries:\n\nRelated reading:\n\nIllustration: Dom Guzman",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-03-20": {
        "0": {
            "title": "GTC 2025 \u2013 Announcements and Live Updates",
            "link": "https://blogs.nvidia.com/blog/nvidia-keynote-at-gtc-2025-ai-news-live-updates/",
            "snippet": "What's next in AI is at GTC 2025. Explore the news and live updates from the show, from the keynote to the final session.",
            "score": 0.9221877455711365,
            "sentiment": null,
            "probability": null,
            "content": "All the news from NVIDIA\u2019s biggest gathering of the year, including new services and hardware, tech demos and what\u2019s next in AI.\n\nWhat\u2019s next in AI is at GTC 2025. Not only the technology, but the people and ideas that are pushing AI forward \u2014 creating new opportunities, novel solutions and whole new ways of thinking. For all of that, this is the place.\n\nHere\u2019s where to find the news, hear the discussions, see the robots and ponder the just-plain mind-blowing. From the keynote to the final session, check back for live coverage from San Jose, California.\n\nQuantum Day: NVIDIA CEO Announces Quantum Computing Lab, and NVIDIA Tech Key to Advancing Quantum Innovations, Industry Leaders Say \ud83d\udd17\n\nNVIDIA is diving deeper into quantum computing with plans to open a dedicated research lab in Boston, founder and CEO Jensen Huang announced Thursday as he kicked off a series of panels at GTC featuring industry leaders.\n\n\u201cIt will likely be the most advanced accelerated computing, hybrid quantum computing research lab in the world,\u201d Huang said.\n\nThe panels at GTC\u2019s inaugural Quantum Day highlighted the critical role accelerated computing will play in advancing the nascent technology.\n\nWhile technologies and approaches to quantum development vary, panelists agreed that GPUs and quantum systems will work hand in hand to unlock new breakthroughs.\n\nNVIDIA\u2019s new lab will serve as a hub for collaboration with leading researchers from institutions such as Harvard University and the Massachusetts Institute of Technology, among others.\n\nThe facility aims to accelerate innovation in quantum computing, a field with vast potential for revolutionizing industries from cryptography to materials science.\n\nThe event, moderated by Huang, included three panels and a long list of companies building quantum systems. Guests included:\n\nAlan Baratz, CEO of D-Wave\n\nBen Bloom, founder and CEO of Atom Computing\n\nJohn Levy, cofounder and CEO of SEEQC\n\nKrysta Svore, technical fellow at Microsoft\n\nLo\u00efc Henriet, CEO of Pasqal\n\nMatthew Kinsella, CEO of Infleqtion\n\nMikhail Lukin, Joshua and Beth Friedman University Professor at Harvard University and cofounder of QuEra Computing\n\nPete Shadbolt, cofounder and chief scientific officer of PsiQuantum\n\nPeter Chapman, executive chair at IonQ\n\nRajeeb Hazra, president and CEO of Quantinuum\n\nRob Schoelkopf, cofounder and chief scientist at Quantum Circuits\n\nSimone Severini, general manager of quantum technologies at AWS\n\nSubodh Kulkarni, CEO of Rigetti\n\nTh\u00e9au Peronnin, cofounder and CEO of Alice & Bob\n\nThey discussed quantum computing progress on reaching double-digit numbers of logical qubits, as well as challenges, such as the needs to reach triple-digit qubits for practical applications and to scale to millions of qubits in the years ahead.\n\nLearn more about quantum computing.\n\nAI Is Changing the Game \u2014 Literally \ud83d\udd17\n\nAI is revolutionizing live sports, unlocking new ways to engage fans, optimize operations and enhance player performance, said sports industry leaders on a panel at GTC.\n\n\u201cAI has been in sports for many years, but the [industry] continue[s] to be early adopters of the latest innovations,\u201d said Sepi Motamedi, senior product marketing manager at NVIDIA.\n\nFor the National Hockey League and La Liga, Spain\u2019s premier football league, AI isn\u2019t just a tool \u2014 it\u2019s a game changer. Le Liga has cameras in all its stadiums, retrieving over 3 million data points per match, according to Javier Gil Fernandez, head of AI implementation and development at the league.\n\nMeanwhile, the NHL is piloting optical tracking to capture players\u2019 skeletal movements. \u201cThe amount of data is incredible, and it\u2019s just going to keep growing,\u201d said Dave Lehanski, executive vice president of business development and innovation at the NHL.\n\nBeyond data, AI is reshaping fan experiences. \u201cWouldn\u2019t it be great if AI automatically served up game storylines in real time?\u201d Lehanski asked.\n\nAI-powered video analysis will let fans instantly search, summarize and relive key moments.\n\nAs AI integration deepens, the sports industry faces challenges \u2014 from legal hurdles to regulation. But one thing is clear: \u201cIf the industry understands AI and can leverage it fully, we will be good to go,\u201d Fernandez said. The future of sports is fast, smart and AI-driven.\n\nBritish Member of Parliament Shares UK\u2019s Sovereign AI Vision \ud83d\udd17\n\nSovereign AI \u2014 a nation\u2019s capacity to develop and deploy AI using domestic infrastructure, datasets and workforces \u2014 was a key theme at GTC, highlighted in a half-day summit headlined by government and industry leaders from countries including Brazil, Denmark, India, Japan, Thailand and the U.K.\n\nIn decades and centuries past, the availability of resources including coal, oil and electricity \u201cdefined which governments and countries grew and which did not,\u201d said Peter Kyle, a U.K. member of parliament and secretary of state for science, innovation and technology, at the summit. \u201cToday, we find ourselves in the midst of another epochal shift. Who swims and who sinks will depend on compute.\u201d\n\nKyle outlined the U.K.\u2019s AI Opportunities Action Plan, which aims to expand compute capacity for AI research at least 30x by 2030 and will transform currently unused facilities and land to develop data centers for building AI models trained on the country\u2019s rich datasets in healthcare and other domains.\n\nThe U.K. government\u2019s Isambard-AI supercomputer, which features 5,448 NVIDIA GH200 Grace Hopper Superchips to deliver a whopping 21 exaflops of AI performance, represents the progress already made toward the nation\u2019s sovereign AI goals. With the emergence of the new test-time scaling law of AI development, Kyle noted, there\u2019s much more ahead.\n\n\u201cThe computer is only as good as the people who are using it and the data that you put into it,\u201d Kyle said. To \u201ctake on the great challenges that will define the century to come will require more than just brute capacity. Building bigger or faster is simply not enough \u2014 in the age of compute, states must build smarter, too.\u201d\n\nNVIDIA Omniverse-Powered Moon Machines to Explore Lunar Surface \ud83d\udd17\n\nAcross the rugged, cratered landscape of the moon, a coordinated fleet of autonomous machines \u2014 drones, excavators and haulers \u2014 seamlessly navigates the harsh vacuum of space, working in sync to map, extract and transport valuable resources.\n\nThis concept for the future of lunar exploration was laid out by Lutz Richter, a space robotics expert, in a captivating presentation at GTC.\n\nRichter, representing premier IT consulting and digital services provider SoftServe, described how his team has helped simulate and model this multi-robot system using the NVIDIA Omniverse platform for physical AI. \u201cWe think space will benefit a lot from the advanced simulation and modeling tools offered by Omniverse,\u201d Richter said.\n\nThe drones would scour the lunar surface, using high-resolution sensors to identify regions of interest for ice extraction. The ground-based excavator would then meticulously dig into the regolith, with SoftServe\u2019s advanced soil mechanics simulation optimizing the process of unearthing the icy deposits. Once loaded, the hauler vehicle would transport the cargo back to the lunar outpost.\n\nRichter also showcased SoftServe\u2019s work simulating the drones\u2019 ability to explore lunar caves. The drones use real-time visual simultaneous localization and mapping to navigate the complex terrain and chart potential ice reserves hidden within.\n\nBy integrating robotics with Omniverse\u2019s powerful simulation and collaboration tools, SoftServe drives a groundbreaking approach to lunar resource exploration.\n\nDemocratizing Immersive Experiences With AI \ud83d\udd17\n\nAt the XR Pavilion, hosted at the Tech Interactive science and technology center, GTC attendees explored immersive demos of digital twins, 3D rendering and virtual reality experiences. If the frequent smiles and exclamations of \u201cwow\u201d are an indication of where things are headed with these visual AI technologies, there\u2019s a lot to look forward to.\n\nRobotics Experts Discuss Outlook for Humanoids \ud83d\udd17\n\nSpeaking to a crowd at San Jose Civic Center, a panel of robotics industry executives weighed in humanoid robot development and what the future holds for making them a reality with consumers.\n\nPanelists discussed the role of large language models, simulation and improvements in robotics hardware in helping develop specialized humanoid robots.\n\n\u201cWe have some of the most extraordinary founders on some of the best robot hardware I\u2019ve ever seen,\u201d said Jim Fan, principal research scientist and senior research manager at NVIDIA. \u201cI think hardware has become a lot better and a lot cheaper.\u201d\n\nAaron Saunders, chief technology officer of Boston Dynamics, said that physics-based simulations have been groundbreaking for the field of robotics. \u201cThe closure of the sim-to-real gap is a big deal,\u201d he said.\n\nLearning by experience is the main change that has happened in robotics, noted Deepak Pathak, cofounder and CEO of Skild AI. \u201cWhat has changed is completely how we approach robotics,\u201d he said.\n\nPanelists also talked about the importance of data sources in training models. \u201cNow we get this global data the way we get an internet, and you can solve all the problems we have today, and all we need is a lot more GPUs,\u201d said Bernt B\u00f8rnich, founder and CEO of 1X.\n\nRobots Galore on the Show Floor \ud83d\udd17\n\nFrom helpful humanoids to robotic arms supporting surgeries, deliveries and industrial processes, physical AI advancements are on full display with interactive demos at GTC.\n\nHow Low-Cost, Self-Driving Microscopes Can Transform Public Health \ud83d\udd17\n\nManu Prakash is on a mission to empower community health workers and citizen scientists around the world with AI-powered microscopes that help detect malaria and other diseases.\n\nPrakash is cofounder of microscopy company Cephla and an associate professor at the Woods Institute for the Environment at Stanford University. He\u2019s integrating AI into low-cost microscopes that can be deployed in grassroots healthcare environments for real-time disease monitoring at scale.\n\n\u201cThis is the first machine that can do something like this in real time,\u201d he said, demonstrating live how Octopi, Cephla\u2019s modular robotic microscopy platform, can rapidly analyze red blood cells to detect malaria parasites. \u201cOne of the phenomenal aspects of this is we can train it not just for malaria \u2014 we can train it for any disease.\u201d\n\nPrakash and his team found that deploying their pipeline on the NVIDIA Jetson Nano module accelerated image processing by 5x compared with using desktop computers \u2014 enabling them to examine about 3 million red blood cells per minute.\n\nBy enabling rapid, low-cost disease detection that can be used by community health workers, Prakash hopes to advance the capabilities of global public health organizations to better tackle challenges such as the emergence of antibiotic-resistant disease strains.\n\nBeyond malaria, his team has collaborated with researchers to adapt their AI pipeline for other disease areas. One organization was able to train an AI pipeline to detect four types of sickle cell disease within just six months. Another trained a pipeline to detect if individuals are actively spreading tuberculosis.\n\n\u201cSometimes, instruments with a little bit of intelligence can go an astronomical distance,\u201d Prakash said.\n\nHow AI Is Unlocking a New Era of Creativity in Filmmaking \ud83d\udd17\n\nAI is making filmmaking more accessible and changing how filmmakers bring their visions to life.\n\nSpeaking at GTC, Haohong Wang, general manager at TCL, shared how AI-powered tools like Runway, Sora and MineStudio are unlocking faster workflows, new artistic possibilities and a more accessible future for content creation.\n\nAI Meets Creativity: Traditional filmmaking can be slow and expensive. Wang predicts that AI-generated content budgets could make it possible to create filmed content at less than 10% of today\u2019s costs, making high-quality storytelling available to more filmmakers.\n\nBut it\u2019s not just about savings. AI allows artists to experiment with new visual styles in real time, letting them generate entire scenes instantly, explore new storytelling techniques and break traditional production barriers.\n\nDirecting AI: AI filmmaking still struggles with consistency, camera control and human movement, Wang noted. Many directors feel they lack the same control as traditional filmmaking. The solution: a structured 3D workflow. By digitizing AI-generated assets \u2014 characters, environments and animations \u2014 filmmakers can blend the best of AI and traditional techniques, ensuring precise control over cinematography and visual consistency.\n\nContent Flywheel: Wang described AI filmmaking as a \u201cflywheel model,\u201d where easier content creation fuels audience growth, attracts advertisers and drives demand for more creative work.\n\n\u201cSo you can see there\u2019s a loop [so] that the advertising money will eventually go back to the creative community,\u201d Wang explained. \u201cOnce this opportunity is there, more and more creatives will join to use AI to produce more original content and get more audiences, get more advertisers.\u201d\n\nWhile some in Hollywood remain skeptical, Wang believes growing adoption will make AI a standard tool for filmmakers \u2014 just as CGI now is.\n\nA Sneak Peek: To show that potential, Wang showed clips from \u201cNext Stop Paris,\u201d a hybrid live-action and AI short film. The story follows two strangers on an adventure \u2014 and a twist reveals AI has been shaping the narrative all along.\n\nHealthcare VP Kimberly Powell Highlights Life Sciences Leaps \ud83d\udd17\n\nAddressing a packed crowd at the Montgomery Theater yesterday, Kimberly Powell, vice president of healthcare and life sciences at NVIDIA, underscored no less than nine GTC announcements delivering industry advances.\n\nThe news included the new NVIDIA DGX Spark AI supercomputer, NVIDIA AgentIQ, the MONAI multimodal and agent framework, NVIDIA Holoscan 3.0, NVIDIA Isaac for Healthcare and partner moves from Sapio Sciences, Cadence and Epic, which is accelerating AI with NVIDIA NIM on Microsoft Azure.\n\nAnd those were only just some of the highlights.\n\n\u201cMany of you have chosen to release some of your most important breakthroughs here at GTC, with several dozen new product announcements and critical breakthroughs,\u201d said Powell.\n\nShe discussed the role of accelerated computing in medical imaging and genomics, as well as in the introduction of the world\u2019s largest biology foundation model, Evo 2, trained on 9 trillion nucleotides, the building blocks for RNA and DNA.\n\nPowell said that biology represents an unsolved problem that\u2019s now positioned to benefit from leaps in progress from AI.\n\n\u201cWe are starting to experience some exponential levels of biology intelligence by being able to represent biology in a computer,\u201d she said.\n\nPowell also discussed the role of AI in drug discovery, the development of AI agents for use in clinical trials and the introduction of physical AI for medical devices.\n\n\u201cWe\u2019re at this next frontier where not only do we have digital agents but we\u2019re going to have physical agents where we\u2019re going to embody AI into physical things,\u201d said Powell.\n\nRead more about how AI is advancing healthcare and watch the talk replay \u2014 virtual registration is free.\n\nUnlock Peak AI Performance With NVIDIA DGX Cloud Benchmarking \ud83d\udd17\n\nAchieving peak AI performance requires more than just powerful hardware. Understanding how hardware and software impact AI workload performance is crucial for technical validation and business planning.\n\nTo help organizations make data-driven decisions and maximize GPU utilization, NVIDIA launched NVIDIA DGX Cloud Benchmarking \u2014 a suite of tools that empowers users to optimize their AI workload and infrastructure investments effectively.\n\nDGX Cloud Benchmarking includes three key components:\n\nPerformance Recipes: Standardized benchmark scripts and baseline results that enable AI platform performance validation across a diverse set of AI workloads.\n\nStandardized benchmark scripts and baseline results that enable AI platform performance validation across a diverse set of AI workloads. Performance Explorer: An interactive tool that provides performance data across AI workloads.\n\nAn interactive tool that provides performance data across AI workloads. Expert guidance and best practices: Work with NVIDIA to simplify the optimization process.\n\nBy providing standardized benchmarks and actionable insights, DGX Cloud Benchmarking helps AI teams optimize their workloads, reduce costs and accelerate innovation.\n\nNVIDIA collaborates with cloud service providers to co-engineer and optimize performance across the entire stack and deliver highly performant AI platforms. This ensures that optimizations are based on real-world scenarios, leading to significant performance improvements over time.\n\nDGX Cloud Benchmarking is designed to evolve with the AI industry, incorporating new models, hardware platforms and software optimizations to ensure customers have access to the latest performance insights.\n\nWhether you\u2019re an AI development team, or an IT team or a cloud service provider validating infrastructure performance, DGX Cloud Benchmarking provides the tools needed to unlock peak AI performance.\n\nLearn more about DGX Cloud Benchmarking.\n\nNVIDIA Expands Benefits for Startups \ud83d\udd17\n\nAs part of a significant expansion of benefits for members of the NVIDIA Inception program for cutting-edge startups, NVIDIA is offering qualified members up to $100,000 in NVIDIA DGX Cloud credits for dedicated NVIDIA H100 GPU capacity.\n\nMembers approved for this benefit will also have access to expert office hours that provide technical guidance and support. With DGX Cloud, a unified AI platform, Inception members can use NVIDIA software, services and AI expertise to build, customize and deploy applications on their preferred cloud.\n\nOther new and enhanced benefits \u2014 encompassing nearly $300,000 in credits and discounts \u2014 offer unparalleled access to cutting-edge technology from NVIDIA Cloud Partners (NCPs), NVIDIA Partner Network independent software vendors (ISVs) and select Inception members. These offers can help Inception startups and members of the NVIDIA Connect program for ISVs build and scale their businesses, fostering innovation and growth across diverse industries.\n\nNCP Offers and Additional Benefits\n\nNCPs such as Nebius, Scaleway and Lintasarta are offering free cloud credits to eligible Inception members, helping them reduce costs and foster innovation. Yotta and Lambda are providing Inception and Connect members with cloud credits to boost scalability and speed time to market.\n\nCompanies adopting NCP services will benefit from enhanced local support, expanded GPU allocation and access, advanced technical capabilities and hands-on assistance, access to local data centers, and flexible pricing and billing to meet the distinct needs of small and growing companies.\n\nNew and expanded NCP collaborations will include onboarding and technical support through training and webinars, as well as access to regional events and opportunities for go-to-market support.\n\nInception members will also gain access to top tools and software from companies like AI21, Beamr, Bria, DeepChecks, Lightning AI, RGo Robotics, Tabnine and Weights & Biases.\n\nMembers of the NVIDIA Connect program for ISVs will also gain access to select benefits, available only through the Inception and Connect member benefits catalog.\n\nJoin the NVIDIA Inception and NVIDIA Connect programs. Additional partners and exclusive offers will be added regularly.\n\nCompanies using and showcasing NVIDIA technology to build compelling tools and services for startup and developer audiences can learn more about the programs and fill out a partner benefit proposal.\n\nResearch Giants Bill Dally, Yann LeCun Share the Stage at GTC \ud83d\udd17\n\nTwo luminaries in AI research \u2014 Bill Dally, chief scientist at NVIDIA, and Yann LeCun, chief AI scientist at Meta and a professor at New York University \u2014 took the stage today at the San Jose Civic auditorium to share their vision for the field\u2019s future.\n\nDally asked LeCun to share his thoughts on artificial general intelligence, the impact of AI on scientific research, his current work on world models and the qualities needed to foster innovation.\n\nLeCun predicted that artificial general intelligence, or AGI \u2014 which he prefers to call advanced machine intelligence, because \u201chuman intelligence is superspecialized, so calling it general is a misnomer\u201d \u2014 will be viable in three to five years.\n\nHe emphasized the importance of open-source projects to support the development of diverse AI assistants.\n\n\u201cWe need assistants that are extremely diverse,\u201d he said. \u201cWe need to speak all languages, understand all the cultures, all the value systems, all the sectors of interest. So we need a platform that anybody can use to build those assistants, a diverse population of assistants \u2014 and right now that can only be done through open-source platforms.\u201d\n\nLeCun also spoke about his work at Meta to develop world models that can understand, reason and plan around physical environments.\n\n\u201cWhat you need is a predictor that, given the state of the world and an action you imagine, can predict the next state of the world,\u201d he said. \u201cAnd if you have such a system, then you can plan a sequence of actions to arrive at a particular outcome.\u201d\n\nDally noted that building world models like these will require significant AI infrastructure powered by NVIDIA GPUs.\n\n\u201cKeep them coming,\u201d LeCun quipped. \u201cWe\u2019re going to need all the computation we can get our hands on.\u201d\n\nWrap-Up: Key Points From the GTC Keynote \ud83d\udd17\n\nHere are the major points NVIDIA founder and CEO Jensen Huang covered in his keynote:\n\nWe\u2019re at a $1 trillion computing inflection point. AI computing demand is accelerating rapidly, driven by the rise of reasoning AI and agentic AI. The scale and complexity of AI workloads are transforming data center investments worldwide.\n\nAI computing demand is accelerating rapidly, driven by the rise of reasoning AI and agentic AI. The scale and complexity of AI workloads are transforming data center investments worldwide. NVIDIA Blackwell is in full production, delivering 40x the performance of Hopper. The Blackwell architecture significantly enhances AI model training and inference, enabling more efficient and scalable AI applications. And the next evolution of the NVIDIA Blackwell AI factory platform, Blackwell Ultra, will be coming to systems in the second half of this year.\n\nThe Blackwell architecture significantly enhances AI model training and inference, enabling more efficient and scalable AI applications. And the next evolution of the NVIDIA Blackwell AI factory platform, Blackwell Ultra, will be coming to systems in the second half of this year. NVIDIA will follow an annual rhythm for the buildout of AI infrastructure. Each year will bring new GPUs, CPUs and accelerated computing advancements, including the upcoming NVIDIA Vera Rubin architecture, designed to drive performance gains and efficiency improvements in AI data centers.\n\nEach year will bring new GPUs, CPUs and accelerated computing advancements, including the upcoming NVIDIA Vera Rubin architecture, designed to drive performance gains and efficiency improvements in AI data centers. AI infrastructure, including photonics and AI-optimized storage, is set to revolutionize the industry. Advanced networking and storage solutions will improve AI scalability, efficiency and energy consumption in large-scale data centers.\n\nAdvanced networking and storage solutions will improve AI scalability, efficiency and energy consumption in large-scale data centers. Physical AI for industrial and robotics is a $50 trillion opportunity. AI-powered robotics and automation are set to transform manufacturing, logistics, healthcare and other industries, with the NVIDIA Isaac and Cosmos platforms leading the way.\n\nWatch the replay:\n\nRead more details about the keynote below and stay tuned for more insights and sessions throughout the conference, running through Friday, March 21.\n\nDive In: NVIDIA Founder and CEO Jensen Huang\u2019s Keynote at GTC \ud83d\udd17\n\nWelcome to NVIDIA\n\nHuang kicked off the keynote by bringing the audience \u201cinto NVIDIA\u2019s headquarters,\u201d with a stunning visual of its lobby that seemed to embrace the audience. He began by talking about where NVIDIA started 25 years ago, with GPUs. He described the growth of AI over the past decade, including the emergence of agentic AI \u2014 which can reason how to solve a problem, plan and take action.\n\nAI at an Inflection Point\n\nHuang then outlined the development of AI able to reason \u201cstep by step by step\u201d and discussed how demand for inference and reinforcement learning is driving demand for AI computing. Demand for GPUs from the top four cloud service providers is surging, as AI is going through an \u201cinflection point.\u201d Huang said he expects the value of data center buildout to reach $1 trillion.\n\nNVIDIA CUDA Ecosystem\n\nNVIDIA CUDA-X GPU accelerated libraries and microservices are now serving every industry, Huang explained. In the future, Huang said, every company will have two factories: one for what they build and another for AI. Ticking through a sampling of NVIDIA\u2019s role in a variety of endeavors, he announced that NVIDIA is going to open-source its cuOpt decision optimization platform. The installed base of CUDA is now \u201ceverywhere,\u201d he said. \u201cWe have reached the tipping point of accelerated computing \u2014 CUDA has made it possible.\u201d\n\nGeneral Motors, NVIDIA Collaborate on AI\n\nAI needs infrastructure, Huang explained. AI is now going out \u201cto the rest of the world,\u201d in robotics and self-driving cars, factories and wireless networks. One of the earliest industries AI went into was autonomous vehicles, Huang said. \u201cWe build technology that almost every single self-driving car company uses,\u201d he added, whether in the data center or the car. Huang announced the next step of that journey: GM, the largest U.S. automaker, is adopting NVIDIA AI, simulation and accelerated computing to develop next-generation vehicles, factories and robots. He also announced NVIDIA Halos, a comprehensive safety system bringing together NVIDIA\u2019s lineup of automotive hardware and software safety solutions with its cutting-edge AI research in AV safety.\n\nData Centers and Inference\n\nNext up: Huang spoke about data centers. He reported that the NVIDIA Blackwell platform is in full production, sharing a shot of systems from a broad range of industry partners. \u201cHow is this not beautiful,\u201d Huang said.\n\nHe described how Blackwell supports extreme scale-up. \u201cThe reason why we wanted to do this is to solve an extreme problem,\u201d Huang said, \u201cand it\u2019s called inference.\u201d\n\nInference, Huang explained, is token generation, which will be critical to businesses. AI factories that generate these tokens have to be built with extreme efficiency and performance. And demand for tokens will only grow, with the latest generation of reasoning models able to think through and solve increasingly complex problems.\n\nTo further accelerate inference on a large scale, Huang announced NVIDIA Dynamo, open-source software for accelerating and scaling AI reasoning models in AI factories. \u201cIt is essentially the operating system of an AI factory,\u201d Huang said.\n\nNVIDIA Blackwell Ultra\n\n\n\nHuang described how Blackwell delivers a \u201cgiant leap\u201d in inference performance. \u201cYou want to make sure you have the most energy-efficient architecture you can possibly get,\u201d Huang said, showing how Blackwell does more work, using less power than the previous generation. \u201cThe more you buy, the more you save,\u201d Huang said, drawing laughter from the audience. \u201cIt\u2019s even better than that \u2014 the more you buy, the more you make.\u201d\n\nHuang then rolled a video showing how a new NVIDIA Omniverse Blueprint can help plan a 1 gigawatt AI factory, letting engineers design, test and optimize a new generation of intelligence manufacturing data centers using digital twins.\n\nThen, he announced the next evolution of the NVIDIA Blackwell AI factory platform, NVIDIA Blackwell Ultra, coming in systems the second half of this year. NVIDIA Blackwell Ultra boosts training and test-time scaling inference \u2014 the art of applying more compute during inference to improve accuracy \u2014 to enable organizations everywhere to accelerate applications such as AI reasoning, agentic AI and physical AI.\n\nNVIDIA Vera Rubin\n\nPaying tribute to astronomer Vera Rubin, Huang outlined a roadmap that will deliver data center performance gains for years to come, Huang offered new details on the next-generation NVIDIA Rubin Ultra GPU and NVIDIA Vera CPU architectures, packed with innovations. \u201cBasically everything is brand new except for the chassis,\u201d Huang said. Systems built on Rubin Ultra, including the Vera Rubin NVL 144, will arrive in the second half of next year. And due for the second half of 2027: systems built on Rubin Ultra. \u201cYou can see that Rubin is going to drive the cost down tremendously,\u201d Huang said.\n\nNVIDIA Photonics\n\nHuang shifted to discussing how NVIDIA will help customers scale out to ever larger systems. Key to that is tightly integrating photonics \u2014 networking technologies that rely on transmitting data using light rather than electrical signals \u2014 into accelerated computing infrastructure. Enabling AI factories to connect millions of GPUs across sites while reducing energy consumption and operational costs, NVIDIA Spectrum-X and NVIDIA Quantum-X silicon photonics networking switches fuse electronic circuits and optical communications. \u201cThis is really crazy technology,\u201d Huang said. NVIDIA photonics switches integrate optics innovations with 4x fewer lasers to deliver 3.5x more power efficiency, 63x greater signal integrity, 10x better network resiliency at scale and 1.3x faster deployment compared with traditional methods.\n\nDGX Spark and DGX Station\n\nEnabling AI developers, researchers, data scientists and students to prototype, fine-tune and inference large models on desktops, NVIDIA unveiled DGX personal AI supercomputers powered by the NVIDIA Grace Blackwell platform. Describing it as the perfect Christmas present, Huang announced DGX Spark \u2014 formerly Project DIGITS \u2014 and DGX Station, a new high-performance NVIDIA Grace Blackwell desktop supercomputer powered by the NVIDIA Blackwell Ultra platform, bringing the power of the Grace Blackwell architecture to the desktop. Users can run these models locally or deploy them on NVIDIA DGX Cloud or any other accelerated cloud or data center infrastructure. \u201cThis is the computer of the age of AI,\u201d Huang said.\n\nAgentic AI\n\n\n\nTransitioning into a discussion of the future of agentic AI, Huang announced the open Llama Nemotron family of models with reasoning capabilities, designed to provide developers and enterprises a business-ready foundation for creating advanced AI agents that can work independently or as connected teams to solve complex tasks. Built on Llama models, the NVIDIA Llama Nemotron reasoning family delivers on-demand AI reasoning capabilities. NVIDIA enhanced the new reasoning model family during post-training to improve multistep math, coding, reasoning and complex decision-making.\n\nPhysical AI and Robotics\n\nDescribing robots as the next $10 trillion industry, Huang said that by the end of this decade, the world is going to be at least 50 million workers short. NVIDIA offers a complete suite of technologies for training, deploying, simulating and testing next-generation robotics.\n\nIn a video, Huang announced the availability of NVIDIA Isaac GR00T N1, the world\u2019s first open, fully customizable foundation model for generalized humanoid reasoning and skills.\n\nNVIDIA also announced a major release of new NVIDIA Cosmos world foundation models, introducing an open and fully customizable reasoning model for physical AI development and giving developers unprecedented control over world generation.\n\n\u201cUsing Omniverse to condition Cosmos, and Cosmos to generate an infinite number of environments, allows us to create data that is grounded, controlled by us and yet systematically infinite at the same time,\u201d Huang said.\n\nHe also introduced the Newton open-source physics engine for robotics simulation, under development with Google DeepMind and Disney Research, before he was joined on stage by a diminutive robot, \u201cBlue,\u201d that emerged from a hatch on the floor, beeping and booping at Huang.\n\n\u2018Let\u2019s Wrap Up\u2019\n\nHuang wound up his talk by emphasizing several major themes.\n\nFirst, Blackwell is in full production \u2014 \u201cand the ramp is incredible, customer demand is incredible,\u201d Huang reported. \u201cAnd for good reason, because there is an inflection point in AI, the amount of computation we have to do in AI is so much greater as a result of reasoning AI, and the training of reasoning AI systems and agentic systems.\u201d\n\nSecond, Blackwell NVL72 with Dynamo offers 40x the AI factory performance of NVIDIA Hopper, and \u201cinference is going to be one of the most important workloads in the next decade as we scale out AI.\u201d\n\nThird, NVIDIA has an \u201cannual rhythm\u201d of roadmaps, so the world can plan its AI infrastructure. NVIDIA is building three AI infrastructures: one for the cloud, a second for the enterprise and a third for robots.\n\nCountdown to Keynote at GTC \ud83d\udd17\n\nThe SAP Center is waking up, bathed in a soothing purple glow. A sea of attendees flows in \u2014 developers, researchers, business leaders \u2014 filling the space with a charge of anticipation.\n\nUp on the big screen: the pre-show with Acquired podcast hosts Ben Gilbert and David Rosenthal, who are interviewing industry figures from the GTC show floor. Earlier today they even got a drop-in from NVIDIA founder and CEO Jensen Huang.\n\nIt\u2019s almost time.\n\nAt 10:00 a.m. PT, Huang takes the stage, setting the tone for everything unfolding at GTC. Crowds in the arena will feel it \u2014 the shuffle of last-minute arrivals, the glow of smartphone screens.\n\nThose tuning in remotely won\u2019t miss a moment. Watch the keynote live. The future of AI, accelerated computing and everything in between is about to be unveiled.\n\nStay with us \u2014 we\u2019re just getting started.\n\nKeynote Today: Tune In Early for the Preshow With Acquired \ud83d\udd17\n\nThe excitement starts long before NVIDIA founder and CEO Jensen Huang takes the stage. Catch the \u201cLive at NVIDIA GTC With Acquired\u201d broadcast now, featuring luminary speakers who\u2019ll offer fascinating insights into NVIDIA\u2019s remarkable journey over the past three decades.\n\nState of AI Art: How Artists Bring AI, Robots Into Creative Process \ud83d\udd17\n\n\u201cConsumption,\u201d a piece by BREAKFAST, on display at GTC.\n\nArtists over millennia have created with every medium available \u2014 fruit and vegetable dyes, blocks of marble, acrylic paints, cameras, rendering software, 3D printers. They\u2019re now harnessing AI and robotics to build immersive experiences that reflect on the ways people respond to and interact with technology.\n\nIn a GTC panel discussion hosted today by Heather Schoell, creative director of AI strategy at NVIDIA, four artists shared how they use AI and robotics in their creative processes.\n\nThe integration of robotics in art \u201cis a very logical continuation in what is a very long spectrum of humans and our relationship to our tools,\u201d said panelist Catie Cuan, founder of consumer AI company Zenie and a postdoc at Stanford University, where she leads art and robotics efforts at the Stanford Robotics Center.\n\nA former professional dancer, Cuan\u2019s works include an eight-hour duet with a robot arm and a symphony of dancing robots that played sounds as they moved.\n\nAnother panelist, Alexander Reben, was OpenAI\u2019s first artist in residence. He\u2019s used large language models, visual generative AI models and NeRFs to create 3D sculptures.\n\nInteractive art by two of the panelists \u2014 Zolty, a kinetic and robotics artist known as BREAKFAST, and Emanuel Gollob, a doctoral researcher at the University of Arts Linz, Austria \u2014 is on display at the main entrance of GTC.\n\nBREAKFAST works with real-time data to create large kinetic art sculptures that are on display aboard a Royal Caribbean cruise ship and the Fontainebleau hotel in Las Vegas. The studio\u2019s piece at GTC, titled \u201cConsumption,\u201d runs on an NVIDIA RTX GPU and features robotic arches that move in response to real-time data about water usage, including rainfall, reservoirs, groundwater levels and municipal supplies.\n\nGollob\u2019s robotic installation, called \u201cDoing Nothing With AI,\u201d uses generative robot control, brainwave measurements and reinforcement learning to move in a way that guides a participant wearing an EEG headband to do nothing and enjoy a moment of inaction.\n\nPanelists Discuss Energy Efficiency in the Age of Generative AI \ud83d\udd17\n\nTo a packed room of attendees, panelists held an illuminating discussion on AI, energy and climate issues today at GTC. Josh Parker, senior director of corporate sustainability at NVIDIA, was joined by Lauren Risi from the Wilson Center, David Sandalow from Columbia University and Bernhard Lorentz of Deloitte.\n\n\u201cIn the world of climate change, there\u2019s an enormous amount of things that AI can do,\u201d said Sandalow. \u201cHow can you mobilize AI tools to mitigate climate change?\u201d\n\nThe industry experts addressed concerns about energy efficiency with the rise of generative AI and the uptick in data centers.\n\n\u201cNVIDIA GPUs are delivering 100,000x better energy efficiency over the past decade,\u201d said Parker, summing up the energy efficiency gains from NVIDIA GPUs.\n\nBMW Group Shows Off 3D Collaboration for Battery Assembly Factories \ud83d\udd17\n\nBMW Group presented the latest updates to its 3D AppStore today at GTC.\n\nThe 3D AppStore, BMW\u2019s cloud streaming service for high-end visualization, is now making it possible to support the automaker\u2019s battery assembly process across various factories worldwide. It allows engineers, designers and others to work in the digital environment collaboratively.\n\n\u201cWith the avatars, they see the factory scene and can be in there together, and then you can basically just ask what is happening here, and the AI gives you a detailed description of what the station here is doing,\u201d said Xaver Freiherr Loeffelholz von Colberg, product owner of 3D AppStore at BMW Group.\n\nOffering a glimpse into the future of automotive design and manufacturing, BMW has two more speaker sessions on Thursday, March 20.\n\nExplaining Tokens \u2014 the Language and Currency of AI \ud83d\udd17\n\nGTC attendees are likely to hear a lot about tokens \u2014 the language and currency of AI.\n\nTokens are units of data processed by AI models during training and inference, enabling prediction, generation and reasoning. The faster tokens can be processed, the faster AI models can learn and respond.\n\nGet up to speed on tokens, tokenization and the ways enterprises can boost revenue by lowering cost per token in our explainer article.\n\nGTC 2025: Real AI, Real Problems, Real Solutions \ud83d\udd17\n\nAI is confronting humanity\u2019s toughest challenges head on. See it unfold next week at the NVIDIA GTC conference in San Jose, California.\n\nFrom transformative healthcare sessions like \u201cRevolutionizing Cardiac MRI Analysis and Diagnosis With AI\u201d and \u201cDesigning the Future: Protein Engineering, AI and Responsible Innovation\u201d to environmental breakthroughs in \u201cAutonomous Systems and Remote Sensing for Better Earth Data,\u201d \u201cThe Role of AI and Accelerated Computing in Understanding and Mitigating Urban Climate Change\u201d and \u201cEnhancing Photovoltaic Power Prediction With High-Resolution Weather Forecasting From NVIDIA Earth-2,\u201d the impact is tangible and global.\n\nThe Future Rolls Into San Jose \ud83d\udd17\n\n\n\nAnyone who\u2019s been in downtown San Jose lately has seen it happening. The banners are up. The streets are shifting. The whole city is getting a fresh coat of NVIDIA green.\n\nFrom March 17-21, San Jose will become a crossroads for the thinkers, tinkerers and true enthusiasts of AI, robotics and accelerated computing. The conversations will be sharp, fast-moving and sometimes improbable \u2014 but that\u2019s the point.\n\nAt the center of it all? NVIDIA founder and CEO Jensen Huang\u2019s keynote, offering a glimpse into the future. It\u2019ll take place at the SAP Center on Tuesday, March 18, at 10 a.m. PT. Expect big ideas, a few surprises, some roars of laughter and the occasional moment that leaves the room silent.\n\nBut GTC isn\u2019t just what happens on stage. It\u2019s a conference that refuses to stay inside its walls. It spills out into sessions at McEnery Convention Center, hands-on demos at the Tech Interactive Museum, late-night conversations at the Plaza de C\u00e9sar Ch\u00e1vez night market and more. San Jose isn\u2019t just hosting GTC. It\u2019s becoming it.\n\nThe speakers are a mix of visionaries and builders \u2014 the kind of people who make you rethink what\u2019s possible:\n\n\ud83e\udde0 Yann LeCun \u2013 chief AI scientist at Meta, professor, New York University\n\n\ud83c\udfc6 Frances Arnold \u2013 Nobel Laureate, Caltech\n\n\ud83d\ude97 RJ Scaringe \u2013 founder and CEO of Rivian\n\n\ud83e\udd16 Pieter Abbeel \u2013 robotics pioneer, UC Berkeley\n\n\ud83c\udf0d Arthur Mensch \u2013 CEO of Mistral AI\n\n\ud83c\udf2e Joe Park \u2013 chief digital and technology officer of Yum! Brands\n\n\u265f\ufe0f Noam Brown \u2013 research scientist at OpenAI\n\nSome are pushing the limits of AI itself; others are weaving it into the world around us.\n\n\ud83d\udce2 Want in? Register now.\n\nCheck back here for what to watch, read and play \u2014 and what it all means. Tune in to all the big moments, the small surprises and the ideas that\u2019ll stick for years to come.\n\nSee you in San Jose. #GTC25",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "March 2025 Nvidia's CPO is the First Step in a Long Journey",
            "link": "https://www.lightcounting.com/research-note/march-2025-nvidias-cpo-is-the-first-step-in-a-long-journey-395",
            "snippet": "Confirming rampant rumors of GTC announcements, Nvidia revealed CPO switches for both InfiniBand and Ethernet, branded Quantum-X Photonics and Spectrum-X...",
            "score": 0.8367998600006104,
            "sentiment": null,
            "probability": null,
            "content": "March 2025 Nvidia's CPO is the First Step in a Long Journey\n\nAbstract\n\nConfirming rampant rumors of GTC announcements, Nvidia revealed CPO switches for both InfiniBand and Ethernet, branded Quantum-X Photonics and Spectrum-X Photonics, respectively. InfiniBand CPO arrives first in 2H25, while Ethernet CPO is due in 2H26. Note that CPO will be optional\u2013Nvidia will continue to offer switch systems with pluggable modules.\n\n\n\nGiven the slow adoption of CPO to date, it's fair to ask why Nvidia is taking this leap. The foremost reason is power savings, which Jensen highlighted in his keynote. The company is claiming a 70% power reduction, from 30W for a 1.6T pluggable transceiver to only 9W per 1.6T CPO port.\n\n\n\nNvidia\u2019s CPO is based on new Micro Ring Modulators (MRM) for an extra gain in power efficiency. Broadcom\u2019s CPO offered 50% power reduction (by removing DSP), but it was based on Mach-Zehnder modulators (MZM)\u2013a standard component in optical transceivers.\n\n\n\nThe Figure below shows an overview of the many other components involved, starting with electronic and photonic ICs built at TSMC and assembled in a 3D stack. TSMC's Compact Universal Photonic Engine (COUPE) technology includes a microlens for surface coupling to a fiber array. In the case of the Quantum-X Photonic platform, the optical-engine assemblies connect to the switch ASIC through an interposer. Nvidia didn't detail what each contributes to the design, but its CPO partner list includes Browave, Coherent, Corning, Fabrinet, Foxconn, Lumentum, Senko, SPIL, Sumitomo, TFC, and TSMC.\n\n\n\n\n\n\n\nTransceiver vendors can breathe a sigh of relief, at least for a couple of years. Nvidia's first CPO design is an InfiniBand switch, and that protocol recently became secondary to Ethernet in the company's AI strategy. In fact, we note that Quantum-X Photonics doesn't even appear in the roadmap. Additionally, Nvidia already offers Quantum-X800 systems with OSFP ports for 1.6T transceivers, so the first CPO deployments are likely to be for in-house clusters. Quantum-X Photonics will serve as an important proof of technology, but it's unlikely to have a meaningful impact on transceiver demand.\n\n\n\nIn his keynote, Jensen referred to Spectrum-X as a \"huge home run,\" bringing InfiniBand-like capabilities to Ethernet. He said Spectrum-X would connect \"many hundreds of thousands\" of GPUs in the Rubin timeframe. Although Nvidia didn't detail the differences, Spectrum-X Photonics uses a different design from Quantum-X Photonics, perhaps representing a second generation.\n\n\n\nIf nothing else, the company's entry will help add momentum behind CPO, which has been evangelized by Broadcom since early 2021. By 2027, both vendors should have 200G/lane CPO switches in production, which should bring greater maturity to the ecosystem.\n\n\n\nScale-out networking is a low-risk CPO entry point, but scale-up optical interconnects are more critical. Fast response times for mixture-of-experts (MoE) models require expert parallelism, which partitions experts across GPUs. Jensen explained the tradeoff between throughput and response time, and his example showed a sweet spot that required expert-parallel 64, meaning an expert is sliced across 64 GPU instances. The Blackwell-generation NVL72 rack creates a 72-GPU scale-up domain using NVLink over a passive-copper backplane (or spine). Nvidia will use a similar design for Vera Rubin NVL144 in 2H26, likely doubling the number of passive cables.\n\n\n\nNvidia withheld details of the Rubin Ultra NVL576 design, but the Kyber rack layout clearly introduces new requirements for the NVLink interconnect. Still, the 144 GPU packages fit in a single rack, so maximum reach is about 2m. The roadmap omitted Feynman's expected NVLink scale, but we believe it will extend to multiple racks, at which point optics will be required.\n\n\n\nNvidia first announced plans for NVLink over fiber at GTC 2022. The company built at least one such cluster internally, but high power consumption of re-timed optical transceivers was a non-starter for wide deployments. Eliminating DSP is one step forward, but a path for future improvements is needed. This is why Nvidia is taking a risk with new technologies such as MRMs. The company is probably working on a wide spectrum of new optical technologies, given how critical the scale-up capabilities are.\n\n\n\nThe 2028 timeframe for NVLink CPO gives Nvidia two generations to prove its technology in scale-out networks. This will lower the risk of the inevitable move to CPO in GPUs, which is a question of when but not if.\n\n\n\nFull text of the research note is available to LightCounting subscribers at: https://www.lightcounting.com/login",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia to Buy \u2018Several Hundred Billion\u2019 Dollars\u2019 Worth of US-Made Electronics",
            "link": "https://www.pymnts.com/artificial-intelligence-2/2025/nvidia-to-buy-several-hundred-billion-dollars-worth-of-us-made-electronics/",
            "snippet": "Nvidia will procure \u201cseveral hundred billion\u201d dollars' worth of chips and electronics manufactured in the U.S. over the next four years.",
            "score": 0.7526037096977234,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia will procure \u201cseveral hundred billion\u201d dollars\u2019 worth of chips and other electronics manufactured in the U.S. over the next four years, CEO Jensen Huang said.\n\nThe chip designer will source these products from suppliers like Taiwan Semiconductor Manufacturing Company (TSMC) and Foxconn, which can manufacture its latest systems in the U.S., Huang told the Financial Times (FT) in a report posted Thursday (March 20).\n\nBy doing so, Nvidia will avoid tariffs and will improve the resiliency of its supply chain, according to the report.\n\nNvidia has been highly dependent on TSMC\u2019s manufacturing facilities in Taiwan, which are subject to the risks of threats from China, earthquakes and potential U.S. tariffs, the report said.\n\nHuang also said that the Trump administration\u2019s policies will give a boost to Nvidia\u2019s efforts to build data centers, which require vast amounts of energy, per the report.\n\n\u201cHaving the support of an administration who cares about the success of this industry and not allowing energy to be an obstacle is a phenomenal result for AI in the U.S.,\u201d Huang told the FT.\n\nHuang\u2019s remarks follow announcements from some other tech companies that are increasing their investment in the U.S.\n\nApple said in February that it plans to invest more than $500 billion in the U.S. in the next four years. The company\u2019s plan includes hiring 20,000 workers and opening a manufacturing facility in Houston to make servers to support its artificial intelligence (AI) system, Apple Intelligence.\n\nThe company also said it plans to keep expanding data center capacity in North Carolina, Iowa, Oregon, Arizona and Nevada, while opening the \u201cApple Manufacturing Academy\u201d in Detroit to help companies pivot to advanced manufacturing.\n\nTSMC said March 4 that plans to expand its investment in advanced semiconductor manufacturing in the U.S. by an additional $100 billion, bringing its total investment in the country to $165 billion. TSMC already has an ongoing $65 billion investment in its operations in Phoenix.\n\nThe company\u2019s latest investment plans include three new fabrication plants, two advanced packaging facilities and a major research and design (R&D) team center. The investment is expected to support 40,000 construction jobs over the next four years and create tens of thousands of tech jobs.\n\nFor all PYMNTS AI coverage, subscribe to the daily AI Newsletter.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia CEO Tries To Make Peace With Quantum Execs",
            "link": "https://www.investors.com/news/technology/quantum-computing-roundtable-nvidia-gtc/",
            "snippet": "Nvidia on Thursday hosted a roundtable discussion on quantum computing at its GTC conference with major players in the space.",
            "score": 0.578677237033844,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia to open quantum computing lab, CEO says",
            "link": "https://www.reuters.com/technology/nvidia-open-quantum-computing-lab-ceo-says-2025-03-20/",
            "snippet": "Nvidia will open a quantum computing research lab in Boston, where it plans to collaborate with scientists from Harvard University and the Massachusetts...",
            "score": 0.8930662870407104,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia Turns Its AI Eye To The Enterprise",
            "link": "https://www.nextplatform.com/2025/03/20/nvidia-turns-its-ai-eye-to-the-enterprise/",
            "snippet": "If you are going to be the full-stack, hardware-to-software-to-developer tools leader in the brave new world of AI, as Nvidia most certainly is, then you.",
            "score": 0.9010241627693176,
            "sentiment": null,
            "probability": null,
            "content": "If you are going to be the full-stack, hardware-to-software-to-developer tools leader in the brave new world of AI, as Nvidia most certainly is, then you have to be able to adapt to the myriad environments that the emerging technology is threading itself through.\n\nDuring his keynote address at Nvidia\u2019s GPU Technical Conference 2025 this week in San Jose, company co-founder and chief executive officer Jensen Huang said the accelerated computing leader planned to do just that.\n\nThere was plenty on Huang\u2019s menu about Nvidia\u2019s \u201cBlackwell\u201d B300 GPUs and their upcoming successors, the \u201cRubin\u201d family of accelerators, and other critical components like networking, which The Next Platform has already discussed. But there also was plenty that was rolled out that addressed the enterprise and the edge and the world of physical AI and, as the CEO pointed out, there were things from Nvidia for all of that.\n\n\u201cThe cloud service providers, of course, they like our leading-edge technology, they like the fact that we have a full stack, because accelerated computing is not about the chip,\u201d he said. \u201cIt\u2019s not even about the chip and the libraries, the programming model. It\u2019s the chip, the programming model, and a whole bunch of software that goes on top of it. That entire stack is incredibly complex. They also love the fact that Nvidia CUDA developers are CSP customers because, in the final analysis, they\u2019re building infrastructure for the world to use.\u201d\n\nThat said, while AI may have gained its momentum in the cloud, it\u2019s not staying there.\n\n\u201cNow that we\u2019re going to take AI out to the rest of the world, the rest of the world has different system configurations, operating environment differences, domain-specific library differences, usage differences,\u201d Huang said. \u201cAI, as it translates to enterprise IT, as it translates to manufacturing, as it translates to robotics and self-driving cars, or even companies that are starting GPU clouds they have their own requirements.\u201d\n\nHe said AI and machine learning have reinvented the computing stack, from the processor through the operating to the applications, and how enterprises run and orchestrate them will be different. Rather than retrieving the data they want and then read it, enterprise workers will ask the AI system a question and it will give them an answer.\n\n\u201cThis is the way enterprise IT will work in the future,\u201d Huang said. \u201cWe\u2019ll have AI agents, which will be part of our digital workforce. There a billion knowledge workers in the world. There will probably be 10 billion digital workers working with us side-by-side. AI agents will be everywhere. How they run, what enterprises run, and how we run them will be fundamentally different. So we need a new line of computers.\u201d\n\nThat tends to start with hardware, in this case two personal AI supercomputers, the DGX Spark \u2013 formerly Project DIGITS \u2013 and DGX Station, Blackwell-powered desktop systems for inference and other tasks that can run locally or in Nvidia\u2019s DGX Cloud or other accelerated cloud environments. DGX Spark is powered by a GB10 Grace Blackwell Superchip, which Nvidia said will deliver up to a thousand-trillion operations per second for AI fine-turning and inference.\n\nDGX Station is a larger desktop powered by the GB300 Grace-Blackwell Ultra Desktop Superchip and containing 784 GB of coherent memory space, Nvidia\u2019s ConnectX-8 SuperNIC, its AI Enterprise software platform, and access to the vendor\u2019s NIM AI microservices.\n\nThe systems not only give enterprise users new tools for running AI workloads, but also are another avenue into the era of AI reasoning models, a step beyond AI agents in their ability to address and solve problems and miles away from the prompt-and-reply nature of ChatGPT and other AI chatbots.\n\n\u201cWe now have Ais that can reason, which is fundamentally about breaking down a problem, step by step,\u201d Huang said. \u201cNow we have Ais that can reason step by step by step using \u2026 technologies called chain of thought, best of N, consistency checking, path planning, a variety of different techniques. We now have Ais that can reason.\u201d\n\nNvidia at the Consumer Electronics Show in January unveiled the Llama Nemotron advanced agentic AI models and Cosmos Nemotron vision language models, making them both available as NIM microservices for developers to build AI agents that can understand language and the world and respond appropriately.\n\nAt GTC, Nvidia unveiled a family of open Llama Nemotron models with improved reasoning capabilities for multistep math, coding, decision-making, and the ability to follow instructions. According to Kari Briski, vice president of generative AI software for the enterprise at Nvidia, the vendor also is supplying datasets \u2013 comprising 60 billion tokens of Nvidia-generated synthetic datasets \u2013 and techniques to further help developers adopt the models.\n\n\u201cJust like humans, agents need to understand context to breakdown complex requests, understand the user\u2019s intent, and adapt in real time,\u201d Briski told journalists in a prebriefing before the keynote.\n\nThe reasoning capability can be toggled on and off and the Nemotron models come in three sizes, with Nano being the smallest and offering the highest accuracy on PCs and edge devices. Super models include high accuracy and throughput on a single GPU, while the Ultra model will run on multiple GPUs. Nano and Super models are available now, with Ultra coming soon, she said.\n\nAmong the additions to Nvidia\u2019s AI Enterprise software platform is AI-Q Blueprint, another NIM-based offering that allow enterprise to connect proprietary data to reasoning AI agents. The open software integrates with Nvidia\u2019s NeMo Retriever tool to query multiple data types, such as text, images, and videos, and allow for the vendor\u2019s accelerated computing to work with third-party storage platforms and software, as well as the Llama Nemotron models.\n\n\u201cFor teams of connected agents, the blueprint provides observability and transparency into agent activity, allowing the developers to improve agents over time,\u201d Briski said. \u201cDevelopers can improve agent accuracy and reduce the completion of these tasks from hours to minutes.\u201d\n\nNvidia\u2019s AI Data Platform is a reference design for enterprise infrastructure that includes AI query agents built via the AI-Q Blueprint.\n\nIn his keynote, Huang also talked about physical AI, an area that calls for integrating AI into physical systems to allow them to perceive and react to the real world and which he said could become the largest space in the AI market.\n\n\u201cAI that understands the physical world, things like friction and inertia, cause and effect, object permanence, that ability to understand the physical world, the three-dimensional world,\u201d Huang said. \u201cIt\u2019s what\u2019s going to enable a new era of physical AI and it\u2019s going to enable robotics.\u201d\n\nThere were a number of announcements in this area, including Nvidia\u2019s AI Dataset aimed at robotics and autonomous vehicles. Developers can use the dataset to pretrain, test, and validate models or for post-training and fine tuning foundation models. It includes both real-world and synthetic data the company uses for its Cosmos world model development platform as well as its Drive AV software, Isaac AI robot development platform, and Metropolis framework for smart cities.\n\nThe first iteration is available on Hugging Face, with 15 terabytes of data for robotics training now and support for autonomous vehicle development coming soon.\n\nHuang also noted Nvidia\u2019s Isaac GROOT N1, an open foundation model trained on real-world and synthetic data for humanoid robots and the result of Project GROOT, which the vendor unveiled at last year\u2019s GTC.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "NVIDIA Blackwell Ultra Fuels AI & HPC Innovation, Efficiency and Capability",
            "link": "https://io-fund.com/artificial-intelligence/semiconductors/nvidia-blackwell-ultra-ai-hpc-gtc-2025",
            "snippet": "NVIDIA's latest Blackwell Ultra GPU, unveiled at NVIDIA GTC 2025, is transforming AI acceleration and high-performance computing (HPC).",
            "score": 0.8345863223075867,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA\u2019s groundbreaking hardware technologies and AI are unlocking unprecedented computational power. At the NVIDIA GTC 2025, NVIDIA unveiled its Blackwell Ultra GPU designed for the \u201cAge of Reasoning\u201d at its 2025 GPU Technology Conference (GTC). AI accelerators like GPUs are well suited for AI training and inference due to parallel processing, which allows for many calculations to be performed simultaneously. Only 30% of the top 500 supercomputers relied on accelerated computing; today, 80% do. The Green 500 ranking of supercomputers by energy efficiency shows an even more pronounced trend.\n\nNVIDIA Blackwell Ultra GPU and GB300 NVL72 server key specifications included.\n\nSource: NVIDIA\n\nBlackwell Ultra GPUs for the Age of Reasoning\n\nAI reasoning models emulate how the brain thinks to render a conclusion, popularized by OpenAI\u2019s o1, Google\u2019s Gemini 2.0 Flash Thinking and DeepSeek\u2019s R1 A1 models. Reasoning models improve responses to queries and more powerful GPUs improve the performance of these models. Blackwell Ultra GPUs are the next generation of the evolution of the GB200 bolstered by more inference power horsepower, packing 50% FLOPS at 1.1 exaFLOPS of FP dense compute.\n\nNVIDIA Blackwell Ultra AI Factory Output chart shows 50x performance increase.\n\nSource: NVIDIA\n\nAt the NVIDIA GTC 2025, in his March 18 presentation titled, \u201cThe Next Frontier of AI Supercomputing: Efficiency With Unprecedented Capability\u201d, NVIDIA\u2019s Vice President of Hyperscale and HPC Computing, Ian Buck, stated, \u201cBlackwell Ultra takes GB200\u2019s 40x data center revenue opportunity to 50x\u201d, citing faster token serving and higher throughput ideal for post-training for models like DeepSeek, which chomp through 100 trillion tokens.\n\nNVIDIA GB300 NVL72 Unleashes Inference Horsepower\n\nNVIDIA\u2019s GB300 superchip combines two Blackwell Ultra GPUs with one Grace CPU. Blackwell Ultra GPUs can be used in the NVL72 rack server, which integrates 72 Blackwell Ultra GPUs and 36 Grace CPUs. The NVIDIA GB300 NVL72 has a fully liquid-cooled rack-scale design. AI factories achieve 50X higher output for reasoning model inference with the NVIDIA GB300 NVL72 compared to the NVIDIA Hopper platform when used with the NVIDIA Quantum-X800 InfiniBand or Spectrum-X Ethernet paired with ConnectX-8 SuperNICS.\n\nBlackwell Ultra\u2019s Silicon Photonics Slashes Power Consumption by Up to 77%\n\nNVIDIA\u2019s Blackwell Ultra GPUs use co-packaged optics with silicon photonics, which integrates optical and silicon components onto a single substrate. This reduces power consumption by eliminating the need for external lasers and pluggable transceivers to achieve a significant reduction in power from 39 watts to 9 watts. Buck said that silicon photonics \"\u2026 gives you that benefit from going from 30 watts of power down to only 9 watts of power for the same number of ports, and that's huge. It doesn't sound like 39 sounds a lot. But if you get 400,000 GPUs in an AI supercomputer, there's like 24 megawatts of lasers like so that's a lot of laser light that could be optimized and made more efficient.\u201d\n\nJoin thousands of investors who trust I/O Fund\u2019s expert stock analysis on AI, semiconductors, cryptocurrency, and adtech \u2014 sign up for free! Click here!\n\nBeth Kindig, Lead Analyst at the IO Fund, pointed out in her \u201cAI Power Consumption: Rapidly Becoming Mission-Critical\u201d blog article that, \"In my analysis last month on the Blackwell architecture, I made the argument these estimates are too low and that my firm expects we will see a $200 billion data center segment by end of CY2025 propelled forward by the B100, B200 and GB200, including the following points: \u201cTaiwan Semi\u2019s CoWos capacity, which is essential for Blackwell\u2019s architecture, is estimated to rise to 40,000 units/month by the end of 2024, which is more than a 150% YoY increase from ~15,000 units/month at the end of 2023. Applied Materials has boosted its forecast for HBM packaging revenue from a prior view for 4X growth to 6X growth this year.\u201d\u201d\n\nThe Next Generation CPU: Vera CPU: Grace\u2019s Successor\n\nNVIDIA\u2019s next-generation CPU is Vera, a follow-on to Grace. With 88 cores (176 threads via spatial multithreading), Vera doubles Grace\u2019s performance 2X, memory bandwidth by 5X per watt, and has a beefier chip-to-chip link for the upcoming Rubin GPU. \u201cEvery core talks to every other core,\u201d Buck stressed, contrasting x86\u2019s front-end focus. Vera\u2019s 12-thread memory saturation trounces traditional CPUs, feeding GPUs for AI and HPC back-end tasks. Vera Rubin will launch in 2026. Vera Rubin NVL 144 will launch in the second half of 2026. FYI, Vera Rubin was an American astronomer who discovered dark matter. Rubin will mark the shift from HBM3/HBM3e to HBM4 and HBM4e for Rubin Ultra.\n\nThe Next Generation GPU Architecture: Rubin Ultra\n\nNVIDIA will be launching Vera Rubin NVL 576 in the second half of 2027, which will have 14X the performance of GB300 NVL72. Rubin will have 1.2 ExaFLOPS of FP8 training compared to just 0.36 ExaFLOPS for B300, resulting in 3.3X compute performance. Bandwidth will improve from 8 TB/s to 13 TB/s. It will have 576 Rubin GPUs in a rack. Compute density is boosted by featuring four dies per package. Rubin Ultra NVL576 will have 365 TB of memory. The inference compute with FP4 rises to 15 ExaFLOPS with 5 ExaFLOPS of FP8 training compute. NVIDIA hinted the next-generation architecture after astronomer Vera Rubin will be named after theoretical physicist Richard Feynman.\n\nThe I/O Fund recently entered five new small and mid-cap positions that we believe will be beneficiaries of this AI spending war. We discuss entries, exits, and what to expect from the broad market every Thursday at 4:30 p.m. in our 1-hour webinar. For a limited time, get $110 off an Annual Pro plan with code PRO110OFF [Learn more here.]\n\nDisclaimer: This is not financial advice. Please consult with your financial advisor in regards to any stocks you buy.\n\nRecommended Reading:",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia CEO Jensen Huang Apologizes for His Quantum Effect",
            "link": "https://www.wsj.com/articles/nvidia-ceo-jensen-huang-apologizes-for-his-quantum-effect-d0fef4d8",
            "snippet": "SAN JOSE, Calif.\u2014Nvidia CEO Jensen Huang offered a quantum-level climb-down on Thursday, after spooking quantum-computing companies earlier this year with...",
            "score": 0.8232370615005493,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia stock just flashed a dreaded technical 'death cross' signal",
            "link": "https://finance.yahoo.com/news/nvidia-stock-just-flashed-dreaded-231333980.html",
            "snippet": "",
            "score": 0.9039190411567688,
            "sentiment": null,
            "probability": null,
            "content": "Getty Images; Chelsea Jia Feng/BI\n\nNvidia shares hit a \"death cross\" on Thursday signaling a potential downtrend after a 948% rally.\n\nA death cross occurs when a stock's 50-day moving average price falls below its 200-day moving average.\n\nAnalyst Ari Wald suggests the signal may not lead to a major decline, citing range-bound trading.\n\nNvidia shares flashed the dreaded \"death cross\" signal on Thursday.\n\nThe technical sell signal occurs when the 200-day moving average rises above the 50-day moving average.\n\nThe stock's 50-day moving average hit $127.39, dipping below the 200-day moving average of $127.73 early in Thursday's trading session before paring losses to rise about 1%.\n\nThe moving average crossover strategy can signal a reversal in a prior trend, suggesting that after a massive 948% rally since October 2022, shares of Nvidia could be on the verge of a downtrend.\n\nThe last time Nvidia flashed a death cross signal was in April 2022, amid a broader bear market for stocks. Shares of Nvidia went on to decline 47% before they bottomed out in October 2022.\n\nAri Wald, head of technical analysis at Oppenheimer & Co., said the death cross signal in Nvidia shares isn't a foolproof signal of a coming decline, and could ultimately be a head fake.\n\n\"While every major decline starts with a 'death cross' not every 'death cross' leads to a major decline,\" Wald told BI.\n\nInstead, the current death cross in Nvidia shares could reflect the stock's range-bound behavior for nearly a year.\n\n\"The stock has shown a loss of momentum for a number of months which can be shown by the fact the stock has made little progress over the last 6-9 months,\" Wald said.\n\nHe added that he is staying on the sidelines with Nvidia stock until the broader market shows signs that it's bottomed after its latest decline. The S&P 500 entered correction territory last week, falling 10% from its peak in February.\n\n\"For now, we'd continue to respect the continued deterioration in the stock's trend, including the most recent 'death cross,'\" Wald said.\n\nWald sees $128 as a key resistance level and $100 as key support for Nvidia shares.\n\nRead the original article on Business Insider",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia CEO says he was surprised that publicly held quantum firms exist",
            "link": "https://fortune.com/2025/03/20/nvidia-ceo-says-he-was-surprised-that-publicly-held-quantum-firms-exist/",
            "snippet": "Nvidia CEO Jensen Huang didn't realize there were publicly traded quantum-computing companies.",
            "score": 0.8250504732131958,
            "sentiment": null,
            "probability": null,
            "content": "\u00a9 2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information\n\nFORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-19": {
        "0": {
            "title": "NVIDIA Honors Americas Partners Advancing Agentic and Physical AI",
            "link": "https://blogs.nvidia.com/blog/partner-network-awards-2025/",
            "snippet": "NVIDIA this week recognized Americas partners for their work advancing agentic and physical AI across industries.",
            "score": 0.8747719526290894,
            "sentiment": null,
            "probability": null,
            "content": "The NVIDIA Partner Network celebrates industry leaders with \u2018Partner of the Year\u2019 awards, including new \u2018Trailblazer,\u2019 \u2018Rising Star Consulting\u2019 and \u2018Innovation\u2019 categories.\n\nNVIDIA this week recognized 14 partners leading the way across the Americas for their work advancing agentic and physical AI across industries.\n\nThe 2025 Americas NVIDIA Partner Network awards \u2014 announced at the GTC 2025 global AI conference \u2014 represent key efforts by industry leaders to help customers become experts in using AI to solve many of today\u2019s greatest challenges. The awards honor the diverse contributions of NPN members fostering AI-driven innovation and growth.\n\nThis year, NPN introduced three new award categories that reflect how AI is driving economic growth and opportunities, including:\n\nTrailblazer , which honors a visionary partner spearheading AI adoption and setting new industry standards.\n\n, which honors a visionary partner spearheading AI adoption and setting new industry standards. Rising Star , which celebrates an emerging talent helping industries harness AI to drive transformation.\n\n, which celebrates an emerging talent helping industries harness AI to drive transformation. Innovation, which recognizes a partner that\u2019s demonstrated exceptional creativity and forward thinking.\n\nThis year\u2019s NPN ecosystem winners have helped companies across industries use AI to adapt to new challenges and prioritize energy-efficient accelerated computing. NPN partners help customers implement a broad range of AI technologies, including NVIDIA-accelerated AI factories, as well as large language models and generative AI chatbots, to transform business operations.\n\nThe 2025 NPN award winners for the Americas are:\n\nGlobal Consulting Partner of the Year \u2014 Accenture is recognized for its impact and depth of engineering with its AI Refinery platform for industries, simulation and robotics, marketing and sovereignty, which helps organizations enhance innovation and growth with custom-built approaches to AI-driven enterprise reinvention.\n\nis recognized for its impact and depth of engineering with its AI Refinery platform for industries, simulation and robotics, marketing and sovereignty, which helps organizations enhance innovation and growth with custom-built approaches to AI-driven enterprise reinvention. Trailblazer Partner of the Year \u2014 Advizex is recognized for its commitment to driving innovation in AI and high-performance computing, helping industries like healthcare, manufacturing, retail and government seamlessly integrate advanced AI technologies into existing business frameworks. This enables organizations to achieve significant operations efficiencies, enhanced decision-making, and accelerated digital transformation.\n\nis recognized for its commitment to driving innovation in AI and high-performance computing, helping industries like healthcare, manufacturing, retail and government seamlessly integrate advanced AI technologies into existing business frameworks. This enables organizations to achieve significant operations efficiencies, enhanced decision-making, and accelerated digital transformation. Rising Star Partner of the Year \u2014 AHEAD is recognized for its leadership, technical expertise and deployment of NVIDIA software, NVIDIA DGX systems, NVIDIA HGX and networking technologies to advance AI, benefitting customers across healthcare, financial services, life sciences and higher education.\n\nis recognized for its leadership, technical expertise and deployment of NVIDIA software, NVIDIA DGX systems, NVIDIA HGX and networking technologies to advance AI, benefitting customers across healthcare, financial services, life sciences and higher education. Networking Partner of the Year \u2014 Computacenter is recognized for advancing high-performance computing and data centers with NVIDIA networking technologies. The company achieved this by using the NVIDIA AI Enterprise software platform, DGX platforms and NVIDIA networking to drive innovation and growth throughout industries with efficient, accelerated data centers.\n\nis recognized for advancing high-performance computing and data centers with NVIDIA networking technologies. The company achieved this by using the NVIDIA AI Enterprise software platform, DGX platforms and NVIDIA networking to drive innovation and growth throughout industries with efficient, accelerated data centers. Solution Integration Partner of the Year \u2014 EXXACT is recognized for its efforts in helping research institutions and businesses tap into generative AI, large language models and high-performance computing. The company harnesses NVIDIA GPUs and networking technologies to deliver powerful computing platforms that accelerate innovation and tackle complex computational challenges across various industries.\n\nis recognized for its efforts in helping research institutions and businesses tap into generative AI, large language models and high-performance computing. The company harnesses NVIDIA GPUs and networking technologies to deliver powerful computing platforms that accelerate innovation and tackle complex computational challenges across various industries. Enterprise Partner of the Year \u2014 World Wide Technology (WWT) is recognized for its leadership in advancing AI adoption of customers across industry verticals worldwide. The company expanded its end-to-end AI capabilities by integrating NVIDIA Blueprints into its AI Proving Ground and has made a $500 million commitment to AI development over three years to help speed enterprise generative AI deployments.\n\nis recognized for its leadership in advancing AI adoption of customers across industry verticals worldwide. The company expanded its end-to-end AI capabilities by integrating NVIDIA Blueprints into its AI Proving Ground and has made a $500 million commitment to AI development over three years to help speed enterprise generative AI deployments. Software Partner of the Year \u2014 Mark III is recognized for the work of its cross-functional team spanning data scientists, developers, 3D artists, systems engineers, and HPC and AI architects, as well as its close collaborations with enterprises and institutions, to deploy NVIDIA software, including NVIDIA AI Enterprise and NVIDIA Omniverse, across industries. These efforts have helped many customers build software-powered pipelines and data flywheels with machine learning, generative AI, high-performance computing and digital twins.\n\nis recognized for the work of its cross-functional team spanning data scientists, developers, 3D artists, systems engineers, and HPC and AI architects, as well as its close collaborations with enterprises and institutions, to deploy NVIDIA software, including NVIDIA AI Enterprise and NVIDIA Omniverse, across industries. These efforts have helped many customers build software-powered pipelines and data flywheels with machine learning, generative AI, high-performance computing and digital twins. Higher Education Research Partner of the Year \u2014 Mark III is recognized for its close engagement with universities, academic institutions and research organizations to cultivate the next generation of leaders across AI, machine learning, generative AI, high-performance computing and digital twins.\n\nis recognized for its close engagement with universities, academic institutions and research organizations to cultivate the next generation of leaders across AI, machine learning, generative AI, high-performance computing and digital twins. Healthcare Partner of the Year \u2014 Lambda is recognized for empowering healthcare and biotech organizations with AI training, fine-tuning and inferencing solutions to speed innovation and drive breakthroughs in AI-driven drug discovery. The company provides AI training, fine-tuning and inferencing solutions at every scale \u2014 from individual workstations to comprehensive AI factories \u2014 that help healthcare providers seamlessly integrate NVIDIA accelerated computing and software into their infrastructure.\n\nis recognized for empowering healthcare and biotech organizations with AI training, fine-tuning and inferencing solutions to speed innovation and drive breakthroughs in AI-driven drug discovery. The company provides AI training, fine-tuning and inferencing solutions at every scale \u2014 from individual workstations to comprehensive AI factories \u2014 that help healthcare providers seamlessly integrate NVIDIA accelerated computing and software into their infrastructure. Financial Services Partner of the Year \u2014 WWT is recognized for driving the digital transformation of the world\u2019s largest banks and financial institutions. The company harnesses NVIDIA AI technologies to optimize data management, enhance cybersecurity and deliver transformative generative AI solutions, helping financial services clients navigate rapid technological changes and evolving customer expectations.\n\nis recognized for driving the digital transformation of the world\u2019s largest banks and financial institutions. The company harnesses NVIDIA AI technologies to optimize data management, enhance cybersecurity and deliver transformative generative AI solutions, helping financial services clients navigate rapid technological changes and evolving customer expectations. Innovation Partner of the Year \u2014 Cambridge Computer is recognized for supporting customers deploying transformative technologies, including NVIDIA Grace Hopper, NVIDIA Blackwell and the NVIDIA Omniverse platform for physical AI.\n\nis recognized for supporting customers deploying transformative technologies, including NVIDIA Grace Hopper, NVIDIA Blackwell and the NVIDIA Omniverse platform for physical AI. Service Delivery Partner of the Year \u2014 SoftServe is recognized for its impact in driving enterprise adoption of NVIDIA AI and Omniverse with custom NVIDIA Blueprints that tap into NVIDIA NIM microservices and NVIDIA NeMo and Riva software. SoftServe helps customers create generative AI services for industries spanning manufacturing, retail, financial services, auto, healthcare and life sciences.\n\nis recognized for its impact in driving enterprise adoption of NVIDIA AI and Omniverse with custom NVIDIA Blueprints that tap into NVIDIA NIM microservices and NVIDIA NeMo and Riva software. SoftServe helps customers create generative AI services for industries spanning manufacturing, retail, financial services, auto, healthcare and life sciences. Distribution Partner of the Year \u2014 TD SYNNEX has been recognized for the second consecutive year for supporting customers in accelerating AI growth through rapid delivery of NVIDIA accelerated computing and software, as part of its Destination AI initiative.\n\nhas been recognized for the second consecutive year for supporting customers in accelerating AI growth through rapid delivery of NVIDIA accelerated computing and software, as part of its Destination AI initiative. Rising Star Consulting Partner of the Year \u2014 Tata Consultancy Services (TCS) is recognized for its growth and commitment to providing industry-specific solutions that help customers adopt AI faster and at scale. Through its recently launched business unit and center of excellence built on NVIDIA AI Enterprise and Omniverse, TCS is poised to accelerate adoption of agentic AI and physical AI solutions to speed innovation for customers worldwide.\n\nis recognized for its growth and commitment to providing industry-specific solutions that help customers adopt AI faster and at scale. Through its recently launched business unit and center of excellence built on NVIDIA AI Enterprise and Omniverse, TCS is poised to accelerate adoption of agentic AI and physical AI solutions to speed innovation for customers worldwide. Canadian Partner of the Year \u2014 Hypertec is recognized for its advancement of high-performance computing and generative AI across Canada. The company has employed the full-stack NVIDIA platform to accelerate AI for financial services, higher education and research.\n\nis recognized for its advancement of high-performance computing and generative AI across Canada. The company has employed the full-stack NVIDIA platform to accelerate AI for financial services, higher education and research. Public Sector Partner of the Year \u2014 Government Acquisitions (GAI) is recognized for its rapid AI deployment and robust customer relationships, helping serve the unique needs of the federal government by adding AI to operations to improve public safety and efficiency.\n\nLearn more about the NPN program.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia CEO says company has not been asked to buy a stake in Intel",
            "link": "https://www.reuters.com/technology/nvidia-ceo-says-orders-36-million-blackwell-gpus-exclude-meta-2025-03-19/",
            "snippet": "Nvidia CEO Jensen Huang said on Wednesday that his company had not been approached about purchasing a stake in Intel .",
            "score": 0.9483460187911987,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Denny\u2019s Debuts New NVIDIA\u00ae Breakfast Bytes",
            "link": "https://www.globenewswire.com/news-release/2025/03/19/3045652/0/en/Denny-s-Debuts-New-NVIDIA-Breakfast-Bytes.html",
            "snippet": "Limited Time Menu Item Launches at NVIDIA GTC Conference, Honoring NVIDIA Founder and CEO Jensen Huang's Favorite Menu Hack and Inspiring Story that Began.",
            "score": 0.8785273432731628,
            "sentiment": null,
            "probability": null,
            "content": "Spartanburg, SC, March 19, 2025 (GLOBE NEWSWIRE) -- Denny\u2019s unveils a special menu item with a tech-inspired twist: the limited-edition NVIDIA\u00ae Breakfast Bytes. The delicious new breakfast item pays tribute to NVIDIA founder and CEO Jensen Huang\u2019s remarkable journey from Denny\u2019s dishwasher and server to a tech titan. This innovative dish debuted at NVIDIA\u2019s GTC Conference in San Jose, Calif., showcasing the connection between a Silicon Valley pioneer and America\u2019s Diner. B-roll of the occasion is available here.\n\nThe new NVIDIA Breakfast Bytes feature four sausage links that guests can wrap in Denny\u2019s signature buttermilk silver dollar pancakes and dip in maple syrup \u2013 mirroring Huang\u2019s favorite way to eat the dish. The culinary creation also fueled the next generation of visionaries and AI enthusiasts at the NVIDIA GTC Conference this week. GTC attendees experienced the first taste of the Breakfast Bytes from Denny\u2019s Mobile Diner before Jensen\u2019s keynote on March 18 as well as at NVIDIA\u2019s GTC Night Market that evening.\n\n\u201cJensen\u2019s journey from Denny\u2019s kitchen and dining room to the pinnacle of the tech world is a testament to the power of dreams and determination,\u201d said Denny\u2019s CEO Kelli Valade. \u201cWe\u2019re deeply honored that America\u2019s Diner played a role in NVIDIA\u2019s origin story as a global AI powerhouse. This new menu item is our way of celebrating Jensen and his groundbreaking work while still keeping Denny\u2019s as part of his inspiring story.\u201d\n\n\u201cDenny\u2019s will always be a special place for me. It\u2019s where I learned that no task was too small to do well,\u201d said NVIDIA CEO Jensen Huang. \u201cThis dish powered me through my long shifts and eventually inspired the birth of NVIDIA at a Denny\u2019s booth right here in San Jose. Seeing it on the Denny\u2019s menu today is a true full-stack moment!\u201d\n\nNVIDIA Breakfast Bytes are available online for carryout or delivery, all day, every day, including late night, for a limited time until May 13 \u2013 only on Dennys.com and Denny's iOS and Android apps. Participating Denny\u2019s restaurants in the San Jose, CA area* will serve NVIDIA Breakfast Bytes for dine-in at limited availability. To save 20% on their next meal, guests can join Denny\u2019s Rewards program at www.dennys.com/rewards. For b-roll of yesterday\u2019s occasion, click here.\n\n*Availability and prices vary by location. Check your local Denny\u2019s.\n\n###\n\nAbout Denny\u2019s Corp\n\nDenny's is a Spartanburg, S.C. - based family dining restaurant brand that has been welcoming guests to our booths for more than 70 years. Our guiding principle is simple: We love to feed people. Denny\u2019s provides craveable meals at a meaningful value across breakfast, lunch, dinner, and late night. Whether it's at our brick-and-mortar locations, via Denny's on Demand (the first delivery platform in the family dining segment), or at The Meltdown, Banda Burrito, and The Burger Den, our three virtual restaurant concepts, Denny\u2019s is ready to delight guests whenever and however they want to order. Our longstanding commitment to supporting our local communities in need is brought to life with our Mobile Relief Diner (that delivers hot meals to our neighbors during times of disaster), Denny's Hungry for Education\u2122 scholarship program, and our annual fundraiser with No Kid Hungry.\n\nDenny's is one of the largest franchised full-service restaurant brands in the world, based on the number of restaurants. As of December 25, 2024, the Denny\u2019s brand consisted of 1,499 restaurants, 1,438 of which were franchised and licensed restaurants and 61 of which were company-operated. This includes 165 restaurants in Canada, Costa Rica, Curacao, El Salvador, Guam, Guatemala, Honduras, Indonesia, Mexico, New Zealand, the Philippines, Puerto Rico, the United Arab Emirates, and the United Kingdom.\n\nTo learn more about Denny's, please visit our brand website at www.dennys.com or the brand's social channels via Facebook, Instagram, TikTok, LinkedIn or YouTube.\n\nNVIDIA is the registered trademark of NVIDIA Corporation in the U.S. and other countries.\n\nAttachments",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia stock rises as GTC leaves Wall Street analysts 'comfortable with long term AI demand'",
            "link": "https://finance.yahoo.com/news/nvidia-stock-rises-as-gtc-leaves-wall-street-analysts-comfortable-with-long-term-ai-demand-132448992.html",
            "snippet": "",
            "score": 0.8323178887367249,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock rose more than 2% Wednesday, stemming losses from a two-day slide that saw shares drop 5% as the AI chipmaker\u2019s annual GTC event failed to excite investors amid a broader market downturn.\n\nNvidia stock\u2019s reversal comes as Wall Street analysts walked away from CEO Jensen Huang\u2019s closely watched GTC 2025 keynote Tuesday optimistic about the company\u2019s roadmap and AI demand, doubling down on their bullish outlooks on the chipmaker in notes to investors. Those outlooks counter broader concerns about long-term AI demand and more efficient AI models reducing the need for computing hardware such as Nvidia\u2019s acclaimed GPUs (graphics processing units, or AI chips).\n\nRead more about Nvidia's stock moves and today's market action.\n\n\u201cWe came out of the keynote reassured in NVIDIA's leadership which if anything seems to be expanding,\u201d Citi analyst Atif Malik wrote, reiterating his Buy rating on Nvidia stock and $163 price target and calling the chipmaker \u201cking of the hill.\u201d\n\nRaymond James analyst Srini Pajjuri also maintained his Strong Buy rating on Nvidia, echoing Citi\u2019s Malik: \u201cOverall, we walked away comfortable with long term AI demand and continue to be impressed with NVDA\u2019s roadmap & technology innovation.\u201d\n\nHuang laid out Nvidia\u2019s upcoming AI chips during his presentation in San Jose, Calif., on Tuesday afternoon: Nvidia will launch its upcoming AI chip, Blackwell Ultra, in the second half of 2025; its next AI superchip, Vera Rubin in the second half of 2026; and the next-gen superchip after that (Vera Rubin Ultra) in the second half of 2027. Huang reiterated that he sees data center spending on compute hardware (i.e., Nvidia\u2019s total addressable market) reaching $1 trillion.\n\n\u201cI've said before that I expect data center build-out to reach $1 trillion. And I am fairly certain we're going to reach that very soon,\u201d he told a large crowd. During a Q&A session with analysts, Huang added that he expects companies to pour \"trillions of dollars\" into building what he calls \"AI factories,\" or colossal data centers dedicated to power artificial intelligence.\n\n\"The world is moving from the general purpose computing platform to GPU accelerated computing platform,\" he told analysts Wednesday. \"Whatever the capex of the world, it is very, very certain that our percentage of that is going to be much higher going forward.\"\n\nBernstein\u2019s Stacy Rasgon wrote in his own note to investors Wednesday morning, \u201cThe roadmap looks really solid, and their capability gap vs competitors across their entire massive stack continues to widen.\u201d",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia Rises As Tech Giant Details Massive AI Opportunity Ahead",
            "link": "https://www.investors.com/news/technology/nvidia-stock-wall-street-analysts-react-to-gtc-news/",
            "snippet": "Nvidia GTC wowed technologists with advancements in AI and accelerated computing on Tuesday. Nvidia stock rose Wednesday.",
            "score": 0.48549073934555054,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Nvidia CEO: Why the Next Stage of AI Needs A Lot More Computing Power",
            "link": "https://www.pymnts.com/artificial-intelligence-2/2025/nvidia-ceo-why-the-next-stage-of-ai-needs-a-lot-more-computing-power/",
            "snippet": "During a speech at Nvidia's developer conference, CEO Jensen Huang said more computer power is needed to power AI's current trajectory.",
            "score": 0.9203829765319824,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang said Tuesday (March 18) that massive computing power is needed to enable the current trajectory of artificial intelligence (AI) \u2014 which is moving toward AI agents and reasoning AI models \u2014 and beyond.\n\n\u201cAI is going through an inflection point,\u201d Huang said in a keynote speech at GTC, Nvidia\u2019s developer conference nicknamed \u201cAI Woodstock\u201d that was held in San Jose, California.\n\nThis shift to agentic and reasoning means \u201cthe amount of computation necessary to train those models, and to inference those models, has grown tremendously,\u201d he said.\n\nTraditional large language models use much less computing power than AI agents and reasoning models, but they also immediately answer. Reasoning models need much more power because they go back and forth reasoning within themselves before answering, which often would take longer.\n\n\u201cIn order for us to keep the model responsive, so that we don\u2019t lose our patience waiting for it to think, we now have to compute 10 times faster,\u201d Huang said. \u201cThe amount of computation we have to do is 100 times more, easily.\u201d\n\nHuang was making the case that the AI industry will still need a lot of Nvidia GPUs. The trajectory of GPU demand was in doubt in late January when startup DeepSeek disclosed that it trained its high-performing foundation AI model using only 2,000 of slower Nvidia H800 chips instead of typically tens of thousands or more for OpenAI and the like.\n\nThe news wiped out Nvidia\u2019s market value by nearly $600 billion in one day, as Wall Street sold off the stock thinking GPU demand was overblown.\n\nBut Huang believes that demand will be even greater in the future because of agentic AI and reasoning. He predicted that in the future, working alongside one billion knowledge workers will be 10 billion AI agents.\n\nAs proof of demand, Huang said in the peak sales year for its older Hopper GPUs, Nvidia shipped 1.3 million chips to the top four cloud computing companies (AWS, Microsoft, Google and Oracle). In comparison, the latest Blackwell chips already shipped 3.6 million chips in its first year.\n\nHuang also showed off a demo pitting Meta\u2019s Llama open-source model against DeepSeek\u2019s R1 reasoning model. The user asked each model to seat family around a 7-seat table at a wedding without putting the parents of the bride and the groom next to each other, plus other constraints.\n\nLlama answered immediately and generated 439 tokens (each token is about 0.75 words). The answer was wrong. R1 got it right, but also took much longer and generated 8,559 tokens. Users pay for the number of tokens.\n\nHuang said that there are techniques to make AI processing more efficient \u2014 hence needing less computing power \u2014 but he believes that demand will stay robust in the foreseeable future.\n\nAI workload efficiency is a challenge being tackled by startups like Inception Labs. Founded by professors from Stanford, UCLA and Cornell, the startup developed a technique that does parallel processing. Instead of generating one token at a time, which takes more GPU hours, it does it in parallel, which reduces the GPU hours needed.\n\nRead more: Silicon Valley Startup Inception Labs Creates Faster LLM\n\nNvidia, GM Announce Partnership\n\nNvidia and General Motors also announced that they\u2019re collaborating to use custom AI systems to build vehicles, factories and robots.\n\nGM\u2019s factories and robots will be optimized with AI using Nvidia Omniverse with Nvidia Cosmos, which are world foundation models. These will be used to create digital twins of assembly lines, enabling the virtual testing and production simulations to reduce downtime, the companies said.\n\nThe two companies also plan to train robots already in use for tasks such as material handling and transport, as well as precision welding.\n\nWithin GM\u2019s vehicles, the automaker plans to use Nvidia\u2019s Drive AGX system for advanced driver assistance and enhanced safety. GM had been using Nvidia GPUs to train its AI models, but now the partnership expands to automotive plant design and operations.\n\nOther Nvidia Collaborations\n\nNvidia also announced a collaboration with Google and its parent, Alphabet, to speed the development of AI in robotics as well as applications in health care, manufacturing and energy. Engineers from both companies are working together to develop robots with grasping skills, reimagine drug discovery, optimize energy grids and more.\n\nMeanwhile, the chipmaker said it is working with GE HealthCare to develop autonomous X-ray technologies and ultrasound applications using Nvidia\u2019s new Isaac for Healthcare medical device simulation platform, which includes pretrained models and physics-based simulations to help train and validate autonomous imaging systems before real-world deployment.\n\nThe partnership aims to expand access to imaging technologies, currently unavailable to nearly two-thirds of the global population, by enhancing systems with robotic capabilities.\n\nNvidia also unveiled desktop supercomputers under the Nvidia DGX brand, powered by its Grace Blackwell platform.\n\nTwo models come under this brand. One is DGX Spark, which Nvidia previously unveiled as a palm-size supercomputer called Project Digits. It can deliver up to 1,000 trillion operations per second of AI compute for fine-tuning and inference. Nvidia also unveiled the DGX Station, which is a desktop supercomputer that offers data center-level performance.\n\nThe chipmaker said AI developers, researchers, data scientists and students can prototype, fine-tune and inference large models on desktops either locally or in the cloud.\n\nAsus, Dell, HP and Lenovo will be manufacturing the computer hardware.\n\nNvidia is also raising the stakes in quantum computing, announcing the creation of the Nvidia Accelerated Quantum Research Center in Boston that will integrate quantum hardware with AI supercomputers to advance quantum computing technologies. The center will start operations in 2025.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia Bets Big on Synthetic Data",
            "link": "https://www.wired.com/story/nvidia-gretel-acquisition-synthetic-training-data/",
            "snippet": "Nvidia has acquired synthetic data startup Gretel to bolster the AI training data used by the chip maker's customers and developers.",
            "score": 0.6811234951019287,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has acquired synthetic data firm Gretel for nine figures, according to two people with direct knowledge of the deal.\n\nThe acquisition price exceeds Gretel\u2019s most recent valuation of $320 million, the sources say, though the exact terms of the purchase remain unknown. Gretel and its team of approximately 80 employees will be folded into Nvidia, where its technology will be deployed as part of the chip giant\u2019s growing suite of cloud-based, generative AI services for developers.\n\nThe acquisition comes as Nvidia has been rolling out synthetic data generation tools, so that developers can train their own AI models and fine-tune them for specific apps. In theory, synthetic data could create a near-infinite supply of AI training data and help solve the data scarcity problem that has been looming over the AI industry since ChatGPT went mainstream in 2022\u2014although experts say using synthetic data in generative AI comes with its own risks.\n\nA spokesperson for Nvidia declined to comment.\n\nGretel was founded in 2019 by Alex Watson, John Myers, and Ali Golshan, who also serves as CEO. The startup offers a synthetic data platform and a suite of APIs to developers who want to build generative AI models, but don\u2019t have access to enough training data or have privacy concerns around using real people\u2019s data. Gretel doesn\u2019t build and license its own frontier AI models, but fine-tunes existing open source models to add differential privacy and safety features, then packages those together to sell them. The company raised more than $67 million in venture capital funding prior to the acquisition, according to Pitchbook.\n\nA spokesperson for Gretel also declined to comment.\n\nUnlike human-generated or real-world data, synthetic data is computer-generated and designed to mimic real-world data. Proponents say this makes the data generation required to build AI models more scalable, less labor intensive, and more accessible to smaller or less-resourced AI developers. Privacy-protection is another key selling point of synthetic data, making it an appealing option for health care providers, banks, and government agencies.\n\nNvidia has already been offering synthetic data tools for developers for years. In 2022 it launched Omniverse Replicator, which gives developers the ability to generate custom, physically accurate, synthetic 3D data to train neural networks. Last June, Nvidia began rolling out a family of open AI models that generate synthetic training data for developers to use in building or fine-tuning LLMs. Called Nemotron-4 340B, these mini-models can be used by developers to drum up synthetic data for their own LLMs across \u201chealth care, finance, manufacturing, retail, and every other industry.\u201d\n\nDuring his keynote presentation at Nvidia\u2019s annual developer conference this Tuesday, Nvidia cofounder and chief executive Jensen Huang spoke about the challenges the industry faces in rapidly scaling AI in a cost-effective way.\n\n\u201cThere are three problems that we focus on,\u201d he said. \u201cOne, how do you solve the data problem? How and where do you create the data necessary to train the AI? Two, what\u2019s the model architecture? And then three, what are the scaling laws?\u201d Huang went on to describe how the company is now using synthetic data generation in its robotics platforms.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "The Rundown: Nvidia\u2019s GTC showcases new AI capabilities that span many industries",
            "link": "https://digiday.com/marketing/the-rundown-nvidias-gtc-showcases-new-ai-capabilities-that-span-many-industries/",
            "snippet": "Nvidia GTC (GPU Technology Conference) 2025 is taking place this week in San Jose, Calif., and the giant chip manufacturer has unveiled a range of AI...",
            "score": 0.7853996157646179,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia GTC (GPU Technology Conference) 2025 is taking place this week in San Jose, Calif., and the giant chip manufacturer has unveiled a range of AI capabilities across a range of industries, with marketing companies like Accenture and S4 Capital\u2019s Monks both showcasing their partnerships.\n\nOn Tuesday\u2019s keynote, Nvidia CEO and founder Jensen Huang detailed numerous updates, claiming AI and machine learning has \u201creinvented the entire computing stack,\u201d which requires different processors, operating systems, applications and data orchestration.\n\n\u201cThe way you access data will be fundamentally different from the past,\u201d he said, citing Perplexity\u2019s model of fusing traditional search methods with conversational AI as the new norm. \u201cInstead of doing retrieval that way, I just ask Perplexity what I want\u2026 This is the way enterprise IT will work in the future as well.\u201d\n\nAmong the launch was an open-source software called Dynamo, which is designed to improve the efficiency and scalability of large language models. Dynamo optimizes GPU resource allocation to accelerate AI processing, reduce costs, increase speed and enhance real-time inference, according to Huang. He added that Dynamo will process computational resources to produce AI tokens \u2014 the building blocks of AI content and enterprise solutions.\n\nAlthough Huang didn\u2019t mention too many examples on the Nvidia GTC 2025 stage, partners mentioned in a blog post about Dynamo include AWS, Google Cloud, Microsoft Azure, Meta, and Perplexity AI.\n\nMonks showcases Nvidia tech\n\nNvidia isn\u2019t a typical advertising-focused tech giant, but major brands and marketing agencies increasingly partner with it to power their own capabilities. One example is Monks, which announced a new 50-person team called the Monks Agentic Advisory that operates as a \u201cnimble advancement and consulting team.\u201d Monks is also certifying 150 engineers to build and deploy custom generative AI models for clients.\n\nTo showcase how it uses Nvidia tech, Monks also unveiled a new AI-generated video for Puma that uses agentic AI to create hyper-realistic content using Monks workflows and generative video tools from AI-startup Runway.\n\nMonks Chief Innovation Officer Henry Cowling said agencies and clients have to consider how AI will impact how people work, workflows, and broader conversations about how organizations are designed. \u201cI don\u2019t think you can talk seriously about enterprise transformation or specifically marketing operations transformation without that conversation leading to Nvidia,\u201d Cowling said. \u201cAt some point, but even if it\u2019s just at the level of the chip stack, eventually you hit Nvidia and they\u2019re obviously working migrating their way sort of up value stack as well.\u201d\n\nMore Nvidia GTC 2025 announcements\n\nT-Mobile: T-Mobile and Cisco will work to develop AI-native wireless capabilities designed for 6G wireless, with Huang claiming AI could revolutionize communication by mimicking how humans use context and prior knowledge to convey meaning.\n\nOracle: Another partnership will see it work with Oracle to help companies more easily organize and search through large volumes of text, images and videos.\n\nGoogle: Nvidia will be the first to adopt SynthID, a Google DeepMind AI technology that embeds watermarks into AI-generated content to enhance content transparency and trust in generative AI by safeguarding against misinformation and misattribution.\n\nAccenture: The consulting firm announced a new AI agent builder that will use the newly released Nvidia Llama Nemoton reasoning models. In a statement about the news, Accenture also said it\u2019s already built 50 industry-specific AI agents for clients like ESPN, HPE and the United Nations.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "NVIDIA Blackwell Powers Real-Time AI for Entertainment Workflows",
            "link": "https://blogs.nvidia.com/blog/media2-rtx-pro-blackwell/",
            "snippet": "AI has been shaping the media and entertainment industry for decades, from early recommendation engines to AI-driven editing and visual effects automation.",
            "score": 0.9100735783576965,
            "sentiment": null,
            "probability": null,
            "content": "AI has been shaping the media and entertainment industry for decades, from early recommendation engines to AI-driven editing and visual effects automation. Real-time AI \u2014 which lets companies actively drive content creation, personalize viewing experiences and rapidly deliver data insights \u2014 marks the next wave of that transformation.\n\nWith the NVIDIA RTX PRO Blackwell GPU series, announced yesterday at the NVIDIA GTC global AI conference, media companies can now harness real-time AI for media workflows with unprecedented speed, efficiency and creative potential.\n\nNVIDIA Blackwell serves as the foundation of NVIDIA Media2, an initiative that enables real-time AI by bringing together NVIDIA technologies \u2014 including NVIDIA NIM microservices, NVIDIA AI Blueprints, accelerated computing platforms and generative AI software \u2014 to transform all aspects of production workflows and experiences, starting with content creation, streaming and live media.\n\nPowering Intelligent Content Creation\n\nAccelerated computing enables AI-driven workflows to process massive datasets in real time, unlocking faster rendering, simulation and content generation.\n\nNVIDIA RTX PRO Blackwell GPUs series include new features that enable unprecedented graphics and AI performance. The NVIDIA Streaming Multiprocessor offers up to 1.5x faster throughput over the NVIDIA Ada generation, and new neural shaders that integrate AI inside of programmable shaders for advanced content creation.\n\nFourth-generation RT Cores deliver up to 2x the performance of the previous generation, enabling the creation of massive photoreal and physically accurate animated scenes. Fifth-generation Tensor Cores deliver up to 4,000 AI trillion operations per second and add support for FP4 precision. And up to 96GB of GDDR7 memory boosts GPU bandwidth and capacity, allowing applications to run faster and work with larger, more complex datasets for massive 3D and AI projects, large-scale virtual-reality environments and more.\n\n\u201cOne of the most exciting aspects of new technology is how it empowers our artists with tools to enhance their creative workflows,\u201d said Steve May, chief technology officer of Pixar Animation Studios. \u201cWith Pixar\u2019s next-generation renderer, RenderMan XPU \u2014 optimized for the NVIDIA Blackwell platform \u2014 99% of Pixar shots can now fit within the 96GB of memory on the NVIDIA RTX PRO 6000 Blackwell GPUs. This breakthrough will fundamentally improve the way we make movies.\u201d\n\n\u201cOur artists were frequently maxing out our 48GB cards with ILM StageCraft environments and having to battle performance issues on set for 6K and 8K real-time renders,\u201d said Stephen Hill, principal rendering engineer at Lucasfilm. \u201cThe new NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition GPU lifts these limitations \u2014 we\u2019re seeing upwards of a 2.5x performance increase over our current production GPUs, and with 96GB of VRAM we now have twice as much memory to play with.\u201d\n\nIn addition, neural rendering with NVIDIA RTX Kit brings cinematic-quality ray tracing and AI-enhanced graphics to real-time engines, elevating visual fidelity in film, TV and interactive media. Including neural texture compression, neural shaders, RTX Global Illumination and Mega Geometry, RTX Kit is a suite of neural rendering technologies that enhance graphics for games, animation, virtual production scenes and immersive experiences.\n\nFueling the Future of Streaming and Data Analytics\n\nData analytics is transforming raw audience insights into actionable intelligence faster than ever. NVIDIA accelerated computing and AI-powered frameworks enable studios to analyze viewer behavior, predict engagement patterns and optimize content in real time, driving hyper-personalized experiences and smarter creative decisions.\n\nWith the new GPUs, users can achieve real-time ingestion and data transformation with GPU-accelerated data loading and cleansing at scale.\n\nThe NVIDIA technologies accelerating streaming and data analytics include a suite of NVIDIA CUDA-X data processing libraries that enable immediate insights from continuous data streams and reduce latency, such as:\n\nNVIDIA cuML: Enables GPU-accelerated training and inference for recommendation models using scikit-learn algorithms, providing real-time personalization capabilities and up-to-date relevant content recommendations that boost viewer engagement while reducing churn.\n\nEnables GPU-accelerated training and inference for recommendation models using scikit-learn algorithms, providing real-time personalization capabilities and up-to-date relevant content recommendations that boost viewer engagement while reducing churn. NVIDIA cuDF: Offers pandas DataFrame operations on GPUs, enabling faster and more efficient NVIDIA-accelerated extract, transform and load operations and analytics. cuDF helps optimize content delivery by analyzing user data to predict demand and adjust content distribution in real time, improving overall user experiences.\n\nAlong with cuML and cuDF, accelerated data science libraries provide seamless integration with the open-source Dask library for multi-GPU or multi-node clusters. NVIDIA RTX Blackwell PRO GPUs\u2019 large GPU memory can further assist with handling massive datasets and spikes in usage without sacrificing performance.\n\nAnd, the video search and summarization blueprint integrates vision language models and large language models and provides cloud-native building blocks to build video analytics, search and summarization applications.\n\nBreathing Life Into Live Media\n\nWith NVIDIA RTX PRO Blackwell GPUs, broadcasters can achieve higher performance than ever in high-resolution video processing, real-time augmented reality and AI-driven content production and video analytics.\n\nNew features include:\n\nNinth-Generation NVIDIA NVENC: Adds support for 4:2:2 encoding, accelerating video encoding speed and improving quality for broadcast and live media applications while reducing costs of storing uncompressed video.\n\nAdds support for 4:2:2 encoding, accelerating video encoding speed and improving quality for broadcast and live media applications while reducing costs of storing uncompressed video. Sixth-Generation NVIDIA NVDEC: Provides up to double H.264 decoding throughput and offers support for 4:2:2 H.264 and HEVC decode. Professionals can benefit from high-quality video playback, accelerate video data ingestion and use advanced AI-powered video editing features.\n\nProvides up to double H.264 decoding throughput and offers support for 4:2:2 H.264 and HEVC decode. Professionals can benefit from high-quality video playback, accelerate video data ingestion and use advanced AI-powered video editing features. Fifth-Generation PCIe : Provides double the bandwidth over the previous generation, improving data transfer speeds from CPU memory and unlocking faster performance for data-intensive tasks.\n\n: Provides double the bandwidth over the previous generation, improving data transfer speeds from CPU memory and unlocking faster performance for data-intensive tasks. DisplayPort 2.1: Drives high-resolution displays at up to 8K at 240Hz and 16K at 60Hz. Increased bandwidth enables seamless multi-monitor setups, while high dynamic range and higher color depth support deliver more precise color accuracy for tasks like video editing and live broadcasting.\n\n\u201cThe NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition GPU is a transformative force in Cosm\u2019s mission to redefine immersive entertainment,\u201d said Devin Poolman, chief product and technology officer at Cosm, a global immersive technology, media and entertainment company. \u201cWith its unparalleled performance, we can push the boundaries of real-time rendering, unlocking the ultra-high resolution and fluid frame rates needed to make our live, immersive experiences feel nearly indistinguishable from reality.\u201d\n\nAs a key component of Cosm\u2019s CX System 12K LED dome displays, RTX PRO 6000 Max-Q enables seamless merging of the physical and digital worlds to deliver shared reality experiences, enabling audiences to engage with sports, live events and cinematic content in entirely new ways.\n\nTo learn more about NVIDIA Media2, watch the GTC keynote and register to attend sessions from NVIDIA and industry leaders at the show, which runs through Friday, March 21.\n\nTry NVIDIA NIM microservices and AI Blueprints on build.nvidia.com.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia to invest billions in US chip production over four years, FT reports",
            "link": "https://www.reuters.com/technology/nvidia-invest-billions-us-chip-production-over-four-years-ft-reports-2025-03-20/",
            "snippet": "Nvidia plans to invest hundreds of billions of dollars in U.S.-made chips and electronics over the next four years, the Financial Times reported on...",
            "score": 0.5433275103569031,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-18": {
        "0": {
            "title": "NVIDIA Announces DGX Spark and DGX Station Personal AI Computers",
            "link": "https://nvidianews.nvidia.com/news/nvidia-announces-dgx-spark-and-dgx-station-personal-ai-computers",
            "snippet": "NVIDIA today unveiled NVIDIA DGX\u2122 personal AI supercomputers powered by the NVIDIA Grace Blackwell platform.",
            "score": 0.9081408977508545,
            "sentiment": null,
            "probability": null,
            "content": "Powered by NVIDIA Grace Blackwell, Desktop Supercomputers Place Accelerated AI in the Hands of Developers, Researchers and Data Scientists; Systems Coming From Leading Computer Makers Including ASUS, Dell Technologies, HP and Lenovo\n\nGTC\u2014NVIDIA today unveiled NVIDIA DGX\u2122 personal AI supercomputers powered by the NVIDIA Grace Blackwell platform.\n\nDGX Spark \u2014 formerly Project DIGITS \u2014 and DGX Station\u2122, a new high-performance NVIDIA Grace Blackwell desktop supercomputer powered by the NVIDIA Blackwell Ultra platform, enable AI developers, researchers, data scientists and students to prototype, fine-tune and inference large models on desktops. Users can run these models locally or deploy them on NVIDIA DGX Cloud or any other accelerated cloud or data center infrastructure.\n\nDGX Spark and DGX Station bring the power of the Grace Blackwell architecture, previously only available in the data center, to the desktop. Global system builders to develop DGX Spark and DGX Station include ASUS, Dell, HP Inc. and Lenovo.\n\n\u201cAI has transformed every layer of the computing stack. It stands to reason a new class of computers would emerge \u2014 designed for AI-native developers and to run AI-native applications,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cWith these new DGX personal AI computers, AI can span from cloud services to desktop and edge applications.\u201d\n\nIgniting Innovation With DGX Spark\n\nDGX Spark is the world\u2019s smallest AI supercomputer, empowering millions of researchers, data scientists, robotics developers and students to push the boundaries of generative and physical AI with massive performance and capabilities.\n\nAt the heart of DGX Spark is the NVIDIA GB10 Grace Blackwell Superchip, optimized for a desktop form factor. GB10 features a powerful NVIDIA Blackwell GPU with fifth-generation Tensor Cores and FP4 support, delivering up to 1,000 trillion operations per second of AI compute for fine-tuning and inference with the latest AI reasoning models, including the NVIDIA Cosmos Reason world foundation model and NVIDIA GR00T N1 robot foundation model.\n\nThe GB10 Superchip uses NVIDIA NVLink\u2122-C2C interconnect technology to deliver a CPU+GPU-coherent memory model with 5x the bandwidth of fifth-generation PCIe. This lets the superchip access data between a GPU and CPU to optimize performance for memory-intensive AI developer workloads.\n\nNVIDIA\u2019s full-stack AI platform enables DGX Spark users to seamlessly move their models from their desktops to DGX Cloud or any accelerated cloud or data center infrastructure \u2014 with virtually no code changes \u2014 making it easier than ever to prototype, fine-tune and iterate on their workflows.\n\nFull Speed Ahead With DGX Station\n\nNVIDIA DGX Station brings data-center-level performance to desktops for AI development. The first desktop system to be built with the NVIDIA GB300 Grace Blackwell Ultra Desktop Superchip, DGX Station features a massive 784GB of coherent memory space to accelerate large-scale training and inferencing workloads. The GB300 Desktop Superchip features an NVIDIA Blackwell Ultra GPU with latest-generation Tensor Cores and FP4 precision \u2014 connected to a high-performance NVIDIA Grace\u2122 CPU via NVLink-C2C \u2014 delivering best-in-class system communication and performance.\n\nDGX Station also features the NVIDIA ConnectX\u00ae-8 SuperNIC, optimized to supercharge hyperscale AI computing workloads. With support for networking at up to 800Gb/s, the ConnectX-8 SuperNIC delivers extremely fast, efficient network connectivity, enabling high-speed connectivity of multiple DGX Stations for even larger workloads, and network-accelerated data transfers for AI workloads.\n\nCombining these state-of-the-art DGX Station capabilities with the NVIDIA CUDA-X\u2122 AI platform, teams can achieve exceptional desktop AI development performance.\n\nIn addition, users gain access to NVIDIA NIM \u2122 microservices with the NVIDIA AI Enterprise software platform, which offers highly optimized, easy-to-deploy inference microservices backed by enterprise support.\n\nAvailability\n\nReservations for DGX Spark systems open today at nvidia.com .\n\nDGX Station is expected to be available from manufacturing partners like ASUS, BOXX, Dell, HP, Lambda and Supermicro later this year.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia GTC 2025: Stock Falling; Jensen Huang Keynote, Blackwell and Rubin Updates",
            "link": "https://www.barrons.com/livecoverage/nvidia-gtc-2025-ai-developers-conference",
            "snippet": "Nvidia's AI event kicks off with CEO Jensen Huang keynote. New AI chips, Vera and Rubin are coming next year. Plus, more on Blackwell Ultra and quantum...",
            "score": 0.9343068599700928,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia Is Hosting the Super Bowl of A.I.",
            "link": "https://www.nytimes.com/2025/03/18/technology/nvidia-gtc-conference-ai.html",
            "snippet": "In 2009, when Nvidia held its first developer conference, the event was something of a science fair. Dozens of academics filled a San Jose, Calif.,...",
            "score": 0.8722819089889526,
            "sentiment": null,
            "probability": null,
            "content": "In 2009, when Nvidia held its first developer conference, the event was something of a science fair. Dozens of academics filled a San Jose, Calif., hotel decorated with white poster boards of computer research. Jensen Huang, the chipmaker\u2019s chief executive, roamed the floor like a judge.\n\nThis year, Nvidia\u2019s developer conference is far different.\n\nMore than 25,000 people were expected to congregate on Tuesday around the event, known as Nvidia GTC. The crowds filled a National Hockey League arena to hear a speech about the future of artificial intelligence from Mr. Huang, who has been nicknamed \u201cA.I. Jesus.\u201d Nvidia, the world\u2019s leading developer of A.I. chips, has also wrapped San Jose in the company\u2019s neon green and black colors, shutting down city streets and sending hotel prices soaring as high as $1,800 a night.\n\nA who\u2019s who of industry leaders attended, including Michael Dell, the chief executive of Dell Technologies; Jeffrey Katzenberg, the co-founder of DreamWorks and WndrCo, a venture capital firm; and Bill McDermott, the chief executive of ServiceNow.\n\n\u201cGTC is jampacked,\u201d Mr. Huang said as he kicked off the conference on Tuesday morning. \u201cThe only way to hold more people at GTC is we\u2019re going to have to grow San Jose.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia CEO Huang says chipmaker well positioned for shift in AI",
            "link": "https://www.reuters.com/technology/artificial-intelligence/nvidia-expected-reveal-details-latest-ai-chip-conference-2025-03-18/",
            "snippet": "Nvidia CEO Jensen Huang said on Tuesday the company was well placed to navigate a shift in the artificial intelligence industry, in which businesses are...",
            "score": 0.8561850786209106,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "NVIDIA Blackwell RTX PRO Comes to Workstations and Servers for Designers, Developers, Data Scientists and Creatives to Build and Collaborate With Agentic AI",
            "link": "https://nvidianews.nvidia.com/news/nvidia-blackwell-rtx-pro-workstations-servers-agentic-ai",
            "snippet": "NVIDIA today announced the NVIDIA RTX PRO\u2122 Blackwell series \u2014 a revolutionary generation of workstation and server GPUs redefining workflows for AI,...",
            "score": 0.8505104780197144,
            "sentiment": null,
            "probability": null,
            "content": "Groundbreaking AI and Graphics Performance Redefine Visualization, Simulation and Scientific Computing From Desktop to Data Center for Millions of Professionals Worldwide\n\nGTC\u2014NVIDIA today announced the NVIDIA RTX PRO\u2122 Blackwell series \u2014 a revolutionary generation of workstation and server GPUs redefining workflows for AI, technical, creative, engineering and design professionals with breakthrough accelerated computing, AI inference, ray tracing and neural rendering technologies.\n\nFor everything from agentic AI, simulation, extended reality, 3D design and complex visual effects to developing physical AI powering autonomous robots, vehicles and smart spaces, the RTX PRO Blackwell series provides professionals across industries the latest and greatest compute power, memory capacity and data throughput right at their fingertips \u2014 from their desktop, on the go with mobile workstations or powered by data center GPUs.\n\nThe new lineup includes:\n\nData center GPU: NVIDIA RTX PRO 6000 Blackwell Server Edition\n\nNVIDIA RTX PRO 6000 Blackwell Server Edition Desktop GPUs: NVIDIA RTX PRO 6000 Blackwell Workstation Edition, NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, NVIDIA RTX PRO 5000 Blackwell, NVIDIA RTX PRO 4500 Blackwell and NVIDIA RTX PRO 4000 Blackwell\n\nNVIDIA RTX PRO 6000 Blackwell Workstation Edition, NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition, NVIDIA RTX PRO 5000 Blackwell, NVIDIA RTX PRO 4500 Blackwell and NVIDIA RTX PRO 4000 Blackwell Laptop GPUs: NVIDIA RTX PRO 5000 Blackwell, NVIDIA RTX PRO 4000 Blackwell, NVIDIA RTX PRO 3000 Blackwell, NVIDIA RTX PRO 2000 Blackwell, NVIDIA RTX PRO 1000 Blackwell and NVIDIA RTX PRO 500 Blackwell\n\n\u201cSoftware developers, data scientists, artists, designers and engineers need powerful AI and graphics performance to push the boundaries of visual computing and simulation, helping tackle incredible industry challenges,\u201d said Bob Pette, vice president of enterprise platforms at NVIDIA. \u201cBringing NVIDIA Blackwell to workstations and servers will take productivity, performance and speed to new heights, accelerating AI inference serving, data science, visualization and content creation.\u201d\n\nNVIDIA Blackwell Technology Comes to Workstations and Data Centers\n\nRTX PRO Blackwell GPUs unlock the potential of generative, agentic and physical AI by delivering exceptional performance, efficiency and scale.\n\nNVIDIA RTX PRO Blackwell GPUs feature:\n\nNVIDIA Streaming Multiprocessor: Offers up to 1.5x faster throughput and new neural shaders that integrate AI inside of programmable shaders to drive the next decade of AI-augmented graphics innovations.\n\nOffers up to 1.5x faster throughput and new neural shaders that integrate AI inside of programmable shaders to drive the next decade of AI-augmented graphics innovations. Fourth-Generation RT Cores: Delivers up to 2x the performance of the previous generation to create photoreal, physically accurate scenes and complex 3D designs with optimizations for NVIDIA RTX\u2122 Mega Geometry.\n\nDelivers up to 2x the performance of the previous generation to create photoreal, physically accurate scenes and complex 3D designs with optimizations for NVIDIA RTX\u2122 Mega Geometry. Fifth-Generation Tensor Cores: Delivers up to 4,000 AI trillion operations per second and adds support for FP4 precision and NVIDIA DLSS 4 Multi Frame Generation, enabling a new era of AI-powered graphics and the ability to run and prototype larger AI models faster.\n\nDelivers up to 4,000 AI trillion operations per second and adds support for FP4 precision and NVIDIA DLSS 4 Multi Frame Generation, enabling a new era of AI-powered graphics and the ability to run and prototype larger AI models faster. Larger, Faster GDDR7 Memory : Boosts bandwidth and capacity \u2014 up to 96GB for workstations and servers and up to 24GB on laptops. This enables applications to run faster and work with larger, more complex datasets for everything from tackling massive 3D and AI projects to exploring large-scale virtual reality environments.\n\n: Boosts bandwidth and capacity \u2014 up to 96GB for workstations and servers and up to 24GB on laptops. This enables applications to run faster and work with larger, more complex datasets for everything from tackling massive 3D and AI projects to exploring large-scale virtual reality environments. Ninth-Generation NVIDIA NVENC: Accelerates video encoding speed and improves quality for professional video applications with added support for 4:2:2 encoding.\n\nAccelerates video encoding speed and improves quality for professional video applications with added support for 4:2:2 encoding. Sixth-Generation NVIDIA NVDEC: Provides up to double the H.264 decoding throughput and offers support for 4:2:2 H.264 and HEVC decode. Professionals can benefit from high-quality video playback, accelerate video data ingestion and use advanced AI-powered video editing features.\n\nProvides up to double the H.264 decoding throughput and offers support for 4:2:2 H.264 and HEVC decode. Professionals can benefit from high-quality video playback, accelerate video data ingestion and use advanced AI-powered video editing features. Fifth-Generation PCIe : Support for fifth-generation PCI Express provides double the bandwidth over the previous generation, improving data transfer speeds from CPU memory and unlocking faster performance for data-intensive tasks.\n\n: Support for fifth-generation PCI Express provides double the bandwidth over the previous generation, improving data transfer speeds from CPU memory and unlocking faster performance for data-intensive tasks. DisplayPort 2.1 : Drives high-resolution displays at up to 4K at 480Hz and 8K at 165Hz. Increased bandwidth enables seamless multi-monitor setups, while high dynamic range and higher color depth support deliver more precise color accuracy for tasks like video editing, 3D design and live broadcasting.\n\n: Drives high-resolution displays at up to 4K at 480Hz and 8K at 165Hz. Increased bandwidth enables seamless multi-monitor setups, while high dynamic range and higher color depth support deliver more precise color accuracy for tasks like video editing, 3D design and live broadcasting. Multi-Instance GPU (MIG): The RTX PRO 6000 data center and desktop GPUs and 5000 series desktop GPUs feature MIG technology, enabling secure partitioning of a single GPU into up to four instances (6000 series) or two instances (5000 series). Fault isolation is designed to prevent workload interference for secure, efficient resource allocation for diverse workloads, maximizing performance and flexibility.\n\nThe new laptop GPUs also support the latest NVIDIA Blackwell Max-Q technologies, which intelligently and continually optimize laptop performance and power efficiency with AI.\n\nWith neural rendering and AI-augmented tools, NVIDIA RTX PRO Blackwell GPUs enable the creation of stunning visuals, digital twins of real-world environments and immersive experiences with unprecedented speed and efficiency. The GPUs are built to elevate 3D computer-aided design and building information model workflows, offering designers and engineers exceptional performance for complex modeling, rendering and visualization.\n\nDesigned for enterprise data center deployments, the RTX PRO 6000 Blackwell Server Edition features a passively cooled thermal design and can be configured with up to eight GPUs per server. For workloads that require the compute density and scale that data centers offer, the RTX PRO 6000 Blackwell Server Edition delivers powerful performance for next-generation AI, scientific and visual computing applications across industries such as healthcare, manufacturing, retail and media and entertainment.\n\nIn addition, this powerful data center GPU can be combined with NVIDIA vGPU\u2122 software to power AI workloads across virtualized environments and deliver high-performance virtual workstation instances to remote users. NVIDIA vGPU support for the NVIDIA RTX PRO 6000 Blackwell Server Edition GPU is expected in the latter half of this year.\n\n\u201cFoster + Partners has tested the NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition GPU on Cyclops, our GPU-based ray-tracing product,\u201d said Martha Tsigkari, head of applied research and development and senior partner at Foster + Partners. \u201cThe new NVIDIA Blackwell GPU has managed to outperform everything we have tested before. For example, when using it with Cyclops, it has performed at 5x the speed of NVIDIA RTX A6000 GPUs. Rendering speeds also increased 5x, allowing tools like Cyclops to provide feedback on how well our design solutions perform in real time as we design them and resulting in intuitive yet informed decision-making from early conceptual stages.\u201d\n\n\u201cEarly evaluation of the RTX PRO 6000 Blackwell technology by GE HealthCare\u2019s engineering team has found the potential for up to 2x GPU processing time improvement on reconstruction algorithms, which could lead to significant benefit to customers,\u201d said Rekha Ranganathan, senior executive and general manager of platforms and digital solutions at GE HealthCare.\n\n\u201cNVIDIA RTX PRO 6000 Blackwell Workstation Edition GPUs enable incredibly sharp and photorealistic graphics,\u201d said Jeff Hammoud, chief design officer at Rivian. \u201cIn conjunction with a Varjo XR4 headset and Autodesk VRED, the system delivered the level of crispness necessary for immersive automotive design reviews. With NVIDIA Blackwell support for PCIe Gen 5, we used two powerful 600W GPUs via VR SLI, allowing us to achieve the highest pixel density and the most stunning visuals we have ever experienced in VR.\u201d\n\n\u201cThe 96GB memory and massive AI processing power in the NVIDIA RTX PRO 6000 Blackwell Workstation Edition GPU has boosted our productivity up to 3x with AI models like Llama 3.3-70B and Mixtral 8x7b, the NVIDIA Omniverse platform and industrial copilots,\u201d said Shaun Greene, director of industry solutions at SoftServe. \u201cWe\u2019ve seen immediate performance improvements and, using workstations, can now handle AI workloads that were previously only possible in the cloud or on rack servers \u2014 unlocking new possibilities for interactive demos and production workloads in retail, manufacturing and industrial edge applications.\u201d\n\nRTX PRO GPUs run on the NVIDIA AI platform and feature larger memory capacity and the latest Tensor Cores to accelerate a deep ecosystem of AI-accelerated applications built on NVIDIA CUDA\u00ae and RTX technology. With everything from the latest AI-based content creation tools and new reasoning models, such as the NVIDIA Llama Nemotron Reason family of models and NVIDIA NIM\u2122 microservices unveiled today, inferencing is faster than ever. And with over 400 NVIDIA CUDA-X\u2122 libraries, developers can easily build, optimize, deploy and scale new AI applications, from workstations to the data center or cloud.\n\nEnterprises can fast-track their AI development and deployments by prototyping locally with an NVIDIA RTX PRO GPU and the NVIDIA Omniverse \u2122 and NVIDIA AI Enterprise platforms, NVIDIA Blueprints and NVIDIA NIM, which gives access to easy-to-use inference microservices backed by enterprise-level support. They can also run these applications at scale on the ultimate universal data center GPU for AI and visual computing, delivering breakthrough acceleration for the most demanding compute-intensive enterprise workloads with the RTX PRO 6000 Blackwell Server Edition.\n\nAvailability\n\nThe NVIDIA RTX PRO 6000 Blackwell Server Edition will soon be available in server configurations from leading data center system partners including Cisco, Dell Technologies, Hewlett Packard Enterprise , Lenovo and Supermicro.\n\nCloud service providers and GPU cloud providers including AWS, Google Cloud, Microsoft Azure and CoreWeave will be among the first to offer instances powered by the NVIDIA RTX PRO 6000 Blackwell Server Edition later this year. In addition, the server edition GPU will be available in data center platforms from ASUS, GIGABYTE, Ingrasys, Quanta Cloud Technology (QCT) and other global system partners.\n\nThe NVIDIA RTX PRO 6000 Blackwell Workstation Edition and NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition will be available through global distribution partners such as PNY and TD SYNNEX starting in April, with availability from manufacturers, such as BOXX , Dell, HP Inc., Lambda and Lenovo, starting in May.\n\nThe NVIDIA RTX PRO 5000, RTX PRO 4500 and RTX PRO 4000 Blackwell GPUs will be available in the summer from BOXX, Dell, HP and Lenovo and through global distribution partners.\n\nNVIDIA RTX PRO Blackwell laptop GPUs will be available from Dell, HP, Lenovo and Razer starting later this year.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA Blackwell Ultra AI Factory Platform Paves Way for Age of AI Reasoning",
            "link": "https://nvidianews.nvidia.com/news/nvidia-blackwell-ultra-ai-factory-platform-paves-way-for-age-of-ai-reasoning",
            "snippet": "NVIDIA today announced the next evolution of the NVIDIA Blackwell AI factory platform, NVIDIA Blackwell Ultra \u2014 paving the way for the age of AI reasoning.",
            "score": 0.5119775533676147,
            "sentiment": null,
            "probability": null,
            "content": "GTC\u2014NVIDIA today announced the next evolution of the NVIDIA Blackwell AI factory platform, NVIDIA Blackwell Ultra \u2014 paving the way for the age of AI reasoning.\n\nNVIDIA Blackwell Ultra boosts training and test-time scaling inference \u2014 the art of applying more compute during inference to improve accuracy \u2014 to enable organizations everywhere to accelerate applications such as AI reasoning, agentic AI and physical AI.\n\nBuilt on the groundbreaking Blackwell architecture introduced a year ago, Blackwell Ultra includes the NVIDIA GB300 NVL72 rack-scale solution and the NVIDIA HGX\u2122 B300 NVL16 system. The GB300 NVL72 delivers 1.5x more AI performance than the NVIDIA GB200 NVL72, as well as increases Blackwell\u2019s revenue opportunity by 50x for AI factories, compared with those built with NVIDIA Hopper\u2122.\n\n\u201cAI has made a giant leap \u2014 reasoning and agentic AI demand orders of magnitude more computing performance,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cWe designed Blackwell Ultra for this moment \u2014 it\u2019s a single versatile platform that can easily and efficiently do pretraining, post-training and reasoning AI inference.\u201d\n\nNVIDIA Blackwell Ultra Enables AI Reasoning\n\nThe NVIDIA GB300 NVL72 connects 72 Blackwell Ultra GPUs and 36 Arm\n\nNeoverse-based NVIDIA Grace\u2122 CPUs in a rack-scale design, acting as a single massive GPU built for test-time scaling. With the NVIDIA GB300 NVL72, AI models can access the platform\u2019s increased compute capacity to explore different solutions to problems and break down complex requests into multiple steps, resulting in higher-quality responses.\n\nGB300 NVL72 is also expected to be available on NVIDIA DGX\u2122 Cloud , an end-to-end, fully managed AI platform on leading clouds that optimizes performance with software, services and AI expertise for evolving workloads. NVIDIA DGX SuperPOD \u2122 with DGX GB300 systems uses the GB300 NVL72 rack design to provide customers with a turnkey AI factory.\n\nThe NVIDIA HGX B300 NVL16 features 11x faster inference on large language models, 7x more compute and 4x larger memory compared with the Hopper generation to deliver breakthrough performance for the most complex workloads, such as AI reasoning.\n\nIn addition, the Blackwell Ultra platform is ideal for applications including:\n\nAgentic AI, which uses sophisticated reasoning and iterative planning to autonomously solve complex, multistep problems. AI agent systems go beyond instruction-following. They can reason, plan and take actions to achieve specific goals.\n\nPhysical AI, enabling companies to generate synthetic, photorealistic videos in real time for the training of applications such as robots and autonomous vehicles at scale.\n\nNVIDIA Scale-Out Infrastructure for Optimal Performance\n\nAdvanced scale-out networking is a critical component of AI infrastructure that can deliver top performance while reducing latency and jitter.\n\nBlackwell Ultra systems seamlessly integrate with the NVIDIA Spectrum-X\u2122 Ethernet and NVIDIA Quantum-X800 InfiniBand platforms, with 800 Gb/s of data throughput available for each GPU in the system, through an NVIDIA ConnectX\u00ae-8 SuperNIC. This delivers best-in-class remote direct memory access capabilities to enable AI factories and cloud data centers to handle AI reasoning models without bottlenecks.\n\nNVIDIA BlueField\u00ae-3 DPUs, also featured in Blackwell Ultra systems, enable multi-tenant networking, GPU compute elasticity, accelerated data access and real-time cybersecurity threat detection.\n\nGlobal Technology Leaders Embrace Blackwell Ultra\n\nBlackwell Ultra-based products are expected to be available from partners starting from the second half of 2025.\n\nCisco, Dell Technologies, Hewlett Packard Enterprise , Lenovo and Supermicro are expected to deliver a wide range of servers based on Blackwell Ultra products, in addition to Aivres , ASRock Rack, ASUS, Eviden, Foxconn, GIGABYTE , Inventec , Pegatron, Quanta Cloud Technology (QCT), Wistron and Wiwynn .\n\nCloud service providers Amazon Web Services, Google Cloud, Microsoft Azure and Oracle Cloud Infrastructure and GPU cloud providers CoreWeave, Crusoe, Lambda, Nebius, Nscale, Yotta and YTL will be among the first to offer Blackwell Ultra-powered instances.\n\nNVIDIA Software Innovations Reduce AI Bottlenecks\n\nThe entire NVIDIA Blackwell product portfolio is supported by the full-stack NVIDIA AI platform. The NVIDIA Dynamo open-source inference framework \u2014 also announced today \u2014 scales up reasoning AI services, delivering leaps in throughput while reducing response times and model serving costs by providing the most efficient solution for scaling test-time compute.\n\nNVIDIA Dynamo is new AI inference-serving software designed to maximize token revenue generation for AI factories deploying reasoning AI models. It orchestrates and accelerates inference communication across thousands of GPUs, and uses disaggregated serving to separate the processing and generation phases of large language models on different GPUs. This allows each phase to be optimized independently for its specific needs and ensures maximum GPU resource utilization.\n\nBlackwell systems are ideal for running new NVIDIA Llama Nemotron Reason models and the NVIDIA AI-Q Blueprint, supported in the NVIDIA AI Enterprise software platform for production-grade AI. NVIDIA AI Enterprise includes NVIDIA NIM \u2122 microservices , as well as AI frameworks, libraries and tools that enterprises can deploy on NVIDIA-accelerated clouds, data centers and workstations.\n\nThe Blackwell platform builds on NVIDIA\u2019s ecosystem of powerful development tools, NVIDIA CUDA-X \u2122 libraries, over 6 million developers and 4,000+ applications scaling performance across thousands of GPUs.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "NVIDIA Launches Family of Open Reasoning AI Models for Developers and Enterprises to Build Agentic AI Platforms",
            "link": "https://nvidianews.nvidia.com/news/nvidia-launches-family-of-open-reasoning-ai-models-for-developers-and-enterprises-to-build-agentic-ai-platforms",
            "snippet": "NVIDIA today announced the open Llama Nemotron family of models with reasoning capabilities, designed to provide developers and enterprises a business-ready...",
            "score": 0.8067002296447754,
            "sentiment": null,
            "probability": null,
            "content": "GTC\u2014NVIDIA today announced the open Llama Nemotron family of models with reasoning capabilities, designed to provide developers and enterprises a business-ready foundation for creating advanced AI agents that can work independently or as connected teams to solve complex tasks.\n\nBuilt on Llama models, the NVIDIA Llama Nemotron reasoning family delivers on-demand AI reasoning capabilities. NVIDIA enhanced the new reasoning model family during post-training to improve multistep math, coding, reasoning and complex decision-making.\n\nThis refinement process boosts accuracy of the models by up to 20% compared with the base model and optimizes inference speed by 5x compared with other leading open reasoning models. The improvements in inference performance mean the models can handle more complex reasoning tasks, enhance decision-making capabilities and reduce operational costs for enterprises.\n\nLeading agent AI platform pioneers \u2014 including Accenture , Amdocs , Atlassian, Box , Cadence , CrowdStrike , Deloitte, IQVIA , Microsoft, SAP and ServiceNow \u2014 are collaborating with NVIDIA on its new reasoning models and software.\n\n\u201cReasoning and agentic AI adoption is incredible,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cNVIDIA\u2019s open reasoning models, software and tools give developers and enterprises everywhere the building blocks to create an accelerated agentic AI workforce.\u201d\n\nNVIDIA Post-Training Boosts Accuracy and Reliability for Enterprise Reasoning\n\nBuilt to deliver production-ready AI reasoning, the Llama Nemotron model family is available as NVIDIA NIM\u2122 microservices in Nano, Super and Ultra sizes \u2014 each optimized for different deployment needs.\n\nThe Nano model delivers the highest accuracy on PCs and edge devices, the Super model offers the best accuracy and highest throughput on a single GPU, and the Ultra model will provide maximum agentic accuracy on multi-GPU servers.\n\nNVIDIA conducted extensive post-training on NVIDIA DGX\u2122 Cloud using high-quality curated synthetic data generated by NVIDIA Nemotron\u2122 and other open models, as well as additional curated datasets cocreated by NVIDIA.\n\nThe tools, datasets and post-training optimization techniques used to develop the models will be openly available, giving enterprises the flexibility to build their own custom reasoning models.\n\nAgentic Platforms Team With NVIDIA to Enhance Reasoning for Industries\n\nAgentic AI platform industry leaders are working with the Llama Nemotron reasoning models to deliver advanced reasoning to enterprises.\n\nMicrosoft is integrating Llama Nemotron reasoning models and NIM microservices into Microsoft Azure AI Foundry. This expands the Azure AI Foundry model catalog with options for customers to enhance services like Azure AI Agent Service for Microsoft 365.\n\nSAP is tapping Llama Nemotron models to advance SAP Business AI solutions and Joule, the AI copilot from SAP. Additionally, it is using NVIDIA NIM and NVIDIA NeMo\u2122 microservices to promote increased code completion accuracy for SAP ABAP programming language models.\n\n\u201cWe are collaborating with NVIDIA to integrate Llama Nemotron reasoning models into Joule to enhance our AI agents, making them more intuitive, accurate and cost effective,\u201d said Walter Sun, global head of AI at SAP. \u201cThese advanced reasoning models will refine and rewrite user queries, enabling our AI to better understand inquiries and deliver smarter, more efficient AI-powered experiences that drive business innovation.\u201d\n\nServiceNow is harnessing Llama Nemotron models to build AI agents that offer greater performance and accuracy to enhance enterprise productivity across industries.\n\nAccenture has made NVIDIA Llama Nemotron reasoning models available on its AI Refinery platform \u2014 including new industry agent solutions announced today \u2014 to enable clients to rapidly develop and deploy custom AI agents tailored to industry-specific challenges, accelerating business transformation.\n\nDeloitte is planning to incorporate Llama Nemotron reasoning models into its recently announced Zora AI agentic AI platform designed to support and emulate human decision-making and action with agents that include deep functional- and industry-specific business knowledge and built-in transparency.\n\nNVIDIA AI Enterprise Delivers Essential Tools for Agentic AI\n\nDevelopers can deploy NVIDIA Llama Nemotron reasoning models with new NVIDIA agentic AI tools and software to streamline the adoption of advanced reasoning in collaborative AI systems.\n\nAll part of the NVIDIA AI Enterprise software platform, the latest agentic AI building blocks include:\n\nThe NVIDIA AI-Q Blueprint , which enables enterprises to connect knowledge to AI agents that can autonomously perceive, reason and act. Built with NVIDIA NIM microservices, the blueprint integrates NVIDIA NeMo Retriever\u2122 for multimodal information retrieval and enables agent and data connections, optimization and transparency using the open-source NVIDIA AgentIQ toolkit .\n\n, which enables enterprises to connect knowledge to AI agents that can autonomously perceive, reason and act. Built with NVIDIA NIM microservices, the blueprint integrates NVIDIA NeMo Retriever\u2122 for multimodal information retrieval and enables agent and data connections, optimization and transparency using the open-source . The NVIDIA AI Data Platform , a customizable reference design for a new class of enterprise infrastructure with AI query agents built with the AI-Q Blueprint.\n\n, a customizable reference design for a new class of enterprise infrastructure with AI query agents built with the AI-Q Blueprint. New NVIDIA NIM microservices , which optimize inference for complex agentic AI applications and enable continuous learning and real-time adaptation across any environment. The microservices ensure reliable deployment of the latest models from leading model builders including Meta, Microsoft and Mistral AI.\n\n, which optimize inference for complex agentic AI applications and enable continuous learning and real-time adaptation across any environment. The microservices ensure reliable deployment of the latest models from leading model builders including Meta, Microsoft and Mistral AI. NVIDIA NeMo microservices, which provide an efficient, enterprise-grade solution to quickly establish and maintain a robust data flywheel that enables AI agents to continuously learn from human- and AI-generated feedback. The NVIDIA AI Blueprint for building a data flywheel will offer a reference architecture for developers to easily build and optimize data flywheels using NVIDIA microservices.\n\nAvailability\n\nThe NVIDIA Llama Nemotron Nano and Super models and NIM microservices are available as a hosted application programming interface from build.nvidia.com and Hugging Face. Access for development, testing and research is free for members of the NVIDIA Developer Program.\n\nEnterprises can run Llama Nemotron NIM microservices in production with NVIDIA AI Enterprise on accelerated data center and cloud infrastructure. Developers can sign up to be notified when NVIDIA NeMo microservices are publicly available.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NVIDIA and Telecom Industry Leaders to Develop AI-Native Wireless Networks for 6G",
            "link": "https://nvidianews.nvidia.com/news/nvidia-and-telecom-industry-leaders-to-develop-ai-native-wireless-networks-for-6g",
            "snippet": "NVIDIA today unveiled partnerships with industry leaders T-Mobile, MITRE, Cisco, ODC, a portfolio company of Cerberus Capital Management, and Booz Allen...",
            "score": 0.6538553237915039,
            "sentiment": null,
            "probability": null,
            "content": "T-Mobile, MITRE, Cisco, ODC and Booz Allen Hamilton to Collaborate on Development of AI-Native Network Stack for 6G on NVIDIA AI Aerial Platform\n\nGTC\u2014NVIDIA today unveiled partnerships with industry leaders T-Mobile, MITRE, Cisco, ODC, a portfolio company of Cerberus Capital Management, and Booz Allen Hamilton on the research and development of AI-native wireless network hardware, software and architecture for 6G.\n\nNext-generation wireless networks must be fundamentally integrated with AI to seamlessly connect hundreds of billions of phones, sensors, cameras, robots and autonomous vehicles. AI-native wireless networks will provide enhanced services for billions of users and set new standards in spectral efficiency \u2014 the rate at which data can be transmitted over a given bandwidth. They will also offer groundbreaking performance and resource utilization while creating new revenue streams for telecommunications companies.\n\n\u201cNext-generation wireless networks will be revolutionary, and we have an unprecedented opportunity to ensure AI is woven in from the start,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cWorking with leaders in the field, we\u2019re building an AI-enhanced 6G network that achieves extreme spectral efficiency.\u201d\n\nOpen Ecosystems Drive Innovation\n\nResearch-driven breakthroughs harnessing the power of AI are necessary to maximize the performance and benefits of AI-native wireless networks. To drive innovation, NVIDIA is collaborating with telco and research leaders to develop an AI-native wireless network stack based on the NVIDIA AI Aerial platform, which provides software-defined radio access networks (RANs) on the NVIDIA accelerated computing platform.\n\nDevelopers across the globe are building AI-RAN as a precursor to AI-native 6G wireless networks. AI-RAN is a technology that brings AI and RAN workloads together on one platform and embeds AI into radio signal processing.\n\nTo deliver enhanced spectral efficiency and lower operational complexity and costs, AI will be fully embedded into the network stack\u2019s software and hosted over a unified accelerated infrastructure, capable of running both network and AI workloads. Also at the solution\u2019s core will be end-to-end security and an open architecture to foster rapid innovation.\n\nT-Mobile and NVIDIA will expand their AI-RAN Innovation Center collaboration announced last year with the goal of providing additional research-based concepts for AI-native 6G network capabilities, working alongside these new industry collaborators.\n\n\u201cThis is an exciting next step to the AI-RAN Innovation Center efforts we began last September at our Capital Markets Day in partnership with NVIDIA,\u201d Mike Sievert, CEO of T-Mobile. \u201cWorking with these additional industry leaders on research to natively integrate AI into the network as we begin the journey to 6G will enable the network performance, efficiency and scale to power the next generation of experiences that customers and businesses expect.\u201d\n\nAs the founding research partner, MITRE, a not-for-profit research and development organization, will research, prototype and contribute open, AI-driven services and applications, such as for agentic network orchestration and security, dynamic spectrum sharing and 6G-integrated sensing and communications.\n\n\u201cMITRE is working with NVIDIA to help make AI-native 6G a reality,\u201d said Mark Peters, president and CEO of MITRE. \u201cBy integrating AI into 6G in the beginning, we can solve a wide range of problems, from enhancing service delivery to unlocking required spectrum availability to fuel wireless growth. Through all of our collaborations with NVIDIA, we look forward to creating impact in 6G, AI, simulation, transportation and more.\u201d\n\nCisco plans to take a lead position in this collaboration as the provider of mobile core and network technologies and will tap into its existing service provider reach and expertise.\n\n\u201cWith 6G on the horizon, it\u2019s critical for the industry to work together to build AI-native networks for the future,\u201d said Chuck Robbins, chair and CEO of Cisco. \u201cCisco is at the forefront of developing secure infrastructure technology for AI, and we are proud to work with NVIDIA and the broader ecosystem to create an AI-enhanced network that improves performance, reliability and security for our customers.\u201d\n\nODC, a portfolio company of Cerberus Capital Management, L.P., will deliver cutting-edge layer 2 and layer 3 software for distributed and centralized units of virtual RAN as part of the AI-native radio access stack. Tapping into decades of experience in large-scale mobile systems, ODC is pioneering next-generation AI-native 5G open RAN (ORAN), surpassing existing networks and seamlessly paving the way for 6G evolution.\n\n\u201cThe mobile industry has always taken advantage of advances in other technology fields, and today, no technology is more central than AI,\u201d said Shaygan Kheradpir, chairman of the advisory board of ODC. \u201cODC is at the forefront of developing and deploying AI-native ORAN 2.0 networks, enabling service providers to on-ramp seamlessly from 5G to 6G by taking advantage of the vast AI ecosystem to redefine the future of connectivity.\u201d\n\nAs a leader in AI and cybersecurity to the federal government, Booz Allen will develop AI RAN algorithms and secure the AI-native 6G wireless platform. Its NextG lab will conduct functional, performance integration and security testing to ensure the resiliency and security of the platform against the most sophisticated adversaries. The company will lead field trials for advanced use cases such as autonomy and robotics.\n\n\u201cThe future of wireless communications starts today, and it\u2019s all about AI,\u201d said Horacio Rozanski, chairman and CEO of Booz Allen. \u201cBooz Allen has the technologies to make AI-native 6G networks a reality and revolutionize secure communications for an entirely new generation of intelligent platforms and applications.\u201d\n\nExpanded Aerial Research Portfolio\n\nThese collaborations build on NVIDIA\u2019s AI-RAN and 6G research ecosystem, supported by advancements in the NVIDIA Aerial\u2122 research portfolio for developing, training, simulating and deploying groundbreaking AI-native wireless innovations.\n\nNew additions to the NVIDIA Aerial Research portfolio, also announced today, include the Aerial Omniverse Digital Twin Service, the Aerial Commercial Test Bed on NVIDIA MGX\u2122, NVIDIA Sionna\u2122 1.0 \u2014 building on the open-source Sionna library, which has nearly 150,000 downloads since its launch in 2022 \u2014 and the Sionna Research Kit on the NVIDIA Jetson\u2122 accelerated computing platform.\n\nThe NVIDIA Aerial Research portfolio serves over 2,000 members through the NVIDIA 6G Developer Program. Industry leaders and more than 150 higher-education and research institutions from the U.S. and around the world are harnessing the platform to accelerate 6G and AI-RAN innovation \u2014 paving the way for AI-native wireless networks.\n\nLearn more by watching the NVIDIA GTC telecom special address and register for sessions from NVIDIA and industry leaders at the show, which runs through March 21.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "NVIDIA and Storage Industry Leaders Unveil New Class of Enterprise Infrastructure for the Age of AI",
            "link": "https://nvidianews.nvidia.com/news/nvidia-and-storage-industry-leaders-unveil-new-class-of-enterprise-infrastructure-for-the-age-of-ai",
            "snippet": "NVIDIA today announced the NVIDIA AI Data Platform, a customizable reference design that leading providers are using to build a new class of AI...",
            "score": 0.5731066465377808,
            "sentiment": null,
            "probability": null,
            "content": "Storage Providers Build Infrastructure Featuring AI Query Agents That Tap NVIDIA Computing, Networking and Software to Reason and Rapidly Generate Accurate Responses to Complex Queries\n\nGTC\u2014NVIDIA today announced the NVIDIA AI Data Platform , a customizable reference design that leading providers are using to build a new class of AI infrastructure for demanding AI inference workloads: enterprise storage platforms with AI query agents fueled by NVIDIA accelerated computing, networking and software.\n\nUsing the NVIDIA AI Data Platform, NVIDIA-Certified Storage providers can build infrastructure to speed AI reasoning workloads with specialized AI query agents. These agents help businesses generate insights from data in near real time, using NVIDIA AI Enterprise software \u2014 including NVIDIA NIM \u2122 microservices for the new NVIDIA Llama Nemotron models with reasoning capabilities \u2014 as well as the new NVIDIA AI-Q Blueprint .\n\nStorage providers can optimize their infrastructure to power these agents with NVIDIA Blackwell GPUs , NVIDIA BlueField \u00ae DPUs , NVIDIA Spectrum-X \u2122 networking and the NVIDIA Dynamo open-source inference library.\n\nLeading data platform and storage providers \u2014 including DDN , Dell Technologies , Hewlett Packard Enterprise , Hitachi Vantara , IBM , NetApp , Nutanix , Pure Storage , VAST Data and WEKA \u2014 are collaborating with NVIDIA to create customized AI data platforms that can harness enterprise data to reason and respond to complex queries.\n\n\u201cData is the raw material powering industries in the age of AI,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cWith the world\u2019s storage leaders, we\u2019re building a new class of enterprise infrastructure that companies need to deploy and scale agentic AI across hybrid data centers.\u201d\n\nNVIDIA AI Data Platform Adds Accelerated Computing and AI to Storage\n\nThe NVIDIA AI Data Platform brings accelerated computing and AI to the millions of businesses using enterprise storage for the data that drives their company.\n\nNVIDIA Blackwell GPUs, BlueField DPUs and Spectrum-X networking provide an accelerated engine to speed AI query agent access to data stored on enterprise systems. BlueField DPUs deliver up to 1.6x higher performance than CPU-based storage while reducing power consumption by up to 50%, providing more than 3x higher performance per watt. Spectrum-X accelerates AI storage traffic up to 48% compared with traditional Ethernet by applying adaptive routing and congestion control.\n\nAI Data Platform storage infrastructure uses the NVIDIA AI-Q Blueprint for developing agentic systems that can reason and connect to enterprise data. AI-Q taps into NVIDIA NeMo Retriever\u2122 microservices to accelerate data extraction and retrieval by up to 15x on NVIDIA GPUs.\n\nAI query agents built with the AI-Q Blueprint connect to data during inference to provide more accurate, context-aware responses. They can access large-scale data quickly and process various data types, including structured, semi-structured and unstructured data from multiple sources, including text, PDF, images and video.\n\nStorage Industry Leaders Building AI Data Platforms With NVIDIA\n\nNVIDIA-Certified Storage partners are collaborating with NVIDIA to build custom AI data platforms.\n\nDDN is architecting AI Data Platform capabilities into its DDN Infinia AI platform.\n\nis architecting AI Data Platform capabilities into its DDN Infinia AI platform. Dell is creating AI data platforms for its family of Dell PowerScale and Project Lightning solutions.\n\nis creating AI data platforms for its family of Dell PowerScale and Project Lightning solutions. Hewlett Packard Enterprise is infusing AI Data Platform capabilities into HPE Private Cloud for AI, HPE Data Fabric, HPE Alletra Storage MP and HPE GreenLake for File Storage.\n\nis infusing AI Data Platform capabilities into HPE Private Cloud for AI, HPE Data Fabric, HPE Alletra Storage MP and HPE GreenLake for File Storage. Hitachi Vantara is bringing AI Data Platform into the Hitachi IQ ecosystem, helping customers innovate with storage systems and data offerings that drive tangible AI outcomes.\n\nis bringing AI Data Platform into the Hitachi IQ ecosystem, helping customers innovate with storage systems and data offerings that drive tangible AI outcomes. IBM is integrating AI Data Platform as part of its content-aware storage capability with IBM Fusion and IBM Storage Scale technology to accelerate retrieval-augmented generation applications.\n\nis integrating AI Data Platform as part of its content-aware storage capability with IBM Fusion and IBM Storage Scale technology to accelerate retrieval-augmented generation applications. NetApp is advancing enterprise storage for agentic AI with the NetApp AIPod solution built with AI Data Platform.\n\nis advancing enterprise storage for agentic AI with the NetApp AIPod solution built with AI Data Platform. Nutanix Cloud Platform with Nutanix Unified Storage will integrate with the NVIDIA AI Data Platform and enable inferencing and agentic workflows deployed across edge, data center and public cloud.\n\nCloud Platform with Nutanix Unified Storage will integrate with the NVIDIA AI Data Platform and enable inferencing and agentic workflows deployed across edge, data center and public cloud. Pure Storage will deliver AI Data Platform capabilities with Pure Storage FlashBlade.\n\nwill deliver AI Data Platform capabilities with Pure Storage FlashBlade. VAST Data is working with AI Data Platform to curate real-time insights with VAST InsightEngine.\n\nis working with AI Data Platform to curate real-time insights with VAST InsightEngine. WEKA Data Platform software integrates with NVIDIA GPUs, DPUs and networking to optimize data access for agentic AI reasoning and insights and deliver a high-performance storage foundation that accelerates AI inference and token processing workloads.\n\nNVIDIA-Certified Storage providers are planning to offer solutions created with the NVIDIA AI Data platform starting this month.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "NVIDIA Dynamo Open-Source Library Accelerates and Scales AI Reasoning Models",
            "link": "https://nvidianews.nvidia.com/news/nvidia-dynamo-open-source-library-accelerates-and-scales-ai-reasoning-models",
            "snippet": "NVIDIA Dynamo Increases Inference Performance While Lowering Costs for Scaling Test-Time Compute; Inference Optimizations on NVIDIA Blackwell Boosts...",
            "score": 0.7827682495117188,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA Dynamo Increases Inference Performance While Lowering Costs for Scaling Test-Time Compute; Inference Optimizations on NVIDIA Blackwell Boosts Throughput by 30x on DeepSeek-R1\n\nGTC\u2014NVIDIA today unveiled NVIDIA Dynamo, an open-source inference software for accelerating and scaling AI reasoning models in AI factories at the lowest cost and with the highest efficiency.\n\nEfficiently orchestrating and coordinating AI inference requests across a large fleet of GPUs is crucial to ensuring that AI factories run at the lowest possible cost to maximize token revenue generation.\n\nAs AI reasoning goes mainstream, every AI model will generate tens of thousands of tokens used to \u201cthink\u201d with every prompt. Increasing inference performance while continually lowering the cost of inference accelerates growth and boosts revenue opportunities for service providers.\n\nNVIDIA Dynamo, the successor to NVIDIA Triton Inference Server\u2122, is new AI inference-serving software designed to maximize token revenue generation for AI factories deploying reasoning AI models. It orchestrates and accelerates inference communication across thousands of GPUs, and uses disaggregated serving to separate the processing and generation phases of large language models (LLMs) on different GPUs. This allows each phase to be optimized independently for its specific needs and ensures maximum GPU resource utilization.\n\n\u201cIndustries around the world are training AI models to think and learn in different ways, making them more sophisticated over time,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cTo enable a future of custom reasoning AI, NVIDIA Dynamo helps serve these models at scale, driving cost savings and efficiencies across AI factories.\u201d\n\nUsing the same number of GPUs, Dynamo doubles the performance and revenue of AI factories serving Llama models on today\u2019s NVIDIA Hopper\u2122 platform. When running the DeepSeek-R1 model on a large cluster of GB200 NVL72 racks, NVIDIA Dynamo\u2019s intelligent inference optimizations also boost the number of tokens generated by over 30x per GPU.\n\nTo achieve these inference performance improvements, NVIDIA Dynamo incorporates features that enable it to increase throughput and reduce costs. It can dynamically add, remove and reallocate GPUs in response to fluctuating request volumes and types, as well as pinpoint specific GPUs in large clusters that can minimize response computations and route queries. It can also offload inference data to more affordable memory and storage devices and quickly retrieve them when needed, minimizing inference costs.\n\nNVIDIA Dynamo is fully open source and supports PyTorch, SGLang, NVIDIA TensorRT\u2122-LLM and vLLM to allow enterprises, startups and researchers to develop and optimize ways to serve AI models across disaggregated inference. It will enable users to accelerate the adoption of AI inference, including at AWS, Cohere, CoreWeave, Dell, Fireworks, Google Cloud, Lambda, Meta, Microsoft Azure, Nebius, NetApp, OCI, Perplexity, Together AI and VAST.\n\nInference Supercharged\n\nNVIDIA Dynamo maps the knowledge that inference systems hold in memory from serving prior requests \u2014 known as KV cache \u2014 across potentially thousands of GPUs.\n\nIt then routes new inference requests to the GPUs that have the best knowledge match, avoiding costly recomputations and freeing up GPUs to respond to new incoming requests.\n\n\u201cTo handle hundreds of millions of requests monthly, we rely on NVIDIA GPUs and inference software to deliver the performance, reliability and scale our business and users demand,\u201d said Denis Yarats, chief technology officer of Perplexity AI. \u201cWe look forward to leveraging Dynamo, with its enhanced distributed serving capabilities, to drive even more inference-serving efficiencies and meet the compute demands of new AI reasoning models.\u201d\n\nAgentic AI\n\nAI provider Cohere is planning to power agentic AI capabilities in its Command series of models using NVIDIA Dynamo.\n\n\u201cScaling advanced AI models requires sophisticated multi-GPU scheduling, seamless coordination and low-latency communication libraries that transfer reasoning contexts seamlessly across memory and storage,\u201d said Saurabh Baji, senior vice president of engineering at Cohere. \u201cWe expect NVIDIA Dynamo will help us deliver a premier user experience to our enterprise customers.\u201d\n\nDisaggregated Serving\n\nThe NVIDIA Dynamo inference platform also supports disaggregated serving, which assigns the different computational phases of LLMs \u2014 including building an understanding of the user query and then generating the best response \u2014 to different GPUs. This approach is ideal for reasoning models like the new NVIDIA Llama Nemotron model family, which uses advanced inference techniques for improved contextual understanding and response generation. Disaggregated serving allows each phase to be fine-tuned and resourced independently, improving throughput and delivering faster responses to users.\n\nTogether AI, the AI Acceleration Cloud, is looking to integrate its proprietary Together Inference Engine with NVIDIA Dynamo to enable seamless scaling of inference workloads across GPU nodes. This also lets Together AI dynamically address traffic bottlenecks at various stages of the model pipeline.\n\n\u201cScaling reasoning models cost effectively requires new advanced inference techniques, including disaggregated serving and context-aware routing,\u201d said Ce Zhang, chief technology officer of Together AI. \u201cTogether AI provides industry-leading performance using our proprietary inference engine. The openness and modularity of NVIDIA Dynamo will allow us to seamlessly plug its components into our engine to serve more requests while optimizing resource utilization \u2014 maximizing our accelerated computing investment. We\u2019re excited to leverage the platform\u2019s breakthrough capabilities to cost-effectively bring open-source reasoning models to our users.\u201d\n\nNVIDIA Dynamo Unpacked\n\nNVIDIA Dynamo includes four key innovations that reduce inference serving costs and improve user experience:\n\nGPU Planner: A planning engine that dynamically adds and removes GPUs to adjust to fluctuating user demand, avoiding GPU over- or under-provisioning.\n\nSmart Router: An LLM-aware router that directs requests across large GPU fleets to minimize costly GPU recomputations of repeat or overlapping requests \u2014 freeing up GPUs to respond to new incoming requests.\n\nLow-Latency Communication Library: An inference-optimized library that supports state-of-the-art GPU-to-GPU communication and abstracts complexity of data exchange across heterogenous devices, accelerating data transfer.\n\nMemory Manager: An engine that intelligently offloads and reloads inference data to and from lower-cost memory and storage devices without impacting user experience.\n\nNVIDIA Dynamo will be made available in NVIDIA NIM\u2122 microservices and supported in a future release by the NVIDIA AI Enterprise software platform with production-grade security, support and stability.\n\nLearn more by watching the NVIDIA GTC keynote, reading this blog on Dynamo and registering for sessions from NVIDIA and industry leaders at the show, which runs through March 21.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-17": {
        "0": {
            "title": "GTC 2025 \u2013 Announcements and Live Updates",
            "link": "https://blogs.nvidia.com/blog/nvidia-keynote-at-gtc-2025-ai-news-live-updates/",
            "snippet": "What's next in AI is at GTC 2025. Explore the news and live updates from the show, from the keynote to the final session.",
            "score": 0.9221877455711365,
            "sentiment": null,
            "probability": null,
            "content": "All the news from NVIDIA\u2019s biggest gathering of the year, including new services and hardware, tech demos and what\u2019s next in AI.\n\nWhat\u2019s next in AI is at GTC 2025. Not only the technology, but the people and ideas that are pushing AI forward \u2014 creating new opportunities, novel solutions and whole new ways of thinking. For all of that, this is the place.\n\nHere\u2019s where to find the news, hear the discussions, see the robots and ponder the just-plain mind-blowing. From the keynote to the final session, check back for live coverage kicking off when the doors open on Monday, March 17, in San Jose, California.\n\nExplaining Tokens \u2014 the Language and Currency of AI \ud83d\udd17\n\nGTC attendees are likely to hear a lot about tokens \u2014 the language and currency of AI.\n\nTokens are units of data processed by AI models during training and inference, enabling prediction, generation and reasoning. The faster tokens can be processed, the faster AI models can learn and respond.\n\nGet up to speed on tokens, tokenization and the ways enterprises can boost revenue by lowering cost per token in our explainer article.\n\nGTC 2025: Real AI, Real Problems, Real Solutions \ud83d\udd17\n\nAI is confronting humanity\u2019s toughest challenges head on. See it unfold next week at the NVIDIA GTC conference in San Jose, California.\n\nFrom transformative healthcare sessions like \u201cRevolutionizing Cardiac MRI Analysis and Diagnosis With AI\u201d and \u201cDesigning the Future: Protein Engineering, AI and Responsible Innovation\u201d to environmental breakthroughs in \u201cAutonomous Systems and Remote Sensing for Better Earth Data,\u201d \u201cThe Role of AI and Accelerated Computing in Understanding and Mitigating Urban Climate Change\u201d and \u201cEnhancing Photovoltaic Power Prediction With High-Resolution Weather Forecasting From NVIDIA Earth-2,\u201d the impact is tangible and global.\n\nThe Future Rolls Into San Jose \ud83d\udd17\n\n\n\nAnyone who\u2019s been in downtown San Jose lately has seen it happening. The banners are up. The streets are shifting. The whole city is getting a fresh coat of NVIDIA green.\n\nFrom March 17-21, San Jose will become a crossroads for the thinkers, tinkerers and true enthusiasts of AI, robotics and accelerated computing. The conversations will be sharp, fast-moving and sometimes improbable \u2014 but that\u2019s the point.\n\nAt the center of it all? NVIDIA founder and CEO Jensen Huang\u2019s keynote, offering a glimpse into the future. It\u2019ll take place at the SAP Center on Tuesday, March 18, at 10 a.m. PT. Expect big ideas, a few surprises, some roars of laughter and the occasional moment that leaves the room silent.\n\nBut GTC isn\u2019t just what happens on stage. It\u2019s a conference that refuses to stay inside its walls. It spills out into sessions at McEnery Convention Center, hands-on demos at the Tech Interactive Museum, late-night conversations at the Plaza de C\u00e9sar Ch\u00e1vez night market and more. San Jose isn\u2019t just hosting GTC. It\u2019s becoming it.\n\nThe speakers are a mix of visionaries and builders \u2014 the kind of people who make you rethink what\u2019s possible:\n\n\ud83e\udde0 Yann LeCun \u2013 chief AI scientist at Meta, professor, New York University\n\n\ud83c\udfc6 Frances Arnold \u2013 Nobel Laureate, Caltech\n\n\ud83d\ude97 RJ Scaringe \u2013 founder and CEO of Rivian\n\n\ud83e\udd16 Pieter Abbeel \u2013 robotics pioneer, UC Berkeley\n\n\ud83c\udf0d Arthur Mensch \u2013 CEO of Mistral AI\n\n\ud83c\udf2e Joe Park \u2013 chief digital and technology officer of Yum! Brands\n\n\u265f\ufe0f Noam Brown \u2013 research scientist at OpenAI\n\nSome are pushing the limits of AI itself; others are weaving it into the world around us.\n\n\ud83d\udce2 Want in? Register now.\n\nCheck back here for what to watch, read and play \u2014 and what it all means. Tune in to all the big moments, the small surprises and the ideas that\u2019ll stick for years to come.\n\nSee you in San Jose. #GTC25",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Stock Falls Ahead Of GTC. Here\u2019s Why.",
            "link": "https://www.investors.com/news/technology/nvidia-stock-falls-ahead-of-gtc-2025/",
            "snippet": "Nvidia stock dipped Monday ahead of the company's GTC conference. Investors are focused on trade restrictions and other issues.",
            "score": 0.7410221695899963,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia stock slides ahead of GTC as investors look to new Blackwell Ultra",
            "link": "https://finance.yahoo.com/news/nvidia-stock-slides-ahead-of-gtc-as-investors-look-to-new-blackwell-ultra-143639940.html",
            "snippet": "Nvidia stock fell Monday ahead of the company's annual GTC conference on March 18.",
            "score": 0.943533718585968,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock slid 1.8% Monday ahead of the AI chipmaker's annual GTC conference on March 18, where CEO Jensen Huang is expected to debut the company's upcoming AI chip, Blackwell Ultra, and a new AI superchip, Vera Rubin.\n\nMonday's decline follows an 8% climb last week. The chipmaker's shares rose more than 5% Friday alone as the stock began to recover from a post-earnings rout in which Nvidia led \"Magnificent 7\" stocks lower amid a tech-driven market downturn.\n\nSeveral of Nvidia's Mag 7 peers \u2014 Amazon (AMZN), Meta (META), Google (GOOG), and Tesla (TSLA) \u2014 also fell Monday.\n\nHuang teased that he would unveil more about Blackwell Ultra and Vera Rubin at GTC during the company's post-earnings call on Feb. 26.\n\n\"Come to GTC, and I'll talk to you about Blackwell Ultra, Vera Rubin, and then show you what's ... after that,\" Huang told analysts.\n\nRead more: How does Nvidia make money?\n\nNvidia has officially scaled production of its current-generation Blackwell AI servers in \"the fastest product ramp\" in its history after they were delayed a quarter amid reports of overheating and glitches. Its Blackwell products generated $11 billion in revenue for Nvidia during the company's fourth quarter. Huang said overcoming the challenges of scaling the current-generation Blackwell systems will make it easier for the company to ramp up production of Blackwell Ultra.\n\n\"This time between Blackwell and Blackwell Ultra, the system architecture is exactly the same. It's a lot harder going from Hopper to Blackwell because ... the chassis, the architecture of the system, the hardware, the power delivery, all of that had to change. This was quite a challenging transition,\" he said in the post-earnings call. Hopper is the GPU Nvidia launched prior to Blackwell.\n\nNvidia CEO Jensen Huang before a baseball game between the San Francisco Giants and the Arizona Diamondbacks in San Francisco on Sept. 3, 2024. (AP Photo/Jeff Chiu) \u00b7 ASSOCIATED PRESS\n\n\"But the next transition will slot right in. Blackwell Ultra will slot right in. We've also already revealed and been working very closely with all of our partners...\u201d\n\nHuang confirmed in the call that Nvidia is slated to launch Blackwell Ultra in the second half of this year.\n\nOther chip stocks, including Samsung (005930.KS), Dell (DELL), Micron (MU), AMD (AMD), and Arm (ARM), rose Monday ahead of GTC.\n\nIn addition to Blackwell Ultra and Rubin, Nvidia is also expected to discuss the company's simulation technologies and more. During last year's event, Nvidia highlighted its latest software efforts around humanoid robots.\n\n\"We ... expect Mr. Huang to update thoughts on evolving AI workloads and provide incremental commentary on emerging areas of AI development such as Physical AI and Robotics,\" wrote Stifel analyst Ruben Roy in a note on Friday.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia's stock slips ahead of Jensen Huang's GTC keynote",
            "link": "https://qz.com/nvidia-stock-fall-gtc-blackwell-rubin-quantum-ai-chips-1851770522",
            "snippet": "Nvidia's (NVDA-1.27% ) stock was down by 1.76% at the market close on Monday ahead of chief executive Jensen Huang's keynote at the GPU Technology...",
            "score": 0.9590484499931335,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia\u2019s (NVDA-1.27% ) stock was down by 1.76% at the market close on Monday ahead of chief executive Jensen Huang\u2019s keynote at the GPU Technology Conference.\n\nDraftKings CEO says he's not making decisions based on their stock price, but looks at it every day CC Share Subtitles Off\n\nEnglish view video DraftKings CEO says he's not making decisions based on their stock price, but looks at it every day\n\nDraftKings CEO says he's not making decisions based on their stock price, but looks at it every day CC Share Subtitles Off\n\nEnglish DraftKings CEO says he's not making decisions based on their stock price, but looks at it every day\n\nThe chipmaker\u2019s shares closed at $119.53 each and are down 11% so far this year. During after-hours trading, the stock was slightly down by around 0.1%.\n\nAdvertisement\n\nAll eyes will be on Huang\u2019s keynote on Tuesday at the chipmaker\u2019s annual developer conference, also known as the GTC, where he is expected to unveil its next artificial intelligence chips.\n\nAdvertisement\n\nIn February, Huang said he will share more about the company\u2019s Blackwell Ultra AI chip, next-generation AI platform Vera Rubin, and other product plans at the conference. Nvidia has \u201csome really exciting things to share\u201d at the GTC about enterprise and agentic AI, reasoning models, and robotics, Huang said during the company\u2019s fiscal fourth quarter earnings call.\n\nAdvertisement\n\nLast week, analysts at Bank of America (BAC+1.89% ) said in a note that they \u201cexpect Nvidia to present attractive albeit well-expected updates on Blackwell Ultra\u201d at the GTC. The analysts added that they anticipate Nvidia will focus on inferencing for reasoning models, which major firms such as OpenAI and Google are racing to develop.\n\nJefferies (JEF+2.50% ) analysts said in a note on Monday that they expect \u201cthe noise around CPO [co-packaged optics] to resurface over next few weeks\u201d going into the GTC and the optical conference. Nvidia will likely discuss how the optical technology will be used in AI data centers, the analysts said.\n\nAdvertisement\n\nMeanwhile, some quantum computing stocks climbed on Monday ahead of the GTC\u2019s first \u201cQuantum Day\u201d that will be held on Thursday.\n\nQuantum Computing (QUBT+13.61% ) stock closed up by 13.12%, while D-Wave was up by 10.15% at the market close.\n\nAdvertisement\n\nExecutives from quantum companies, including D-Wave, are expected to join Huang to discuss \u201cwhat businesses should expect from quantum computing in the coming decades \u2014 mapping the path toward useful quantum applications,\u201d Nvidia said.\n\nShares of IonQ (IONQ+0.28% ) and Rigetti Computing (RGTI-1.33% ) , both of which are also expected to join Huang at the GTC, were down by 0.28% and 0.53% respectively at the close.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia's AI Event Kicks Off Tuesday With Jensen Huang Keynote. Here Are 5 Things to Expect.",
            "link": "https://www.barrons.com/livecoverage/nvidia-gtc-2025-ai-developers-conference/card/nvidia-s-ai-event-to-kick-off-with-jensen-huang-keynote-here-are-5-things-to-expect--ORoeayiuJek38emEO0gC",
            "snippet": "The biggest artificial intelligence event of the year is here: Nvidia's GTC 2025. The conference brings together thousands of engineers, scientists,...",
            "score": 0.9171791076660156,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Quantum computing, AI stocks rise as Nvidia kicks off annual conference",
            "link": "https://www.reuters.com/technology/quantum-computing-ai-stocks-rise-nvidia-kicks-off-annual-conference-2025-03-17/",
            "snippet": "Shares of quantum computing and artificial intelligence companies rose on Monday, as investors hoped that Nvidia would blow some life back into the...",
            "score": 0.5916532874107361,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Markets are hoping Nvidia's big AI event this week will be a badly needed positive catalyst for the stock",
            "link": "https://www.businessinsider.com/nvidia-stock-price-prediction-nvda-gtc-conference-blackwell-gpu-chips-2025-3",
            "snippet": "With the shares down 13% in 2025, investors are looking for fresh updates from the GTC conference to help restart the rally for Nvidia.",
            "score": 0.9216347336769104,
            "sentiment": null,
            "probability": null,
            "content": "Investors will have their eyes on Nvidia's big GTC conference this week.\n\nThe big AI event could give a badly needed boost to Nvidia stock after a 20% sell-off.\n\nWall Street is looking for updates on the rollout of the next-gen Blackwell GPU and other endeavors.\n\nNvidia's big artificial intelligence event this week could provide some much-needed fuel for the chipmaker's stock.\n\nThe Jensen Huang-led firm kicked off its annual GPU Technology Conference on Monday, its annual AI chips conference.\n\nThe hotly anticipated event, which takes place in San Jose, will include a keynote speech from Huang on Tuesday, during which investors are expecting key updates on the firm's chip sales and the company's other projects.\n\nFor Wall Street, the hope is that the event brings a fresh catalyst for Nvidia shares, which have been beaten down in recent months after soaring in 2024. Shares were trading around $119 on Monday, down 20% from their high of $149.42 on January 6.\n\nLouis Navellier, the chief investment officer of Navellier & Associates, told Business Insider he would be disappointed if Nvidia shares didn't jump at least 5% by the end of the week.\n\nHe's expecting Nvidia to give major updates during the conference on its advancements in quantum computing, a big \"next step\" as the firm looks to speed up its AI processing.\n\n\"I expect that the Nvidia AI Developers Conference will just reaffirm NVDA's dominance on regenerative AI,\" Naveillier said to BI in an email. \"On the last day of the conference, quantum computing will be the focus and Nvidia is expected to announce business with one of the winners of its recent quantum computing development contest.\"\n\nBank of America is also feeling positive ahead of the GTC conference. Analysts lifted their price target for Nvidia stock to $200, up from $190 a share.\n\nThe bank said it expects key updates on Blackwell, as well as updates on Nvidia's robotics and quantum computing projects. Investors will be particularly focused on updates related to the outlook for GM, given the carmaker's partnership with Nvidia. Also in focus is demand for Nvidia's chips in China, competition with alternative chipmakers, and the company's expected growth for the 2026 year, analysts said.\n\nThe BofA team said they reiterate their \"buy\" rating for the stock and see shares rally even as volatility related to China creates near-term headwinds.\n\nMelius Research also reiterated its \"buy\" rating ahead of the GTC conference.\n\nAt the event, Nvidia is expected to show how major cloud companies and enterprise businesses are adopting its tech. The company could also potentially unveil its Blackwell Ultra, the firm said.\n\nThe conference will likely bolster confidence that Nvidia can grow 10% quarter-over-quarter, Ben Reitzes, the head of technology research at Melius, said.\n\n\"Nvidia's forward PE is now 41% lower than it was on the day ChatGPT was launched on Nov. 30th, 2022. Of course, in FY25 its net income was up 788% vs. FY23 (ended 1/23) and revenues are up 384% since then too. Just doesn't feel right, does it?\" Reitzes said in a note last week, noting that Apple stock experienced a similar trajectory in 2008 before the stock rebounded.\n\n\"If Nvidia duplicates its own version of this industry stewardship, we could look back at this period of uncertainty and have a good chuckle,\" he added.\n\nStacy Rasgon, a senior analyst at Bernstein Research, said he is bullish on the stock heading into the GTC conference. He's expecting the company to give key updates on Blackwell, as well as Rubin, the chip following Blackwell in Nvidia's product cycle.\n\nUpdates on Blackwell could be particularly impactful, Rasgon suggested, adding that he believed the chip could represent the \"biggest product cycle\" in the history of the company.\n\nOver the last 10 years, when investors bought Nvidia at similar valuation levels, gains have always been positive over the next 12-month period, he added.\n\n\"I really like it at these valuations,\" Rasgon told CNBC last week.\n\nInvestors have been anxious about the strength of the AI trade in recent months since DeepSeek, a Chinese startup, appeared to create an AI model to to rival US peers despite claiming to use older and cheaper GPUs to train it.\n\nMega-cap tech shares have also been hit the hardest by the recent bout of tariff-induced volatility that sent the S&P 500 into correction last week.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Nvidia GTC 2025: What to expect from this year\u2019s show",
            "link": "https://techcrunch.com/2025/03/17/nvidia-gtc-2025-what-to-expect-from-this-years-show/",
            "snippet": "GTC, Nvidia's biggest conference of the year, begins Monday and runs till Friday in San Jose, with the biggest announcements probably coming Tuesday.",
            "score": 0.9166229963302612,
            "sentiment": null,
            "probability": null,
            "content": "GTC, Nvidia\u2019s biggest conference of the year, begins Monday and runs till Friday in San Jose, with the biggest announcements probably coming Tuesday. TechCrunch will be on the ground covering the news as it happens \u2014 and we\u2019re expecting a healthy dose of announcements. And we\u2019re making it easy for you to follow along.\n\nCEO Jensen Huang will give a keynote address at the SAP Center on Tuesday at 10 a.m. PT, focusing on \u2014 what else? \u2014 AI and accelerating computing technologies, according to Nvidia. The company is also teasing reveals related to robotics, sovereign AI, AI agents, and automotive \u2014 plus 1,000 sessions with 2,000 speakers and close to 400 exhibitors.\n\nHere\u2019s how to watch the Nvidia GTC 2025 keynote online, along with many other sessions, talks, and panels.\n\nSo what do we expect to see at GTC? Well, Nvidia typically reserves a big chunk of the conference for GPU-related debuts. A new, upgraded iteration of the company\u2019s Blackwell chip lineup seems likely.\n\nDuring Nvidia\u2019s most recent earnings call, Huang confirmed that the upcoming Blackwell B300 series, codenamed Blackwell Ultra, is slated for release in the second half of this year. In addition to higher computing performance, Blackwell Ultra cards pack more memory (288GB), an attractive feature for customers looking to run and train memory-hungry AI models.\n\nRubin, Nvidia\u2019s next-gen GPU series, is almost certain to get a mention at GTC alongside Blackwell Ultra. Due out in 2026, Rubin promises to deliver what Huang has described as a \u201cbig, big, huge step up\u201d in computing power.\n\nHuang said during the aforementioned Nvidia earnings call that he\u2019d talk about post-Rubin products at GTC, as well. That could be Rubin Ultra GPUs, or perhaps the GPU architecture that\u2019ll come after the Rubin family. (The chips are named after Vera Rubin, the astronomer who discovered dark matter.)\n\nBeyond GPUs, Nvidia may illuminate its approach to recent quantum computing advancements. The company has scheduled a \u201cquantum day\u201d for GTC, during which it\u2019ll host execs from prominent companies in the space to \u201c[map] the path toward useful quantum applications.\u201d\n\nOne thing\u2019s for sure: Nvidia could use a win.\n\nEarly Blackwell cards reportedly suffered from severe overheating issues, causing customers to cut their orders. U.S. export controls and fears of tariffs have massively depressed Nvidia\u2019s stock price in recent months. At the same time, the success of Chinese AI lab DeepSeek, which developed efficient models competitive with models from leading AI labs, has prompted investors to worry about the demand for powerful GPUs like Blackwell.\n\nHuang has asserted that DeepSeek\u2019s rise to prominence will in fact be a net positive for Nvidia because it\u2019ll accelerate the broader adoption of AI technology. He has also pointed to the growth of power-hungry so-called \u201creasoning\u201d models like OpenAI\u2019s o1 as Nvidia\u2019s next mountain to climb.\n\nTo be clear, Nvidia isn\u2019t exactly hurting. The company reported a record-breaking quarter in February, notching $39.3 billion in revenue and projecting $43 billion in revenue for the subsequent quarter. While rivals such as AMD have begun to encroach on the company\u2019s territory, Nvidia still commands an estimated 82% of the GPU market. But Huang is reportedly feeling impatient to see AI applications that matter beyond the tech industry.\n\nThis story originally published March 11.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "What to expect at NVIDIA's annual GTC conference with CEO Jensen Huang",
            "link": "https://www.engadget.com/computing/what-to-expect-at-nvidias-annual-gtc-conference-with-ceo-jensen-huang-183038411.html",
            "snippet": "NVIDIA's GPU Technology Conference, also known as GTC, is happening from March 17-21.",
            "score": 0.9354913234710693,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's GPU Technology Conference, also known as GTC, is coming up. The event is happening March 17-21 in San Jose, California, but you can also follow along with all the big developments here at Engadget. We'll have a liveblog for the keynote with CEO Jensen Huang on March 18 at 1PM ET (or 10AM PT), which is when most of the big news will drop. His speech will also be livestreamed for free, so you can watch it on NVIDIA's website too.\n\nAnd for those of you who want it saved to your YouTube watch history, here's the company's livestream on YouTube as well.\n\nIf you prefer to catch up without video, our liveblog will be the best place for primarily text and image updates, peppered with contextual information and expert analysis from our senior writer Devindra Hardawar, so make sure you come back tomorrow for that.\n\nADVERTISEMENT Advertisement\n\nFrom a quick glance at the session catalog, we can make a few educated guesses as to what might be covered. Topics range from quantum computing and \"physical AI\" to robotics, healthcare and Agentic AI. Huang himself is hosting the quantum computing chat with a bevy of participants from companies like Microsoft and Amazon, too.\n\nLet's not forget, either, that this is ultimately an event that NVIDIA is billing as a \"premier\" AI conference for developers, which means most of the week will revolve around workshops, training labs and networking opportunities (that the schedule calls \"Dinner with Strangers\"). As the in-person audience is likely to be filled with developers, it's possible, as speculated by our senior reporter Devindra Hardawar, that Huang could get down and nerdy and get into more specifics than he did at the CES keynote this year.\n\nIt's a safe bet that you'll hear a whole lot about artificial intelligence during the week, but with all the changes in the computing landscape over the past 12 months, the stakes might be higher for the company to make serious waves at this conference. Plus, with NVIDIA recently becoming a Wall Street darling, more eyes are on the company than ever.\n\nWhat to expect at NVIDIA GTC 2025\n\nNVIDIA has been going all-in on AI for years now, and that makes it a regular highlight for GTC programming. Last year saw the company unveiling its Blackwell line of GPUs for faster and less demanding computations. We're guessing that Huang will introduce another iteration of Blackwell GPUs with even better specs this time around. NVIDIA is also likely to share updates on its projects in automotive, robotics and quantum computing.\n\nADVERTISEMENT Advertisement\n\nBut the company is in a very different situation in early 2025 than it was going into last year's conference. NVIDIA is no longer sitting quite so comfortably at the top of the heap. The emergence of DeepSeek's reasoning model caused a plunge for tech stocks, including NVIDIA's, earlier this year. There have been lots of issues related to its latest RTX product launches and splashy tech for AI-generated NPCs in gaming are, unsurprisingly, pretty soulless.\n\nBasically, NVIDIA needs a win. This would be the time for Huang to drop something surprising and exciting. Hopefully he delivers.\n\nUpdate, March 12 2025, 1:18PM ET: This story has been updated to add information gleaned from NVIDIA's session catalog to the intro.\n\nUpdate, March 13 2025, 2:10PM ET: This story has been updated to add a link to the NVIDIA livestream site, as well as more information about GTC as a developer conference.\n\nUpdate, March 17 2025, 1:08PM ET: This story has been updated to embed a YouTube video of NVIDIA's livestream, plus information about Engadget's own liveblog.\n\nIf you buy something through a link in this article, we may earn commission.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Hyperfine to collaborate with Nvidia",
            "link": "https://www.auntminnie.com/clinical-news/mri/article/15740106/hyperfine-to-collaborate-with-nvidia",
            "snippet": "Portable MRI developer Hyperfine and graphics processing unit (GPU) technology developer Nvidia have inked a partnership to develop AI-driven neuroimaging...",
            "score": 0.607378363609314,
            "sentiment": null,
            "probability": null,
            "content": "Portable MRI developer Hyperfine and graphics processing unit (GPU) technology developer Nvidia have inked a partnership to develop AI-driven neuroimaging for portable MRI.\n\nHyperfine\n\nThe collaboration will allow Hyperfine to integrate Nvidia's AI expertise and computing capabilities into its Swoop system, the company said. It will focus on providing AI-powered image reconstruction and real-time clinical decision support into the portable MRI workflow; Nvidia's Dali and Monai training tools will be incorporated into the device, according to Hyperfine.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-16": {
        "0": {
            "title": "Nvidia's giant tech conference will show off AI's future",
            "link": "https://www.thestreet.com/technology/nvidias-giant-tech-conference-will-show-off-ais-future",
            "snippet": "The company's annual GTC conference will draw thousands to San Jose for a week of looking into where the technology is headed.",
            "score": 0.5976629257202148,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "Nvidia\u2019s Next Act Needs To Be Even Bigger",
            "link": "https://www.wsj.com/tech/ai/nvidia-growth-ai-gtc-conference-f9f7881f",
            "snippet": "Chips to be unveiled at developer conference expected to drive growth next year and beyond, but will depend on Big Tech's spending spigot staying on full...",
            "score": 0.9217104911804199,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Stock's Been Growing for Years. Just Look At Its 100,000% Return",
            "link": "https://www.kiplinger.com/investing/nvidia-stock-a-grower-100-000-percent-return",
            "snippet": "Nvidia shareholders have had to stomach intense volatility over the years, but they have come out on top thanks to the AI chipmaker's bellwether status.",
            "score": 0.7487561106681824,
            "sentiment": null,
            "probability": null,
            "content": "Editor's note: This is part 10 of a 13-part series about companies whose shares have amassed 100,000% returns for investors and the path taken to generate such impressive gains over the long term. See below for links to the other stocks in this series.\n\nThe AI boom occurring right now may be more transformational than the internet boom that occurred at the turn of this century.\n\nWhile the emergence of the internet was truly transformational, the age was ushered in with a bubble that was catastrophic when it burst. I suspect that something similar may occur with AI, but Nvidia (NVDA) investors will not be among the collateral damage.\n\nSubscribe to Kiplinger\u2019s Personal Finance Be a smarter, better informed investor. Save up to 74% Sign up for Kiplinger\u2019s Free E-Newsletters Profit and prosper with the best of expert advice on investing, taxes, retirement, personal finance and more - straight to your e-mail. Profit and prosper with the best of expert advice - straight to your e-mail. Sign up\n\nThere is a caveat here. Long-term owners of Nvidia have already experienced extreme volatility in their holdings. Between November of 2021 and September of 2022, shares of Nvidia fell by nearly two-thirds.\n\nAnd it wasn't the first time Nvidia shares took a perilous dip. Between August 2018 and the end of that year, Nvidia shares fell by 50%. This could happen again. Markets get twitchy, and when they do anything is possible.\n\nBut as we go to press, NVDA is our largest holding, and I expect it will remain so for the foreseeable future. This is because Nvidia has a lock on the semiconductor chips that drive AI and there are no signs that its grip will loosen.\n\nIf anything, as the summer of 2024 came to a close, NVDA's grip strengthened with the release of its Blackwell GB200 graphic processing units. Notably, Blackwell is the third generation of AI chips from Nvidia, following the Hopper and Lovelace releases.\n\nThere are other chipmakers of course, but for now, they have been relegated to lower-grade chip sets for lower-grade AI applications.\n\nEvidence that the competitors may never close the gap on Nvidia surfaces in the amounts that Intel (INTC) and Advanced Micro Devices (AMD) are now spending on research and development to catch up.\n\nAdvanced Micro Devices' research and development expenses were 25.9% of revenues in 2023. At Intel, it's even higher with total research and development expenses at 29.6% of 2023 revenues.\n\nBut it's not as if NVDA is sitting still while others play catch up. Even with its \"lighter\" R&D expense of 14.2% of revenues for fiscal 2023, Nvidia expects to upgrade its flagship AI chipset every year until the end of the decade.\n\nThe potential of being an undisputed leader in an industry that will indisputably mushroom is one reason Nvidia stock has increased by over 1,000% from the fall of 2022 midway into 2024.\n\nBut that's potential. Over time, the growth in AI needs to materialize and Nvidia must replace the return predicated on potential with a solid fundamental performance.\n\nBut this is another part of the story that makes NVDA compelling. This is a company that has already and unequivocally demonstrated that it knows how to make money.\n\nBefore AI consumed the potential of the company, NVDA was focused on advanced semiconductor chips for a number of end markets including automotive, robotics and data centers.\n\nNvidia was also dominant in the graphics market for gaming and industrial applications.\n\nAnd for the years before AI's dominance over the future of the company, say 2022, Nvidia earned $9.7 billion on revenues of $26.9 billion. Further, in just two years, Nvidia grew total revenues to $60.9 billion and earned $33.8 billion, for an unheard-of net margin of close to 50%.\n\nYes, NVDA has always been a grower. From earnings per share of just 1 cent in 2005, the company reported earnings per share of $1.21 for its fiscal year 2023. That represents an average annual growth of 27.1% per year.\n\nAll this for a company that is just 32 years old. If it can generate a 272,000% return in that short span, imagine what it might do over the next 32 years.\n\nNote: This content first appeared in Louis Navellier's latest book, The Sacred Truths of Investing: Finding Growth Stocks that Will Make You Rich , which was published by John Wiley & Sons, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nice Bounce, Now Wait For Bullish Signal; Nvidia, Fed Due",
            "link": "https://www.investors.com/market-trend/stock-market-today/dow-jones-futures-market-rallies-trump-tariff-nvidia-gtc-ai-stock/",
            "snippet": "Dow Jones futures: Friday's bounce only pared big weekly stock market losses from Trump tariffs. Will Nvidia GTC revive the AI stock boom?",
            "score": 0.7640206813812256,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia is about to drop new AI chips. Here's what to expect",
            "link": "https://qz.com/nvidia-gtc-2025-blackwell-ultra-rubin-ai-chips-quantum-1851770112",
            "snippet": "Nvidia CEO Jensen Huang will deliver the keynote address at the AI chipmaker's GTC developers conference.",
            "score": 0.5429453253746033,
            "sentiment": null,
            "probability": null,
            "content": "In This Story NVDA BAC GOOGL RGTI JEF\n\nAll eyes will be on Nvidia\u2019s (NVDA) GPU Technology Conference this week, where the company is expected to unveil its next artificial intelligence chips.\n\nDraftKings CEO on March Madness, AI, and the future of sports gambling CC Share Subtitles Off\n\nEnglish view video DraftKings CEO on March Madness, AI, and the future of sports gambling\n\nDraftKings CEO on March Madness, AI, and the future of sports gambling CC Share Subtitles Off\n\nEnglish DraftKings CEO on March Madness, AI, and the future of sports gambling\n\nNvidia chief executive Jensen Huang said he will share more about the upcoming Blackwell Ultra AI chip, Vera Rubin platform, and plans for following products at the annual conference, known as the GTC, during the company\u2019s fiscal fourth quarter earnings call.\n\nAdvertisement\n\nOn the earnings call, Huang said Nvidia has \u201csome really exciting things to share\u201d at the GTC about enterprise and agentic AI, reasoning models, and robotics.\n\nAdvertisement\n\nThe chipmaker introduced its highly anticipated Blackwell AI platform at last year\u2019s GTC, which has \u201csuccessfully ramped up\u201d large-scale production, and made \u201cbillions of dollars in sales in its first quarter,\u201d according to Huang.\n\nAdvertisement\n\nAnalysts at Bank of America (BAC) said in a note on Wednesday that they \u201cexpect Nvidia to present attractive albeit well-expected updates on Blackwell Ultra,\u201d with a focus on inferencing for reasoning models, which major firms such as OpenAI and Google are racing to develop.\n\nThe analysts also anticipate the chipmaker to share more information on its next-generation networking technology, and long-term opportunities in autonomous cars, physical AI such as robotics, and quantum computing.\n\nAdvertisement\n\nIn January, Nvidia announced that it would host its first Quantum Day at the GTC, and have executives from D-Wave and Rigetti (RGTI) \u201cdiscuss where quantum computing is headed.\u201d The company added that it will unveil quantum computing \u201cadvances shortening the timeline to useful applications.\u201d\n\nThe same month, quantum computing stocks tanked after Huang expressed doubts over the technology\u2019s near-term potential during the chipmaker\u2019s financial analyst day at the Consumer Electronics Show, saying useful quantum computers are likely decades away.\n\nAdvertisement\n\nJefferies (JEF) analysts said in a Thursday note that they expect Nvidia\u2019s product announcements at the GTC \u201cto be another positive catalyst\u201d for the company \u201cas investors move past supply-chain noise and can once again get excited about the technology roadmap.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia: Simply Irresistible At This Price (NASDAQ:NVDA)",
            "link": "https://seekingalpha.com/article/4767929-nvidia-simply-irresistible-at-this-price",
            "snippet": "Nvidia's Blackwell chips are set to boost earnings. Read why NVDA stock is an attractive investment opportunity amid generative AI market expansion.",
            "score": 0.9434841275215149,
            "sentiment": null,
            "probability": null,
            "content": "A financial researcher and avid investor with a keen eye for innovation and disruption, as well as growth buy-outs and value stocks. Keeping an eye on the pace of high tech and early growth companies, I write about current events and the biggest news surrounding the industry, and strive to provide readers with ample research and investment opportunities.\n\nAnalyst\u2019s Disclosure: I/we have a beneficial long position in the shares of NVDA either through stock ownership, options, or other derivatives. I wrote this article myself, and it expresses my own opinions. I am not receiving compensation for it (other than from Seeking Alpha). I have no business relationship with any company whose stock is mentioned in this article.\n\nSeeking Alpha's Disclosure: Past performance is no guarantee of future results. No recommendation or advice is being given as to whether any investment is suitable for a particular investor. Any views or opinions expressed above may not reflect those of Seeking Alpha as a whole. Seeking Alpha is not a licensed securities dealer, broker or US investment adviser or investment bank. Our analysts are third party authors that include both professional investors and individual investors who may not be licensed or certified by any institute or regulatory body.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Think It's Too Late to Buy Nvidia? Here's the Biggest Reason Why There's Still Time.",
            "link": "https://www.fool.com/investing/2025/03/16/think-its-too-late-to-buy-nvdia-heres-the-big/",
            "snippet": "You could be forgiven if you thought that Nvidia (NVDA 5.27%) was a young company. It's rare for the stock of a mature business to suddenly take off after...",
            "score": 0.8976672291755676,
            "sentiment": null,
            "probability": null,
            "content": "You could be forgiven if you thought that Nvidia (NVDA 5.27%) was a young company. It's rare for the stock of a mature business to suddenly take off after it's been on the market for a few years. But Nvidia actually went public in 1999. It had been a solid gainer for many years before generative artificial intelligence (AI) set it off higher, but it was hardly a household name.\n\nOver the past two years, though, it's gained 374% as generative AI becomes part of regular parlance. Armchair analysts have been dissecting Nvidia's continued potential. Is the ride over? Is it still worth buying today? No, and yes, in that order. Here's why.\n\nInestimable potential\n\nThere are varied estimates for how big the AI opportunity is going to be, but they have one thing in common: It's going to be BIG. McKinsey estimates the long-term opportunity at $4.4 trillion. According to a Harvard Business Review study in collaboration with Amazon Web Services (AWS), 81% of respondents say Generative AI is going to transform their industry, and 83% feel that if they don't use it, they're going to fall behind.\n\nNvidia makes the hardware that powers generative AI. There are competitors, but Nvidia is the clear leader, and partners like Amazon and Alphabet rely on it for their competitive AI programs.\n\nThe market was in suspense about Nvidia's latest earnings report, concerned that with new competition and already high sales, growth might decelerate. The company put any idea of a slowdown to rest with a blowout earnings report demonstrating a 78% year-over-year increase in revenue and an 8% increase in earnings per share (EPS).\n\nWhen the DeepSeek news sent Nvidia stock tumbling a few months ago, Nvidia CEO Jensen Huang smartly observed that progress in the space was a win for all of the players. As generative AI takes a foothold in almost every industry, Nvidia's products become even more important, and its market becomes that much larger.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "\u2018History Repeating Itself,\u2019 Says Investor About Nvidia Stock",
            "link": "https://www.tipranks.com/news/history-repeating-itself-says-investor-about-nvidia-stock",
            "snippet": "Trade tensions have sowed havoc throughout the market, with impacts felt far and wide. This has certainly been the case when it comes to the semiconductor...",
            "score": 0.8980534672737122,
            "sentiment": null,
            "probability": null,
            "content": "Trade tensions have sowed havoc throughout the market, with impacts felt far and wide. This has certainly been the case when it comes to the semiconductor industry, and Nvidia Corporation (NASDAQ:NVDA) has been among those buffeted by the darkening clouds of international commerce.\n\nThe company shares are down some 9% since the start of the year, reflecting investor worries over tariffs, China export restrictions, and a DeepSeek-inspired slowdown in capex investments by hyperscalers, among other concerns.\n\nBelieving that the tariffs and cuts in federal spending are going to drive the economy into an economic contraction, investor Paul Franke cautions that this \u201cbubble stock\u201d is one to avoid.\n\n\u201cIf we are sliding into recession, and we experience a deep or prolonged one, Nvidia\u2019s future price direction could be straight down, just like late 2007\u2019s setup,\u201d explains the 5-star investor.\n\nIn other words, Franke thinks that Nvidia is about to go down a path it previously tread when the Great Recession hit almost two decades ago. The investor notes that recessions are particularly rough on cyclical stocks like semiconductors.\n\nFranke further details that analysts are not factoring in a possible recession when forecasting the company\u2019s sales and EPS growth. Though a 1-year forward Price-to-Earnings multiple south of 20x might appear attractive, this is likely a \u201cbest-case scenario\u201d if a recession sparks cancelled orders and decreased capex spending.\n\n\u201cIf expected growth actually reverses into sliding sales and profitability from a recession into 2026, Nvidia\u2019s valuation is still way, way too high,\u201d adds Franke.\n\nThe investor also points to increasing competition for Nvidia\u2019s products, as other semiconductor firms and hyperscalers are looking to develop their own chips. It follows that this would naturally decrease demand for NVDA GPUs.\n\nAnother worry for the investor? The potential for quantum chips to make GPUs obsolete, in which case Nvidia\u2019s sales and income would take a massive hit.\n\n\u201cA monster collapse in both could be the long-term future of the business. Don\u2019t say such is impossible. Technology changes happen rapidly,\u201d cautions Franke.\n\nThe investor is therefore rating NVDA a Strong Sell. (To watch Franke\u2019s track record, click here)\n\nWall Street, on the other hand, holds a completely opposite viewpoint. With 39 Buy and 3 Hold ratings, NVDA enjoys a Strong Buy consensus rating. Its 12-month average price target of $177.23 would yield gains of almost 50% in the coming year. (See NVDA stock forecast)\n\nTo find good ideas for AI stocks trading at attractive valuations, visit TipRanks\u2019 Best Stocks to Buy, a tool that unites all of TipRanks\u2019 equity insights.\n\nDisclaimer: The opinions expressed in this article are solely those of the featured investor. The content is intended to be used for informational purposes only. It is very important to do your own analysis before making any investment.\n\nQuestions or Comments about the article? Write to editor@tipranks.com",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "ZEDEDA deepens NVIDIA integration to streamline enterprise edge AI deployment",
            "link": "https://www.edgeir.com/zededa-deepens-nvidia-integration-to-streamline-enterprise-edge-ai-deployment-20250316",
            "snippet": "ZEDEDA has enhanced its integration with NVIDIA's edge AI platform, including support for NVIDIA Jetson systems, NGC catalog, and TAO toolkit, to streamline...",
            "score": 0.8052654266357422,
            "sentiment": null,
            "probability": null,
            "content": "This will enable enterprises to deploy, secure, and manage AI models at the edge, addressing challenges like resource constraints, heterogeneous hardware, and intermittent connectivity.\n\nZEDEDA has enhanced its integration with NVIDIA\u2019s edge AI platform, including support for NVIDIA Jetson systems, NGC catalog, and TAO toolkit, to streamline edge AI deployment and management.\n\nDirect integration with NVIDIA NGC catalog, optimization with TAO toolkit, zero-touch edge management, advanced observability, and native support for NVIDIA Jetson GPUs are all features of this recent development.\n\n\u201cEdge AI deployments present unique challenges that can\u2019t be solved with traditional cloud-native tools alone,\u201d says Said Ouissal, ZEDEDA\u2019s CEO and founder. \u201cBy expanding our integration with NVIDIA\u2019s AI platform, we\u2019re enabling enterprises to innovate faster with simplified AI deployment workflows, reduce risk through zero-touch management and security, and lower costs by automating manual processes at the edge. Our edge-first approach abstracts away the infrastructural complexity, allowing organizations to focus on driving transformation through innovation.\u201d\n\nZEDEDA\u2019s platform simplifies AI workflows, enhances security, and reduces operational costs, enabling scalability from proof-of-concept to large-scale deployments. The platform supports industries including manufacturing, energy, retail, transportation, and robotics with applications such as predictive maintenance, safety detection, and quality control.\n\nZEDEDA\u2019s open architecture and marketplace provide flexibility for deploying and managing diverse edge AI workloads while maintaining security and edge orchestration.\n\nPartnerships with industry leaders such as Wind River and AHEAD demonstrate ZEDEDA\u2019s capabilities in mission-critical and vision-based AI applications.\n\nZEDEDA will showcase these advancements at NVIDIA GTC 2025 this week and continue to expand its ecosystem.\n\nRelated\n\nArticle Topics\n\nAI/ML | edge AI | edge orchestration | edge platform | Nvidia | Zededa",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "This Analyst Thinks Nvidia Stock Can Surge 80% From Here. Should You Buy NVDA Now?",
            "link": "https://www.theglobeandmail.com/investing/markets/stocks/NVDA/pressreleases/31423898/this-analyst-thinks-nvidia-stock-can-surge-80-from-here-should-you-buy-nvda-now/",
            "snippet": "Detailed price information for Nvidia Corp (NVDA-Q) from The Globe and Mail including charting and trades.",
            "score": 0.7104666829109192,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-15": {
        "0": {
            "title": "Report: Nvidia Aims to Expand AI Efforts Beyond Chips",
            "link": "https://www.pymnts.com/artificial-intelligence-2/2025/report-nvidia-aims-to-expand-ai-efforts-beyond-chips/",
            "snippet": "Nvidia CEO Jensen Huang is reportedly working to make sure the chip maker has a secure foundation in case AI demand slows down.",
            "score": 0.9298236966133118,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang is reportedly working to make sure the chip maker has a secure foundation in case the demand driven by artificial intelligence (AI) systems slows down.\n\nThe AI boom has made Nvidia a multitrillion-dollar company and Huang the world\u2019s 15th-wealthiest person, Bloomberg reported Friday (March 14).\n\nAt the same time, Huang is aware that tech infrastructure companies\u2019 products can become commodities and that there is a history of the industry experiencing booms and busts, according to the report.\n\nNvidia is already seeing competitors try to undercut its price, customers try to build their own chips, tariffs cause complications and national security concerns threaten sales to China, the report said.\n\nIn addition, the recent debut of an AI model from DeepSeek that claimed to be as powerful as its competitors while costing much less, led to concerns that the AI boom has peaked, per the report. After the debut of that model, Nvidia experienced the biggest single-day drop in market ever seen by a company: almost $600 billion.\n\nWith Nvidia\u2019s annual conference set to be held next week, it is expected that Huang will highlight the company\u2019s wide-ranging efforts to find \u201cthe next frontier in AI,\u201d according to the report.\n\nThe company aims to build not only chips but also software that will deliver benefits in a variety of industries and encourage other companies to continue making large investments in AI, per the report.\n\nThe Bloomberg report came after a Feb. 26 earnings call in which Huang said that sales of the company\u2019s most advanced chip architecture hit a record in the fourth quarter and that these results were a harbinger of even more demand ahead because the AI era has just begun.\n\n\u201cAI is advancing at light speed,\u201d Huang said during the call. \u201cWe\u2019re just at the start of the age of AI.\u201d\n\nHuang said the world is just two years into the current wave of AI advancements, starting with generative AI, which powered consumer use of AI, and is now moving into AI agents, which will power business use of AI. Following these two trends, he sees the next stage is physical AI like robots.\n\n\u201cAI has gone mainstream\u201d and one day it will be embedded in all industries, Huang said.\n\nFor all PYMNTS AI coverage, subscribe to the daily AI Newsletter.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "The Next Few Days Could Make or Break Nvidia Stock (Here's Why)",
            "link": "https://www.nasdaq.com/articles/next-few-days-could-make-or-break-nvidia-stock-heres-why",
            "snippet": "In today's video, I discuss Nvidia (NASDAQ: NVDA) and recent updates impacting the AI market. To learn more, check out the short video, consider subscribing...",
            "score": 0.7621397972106934,
            "sentiment": null,
            "probability": null,
            "content": "In today's video, I discuss Nvidia (NASDAQ: NVDA) and recent updates impacting the AI market. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the after-market prices of March 13, 2025. The video was published on March 13, 2025.\n\nWhere to invest $1,000 right now? Our analyst team just revealed what they believe are the 10 best stocks to buy right now. Learn More \u00bb\n\nWhere to invest $1,000 right now\n\nWhen our analyst team has a stock tip, it can pay to listen. After all, Stock Advisor\u2019s total average return is 803% \u2014 a market-crushing outperformance compared to 160% for the S&P 500.*\n\nThey just revealed what they believe are the 10 best stocks for investors to buy right now, available when you join Stock Advisor.\n\nSee the 10 stocks \u00bb\n\n*Stock Advisor returns as of March 14, 2025\n\nJose Najarro has positions in Nvidia. The Motley Fool has positions in and recommends Nvidia. The Motley Fool has a disclosure policy. Jose Najarro is an affiliate of The Motley Fool and may be compensated for promoting its services. If you choose to subscribe through their link they will earn some extra money that supports their channel. Their opinions remain their own and are unaffected by The Motley Fool.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "NVIDIA Shows Off Trio of Upcoming Action Games Running at Up to 500 FPS",
            "link": "https://wccftech.com/nvidia-shows-off-trio-of-upcoming-action-games-running-at-up-to-500-fps/",
            "snippet": "At GDC 2025, NVIDIA shared new trailers for three upcoming action games: Lost Soul Aside, Stellar Blade, and Tides of Annihilation.",
            "score": 0.8485045433044434,
            "sentiment": null,
            "probability": null,
            "content": "Shortly after their slew of GDC 2025 announcements, NVIDIA also shared brief new clips for a trio of highly anticipated upcoming action games: Lost Soul Aside, Stellar Blade, and Tides of Annihilation. All of them are made by Asian developers, and they will support NVIDIA DLSS 4 Multi Frame Generation to guarantee exceedingly high frame rates, up to over 500 FPS in Lost Soul Aside.\n\nThe latter game has been in development for a long time. We first covered it nearly seven years ago, when it was being developed for the PlayStation 4 Pro. Unlike other long-delayed games, though, there's a very good reason - it all started as a one-man project of Yang Bing before it gained a lot of attention and support. At long last, Lost Soul Aside will launch on May 30 for PC and PlayStation 5, making it the first single player game published by Sony Interactive Entertainment to release day and date on the two platforms. The action title, which still runs on Unreal Engine 4, also supports ray tracing; you can find detailed system requirements here.\n\nStrictly in order of release, the next game is Stellar Blade from Korean developer Shift Up. This one (also published by Sony Interactive Entertainment like Lost Soul Aside) launched a year ago on the PlayStation 5 to excellent sales and reviews, though developer and publisher have both since been sued for trademark infringement.\n\nAs seen in the brief footage, Stellar Blade, which is also powered by Unreal Engine 4, runs at up to 371 frames per second with NVIDIA DLSS 4 Multi Frame Generation. The PC release is expected at some point in June. We don't have the official system requirements yet.\n\nLastly, we have the most recently announced game: Tides of Annihilation. This action/adventure title inspired by Arthurian myths is in development at Eclipse Glow Games with the Unreal Engine 5, tentatively slated for a 2026 launch. In the trailer, we see it running at up to 359 frames per second with the aid of NVIDIA DLSS 4 Multi Frame Generation.\n\nNeedless to say, all these trailers were captured on the new NVIDIA flagship GPU, the GeForce RTX 5090. Moreover, they will have selected Performance Mode at 4K resolution to maximize frame rates. Still, even with these small caveats, they are impressive frame rate figures. We'll still have to wait to actually play the games before judging their smoothness, though, as stuttering could nonetheless mar even a high frame rate experience.\n\nWhich of these games are you most eager to play? Let us know below.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "The Next Few Days Could Make or Break Nvidia Stock (Here's Why)",
            "link": "https://www.fool.com/investing/2025/03/15/the-next-few-days-could-make-or-break-nvidia-stock/",
            "snippet": "In today's video, I discuss Nvidia (NASDAQ: NVDA) and recent updates impacting the AI market. To learn more, check out the short video, consider subscribing...",
            "score": 0.7621397972106934,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's GTC event will bring to light all the innovations Nvidia has been working on.\n\nIn today's video, I discuss Nvidia (NASDAQ: NVDA) and recent updates impacting the AI market. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the after-market prices of March 13, 2025. The video was published on March 13, 2025.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "This Nvidia-Backed Company Is Seeking a $35 Billion Valuation in One of the Most Eagerly Anticipated IPOs of 2025: 3 Things You Should Know",
            "link": "https://www.yahoo.com/finance/news/nvidia-backed-company-seeking-35-112000642.html",
            "snippet": "Many market strategists have predicted a revival in initial public offerings (IPO) after a sluggish market over the last few years.",
            "score": 0.8516895174980164,
            "sentiment": null,
            "probability": null,
            "content": "Many market strategists have predicted a revival in initial public offerings (IPO) after a sluggish market over the last few years. While it still could take some time for the IPO market to thaw and the floodgates open, especially after recent turbulence in the market, more private companies are starting to emerge.\n\nOne recent company that announced plans to go public is the artificial intelligence infrastructure firm Coreweave (CRWV). Coreweave is reportedly seeking to raise $4 billion, which would value the company at $35 billion, making it one of the larger IPOs in recent years. Given how successful AI stocks have been, the company is likely to draw a lot of interest and is definitely one of the most eagerly anticipated IPOs in 2025. Here are three things you should know.\n\n1. Data centers built for AI\n\nInterestingly, Coreweave started as an infrastructure play targeting crypto miners. The company previously used data centers in New Jersey to mine Ethereum when the world's second-largest cryptocurrency used the energy-intensive proof-of-work mining system that required lots of computing power to mint new blocks of Ethereum and collect the associated rewards. However, when crypto crashed in 2018, Coreweave pivoted and now provides data centers made for supporting generative AI workloads.\n\nCoreweave's data centers provide Nvidia's latest graphics processing units (GPU), liquid cooling capabilities, storage optimized for AI, and robust security. The company essentially provides infrastructure-as-a-service for companies looking to develop and roll out AI solutions and capabilities. Without Coreweave, many companies would be faced with building their own infrastructure, which is a significant undertaking and capital investment. At the end of 2024, Coreweave ran 32 data centers operating more than 250,000 GPUs.\n\nImage source: Getty Images.\n\n2. Financials are impressive\n\nWhen popular tech and AI companies go public, you'll often notice that most are losing money and really don't have the best-looking set of financials, other than the promise of massive growth that will eventually turn into profits down the line. Coreweave is still not profitable and reported a loss of over $863 million in 2024.\n\nHowever, the company had roughly $863 million of depreciation and amortization during the year, which makes the financials look much better on an operating basis. Coreweave reported operating income of over $324 million in 2024, a tremendous improvement from a $14.5 million operating loss in 2023. Meanwhile, revenue exploded roughly 740% in 2024.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA board partners focus on OC Models as GPU and VRAM make up 80% of costs",
            "link": "https://videocardz.com/newz/nvidia-board-partners-focus-on-oc-models-as-gpu-and-vram-make-up-80-of-costs",
            "snippet": "Normally, in our roundups, we include one or two reviews from each media outlet at launch. Some reviewers have better industry contacts, allowing them to...",
            "score": 0.9068089127540588,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Prediction: This Will Be Nvidia's Next Big Move (and It Will Start on March 18)",
            "link": "https://www.msn.com/en-us/money/topstocks/prediction-this-will-be-nvidia-s-next-big-move-and-it-will-start-on-march-18/ar-AA1AXOPZ?ocid=finance-verthp-feeds",
            "snippet": "Nvidia (NASDAQ: NVDA) has wowed investors with many of its moves over the past few years, from introducing game-changing new products into the artificial...",
            "score": 0.9029468894004822,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Rep. Jefferson Shreve Purchases Shares of NVIDIA Co. (NASDAQ:NVDA)",
            "link": "https://www.marketbeat.com/instant-alerts/nvidia-nasdaqnvda-stock-acquired-rep-jefferson-shreve-2025-03-11/",
            "snippet": "Representative Jefferson Shreve (R-Indiana) recently bought shares of NVIDIA Co. (NASDAQ:NVDA). In a filing disclosed on March 09th, the Representative...",
            "score": 0.9364852905273438,
            "sentiment": null,
            "probability": null,
            "content": "Representative Jefferson Shreve (R-Indiana) recently bought shares of NVIDIA Co. NASDAQ: NVDA. In a filing disclosed on March 09th, the Representative disclosed that they had bought between $15,001 and $50,000 in NVIDIA stock on February 24th. The trade occurred in the Representative's \"CRT - STANDARD UNIT TRUST\" account.\n\nGet NVIDIA alerts: Sign Up\n\nRepresentative Jefferson Shreve also recently made the following trade(s):\n\nPurchased $15,001 - $50,000 in shares of RTX NYSE: RTX on 2/24/2025.\n\non 2/24/2025. Purchased $15,001 - $50,000 in shares of Royal Gold NASDAQ: RGLD on 2/24/2025.\n\non 2/24/2025. Purchased $15,001 - $50,000 in shares of QUALCOMM NASDAQ: QCOM on 2/24/2025.\n\non 2/24/2025. Purchased $15,001 - $50,000 in shares of Pfizer NYSE: PFE on 2/24/2025.\n\non 2/24/2025. Sold $15,001 - $50,000 in shares of Norfolk Southern NYSE: NSC on 2/24/2025.\n\non 2/24/2025. Purchased $15,001 - $50,000 in shares of CoStar Group NASDAQ: CSGP on 2/24/2025.\n\non 2/24/2025. Purchased $50,001 - $100,000 in shares of AbbVie NYSE: ABBV on 2/24/2025.\n\non 2/24/2025. Purchased $15,001 - $50,000 in shares of Johnson & Johnson NYSE: JNJ on 2/24/2025.\n\non 2/24/2025. Purchased $15,001 - $50,000 in shares of The Cigna Group NYSE: CI on 2/24/2025.\n\non 2/24/2025. Purchased $15,001 - $50,000 in shares of Adobe NASDAQ: ADBE on 2/24/2025.\n\nNVIDIA Price Performance\n\nNVIDIA stock traded up $6.09 during mid-day trading on Friday, hitting $121.67. 274,813,566 shares of the company were exchanged, compared to its average volume of 323,790,763. NVIDIA Co. has a 1-year low of $75.61 and a 1-year high of $153.13. The company has a quick ratio of 3.64, a current ratio of 4.10 and a debt-to-equity ratio of 0.13. The business has a fifty day moving average of $129.71 and a 200 day moving average of $131.20. The company has a market capitalization of $2.97 trillion, a PE ratio of 47.88, a price-to-earnings-growth ratio of 1.76 and a beta of 1.77.\n\nNVIDIA (NASDAQ:NVDA - Get Free Report) last released its quarterly earnings results on Wednesday, February 26th. The computer hardware maker reported $0.89 EPS for the quarter, topping analysts' consensus estimates of $0.84 by $0.05. NVIDIA had a return on equity of 114.83% and a net margin of 55.69%. The business had revenue of $39.33 billion for the quarter, compared to analyst estimates of $38.16 billion. As a group, sell-side analysts expect that NVIDIA Co. will post 2.77 earnings per share for the current year.\n\nNVIDIA Announces Dividend\n\nThe business also recently declared a quarterly dividend, which will be paid on Wednesday, April 2nd. Shareholders of record on Wednesday, March 12th will be given a $0.01 dividend. This represents a $0.04 dividend on an annualized basis and a yield of 0.03%. The ex-dividend date is Wednesday, March 12th. NVIDIA's dividend payout ratio (DPR) is 1.36%.\n\nAnalyst Ratings Changes\n\nNVDA has been the subject of several research reports. DZ Bank raised NVIDIA from a \"hold\" rating to a \"strong-buy\" rating in a research note on Friday, February 28th. Rosenblatt Securities reaffirmed a \"buy\" rating and issued a $220.00 target price on shares of NVIDIA in a research note on Monday, February 24th. Loop Capital restated a \"buy\" rating and issued a $175.00 price objective on shares of NVIDIA in a report on Wednesday, November 20th. The Goldman Sachs Group lifted their target price on shares of NVIDIA from $150.00 to $165.00 and gave the stock a \"buy\" rating in a report on Thursday, November 21st. Finally, JPMorgan Chase & Co. increased their price target on NVIDIA from $155.00 to $170.00 and gave the company an \"overweight\" rating in a research note on Thursday, November 21st. Four equities research analysts have rated the stock with a hold rating, thirty-seven have assigned a buy rating and two have assigned a strong buy rating to the company. Based on data from MarketBeat, NVIDIA has an average rating of \"Moderate Buy\" and an average target price of $171.51.\n\nRead Our Latest Report on NVIDIA\n\nInstitutional Trading of NVIDIA\n\nInstitutional investors and hedge funds have recently bought and sold shares of the business. Keystone Financial Services LLC acquired a new position in shares of NVIDIA during the 4th quarter worth approximately $1,020,000. Lansforsakringar Fondforvaltning AB publ bought a new position in NVIDIA during the fourth quarter worth $1,288,798,000. Silver Oak Wealth Advisors Services LLC acquired a new position in shares of NVIDIA in the fourth quarter valued at $591,000. PKO Investment Management Joint Stock Co bought a new position in shares of NVIDIA in the fourth quarter valued at about $21,218,000. Finally, Northstar Financial Companies Inc. acquired a new stake in shares of NVIDIA during the 4th quarter worth about $565,000. Institutional investors own 65.27% of the company's stock.\n\nInsider Activity at NVIDIA\n\nIn other NVIDIA news, Director Tench Coxe sold 1,000,000 shares of the stock in a transaction on Monday, December 16th. The stock was sold at an average price of $131.26, for a total transaction of $131,260,000.00. Following the transaction, the director now owns 28,671,360 shares in the company, valued at $3,763,402,713.60. This trade represents a 3.37 % decrease in their position. The transaction was disclosed in a filing with the Securities & Exchange Commission, which is available at this hyperlink. Also, EVP Ajay K. Puri sold 36,462 shares of the business's stock in a transaction dated Monday, January 6th. The shares were sold at an average price of $151.10, for a total transaction of $5,509,408.20. Following the completion of the sale, the executive vice president now directly owns 3,902,888 shares of the company's stock, valued at approximately $589,726,376.80. The trade was a 0.93 % decrease in their position. The disclosure for this sale can be found here. In the last 90 days, insiders have sold 1,039,125 shares of company stock worth $137,062,338. Insiders own 4.23% of the company's stock.\n\nAbout Representative Shreve\n\nJefferson Shreve (Republican Party) is a member of the U.S. House, representing Indiana's 6th Congressional District. He assumed office on January 3, 2025. His current term ends on January 3, 2027. Shreve (Republican Party) is running for re-election to the U.S. House to represent Indiana's 6th Congressional District. He declared candidacy for the 2026 election. Email editor@ballotpedia.org to notify us of updates to this biography. Jefferson Shreve earned a bachelor's degree from Indiana University in 1989, a graduate degree from the University of London in 1998, and a graduate degree from Purdue University in 2003. Shreve's career experience includes working as a real estate executive. Shreve was elected 7th District Vice Chairman of the Indiana Republican State Committee on January 17, 2018, and was re-elected on March 6, 2021. In 2020, he was selected as a district-level delegate to the 2020 Republican National Convention.\n\nNVIDIA Company Profile\n\nNVIDIA Corporation provides graphics and compute and networking solutions in the United States, Taiwan, China, Hong Kong, and internationally. The Graphics segment offers GeForce GPUs for gaming and PCs, the GeForce NOW game streaming service and related infrastructure, and solutions for gaming platforms; Quadro/NVIDIA RTX GPUs for enterprise workstation graphics; virtual GPU or vGPU software for cloud-based visual and virtual computing; automotive platforms for infotainment systems; and Omniverse software for building and operating metaverse and 3D internet applications.\n\nSee Also\n\nBefore you consider NVIDIA, you'll want to hear this.\n\nMarketBeat keeps track of Wall Street's top-rated and best performing research analysts and the stocks they recommend to their clients on a daily basis. MarketBeat has identified the five stocks that top analysts are quietly whispering to their clients to buy now before the broader market catches on... and NVIDIA wasn't on the list.\n\nWhile NVIDIA currently has a Moderate Buy rating among analysts, top-rated analysts believe these five stocks are better buys.\n\nView The Five Stocks Here",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Unmasking a Tech Titan: Author Stephen Witt on Nvidia Founder Jensen Huang's Journey",
            "link": "https://news.cgtn.com/news/2025-03-15/Author-Stephen-Witt-on-Nvidia-Founder-Jensen-Huang-s-Journey-1BLbydT9MJi/p.html",
            "snippet": "With the appointment of Lip-Bu Tan as Intel's new CEO, four major U.S. semiconductor giants - Intel, NVIDIA, AMD and Broadcom - are now led by CEOs of...",
            "score": 0.9248502254486084,
            "sentiment": null,
            "probability": null,
            "content": "Open in CGTN APP for better experience",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Why Nvidia Stock Investors Should Hope AI Gets Extremely Cheap",
            "link": "https://www.nasdaq.com/articles/why-nvidia-stock-investors-should-hope-ai-gets-extremely-cheap",
            "snippet": "In today's video, I discuss Nvidia (NASDAQ: NVDA) and recent updates impacting the AI market. To learn more, check out the short video, consider subscribing...",
            "score": 0.8459967970848083,
            "sentiment": null,
            "probability": null,
            "content": "In today's video, I discuss Nvidia (NASDAQ: NVDA) and recent updates impacting the AI market. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the after-market prices of March 12, 2025. The video was published on March 12, 2025.\n\nWhere to invest $1,000 right now? Our analyst team just revealed what they believe are the 10 best stocks to buy right now. Learn More \u00bb\n\nShould you invest $1,000 in Nvidia right now?\n\nBefore you buy stock in Nvidia, consider this:\n\nThe Motley Fool Stock Advisor analyst team just identified what they believe are the 10 best stocks for investors to buy now\u2026 and Nvidia wasn\u2019t one of them. The 10 stocks that made the cut could produce monster returns in the coming years.\n\nConsider when Nvidia made this list on April 15, 2005... if you invested $1,000 at the time of our recommendation, you\u2019d have $708,400!*\n\nStock Advisor provides investors with an easy-to-follow blueprint for success, including guidance on building a portfolio, regular updates from analysts, and two new stock picks each month. The Stock Advisor service has more than quadrupled the return of S&P 500 since 2002*. Don\u2019t miss out on the latest top 10 list, available when you join Stock Advisor.\n\nSee the 10 stocks \u00bb\n\n*Stock Advisor returns as of March 14, 2025\n\nJose Najarro has positions in Nvidia. The Motley Fool has positions in and recommends Nvidia. The Motley Fool has a disclosure policy. Jose Najarro is an affiliate of The Motley Fool and may be compensated for promoting its services. If you choose to subscribe through their link they will earn some extra money that supports their channel. Their opinions remain their own and are unaffected by The Motley Fool.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-14": {
        "0": {
            "title": "Will Nvidia Stock Get Lift From AI Conference?",
            "link": "https://www.investors.com/news/technology/nvidia-stock-gtc-ai-conference-could-provide-lift/",
            "snippet": "Wall Street analysts are hoping that Nvidia's upcoming GTC conference will provide a boost for Nvidia stock.",
            "score": 0.5703201293945312,
            "sentiment": null,
            "probability": null,
            "content": "The last time Nvidia (NVDA) Chief Executive Jensen Huang gave a major speech his company's stock was trading in record high territory. Ahead of his next big speech on Tuesday, Nvidia stock is in a slump.\n\nYear to date, through Friday's close, Nvidia stock is down 9.4%.\n\nHowever, on the stock market today, Nvidia rose 5.3% to close at 121.67.\n\nHuang is scheduled to give a keynote address at the company's semiannual GTC conference in San Jose, Calif. He plans to discuss what's next in agentic AI, robotics, accelerated computing and more, the company said.\n\nAnalysts expect Nvidia to unveil its GB300 AI chip at the show. That chip could start shipping in May. Huang also is likely to preview the company's next-generation graphics processing unit, or GPU, called Rubin. Rubin is due out in 2026 and will follow the current Blackwell series processors.\n\nNvidia is touting GTC as \"the world's premier AI conference.\" GTC is short for GPU Technology Conference. It will run March 17-21 and feature over 1,000 sessions, 2,000 speakers and nearly 400 exhibitors, mostly focused on artificial intelligence. Nvidia expects 25,000 in-person attendees at the show, with 300,000 attending virtually.\n\nThis year's GTC will feature a day devoted to quantum computing. Quantum Day is taking place on Thursday.\n\nNvidia Stock Down From Record Heights\n\nThe last time Huang gave a keynote speech was at CES 2025 in Las Vegas on Jan. 6. Nvidia stock hit a record high of 153.13 intraday on Jan. 7.\n\nBut since then, Nvidia stock has declined on concerns about hyperscale cloud service providers rethinking their massive capex plans for AI data centers. China's DeepSeek and other AI systems have shown that artificial intelligence models can be operated with fewer chips and less cutting-edge chips.\n\nStill, Wall Street analysts are hoping that the GTC show will provide a boost for Nvidia stock.\n\nBofA Securities analyst Vivek Arya reiterated his buy rating on Nvidia stock with a price target of 200 on Tuesday. He expects important product updates from Nvidia at the show.\n\n\"Despite heavy ASIC (application-specific integrated circuit) competition, we expect Nvidia to maintain its leading 80-85% share\" in AI processors, he said in a report. He noted Nvidia's broad product lineup covering computing and networking, its large software developer base, incumbency and other advantages.\n\nNvidia's GTC is \"typically more a catalyst for the supply chain than Nvidia's stock,\" TD Cowen analyst Joshua Buchalter said in a client note Thursday. He rates Nvidia stock as buy with a price target of 175.\n\nMeanwhile, Mizuho Securities analyst Vijay Rakesh on Thursday lowered his price target on Nvidia stock to 168 from 175 but kept his outperform rating. He cited \"multiple compression in the AI space overall due to fears of slowing growth\" as the reason for his price-target cut.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nApple Stock Bulls Are Losing Confidence In iPhone Upgrade Cycle\n\nIntel Stock Rises On New CEO, But Rough Road Ahead\n\nHow To Know It's Time To Sell Your Favorite Stock\n\nWhen To Sell Growth Stocks: This Could Be Your No. 1 Rule\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open\n\nMarket Rallies Amid Trump Tariff Sell-Off, Nvidia GTC On Tap",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "What To Expect From Nvidia's GPU Technology Conference",
            "link": "https://www.investopedia.com/what-to-expect-from-nvidia-gpu-technology-conference-11697121",
            "snippet": "Nvidia is set to kick off its weeklong GPU Technology Conference in San Jose, California on Monday, with a keynote address from CEO Jensen Huang on Tuesday.",
            "score": 0.924755334854126,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia is set to kick off its weeklong GPU Technology Conference in San Jose, California on Monday, with a keynote address from CEO Jensen Huang on Tuesday.\n\nInvestors will be watching for updates on Nvidia's Blackwell Ultra chips, as well as its next-generation Rubin architecture.\n\nThe chipmaker's stock has struggled in 2025, creating a \"compelling valuation\" ahead of the conference, one analyst said.\n\nNvidia (NVDA) is set to kick off its weeklong GPU Technology Conference in San Jose, California on Monday, with a keynote address from CEO Jensen Huang on Tuesday.\n\nInvestors and analysts will likely be watching for updates on the company\u2019s latest artificial intelligence chips, upcoming releases, and developments in gaming and robotics.\n\nThe AI chipmaker is expected to showcase its Blackwell Ultra GB300 family of chips, which Deutsche Bank analysts said is expected to deliver over 50% more memory capacity and significantly higher performance than its earlier Blackwell offerings. The timing of GB300\u2019s rollout will be a focus, the analysts said, particularly as Nvidia has faced delays in fully ramping up Blackwell production.\n\nNvidia could also offer more details on its Rubin GPU, the successor to Blackwell expected in 2026, along with its associated Vera CPU, and the Rubin Vera platform. It\u2019s possible Huang\u2019s keynote could offer breadcrumbs at what lies a generation beyond Rubin, analysts said.\n\nGTC comes as Nvidia\u2019s stock has fallen nearly 10% so far in 2025, creating a \u201ccompelling valuation\u201d heading into the conference, analysts at Bank of America said. The analysts reiterated a \u201cbuy\u201d rating and $200 price target, above the average of analysts tracked by Visible Alpha. The consensus target at $177 would suggest over 45% upside from Nvidia\u2019s closing price of $121.67 Friday.\n\nUPDATE\u2014March 14, 2025: This article has been updated since it was first published to reflect more recent share price values.\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Report: Nvidia Aims to Expand AI Efforts Beyond Chips",
            "link": "https://www.pymnts.com/artificial-intelligence-2/2025/report-nvidia-aims-to-expand-ai-efforts-beyond-chips/",
            "snippet": "Nvidia CEO Jensen Huang is reportedly working to make sure the chip maker has a secure foundation in case AI demand slows down.",
            "score": 0.9298236966133118,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang is reportedly working to make sure the chip maker has a secure foundation in case the demand driven by artificial intelligence (AI) systems slows down.\n\nThe AI boom has made Nvidia a multitrillion-dollar company and Huang the world\u2019s 15th-wealthiest person, Bloomberg reported Friday (March 14).\n\nAt the same time, Huang is aware that tech infrastructure companies\u2019 products can become commodities and that there is a history of the industry experiencing booms and busts, according to the report.\n\nNvidia is already seeing competitors try to undercut its price, customers try to build their own chips, tariffs cause complications and national security concerns threaten sales to China, the report said.\n\nIn addition, the recent debut of an AI model from DeepSeek that claimed to be as powerful as its competitors while costing much less, led to concerns that the AI boom has peaked, per the report. After the debut of that model, Nvidia experienced the biggest single-day drop in market ever seen by a company: almost $600 billion.\n\nWith Nvidia\u2019s annual conference set to be held next week, it is expected that Huang will highlight the company\u2019s wide-ranging efforts to find \u201cthe next frontier in AI,\u201d according to the report.\n\nThe company aims to build not only chips but also software that will deliver benefits in a variety of industries and encourage other companies to continue making large investments in AI, per the report.\n\nThe Bloomberg report came after a Feb. 26 earnings call in which Huang said that sales of the company\u2019s most advanced chip architecture hit a record in the fourth quarter and that these results were a harbinger of even more demand ahead because the AI era has just begun.\n\n\u201cAI is advancing at light speed,\u201d Huang said during the call. \u201cWe\u2019re just at the start of the age of AI.\u201d\n\nHuang said the world is just two years into the current wave of AI advancements, starting with generative AI, which powered consumer use of AI, and is now moving into AI agents, which will power business use of AI. Following these two trends, he sees the next stage is physical AI like robots.\n\n\u201cAI has gone mainstream\u201d and one day it will be embedded in all industries, Huang said.\n\nFor all PYMNTS AI coverage, subscribe to the daily AI Newsletter.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Is NVIDIA Corporation (NVDA) the Best Quality Stock to Invest in Now?",
            "link": "https://finance.yahoo.com/news/nvidia-corporation-nvda-best-quality-021738436.html",
            "snippet": "We recently published a list of 12 Best Quality Stocks to Invest in Now. In this article, we are going to take a look at where NVIDIA Corporation...",
            "score": 0.8166135549545288,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of 12 Best Quality Stocks to Invest in Now. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against other best quality stocks to invest in now.\n\nTom Lee, co-founder of Fundstrat Global Advisors, believes that a significant rebound is in the offing, despite the tough start to the year. The strategist opines that there is a possibility of ~10% \u2013 15% bounce over the coming months. In an interview with CNBC, he stated his expectations about March, April, and May witnessing the rally. Therefore, missing critical trading days can be a mistake for investors.\n\nWhat Lies Ahead?\n\nLee believes that investors can consider buying as the markets are unsettled. In an interview with CNBC, he went on to explain that the market\u2019s 10 best days last year resulted in the addition of up to 20 percentage points for the broader S&P 500. However, if we exclude these 10 days, the index increased by only 4%. According to him, the best days might be around the corner. If there are tensions related to the growth or related to the employment market, Trump or the US Fed can intervene to bring some stability. These are the favourable catalysts for the upcoming weeks, says Lee. Overall, he believes that a large chunk of the bad news has been priced in as markets have seen a significant decline.\n\nREAD ALSO: 7 Best Stocks to Buy For Long-Term and 8 Cheap Jim Cramer Stocks to Invest In.\n\nSigns of Economic Strength Remain\n\nMorningstar, while echoing the views of Adam Hetts (Janus Henderson Investors portfolio manager), mentioned that the longer-term perspective is expected to provide investors a silver lining. Hetts believes that there are bright spots in the international markets such as Europe and China, that have surpassed the performance of the US stocks since the beginning of the year. Even though the mega-cap tech stocks have been declining, they are getting cheaper as compared with the elevated valuations just a few months ago.\n\nAccording to experts, there are several signs of economic strength. Gus Faucher, chief economist for PNC Financial Services Group, told Morningstar that consumers continue to spend amidst weakness in the sentiment data. Also, the labor market has been holding up. Overall, the chief economist doesn\u2019t see any sort of fundamental weaknesses in the broader economy that can signal a problem. As per Morningstar chief US market strategist David Sekera, the investors are required to focus on the fundamentals and valuations, while maintaining a long-term view.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Why Shares of Nvidia Are Rising to End the Week",
            "link": "https://www.fool.com/investing/2025/03/14/why-shares-of-nvidia-are-rising-to-end-the-week/",
            "snippet": "Shares of artificial intelligence (AI) giant Nvidia (NVDA 5.27%) traded nearly 4.5% higher in the final half hour of trading on Friday.",
            "score": 0.8927550911903381,
            "sentiment": null,
            "probability": null,
            "content": "Shares of artificial intelligence (AI) giant Nvidia (NVDA 5.27%) traded nearly 4.5% higher in the final half hour of trading on Friday. While there was no obvious catalyst behind the move, the stock market rebounded today after another difficult week and Nvidia also seems to be benefiting after a positive update from one of its suppliers.\n\nAI demand still on solid footing\n\nTaiwan-based Foxconn, also known as Hon Hai Precision Industry, reported fourth-quarter net income of $1.41 billion, which missed analyst estimates and also declined 13% year over year. However, management on an earnings call said it expects demand from its AI server business to more than double in the first quarter of 2024, according to Reuters.\n\nChairman Young Liu said on the call:\n\nWe have not seen CSP [cloud service providers] demand slowing down. There are [market] rumors that CSP demand will peak this year, and then it will go down next year. But we are not seeing that ... at least for Foxconn.\n\nLiu added that AI server revenue should make up over half of Foxconn's total server revenue in 2025 and that it is increasing production for Nvidia. Late last year, Foxconn announced that it would build a large plant in Mexico in order to keep up with the intense demand for Nvidia's Blackwell graphics processing units.\n\n\"Some tariff factors may affect demand ... But we have planned in different regions over the past years and have made our supply chain resilience better than it was eight years ago during President Trump's first term,\" Liu said.\n\nUncertainty still high\n\nWhile Friday marked a quieter day on the tariff front, at least as of this writing, I don't think the trade war is done yet. Nvidia will also likely continue to deal with uncertainties from export controls and potential weakness in the economy.\n\nWhile I'm not personally a buyer of Nvidia right now, the stock trades at just under 27 times forward earnings, which is a much more attractive valuation than in the past when it could trade upward of 50 times forward earnings, especially if AI demand stays hot.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Nvidia surpasses Tesla as the No. 1 held stock on Robinhood, fueled by Gen Z interest",
            "link": "https://fortune.com/2025/03/14/nvidia-stock-gen-z-investment-robinhood/",
            "snippet": "Is Nvidia still a worthwhile investment? Despite recent market headwinds, investors\u2014especially young ones\u2014are still bullish.",
            "score": 0.9266683459281921,
            "sentiment": null,
            "probability": null,
            "content": "\u00a9 2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information\n\nFORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia Stock Rises. New Evidence Shows the AI Trade Is Alive and Well.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-ai-8d9f33fe",
            "snippet": "Nvidia stock could do with a boost, having tumbled 15% over the past month.",
            "score": 0.6770842671394348,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "What Nvidia must do at its biggest event of the year next week to win back investors",
            "link": "https://www.cnbc.com/2025/03/14/nvidia-holds-gtc-next-week-what-jim-cramer-expects-from-ceo-jensen-huang-.html",
            "snippet": "Jensen Huang, co-founder and chief executive officer of Nvidia Corp., speaks during the Nvidia GPU Technology Conference in San Jose, California, March 19,...",
            "score": 0.7959465980529785,
            "sentiment": null,
            "probability": null,
            "content": "What a difference a year makes. Nvidia entered last year's pivotal GTC conference \u2014 its new product showcase and software developer workshop \u2014 riding high, with its stock already up a blistering 77% in 2024. The artificial intelligence boom was just over a year old, and optimism reigned supreme among tech investors. Shares of the AI chip king head into next week's GTC bruised and battered. As of Thursday's close, they were down more than 20% from their all-time closing high in early January. The decline would look even worse \u2014 if not for a bounce in recent sessions. During Friday's Morning Meeting for Club members, Jim Cramer pointed out that Nvidia stock tends to trend higher into GTC. True to form, shares were up more than 7% for the week. \"The keynote is very important,\" he added. \"You've got to reestablish that it's actually a company that is a factory for the future.\" This year, skepticism about the sustainability of AI investments is much easier to find in the wake of the DeepSeek panic . Nvidia's revenue growth is slowing, to 78% last quarter and a projected 65% for the February-to-April period. As impressive as that growth rate is for a company worth trillions, triple-digit expansion seems to be a thing of the past. Investors don't hesitate to pull the what-have-you-done-for-me-lately card. Then there's the list of things weighing on Nvidia's stock far outside the company's control: Tariffs, geopolitics affecting its ability to sell products to China, and a rise in uncertainty about the direction of the U.S. economy, which has prompted investors to get more defensive and reduce their riskier bets. Nvidia is far from the only momentum stock to go in reverse in recent weeks. Just take a look at the charts of Palantir and Tesla . Traders taking advantage of highly speculative zero-day options represent another challenge for Nvidia shares. Needless to say, the backdrop for GTC is quite muddy. Is there anything Nvidia CEO Jensen Huang can say or announce next week that will clear it up and allow the conference to, once again , be a positive catalyst for the stock? We're about to find out \u2014 and Jim will be out in Silicon Valley to take it all in. \"The risk-reward is fairly decent here,\" he said during Thursday's Monthly Meeting , because Nvidia's valuation has gone down alongside its share price. The stock now sells at about 25 times forward earnings, down from 31 at the start of the year, and a sizable discount to its five-year average of 40, according to FactSet. \"Let me find out what I can next week and make a better decision [on what to do next] for you,\" Jim said. NVDA 1Y mountain Nvidia's stock performance over the past 12 months. At Huang's keynote presentation on Tuesday afternoon, investors should plan to hear about an updated version of the company's new Blackwell AI chip platform dubbed Blackwell Ultra, which is slated for release in the fall. New details also are expected on Nvidia's next-generation Vera Rubin offering , the successor to Blackwell that is targeted for a 2026 debut. For both products, the focus is on their performance specifications, specifically for the day-to-day use of AI models known as \"inference.\" Are these new products going to incentivize companies to shell out more money to get the newest hardware after spending a lot on the current version of Blackwell and the prior generation known as Hopper? Are they good enough to assuage some investors' concerns about the threat that custom chips present to Nvidia's long-term growth outlook? Huang's keynote and GTC as a whole also will likely generate headlines around robotics and autonomous vehicles, which Nvidia calls \"physical AI.\" While those are both considered promising areas for Nvidia down the road, updates pertaining to its data center business are more significant drivers of the stock right now. Even if the stock does receive a GTC-related lift, it may be short-lived. Such was the case in March 2024 : A six-session win streak that began on the day GTC started soon gave way to a nearly 20% pullback that culminated on April 19. Nvidia's stock has been no stranger to volatility over the years, but the current moment presents distinct difficulties. The doubts over the durability of AI spending \u2014 and by extension orders for Nvidia's latest and greatest chips \u2014 won't be erased overnight. The fate of external overhangs, including trade policy and geopolitics around exports to China, is a guessing game. The good news is that GTC is a real opportunity for Nvidia and Huang to add some clarity to a fuzzy situation and steer at least some of the conversation back toward the company's market-leading technology. When that is the focus, there's a lot to like. (Jim Cramer's Charitable Trust is long NVDA. See here for a full list of the stocks.) As a subscriber to the CNBC Investing Club with Jim Cramer, you will receive a trade alert before Jim makes a trade. Jim waits 45 minutes after sending a trade alert before buying or selling a stock in his charitable trust's portfolio. If Jim has talked about a stock on CNBC TV, he waits 72 hours after issuing the trade alert before executing the trade. THE ABOVE INVESTING CLUB INFORMATION IS SUBJECT TO OUR TERMS AND CONDITIONS AND PRIVACY POLICY , TOGETHER WITH OUR DISCLAIMER . NO FIDUCIARY OBLIGATION OR DUTY EXISTS, OR IS CREATED, BY VIRTUE OF YOUR RECEIPT OF ANY INFORMATION PROVIDED IN CONNECTION WITH THE INVESTING CLUB. NO SPECIFIC OUTCOME OR PROFIT IS GUARANTEED.\n\nJensen Huang, co-founder and chief executive officer of Nvidia Corp., speaks during the Nvidia GPU Technology Conference in San Jose, California, March 19, 2024. David Paul Morris | Bloomberg | Getty Images",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia at a Crossroads: Futureproof or Fizzle",
            "link": "https://www.bloomberg.com/news/articles/2025-03-14/nvidia-ceo-jensen-huang-eyes-next-move-to-outlast-current-ai-boom",
            "snippet": "Jensen Huang, co-founder and chief executive officer of Nvidia Corp., speaks while holding the company's new GeForce RTX 50 series graphics cards and a Thor...",
            "score": 0.9013899564743042,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NVIDIA GTC 25: What To Expect From the Year\u2019s Biggest Artificial Intelligence Event",
            "link": "https://biztechmagazine.com/article/2025/03/nvidia-gtc-25-what-expect-years-biggest-artificial-intelligence-event",
            "snippet": "Speakers will focus on the company's latest advancements and how businesses of all sizes can take advantage.",
            "score": 0.9096373319625854,
            "sentiment": null,
            "probability": null,
            "content": "Several key themes and announcements are expected to shape the discussions:\n\nThe new Blackwell architecture. NVIDIA\u2019s latest chips, named for mathematician David Blackwell, are now in production and shipping to customers. Blackwell represents a leap forward in GPU design, the company says. During his March 18 keynote presentation, CEO Jensen Huang will surely tout the technical enhancements of Blackwell, including improvements in speed, memory and power efficiency.\n\nProject DIGITS. The company plans to sell a first-of-its kind personal AI supercomputer called DIGITS that anyone might purchase for a home office or that businesses of any size might find useful. \u201cNormally a workstation with the same computing power would cost $30,000,\u201d says Sana Gutierrez, senior manager for data and artificial intelligence at CDW. \u201cNow you can have it in this different form factor, and NVIDIA has said, preliminarily, that it\u2019s going to be like $3,000.\u201d The company expects to begin shipping its DIGITS workstations in May.\n\nNVIDIA Inference Microservices. Better known as NIMs, NVIDIA Inference Microservices are prepackaged software kits that help organizations shorten development cycles. \u201cIt\u2019s the Legos that you use to build the plane,\u201d Gutierrez explains. \u201cYou still have to have some knowledge and capability to build what you need, but it makes it a lot easier. It\u2019s not out of a box; there\u2019s always going to be some development work, but the speed at which you can develop with NIMs is much quicker.\u201d\n\nDIG DEEPER: Unlock the value of generative artificial intelligence in your organization.\n\nHow Businesses Can Build Their Own AI Solutions\n\nThere\u2019s no limit to what organizations can create using the NIM tools. Many businesses in the manufacturing, warehouse and logistics sectors have used NIMs to build digital twins, copies of factory or warehouse floors where companies can experiment with new processes or layouts.\n\nCDW has used NIM tools to build several AI solutions, Gutierrez notes, including a virtual assistant, a product description generator, a corporate briefing manager that can speak to outsiders about the business and a customer service agent.\n\n\u201cWe\u2019ve built some really cool things using those blueprints, and it highlights our capability to build. But we also know that no two customer situations are the same,\u201d she says, adding that CDW is available to help customers develop their own AI solutions.\n\nBetween Project DIGITS, NIM, and NVIDIA\u2019s advancements in the area of robotics, Gutierrez says that GTC is an opportunity for the public to glimpse \u201creally futuristic stuff,\u201d and it\u2019s worth considering how far technology has come in just a few years. At last year\u2019s event, \u201cI don\u2019t think there was nearly enough oohing and aahing at the fact that in our day and age, there are technology companies that are truly defying what we thought was possible, and NVIDIA is one of those.\u201d\n\nClick the banner below to learn how organizations are deploying artificial intelligence.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-13": {
        "0": {
            "title": "NVIDIA Reveals Neural Rendering, AI Advancements at GDC 2025",
            "link": "https://blogs.nvidia.com/blog/gdc-2025-ai-neural-rendering-game-development/",
            "snippet": "At this year's GDC conference, NVIDIA is revealing new AI tools and technologies to supercharge the next era of graphics in games.",
            "score": 0.8793550729751587,
            "sentiment": null,
            "probability": null,
            "content": "New neural rendering tools, rapid NVIDIA DLSS 4 adoption, \u2018Half-Life 2 RTX\u2019 demo and digital human technology enhancements are among NVIDIA\u2019s announcements at the premier conference for game developers.\n\nAI is leveling up the world\u2019s most beloved games, as the latest advancements in neural rendering, NVIDIA RTX and digital human technologies equip game developers to take innovative leaps in their work.\n\nAt this year\u2019s GDC conference, running March 17-21 in San Francisco, NVIDIA is revealing new AI tools and technologies to supercharge the next era of graphics in games.\n\nKey announcements include new neural rendering advancements with Unreal Engine 5 and Microsoft DirectX; NVIDIA DLSS 4 now available in over 100 games and apps, making it the most rapidly adopted NVIDIA game technology of all time; and a Half-Life 2 RTX demo coming Tuesday, March 18.\n\nPlus, the open-source NVIDIA RTX Remix modding platform has now been released, and NVIDIA ACE technology enhancements are bringing to life next-generation digital humans and AI agents for games.\n\nNeural Shaders Enable Photorealistic, Living Worlds With AI\n\nThe next era of computer graphics will be based on NVIDIA RTX Neural Shaders, which allow the training and deployment of tiny neural networks from within shaders to generate textures, materials, lighting, volumes and more. This results in dramatic improvements in game performance, image quality and interactivity, delivering new levels of immersion for players.\n\nAt the CES trade show earlier this year, NVIDIA introduced RTX Kit, a comprehensive suite of neural rendering technologies for building AI-enhanced, ray-traced games with massive geometric complexity and photorealistic characters.\n\nNow, at GDC, NVIDIA is expanding its powerful lineup of neural rendering technologies, including with Microsoft DirectX support and plug-ins for Unreal Engine 5.\n\nNVIDIA is partnering with Microsoft to bring neural shading support to the DirectX 12 Agility software development kit preview in April, providing game developers with access to RTX Tensor Cores to accelerate the performance of applications powered by RTX Neural Shaders.\n\nPlus, Unreal Engine developers will be able to get started with RTX Kit features such as RTX Mega Geometry and RTX Hair through the experimental NVIDIA RTX branch of Unreal Engine 5. These enable the rendering of assets with dramatic detail and fidelity, bringing cinematic-quality visuals to real-time experiences.\n\nNow available, NVIDIA\u2019s \u201cZorah\u201d technology demo has been updated with new incredibly detailed scenes filled with millions of triangles, complex hair systems and cinematic lighting in real time \u2014 all by tapping into the latest technologies powering neural rendering, including:\n\nReSTIR Path Tracing\n\nReSTIR Direct Illumination\n\nRTX Mega Geometry\n\nRTX Hair\n\nAnd the first neural shader, Neural Radiance Cache, is now available in RTX Remix.\n\nOver 100 DLSS 4 Games and Apps Out Now\n\nDLSS 4 debuted with the release of GeForce RTX 50 Series GPUs. Over 100 games and apps now feature support for DLSS 4. This milestone has been reached two years quicker than with DLSS 3, making DLSS 4 the most rapidly adopted NVIDIA game technology of all time.\n\nDLSS 4 introduced Multi Frame Generation, which uses AI to generate up to three additional frames per traditionally rendered frame, working with the complete suite of DLSS technologies to multiply frame rates by up to 8x over traditional brute-force rendering.\n\nThis massive performance improvement on GeForce RTX 50 Series graphics cards and laptops enables gamers to max out visuals at the highest resolutions and play at incredible frame rates.\n\nIn addition, Lost Soul Aside, Mecha BREAK, Phantom Blade Zero, Stellar Blade, Tides of Annihilation and Wild Assault will launch with DLSS 4, giving GeForce RTX gamers the definitive PC experience in each title. Learn more.\n\nDevelopers can get started with DLSS 4 through the DLSS 4 Unreal Engine plug-in.\n\n\u2018Half-Life 2 RTX\u2019 Demo Launch, RTX Remix Official Release\n\nHalf-Life 2 RTX is a community-made remaster of the iconic first-person shooter Half-Life 2.\n\nA playable Half-Life 2 RTX demo will be available on Tuesday, March 18, for free download from Steam for Half-Life 2 owners. The demo showcases Orbifold Studios\u2019 work in the eerily sensational maps of Ravenholm and Nova Prospekt, with significantly improved assets and textures, full ray tracing, DLSS 4 with Multi Frame Generation and RTX neural rendering technologies.\n\nHalf-Life 2 RTX was made possible by NVIDIA RTX Remix, an open-source platform officially released today for modders to create stunning RTX remasters of classic games.\n\nUse the platform now to join the 30,000+ modders who\u2019ve experimented with enhancing hundreds of classic titles since its beta release last year, enabling over 1 million gamers to experience astonishing ray-traced mods.\n\nNVIDIA ACE Technologies Enhance Game Characters With AI\n\nThe NVIDIA ACE suite of RTX-accelerated digital human technologies brings game characters to life with generative AI.\n\nNVIDIA ACE autonomous game characters add autonomous teammates, nonplayer characters (NPCs) and self-learning enemies to games, creating new narrative possibilities and enhancing player immersion.\n\nACE autonomous game characters are debuting in two titles this month:\n\nIn inZOI, \u201cSmart Zoi\u201d NPCs will respond more realistically and intelligently to their environment based on their personalities. The game launches with NVIDIA ACE-based characters on Friday, March 28.\n\nAnd in NARAKA: BLADEPOINT MOBILE PC VERSION, on-device NVIDIA ACE-powered teammates will help players battle enemies, hunt for loot and fight for victory starting Thursday, March 27.\n\nDevelopers can start building with ACE today.\n\nJoin NVIDIA at GDC.\n\nSee notice regarding software product information.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia\u2019s next chips are named after Vera Rubin, astronomer who discovered dark matter",
            "link": "https://www.cnbc.com/2025/03/13/nvidia-to-detail-vera-rubin-chips-at-gtc-conference.html",
            "snippet": "Nvidia CEO Jensen Huang is expected to reveal details about Rubin, the chipmaker's next AI GPU, on Tuesday at the company's annual GTC conference.",
            "score": 0.9396201372146606,
            "sentiment": null,
            "probability": null,
            "content": "Jensen Huang, co-founder and CEO of Nvidia, displays the new Blackwell GPU chip during the Nvidia GPU Technology Conference in San Jose, California, on March 18, 2024.\n\nNvidia CEO Jensen Huang is expected to reveal details about Rubin, the chipmaker's next AI graphics processor, on Tuesday at the company's annual GTC conference.\n\nWhile other tech companies usually name their products using combinations of inscrutable letters and numbers, most of Nvidia's most recent GPU architectures have been named after famous women scientists.\n\nNvidia is naming its next critical AI chip platform after Vera Rubin, an American astronomer.\n\nThe company has never explained its naming convention, and hasn't emphasized the diversity aspect of its choices, but Nvidia's chip names that highlight women and minority scientists are one of the most visible efforts to honor diversity in the tech industry during a period where diversity, equity and inclusion, or DEI, initiatives are being slashed by the Trump administration.\n\nRubin discovered a lot of what is known about \"dark matter,\" a form of matter that could make up a quarter of the matter of the universe and which doesn't emit light or radiation, and she advocated for women in science throughout her career.\n\nNvidia has been naming its architectures after scientists since 1998, when its first chips were based on the company's \"Fahrenheit\" microarchitecture. It's part of the company's culture \u2013 Nvidia used to sell an employee-only t-shirt with cartoons of several famous scientists on it.\n\nIt's one of Nvidia's quirks that has received more attention as it's risen to become one of the three most-valuable tech companies and one of the most important suppliers to Google , Microsoft , Amazon , OpenAI, Tesla and Meta .\n\nInvestors want to hear on Tuesday how fast the Rubin chips will be, what configurations it will come in and when it might start shipping.\n\nBefore revealing a new architecture, Nvidia CEO Jensen Huang usually gives a one-sentence biography of the scientist it's named after.\n\n\"I'd like to introduce you to a very, very big GPU named after David Blackwell, mathematician, game theorist, probability,\" Huang said at last year's GTC conference. \"We thought it was a perfect name.\"\n\nRubin is a fitting name for Nvidia's next chip, which comes as the company tries to solidify the gains it has made in recent years as the leader in AI hardware. \"Vera\" will refer to Nvidia's next-generation central processor, and \"Rubin\" will refer to Nvidia's new GPU.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Stock Held Up Better Than Mag 7 Peers During Thursday's Rout\u2014Watch These Key Levels",
            "link": "https://www.investopedia.com/nvidia-stock-held-up-better-than-mag-7-peers-during-thursday-rout-watch-these-key-levels-11696055",
            "snippet": "Nvidia shares held up better than other Magnificent 7 stocks on Thursday after surging 6% the previous session, as investors have sought dip-buying...",
            "score": 0.9500192403793335,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia shares held up better than other Magnificent 7 stocks on Thursday after surging 6% the previous session, as investors have sought dip-buying opportunities in the AI chipmaker.\n\nSince setting a record high in early January, the stock has traded within a descending channel, with the price recently finding buying interest near the pattern's lower trendline.\n\nInvestors should watch key support levels on Nvidia's chart around $105 and $96, while also monitoring important resistance levels near $130 and $153.\n\nNvidia (NVDA) shares held up better than other Magnificent 7 stocks on Thursday after surging 6% the previous session, as investors have sought dip-buying opportunities in the chipmaker.\n\nThe AI favorite has been under pressure since late January after AI competition from China sparked fears of overspending by technology giants on the infrastructure that Nvidia sells. More recently, worries that tariffs, a flare-up of inflation, and further export curbs could drag down chip sales have also weighed on sentiment.\n\nYesterday\u2019s bounce coincided with a report that the chipmaker, along with Advanced Micro Devices (AMD), and Broadcom (AVGO), has been approached by Taiwan Semiconductor Manufacturing Company (TSM) about forming a joint venture to own and run Intel\u2019s (INTC) foundry division.\n\nOn Thursday, Nvidia shares closed 0.1% lower at $115.58, while its Mag 7 counterparts all fell sharply amid a broader sell-off that sent the S&P 500 into correction for the first time since 2023. Nvidia shares are down 14% since the start of the year, with the lion\u2019s shares of that loss occurring over the last month.\n\nBelow, we take a closer look at Nvidia\u2019s chart and apply technical analysis to point out key price levels that investors may be watching.\n\nDescending Channel Takes Shape\n\nSince setting a record high in early January, Nvidia shares have traded within a descending channel, with volume picking up in the second half of February.\n\nMore recently, the stock found buying interest near the channel\u2019s lower trendline, coinciding with an uptick in the relative strength index (RSI) as the indicator moves back towards neutral territory.\n\nLooking ahead, as the 50-day moving average (MA) converges towards the 200-Day MA, investors should watch for a potential death cross, a signal that forecasts further downside.\n\nLet\u2019s identify several key support and resistance levels on Nvidia\u2019s chart that could come into play during future price swings.\n\nKey Support Levels to Watch\n\nThe first key support level to watch sits at $105. This area, currently in the vicinity of the descending channel\u2019s lower trendline, could attract buying interest near this month\u2019s low and the September trough.\n\nA close below this location could see the shares revisit lower support around $96. Investors may look to accumulate shares in this region near last year\u2019s March twin peaks, which closely align with the early August swing low.\n\nImportant Resistance Levels to Monitor\n\nUpon a move higher, it\u2019s worth monitoring how the shares respond to the $130 level. This area may provide overhead selling pressure near the descending channel\u2019s upper trendline, the moving averages, and several peaks and troughs on the chart stretching back to June last year.\n\nFinally, further upside could see Nvidia shares climb to around $153. Investors who have bought at lower prices may seek exit points in this region near several peaks situated just below the stock\u2019s all-time high.\n\nThe comments, opinions, and analyses expressed on Investopedia are for informational purposes only. Read our warranty and liability disclaimer for more info.\n\nAs of the date this article was written, the author does not own any of the above securities.\n\n",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "The latest rumors about the Nvidia RTX 5060: Release date, specs, pricing, and more",
            "link": "https://mashable.com/article/rumors-nvidia-rtx-5060-date-specs-pricing",
            "snippet": "We don't know much for certain just yet, but there have been lots of rumors about Nvidia's RTX 5060.",
            "score": 0.9065498113632202,
            "sentiment": null,
            "probability": null,
            "content": "Customers looking for a budget-friendly graphic card from Nvidia are eagerly awaiting the arrival of the final members of its RTX 50 Series graphics card. With all that interest, we\u2019ve already seen a number of leaks and rumors about what to expect.\n\nThere has still not been an official announcement, so for the time being, alleged leaked specs are all we have. The RTX 5060 is allegedly coming with 8GB of VRAM, although later rumors say that a 12GB model may also exist. The RTX 5060 Ti will likely have two variants, one with 8GB and one with 16GB. All four cards are rumored to come with a 128-bit memory bus, putting them on par with the RTX 4060 and 4060 Ti. There is also an RTX 5050 rumored to be coming with 8GB of VRAM.\n\nPricing on the cards hasn\u2019t been revealed, but a Chinese retailer appears to have listed the cards , giving us our best glimpse yet. Per the website, the RTX 5060 with 12GB of VRAM would go for around $525, while the Ti variant is listed at around $600 once the prices were converted from Chinese yuan to U.S. dollars.\n\nMashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Loading... Sign Me Up By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy Thanks for signing up!\n\nSEE ALSO: Where to buy Nvidia RTX 5070 at launch\n\nVideoCardz notes that these prices are likely for AIB models \u2014 like Asus, Asrock, etc \u2014 and not the actual MSRP. The prevailing opinion is that the RTX 5060 series of cards will sell for close to what the RTX 4060 series sold for, which would put prices at $299 for the base 5060 up to $499 for the RTX 4060 Ti. At those prices, it makes the RTX 5070 look pretty good at its $550 MSRP, assuming you can find one at that price.\n\nIt had been rumored that Nvidia would be announcing the final members of its RTX 50 Series graphics cards on Thursday. It turned out that this time, it was only a rumor.\n\nThe original rumor was posted by VideoCardz , which reported that Nvidia would finally announce the most budget-friendly members of the RTX 50 Series graphics cards, including the RTX 5060, 5060 Ti (8GB), and another RTX 5060 Ti with 16GB of VRAM.\n\nTo their credit, VideoCardz did own up to their error, stating on their website that \u201cit appears our information was incorrect; Nvidia has not announced the RTX 5060 series on March 13. We regret the error.\u201d The publication did initially report, however, that Nvidia had reached out to the media about the cards, so they could still be coming soon.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia\u2019s GTC keynote will emphasize AI over gaming",
            "link": "https://venturebeat.com/games/nvidias-gtc-keynote-will-emphasize-ai-over-gaming/",
            "snippet": "Nvidia's GPU Technology Conference (GTC) takes place in San Jose next week, not terribly far from San Franciso concurrently hosting the Game Developer's...",
            "score": 0.8543041348457336,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia\u2019s GPU Technology Conference (GTC) takes place in San Jose next week, not terribly far from San Franciso concurrently hosting the Game Developer\u2019s Conference in the heart of the city. Despite geographic proximity, the subject matter of both conferences will likely be a world apart, as Nvidia CEO Jen Huang seems to be aiming for less of a talk about what Nvidia will do for gaming and more what it will do for AI.\n\nIn Nvidia\u2019s GTC session catalog, the keynote is described succinctly: \u201cDon\u2019t miss this keynote from NVIDIA founder and CEO Jensen Huang. He\u2019ll share how NVIDIA\u2019s accelerated computing platform is driving the next wave in AI, digital twins, cloud technologies, and sustainable computing.\u201d\n\nWith the recent launch of the 5090, it makes sense that Nvidia would be somewhat mum about new graphics capabilities for the latest games. At this point, improvements are likely to err more toward an incrementalist game of inches than a massive revolution every few years. But it does indicate that Nvidia is aware which audience is buttering their bread right now and is aiming their focus toward that.\n\nMoreover, Nvidia and Huang likely feel they desperately need to win back the faith of the AI market. The picture of AI has changed dramatically since DeepSeek was revealed a few months ago, making Nvidia\u2019s argument of being the best requiring expensive hardware straight from the source feel a bit shaky and that feeling was born out by the stock market.\n\nNvidia is going into GTC looking for a win and it is more likely to get that from AI enthusiasts and investors than gamers. This is not to say the company will not have any new information about how to get the best graphics out of all of 2025 and beyond\u2019s shiny new titles, but the core focus lies elsewhere for the keynote at least.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "RTX 5060 listed for over $500. Has Nvidia lost their minds?",
            "link": "https://hardforum.com/threads/rtx-5060-listed-for-over-500-has-nvidia-lost-their-minds.2040292/",
            "snippet": "Just as the headline says. Nvidia now wants over $500 for the 5060 model and nearly $600 for the 5060 Ti according to leaked pricing. This feels extremely.",
            "score": 0.6729977130889893,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Kioxia And Pliops Storage Announcements For The 2025 Nvidia GTC",
            "link": "https://www.forbes.com/sites/tomcoughlin/2025/03/13/kioxia-and-pliops-storage-announcements-for-the-2025-nvidia-gtc/",
            "snippet": "At next week's Nvidia GTC, Kioxia will be showing its high-capacity PCIe 5.0 SSD, and Pliops will be showing its shared storage used to improve LLM...",
            "score": 0.9366942048072815,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Fund manager sends blunt message on Nvidia stock before conference",
            "link": "https://www.thestreet.com/investing/stocks/veteran-fund-managers-blunt-take-on-nvidia-raises-eyebrows",
            "snippet": "Here's what could happen to Nvidia's stock next.",
            "score": 0.6006693243980408,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "As The Dow Skids, Nvidia And This Aerospace Leader Rebound",
            "link": "https://www.investors.com/news/boeing-stock-dow-jones-nvidia-rebound-nvda-ba-march-2025/",
            "snippet": "The Dow Jones Industrial Average retreated 3.5% this week. Nvidia, Boeing climb to buck the trend. Analysts see 50% upside to Boeing stock.",
            "score": 0.7448531985282898,
            "sentiment": null,
            "probability": null,
            "content": "Trade wars, recession worries and broad market uncertainty sent the Dow Jones Industrial Average down 4.6% for the week through Thursday. The S&P 500 shed 4.3%. In the shadow of those losses, AI-play Nvidia (NVDA) rebounded 2.7% this week. Boeing stock is outpacing those gains as shares push off 2025 lows and toward their first weekly advance in four weeks.\u2026",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia Stock Gains. This Could Stop a Full-Blown Rebound.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-sell-5a7b6c5c",
            "snippet": "The chip maker's shares are down 14% in 2025, tumbling as Donald Trump's tariffs have triggered a rotation out of tech.",
            "score": 0.5777995586395264,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-03-12": {
        "0": {
            "title": "Exclusive: TSMC pitched Intel foundry JV to Nvidia, AMD and Broadcom, sources say",
            "link": "https://www.reuters.com/technology/tsmc-pitched-intel-foundry-jv-nvidia-amd-broadcom-sources-say-2025-03-12/",
            "snippet": "SINGAPORE/NEW YORK/TAIPEI, March 12 (Reuters) - TSMC (2330.TW) , opens new tab has pitched U.S. chip designers Nvidia (NVDA.O) , opens new tab,...",
            "score": 0.8940325379371643,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia won the AI training race, but inference is still anyone's game",
            "link": "https://www.theregister.com/2025/03/12/training_inference_shift/",
            "snippet": "Comment With the exception of custom cloud silicon, like Google's TPUs or Amazon's Trainium ASICs, the vast majority of AI training clusters being built...",
            "score": 0.9098249673843384,
            "sentiment": null,
            "probability": null,
            "content": "Comment With the exception of custom cloud silicon, like Google's TPUs or Amazon's Trainium ASICs, the vast majority of AI training clusters being built today are powered by Nvidia GPUs. But while Nvidia may have won the AI training battle, the inference fight is far from decided.\n\nUp to this point, the focus has been on building better, more capable, and trustworthy models. Most inference workloads, meanwhile, have taken the form of proof-of-concepts and low-hanging fruit like AI chatbots and image generators. Because of this, most AI compute has been optimized for training rather than inference.\n\nBut as these models get better, as applications become more complex, and as AI infiltrates deeper into our daily lives, this ratio is poised to change dramatically over the next couple of years. In light of this change, many of the chip companies that missed the boat on AI training are now salivating at the opportunity to challenge Nvidia's market dominance.\n\nFinding a niche\n\nCompared to training, which pretty much universally requires gobs of compute, often spanning entire data halls and consuming megawatts of power for days or weeks at a time, inference is a far more diverse workload.\n\nWhen it comes to inference, performance is predominantly determined by three core factors:\n\nMemory capacity dictates what models you can run.\n\nMemory bandwidth influences how quickly the response is generated.\n\nCompute affects how long it takes for the model to respond and how many requests it can serve at a time.\n\nBut which of these you prioritize depends heavily on your model's architecture, parameter count, hosting location, and target audience.\n\nFor example, a small latency-sensitive model might be better suited to a low-power NPU or even a CPU, while a multi-trillion-parameter LLM is going to need datacenter-class hardware with terabytes of incredibly fast memory.\n\nThe latter example is exactly what AMD appears to have targeted with its MI300-series GPUs, which boast between 192 GB and 256 GB of speedy HBM. More memory means AMD is able to cram larger frontier models into a single server than Nvidia, which might explain why companies like Meta and Microsoft were so keen to adopt them.\n\nOn the other end of the spectrum, companies like Cerebras, SambaNova, and Groq \u2013 not to be confused with xAI's Grok series of models \u2013 have prioritized speed, leaning on their SRAM-heavy chip architectures and tricks like speculative decoding to run models five, ten, or even 20 times faster than the best GPU-based inference-as-a-service vendors have managed to achieve thus far.\n\nDeepSeek means companies need to consider AI investment more carefully READ MORE\n\nWith the rise of chain-of-thought reasoning models, which might need to generate thousands of words \u2013 or more specifically, tokens \u2013 to answer a question, lightning-fast inference goes from being a neat gimmick to something legitimately useful.\n\nSo it's no surprise that startups like d-Matrix and others are looking to get in on the \"fast inference\" game as well. The company expects its Corsair accelerators, due out in Q2, will be able to run models like Llama 70B at latencies as low as 2 ms per token, which, by our estimate, works out to 500 tokens a second. The company has set its sights on even larger models for its next-gen Raptor series of chips, which we're told will use vertically stacked DRAM to boost memory capacity and bandwidth.\n\nAt the lower end of the spectrum, we've seen a growing number of vendors like Hailo AI, EnCharge, and Axelera developing low-power, high-performance chips for the edge and PC markets.\n\nSpeaking of the PC market, more established chipmakers like AMD, Intel, Qualcomm, and Apple are racing to integrate ever more powerful NPUs into their SoCs to support AI-augmented workflows.\n\nFinally, we can't ignore the cloud and hyperscaler providers, many of which will continue buying Nvidia hardware while simultaneously hedging their bets on in-house silicon.\n\nDon't count Nvidia out yet\n\nWhile Nvidia certainly is facing more competition than it ever has, it's still the biggest name in AI infrastructure. With its latest generation of GPUs, it's clearly preparing for the transition to large-scale inference deployments.\n\nIn particular, Nvidia's GB200 NVL72, unveiled last year, expanded its NVLink compute domain to 72 GPUs, totaling more than 1.4 exaFLOPS and 13.5 TB of memory.\n\nPrior to this, Nvidia's most powerful systems topped out at just eight GPUs per node and between 640 GB and 1.1 TB of vRAM. This meant that large-scale, frontier-class models, like GPT-4, had to be distributed across multiple systems not just to fit all the parameters in memory, but to achieve reasonable throughput.\n\nIf Nvidia's projections are to be believed, the NVL72's high-speed interconnect fabric will enable it to deliver a 30x improvement in throughput for 1.8 trillion parameter-scale mixture-of-expert models \u2013 like GPT-4 \u2013 compared to an eight-node, 64-GPU cluster of H100s.\n\nMore importantly, these are general-purpose GPUs, which means they're not limited to just training or inference. They can be used to train new models and later re-tasked to run them \u2013 something that isn't necessarily true of every silicon upstart vying for a piece of Jensen's turf.\n\nWith GTC set to kick off next week, Nvidia is expected to detail its next-gen Blackwell-Ultra platform, which, if it's anything like its H200 generation of GPUs, should be tuned specifically with inference in mind.\n\nGiven the launch of Nvidia's Blackwell-based RTX cards earlier this year, we also wouldn't be surprised to see a L40 successor or even some refreshed workstation-class cards.\n\nIn the end, inference is a tokens-per-dollar game\n\nWhatever hardware AI service providers end up packing their bit barns with, the economics of inferencing ultimately boils down to tokens per dollar.\n\nWe're not saying developers won't be willing to pay extra for access to the latest models, or higher throughput, especially if it helps their app or service to stand out.\n\nBut from a developer standpoint, these services amount to little more than an API spigot to which they connect their app, allowing tokens to flow on demand.\n\nThe fact they're using Nvidia's Blackwell parts or some bespoke accelerator you've never heard of is completely abstracted behind what usually ends up being an OpenAI-compatible API endpoint. \u00ae",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Stock Attempts Rebound As Bulls Pound The Table",
            "link": "https://www.investors.com/news/technology/nvidia-stock-attempts-rebound-bulls-pound-table/",
            "snippet": "After retreating 27% in the past three weeks, Nvidia (NVDA) stock is attempting a rebound. Several Wall Street bulls say shares of the AI chipmaker are...",
            "score": 0.8734564781188965,
            "sentiment": null,
            "probability": null,
            "content": "After retreating 27% in the past three weeks, Nvidia (NVDA) stock is attempting a rebound. Several Wall Street bulls say shares of the AI chipmaker are trading at an attractive valuation now.\n\nOn Tuesday, Nvidia stock hit a six-month low of 104.77 before staging a comeback later in the session. It closed at 108.76 on Tuesday, up 1.7% for the day.\n\nOn the stock market today, Nvidia stock jumped 6.4% to end the regular session at 115.74.\n\nBofA Securities reiterated its buy rating and price target of 200 on Nvidia stock on Wednesday. In a client note, the firm said Nvidia is trading \"at a very compelling price\" ahead of the company's flagship GPU Tech Conference, known as GTC, next week.\n\nAt the conference, Nvidia is expected to present updates on its Blackwell Ultra and Rubin processors and next-generation networking technology. It also will discuss long-term opportunities in autonomous cars, physical AI, robotics and quantum computing.\n\nWedbush Securities analyst Daniel Ives said investors need to think long-term about stocks involved in the artificial intelligence megatrend.\n\n\"Our bullish calls on Nvidia, and many of the Mag 7, have been upside down this year,\" Ives said in a client note Wednesday. \"But our stock calls are not for the next few months, (they are) for where we see these tech names over the next 1, 3, and 5 years.\"\n\nNvidia Stock Catalyst: GTC Conference News\n\nNext week's GTC conference could be \"a turning point for tech stocks\" as investors start to refocus on \"the AI Revolution and the massive tech spending ahead,\" Ives said.\n\nWells Fargo Securities analyst Aaron Rakers maintained his overweight, or buy, rating on Nvidia stock with a price target of 185 on Tuesday.\n\nIn a client note, he said the market turmoil in the wake of actions by the Trump administration have created a \"buying opportunity\" for Nvidia stock.\n\n\"GTC 2025 can't come fast enough,\" Rakers said.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nThis Stock Called Top Defensive Play In Current Market Upheaval\n\nApple Dominates Smart Tag Market With AirTags\n\nHow To Know It's Time To Sell Your Favorite Stock\n\nWhen To Sell Growth Stocks: This Could Be Your No. 1 Rule\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Block Becomes First Company in North America to Deploy the Latest NVIDIA GB200 Systems for Frontier Models",
            "link": "https://www.businesswire.com/news/home/20250312684830/en/Block-Becomes-First-Company-in-North-America-to-Deploy-the-Latest-NVIDIA-GB200-Systems-for-Frontier-Models",
            "snippet": "Today, Block, Inc. (NYSE: XYZ) announced it will be the first company in North America to deploy the NVIDIA DGX SuperPOD with DGX GB200 systems. Upon.",
            "score": 0.6964901089668274,
            "sentiment": null,
            "probability": null,
            "content": "DISTRIBUTED-WORK-MODEL/OAKLAND, Calif.--(BUSINESS WIRE)--Today, Block, Inc. (NYSE: XYZ) announced it will be the first company in North America to deploy the NVIDIA DGX SuperPOD with DGX GB200 systems. Upon deployment at an Equinix data center, this new infrastructure will be used in the research and training of open source generative AI models with novel capabilities in underexplored areas.\n\n\u201cThe industry, and the world, is undergoing a seismic shift with adopting AI tools. At Block, we think it\u2019s essential not only to apply AI to existing problems, but also to explore, learn, and build in the open so that we can advance the frontier of AI in a way that truly levels the playing field for our customers and community,\u201d said Dhanji R. Prasanna, CTO of Block. \u201cWe\u2019re excited to deploy an NVIDIA Grace Blackwell DGX SuperPOD and start exploring novel solutions for our customers. We're committed to an open source approach, sharing our learnings and results along the way.\u201d\n\nBlock\u2019s AI research has previously worked on deepfake detection, and attracted widespread attention for one of the first examples in the world of hyper-realistic generated audio.\n\n\u201cBlock is a technology company first, and engineering is at the heart of everything we do. Our past work on generative speech models means Block is well-positioned for innovation surrounding Gen AI models, and all possible applications that come with it,\u201d said Prasanna.\n\nThe Grace Blackwell-powered NVIDIA DGX SuperPOD is purpose-built for state-of-the-art generative AI models, bringing advanced capabilities for training and inference. These systems address key AI challenges, enabling larger and more advanced models to be trained, with faster innovation and greater capabilities for AI as a result.\n\n\u201cAs AI models grow in complexity and scale, businesses need powerful infrastructure that can match the pace of innovation,\u201d said Charlie Boyle, Vice President, DGX platforms, NVIDIA. \u201cWith NVIDIA DGX GB200 systems, Block engineering and research teams can develop frontier open source AI models that can tackle complex, real-world challenges with state-of-the-art AI supercomputing.\u201d\n\nA key to select the right AI Cloud partner for Block and prove out hypotheses before scaling was the ability to access hundreds of interconnected NVIDIA GPUs, for a short amount of time and supported by ML engineers: Lambda 1-Click Clusters. These GPU clusters are now available with NVIDIA Blackwell.\n\nThe new DGX SuperPOD will be deployed at one of Equinix\u2019s AI-ready data centers. These purpose-built, globally interconnected facilities provide a unique environment of data privacy and sovereignty compliance, high-performance, flexibility and low-latency edge connectivity to thousands of ecosystem partners and clouds.\n\n\u201cFrontier models represent the cutting edge of artificial intelligence technology, pushing the boundaries of what AI can achieve, and they require the latest in AI chips \u2013 like NVIDIA\u2019s new DGX SuperPOD,\u201d said Jon Lin, Chief Business Officer at Equinix. \u201cBy deploying at Equinix\u2019s neutral, cloud-adjacent platform, companies like Block can unlock expanded compute scale and flexibility. This enables the customization of AI solutions with a choice of infrastructure, cloud, models and cooling at our neutral exchange.\u201d\n\nAdopting NVIDIA\u2019s latest AI infrastructure is a fast follow from Block\u2019s launch of codename goose, an open source, interoperable AI agent framework that enables users to connect large language models (LLMs) to real-world actions. Its first use cases are related to software engineering, but developers both within Block and the broader open source community are exploring other non-engineering use cases as well.\n\nAbout Block\n\nBlock, Inc. (NYSE: XYZ) builds technology to increase access to the global economy. Each of our brands unlocks different aspects of the economy for more people. Square makes commerce and financial services accessible to sellers. Cash App is the easy way to spend, send, and store money. Afterpay is transforming the way customers manage their spending over time. TIDAL is a music platform that empowers artists to thrive as entrepreneurs. Bitkey is a simple self-custody wallet built for bitcoin. Proto is a suite of bitcoin mining products and services. Together, we\u2019re helping build a financial system that is open to everyone. Block.xyz",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia GeForce RTX 5090 departs from RTX 3090 Ti and RTX 4090 flagship tradition, drops VRAM ECC for pro workloads",
            "link": "https://hardforum.com/threads/nvidia-geforce-rtx-5090-departs-from-rtx-3090-ti-and-rtx-4090-flagship-tradition-drops-vram-ecc-for-pro-workloads.2040253/",
            "snippet": "Missing ROPs, no PhysX, and also no ECC Felt somethin was off and confirmed it today \"However, it is not clear whether Blackwell's memory controller...",
            "score": 0.8879379034042358,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia GTC 2025: What to expect from this year\u2019s show",
            "link": "https://techcrunch.com/2025/03/12/nvidia-gtc-2025-what-to-expect-from-this-years-show/",
            "snippet": "Nvidia is hosting its GTC 2025 conference in San Jose, California from March 17 to March 21. Here's what to expect, including new Blackwell GPU reveals.",
            "score": 0.9166229963302612,
            "sentiment": null,
            "probability": null,
            "content": "GTC, Nvidia\u2019s biggest conference of the year, begins Monday and runs till Friday in San Jose. TechCrunch will be on the ground covering the news as it happens \u2014 and we\u2019re expecting a healthy dose of announcements.\n\nCEO Jensen Huang will give a keynote address at the SAP Center on Tuesday at 10 a.m. Pacific, focusing on \u2014 what else? \u2014 AI and accelerating computing technologies, according to Nvidia. The company is also teasing reveals related to robotics, sovereign AI, AI agents, and automotive \u2014 plus 1,000 sessions with 2,000 speakers and close to 400 exhibitors.\n\nHere\u2019s how to watch the Nvidia GTC 2025 keynote online, along with many other sessions, talks, and panels.\n\nSo what do we expect to see at GTC? Well, Nvidia typically reserves a big chunk of the conference for GPU-related debuts. A new, upgraded iteration of the company\u2019s Blackwell chip lineup seems likely.\n\nDuring Nvidia\u2019s most recent earnings call, Huang confirmed that the upcoming Blackwell B300 series, codenamed Blackwell Ultra, is slated for release in the second half of this year. In addition to higher computing performance, Blackwell Ultra cards pack more memory (288GB), an attractive feature for customers looking to run and train memory-hungry AI models.\n\nRubin, Nvidia\u2019s next-gen GPU series, is almost certain to get a mention at GTC alongside Blackwell Ultra. Due out in 2026, Rubin promises to deliver what Huang has described as a \u201cbig, big, huge step up\u201d in computing power.\n\nHuang said during the aforementioned Nvidia earnings call that he\u2019d talk about post-Rubin products at GTC, as well. That could be Rubin Ultra GPUs, or perhaps the GPU architecture that\u2019ll come after the Rubin family.\n\nBeyond GPUs, Nvidia may illuminate its approach to recent quantum computing advancements. The company has scheduled a \u201cquantum day\u201d for GTC, during which it\u2019ll host execs from prominent companies in the space to \u201c[map] the path toward useful quantum applications.\u201d\n\nOne thing\u2019s for sure: Nvidia could use a win.\n\nEarly Blackwell cards reportedly suffered from severe overheating issues, causing customers to cut their orders. U.S. export controls and fears of tariffs have massively depressed Nvidia\u2019s stock price in recent months. At the same time, the success of Chinese AI lab DeepSeek, which developed efficient models competitive with models from leading AI labs, has prompted investors to worry about the demand for powerful GPUs like Blackwell.\n\nHuang has asserted that DeepSeek\u2019s rise to prominence will in fact be a net positive for Nvidia because it\u2019ll accelerate the broader adoption of AI technology. He has also pointed to the growth of power-hungry so-called \u201creasoning\u201d models like OpenAI\u2019s o1 as Nvidia\u2019s next mountain to climb.\n\nTo be clear, Nvidia isn\u2019t exactly hurting. The company reported a record-breaking quarter in February, notching $39.3 billion in revenue and projecting $43 billion in revenue for the subsequent quarter. While rivals such as AMD have begun to encroach on the company\u2019s territory, Nvidia still commands an estimated 82% of the GPU market.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia\u2019s stock is rebounding, but the key to a real recovery may surprise investors",
            "link": "https://www.marketwatch.com/story/nvidias-stock-is-rebounding-but-the-key-to-a-real-recovery-may-surprise-investors-5fec7081",
            "snippet": "Amid all the fearful talk on Wall Street about the potential for lower spending on artificial-intelligence infrastructure, one analyst believes Nvidia Corp.",
            "score": 0.9177612066268921,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Samsung Advances AI in Mobile Networks With NVIDIA",
            "link": "https://news.samsung.com/global/samsung-advances-ai-in-mobile-networks-with-nvidia",
            "snippet": "Samsung showcased significant progress in technology and ecosystem development of AI-RAN, unlocking the full potential of software-based networks with...",
            "score": 0.7817824482917786,
            "sentiment": null,
            "probability": null,
            "content": "Samsung showcased significant progress in technology and ecosystem development of AI-RAN, unlocking the full potential of software-based networks with NVIDIA\u2019s AI platform\n\nSamsung Electronics today announced that the company is working with NVIDIA to advance AI-RAN technologies. The collaboration underscores Samsung\u2019s commitment to fostering a robust ecosystem and diversifying the computing platforms available. This endeavor aims to support a smooth and easy adoption of AI in mobile networks by expanding the ecosystem of Central Processing Unit (CPU) and bolstering partnerships with Graphics Processing Unit (GPU) companies.\n\nTo maximize and bring the power of AI into Radio Access Networks (RAN), Samsung has made significant technological progress leveraging its in-house AI and radio expertise since the beginning of 2024. One of the pivotal milestones achieved was the interoperability between Samsung\u2019s O-RAN compliant virtualized RAN (vRAN) and NVIDIA\u2019s accelerated computing, which took place in Samsung Research\u2019s lab in late last year. Samsung has successfully demonstrated a proof-of-concept to verify how NVIDIA\u2019s accelerated computing can be seamlessly integrated into software-based networks to help enhance AI capabilities.\n\nThis achievement further solidifies Samsung\u2019s strides in moving ahead with its unique innovation of combining AI and RAN. With the baseline, Samsung can seamlessly deliver AI-RAN by integrating its vRAN (virtualized Distributed Unit, vDU) with NVIDIA\u2019s accelerated computing into a commercial-off-the-shelf (COTS) server where Samsung\u2019s vRAN software is installed.\n\nIn addition, the companies will continue to explore the best-of-breed combinations of AI-RAN options leveraging Samsung vRAN with NVIDIA\u2019s Grace CPU and/or GPU based AI platform using Compute Unified Device Architecture (CUDA) technologies. All of these are optimal for each network deployment environment \u2014 from rural, sub-urban to dense urban.\n\nAt MWC 2025, Samsung presented its leadership in AI-For-RAN innovations, as exemplified through two AI-RAN demonstrations, endorsed by the AI-RAN Alliance and developed in collaboration with various members including NVIDIA. The demonstrations included AI-based physical uplink shared channel (PUSCH) estimation and non-uniform modulation, showing a glimpse in innovative ways to infuse AI into mobile networks.\n\n\u201cWhile AI is reshaping the telecommunications landscape, Samsung is helping operators to build the right network architecture and environment where AI can thrive, all powered by our proven and AI-powered vRAN,\u201d said June Moon, Executive Vice President, Head of R&D, Networks Business at Samsung Electronics. \u201cThis collaboration with NVIDIA signifies our continued efforts to expand GPU and CPU ecosystem, and we look forward to exploring more possibilities in the future.\u201d\n\n\u201cAI-RAN is a critical technology that delivers transformative gains in network utilization, efficiency and performance, while enabling new AI services,\u201d said Ronnie Vasishta, Senior Vice President for Telecom at NVIDIA. \u201cSamsung is a frontrunner in AI-RAN development. Their vRAN expertise and software integrated with the NVIDIA\u2019s AI accelerated computing will accelerate the path to AI-native wireless networks.\u201d\n\nAs one of the founding members of the AI-RAN Alliance forged in 2024, Samsung is actively participating in advancing AI-RAN technologies with academic institutions and industry leaders like NVIDIA. As an elected Vice Chair of Board of Directors as well as the Working Group #3 (AI-on-RAN), Samsung is leading industry towards AI-powered next-generation networks.\n\nSamsung\u2019s end-to-end software-based network architecture is the optimal foundation to easily deploy and adopt AI across every layer of the network. As such, Samsung can empower operators with its flexible networks and sharpen their competitive edge to stay at the front of the AI era. This advancement opens doors for utilizing network infrastructure not only for mobile communications but also for processing general workloads, providing a data center-like network architecture that will present new business opportunities.\n\nSamsung has pioneered the successful delivery of 5G end-to-end solutions, including chipsets, radios and cores. Through ongoing research and development, Samsung drives the industry to advance 5G networks with its market-leading product portfolio, from vRAN 3.0, Open RAN, core to private network solutions and AI-powered automation tools and applications. The company currently provides innovative network solutions to mobile operators and enterprises that deliver boundless connectivity to hundreds of millions of users worldwide.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "What To Expect From Nvidia\u2019s GTC Conference Next Week",
            "link": "https://www.forbes.com/sites/timbajarin/2025/03/12/what-to-expect-from-nvidias-gtc-conference-next-week/",
            "snippet": "Nvidia's GPU Technology Conference, set for Mar. 17-21, 2025, is garnering significant attention this year. As a key player in the AI revolution, Nvidia has...",
            "score": 0.9381968975067139,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "\u2018Dean of Valuation\u2019 Says Nvidia (NVDA) Doesn\u2019t Have \u2018Capacity\u2019 for Earnings, Cash Flow Generation to Justify Market Cap",
            "link": "https://finance.yahoo.com/news/dean-valuation-says-nvidia-nvda-183147743.html",
            "snippet": "We recently published a list of Top 10 Stocks Wall Street is Discussing. In this article, we are going to take a look at where NVIDIA Corporation...",
            "score": 0.5373795628547668,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of Top 10 Stocks Wall Street is Discussing. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against other top stocks Wall Street is discussing.\n\nBill Strazzullo, Bell Curve Trading chief market strategist, said in a latest program on CNBC that the market rally that started during the peak of the pandemic driven by fiscal stimulus seems to have \u201ctapped out.\u201d The analyst sees more pain ahead:\n\n\u201cThe bottom line of all this is that we\u2019ve only started this. We\u2019ll be lucky if we get out of this top to bottom only down 20%. I think eventually, across the indices\u2014Dow, S&P, NASDAQ 100\u2014we\u2019ll end up being down 25% before it\u2019s all done.\u201d\n\nAsked what he would advise to long-term investors, the analyst recommended taking some money off the table and bracing for more impact:\n\n\u201cIt\u2019s not anything very esoteric. Take some money off the table\u2014you\u2019ll be able to deploy that capital at much better levels later in the year. But right now, Trump is taking us not only into a trade war but into a full-blown recession. I think the mistake people are making is that we\u2019ve been spoiled with these V bottoms\u2014we go down 10 or 12% and then right back up. This is not going to be like that. We are going to be in for a much deeper drawdown and I think something that\u2019s going to last a significant amount of time.\u201d\n\nREAD ALSO: 7 Best Stocks to Buy For Long-Term and 8 Cheap Jim Cramer Stocks to Invest In\n\nFor this article, we picked 10 stocks making moves on important news. With each stock we have mentioned the number of hedge fund investors. Why are we interested in the stocks that hedge funds pile into? The reason is simple: our research has shown that we can outperform the market by imitating the top stock picks of the best hedge funds. Our quarterly newsletter\u2019s strategy selects 14 small-cap and large-cap stocks every quarter and has returned 373.4% since May 2014, beating its benchmark by 218 percentage points (see more details here).\n\n\u2018Dean of Valuation\u2019 Says Nvidia (NVDA) Doesn\u2019t Have \u2018Capacity\u2019 for Earnings, Cash Flow Generation to Justify Market Cap\n\nNVIDIA Corp (NASDAQ:NVDA)\n\nNumber of Hedge Funds Investors: 193\n\nAswath Damodaran, NYU Stern School of Business professor of finance, said before NVIDIA Corp\u2019s (NASDAQ:NVDA) latest earnings report that the company will beat Wall Street estimates and still \u201cdisappoint\u201d investors amid high expectations.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-03-11": {
        "0": {
            "title": "Analysts turn heads with Nvidia stock price target move",
            "link": "https://www.thestreet.com/technology/analysts-turn-heads-with-nvidia-stock-price-target-move",
            "snippet": "This is what could happen next to Nvidia shares.",
            "score": 0.571956992149353,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia\u2019s stock has been \u2018brutalized.\u2019 Here\u2019s the big disconnect.",
            "link": "https://www.marketwatch.com/story/nvidias-stock-has-been-brutalized-heres-the-big-disconnect-10ea3d1f",
            "snippet": "Can Nvidia Corp.'s \u201cbrutalized\u201d stock meaningfully turn things around? Shares of Nvidia \u2014 and Broadcom Inc., for that matter \u2014 \u201care looking attractive for...",
            "score": 0.8029687404632568,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Wells Fargo Says Nvidia's Drop Is a 'Golden Opportunity'-Here's What to Watch at GTC",
            "link": "https://finance.yahoo.com/news/wells-fargo-says-nvidias-drop-203400294.html",
            "snippet": "Wells Fargo sees Nvidia's stock dip as a buying opportunity, with GTC 2025 expected to showcase major AI and chip advancements.",
            "score": 0.6744477152824402,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's (NASDAQ:NVDA) recent stock decline isn't a cause for concern, it's a buying opportunity, according to Wells Fargo, which sees the upcoming GTC conference as a potential catalyst for the chipmaker.\n\nAnalyst Aaron Rakers, who has an Overweight rating and a $185 price target on Nvidia, highlighted that over the past five years, Nvidia shares have consistently outperformed the Philadelphia Semiconductor Index around GTC, with an average return of over 6% during the event week and more than 4% in the following two weeks.\n\nOne major focus for investors is co-package optics (CPO), though Rakers doesn't expect Nvidia to introduce it at the GPU level just yet. However, he believes CPO integration could debut in Nvidia's Quantum InfiniBand networking portfolio at GTC.\n\nOther expected highlights include Blackwell Ultra (GB300), which will focus on inferencing, and advancements in Nvidia's NVLink and Spectrum Ultra X800. Investors will also be watching for updates on Nvidia's AI software monetization and Project DIGITS expansion.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia\u2019s RTX 5060 and 5060 Ti GPUs May Arrive Real Soon",
            "link": "https://gizmodo.com/nvidias-rtx-5060-and-5060-ti-gpus-may-arrive-real-soon-2000574616",
            "snippet": "Take your bets whether you'll be able to find Nvidia's lower-end RTX 5060 and RTX 5050 in stock anywhere.",
            "score": 0.699666440486908,
            "sentiment": null,
            "probability": null,
            "content": "Like Bigfoot or the chupacabra, the Nvidia GeForce RTX 5060 is being spotted in the wild by ravenous GPU buyers ready to believe any hint of a footprint indicates how powerful a beast they are. If anything, the latest leaks suggest we may be on track for a full reveal as soon as this week. The first hints of the card found in a pre-built desktop tower indicate the next Blackwell GPU could be relatively cheap, but that may not matter much if Team Green\u2019s next graphics card is limited on VRAM.\n\nFrench retailers are already promoting pre-configured desktop towers sporting a Nvidia GeForce RTX 5060. As first spotted by the sleuths at Videocardz, there\u2019s an Acer Nitro 50 tower containing a 14th-gen Intel Core i7-14700F CPU alongside Nvidia\u2019s new lower-end graphics card (it\u2019s listed as a PCIe x16 card, which may be incorrect but we\u2019ll find out soon enough). The French seller EvoPC is selling the tower for 1,590 euros, or a little more than $1,737. The jury is still out on whether that config is worth it, even with 32 GB of RAM.\n\nOn Thursday, an Acer spokesperson sent us a note saying the Nitro 50 was posted \u201cin error\u201d and the listing has since been pulled down.\n\nA similar config with the Acer Nitro 60 on Amazon containing a last-gen RTX 4060 may go for between $1,520 and $1,541 from various sellers. That may not mean much, considering the Nitro 50 and 60 configs have been around since 2022. Since it\u2019s a French seller, those prices may not reflect Acer\u2019s commitment to a 10% markup for U.S. customers due to Trump tariffs.\n\nThe other element the listing potentially confirms is the 8GB of GDDR7 VRAM. Earlier this week, the usually accurate Nvidia leaker that goes by kopite7kimi posted supposed specs for an RTX 5050, RTX 5060, and RTX 5060 Ti. The RTX 5060 may hold 3,840 CUDA cores and a 150 W power draw. That\u2019s a 25% uptick from the Ada Lovelace architecture at the same level from 2022. That sounds good until you notice the 8 GB of VRAM. The Nvidia GeForce RTX 5070 was already relatively limited at higher resolutions, and the squeezed memory didn\u2019t help it.\n\nAs for the 5060 Ti, the core counts are less dramatic, going from 4,352 on the 4060 Ti to 4,608. Just remember that core counts don\u2019t offer more than a small window into the full picture of the GPU\u2019s capabilities. The general fear is that the 5060 Ti won\u2019t be a major uplift in gen-on-gen performance, similar to the RTX 4070 Ti Super compared to the RTX 5070 Ti. At the very least, according to the leaks, you can get the RTX 5060 Ti in both 8 GB and 16 GB GDDR7 RAM variants. It will have a 180W power draw.\n\nLike the RTX 5070 Ti, Nvidia doesn\u2019t seem willing to wait to share its \u201ctitanium\u201d edition GPUs. If we believe the leaks, other changes will come to Nvidia\u2019s desktop GPU slate. Those leaks indicate the RTX 5050 could come in desktop form, where it was relegated to laptops on the RTX 40-series. That, too, will sit at a mere 8 GB VRAM and 2,560 of the Blackwell CUDA cores. What this means for the low-end GPU is still in flux. Each of these GPUs should be able to use DLSS 4 and multi-frame gen, so we\u2019ll see if that can push the envelope more than it did while trying to eek 4K performance from the 5070.\n\nMeanwhile, AMD seems to have a plan to stick 16GB of GDDR6 VRAM into all its cards. Rumors reported by Videocardz indicate the 9060 wants to compete at the 60-level just as well as it did against the 70s. None of this really changes the landscape until we get an idea about the price. Nvidia still has its work cut out for itself, trying to make sure there\u2019s enough supply for this launch.\n\nUpdate 03/13/25 at 4:30 p.m. ET: This post was updated to include a note from Acer that the EvoPC listing has since been pulled.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia Stock Sees Wild Swings. How Trump Could End the AI Chips Slump.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-trump-ai-chips-c262f606",
            "snippet": "Nvidia stock dropped 5.1% Monday for its lowest close since September. Worries about President Donald Trump's tariffs are hitting the AI trade.",
            "score": 0.7968186736106873,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Will Nvidia Stock Keep Dropping in 2025?",
            "link": "https://www.fool.com/investing/2025/03/11/will-nvidia-stock-keep-dropping-in-2025/",
            "snippet": "With shares down 20% year to date, Nvidia (NVDA 5.27%) has been off to a bad start in 2025, even though its chip business continues to fire on all cylinders...",
            "score": 0.9282535910606384,
            "sentiment": null,
            "probability": null,
            "content": "With shares down 20% year to date, Nvidia (NVDA 5.27%) has been off to a bad start in 2025, even though its chip business continues to fire on all cylinders. Is the market predicting an end to the generative artificial intelligence (AI) hype cycle? Let's dig deeper to determine what the next nine months might have in store for the industry's leader.\n\nFourth-quarter earnings were not bad\n\nWhen a company's stock starts falling, it's easy to blame poor operational performance. But that hasn't been the case for Nvidia. Fourth-quarter earnings show a business that is still firing on all cylinders. Revenue soared 78% from a year ago to a quarterly record of $39.3 billion, driven by strength in the company's data center segment, where it makes advanced AI chips for running and training large language models (LLMs).\n\nWhile Nvidia started as a gaming chip designer, expensive data center hardware has become its bread and butter. Its latest AI chip, Blackwell, is estimated to cost a whopping $30,000 to $50,000 per unit. But these new products boast dramatic improvements in power and efficiency compared to their predecessors, potentially allowing clients to save money using them. According to CEO Jensen Huang, demand is \"insane.\"\n\nThat said, everything isn't peaches and cream. Nvidia experienced a three-point drop in fourth-quarter gross margins (to 73%), but that was likely because of temporary challenges associated with the rollout of its new Blackwell chips. Management expects the trend to continue in the first quarter, with gross margins dropping to 71% as it ramps up production.\n\nThe market isn't impressed\n\nAt the time of writing, Nvidia's shares are down 14% from the release of earnings on Feb. 26. This dip suggests the market isn't very impressed with the company despite its high growth rate and the successful rollout of its Blackwell chips. Some might blame this on the falling gross margins. However, the bigger challenge may come from long-term demand.\n\nIn February, Microsoft shocked the tech world when it scrapped some leases for data centers in the U.S. The software giant is believed to be one of Nvidia's top consumers, and a reduction in its data center capacity could indicate a desire to reduce its exposure to the industry.\n\nWhile Microsoft probably won't give up on AI, its CEO, Satya Nadella, suggests the technology isn't creating much meaningful value yet. If one of the industry leaders says this openly, other Nvidia clients (such as Alphabet and Meta Platforms) may feel the same way behind the scenes.\n\nFurther alarm bells are coming from another major Nvidia client, OpenAI. Last month, the generative AI start-up finalized a design with TSMC to make its own custom chips to reduce its reliance on third-party suppliers. Custom chips are designed for specialized workloads, so they have fewer unnecessary components, making them cheaper and more efficient compared to the one-size-fits-all GPUs typically provided by Nvidia.\n\nIf major clients scale back their AI investments and turn to custom chips, Nvidia's growth potential could be seriously eroded.\n\nWhat's next for Nvidia?\n\nWith a market cap of $2.6 trillion, Nvidia is already a huge company. And future upside looks limited, especially as investors become more concerned with challenges like falling gross margins and potential demand erosion. That said, shares also look unlikely to crash in 2025.\n\nWith a forward price-to-earnings (P/E) multiple of just 25.5, Nvidia shares are valued only slightly higher than the Nasdaq-100 estimate of 25, making them quite affordable and reducing the risk of downside. The stock is likely to remain flat this year unless there is a severe deterioration in macroeconomic conditions, such as a recession.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Exclusive: Meta begins testing its first in-house AI training chip",
            "link": "https://www.reuters.com/technology/artificial-intelligence/meta-begins-testing-its-first-in-house-ai-training-chip-2025-03-11/",
            "snippet": "Facebook owner Meta is testing its first in-house chip for training artificial intelligence systems, a key milestone as it moves to design more of its own...",
            "score": 0.8116359114646912,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Leaked GeForce RTX 5060 and 5050 specs suggest Nvidia will keep playing it safe",
            "link": "https://arstechnica.com/gadgets/2025/03/leaked-geforce-rtx-5060-and-5050-specs-suggest-nvidia-will-keep-playing-it-safe/",
            "snippet": "Nvidia has launched all of the GeForce RTX 50-series GPUs that it announced at CES, at least technically\u2014whether you're buying from Nvidia, AMD, or Intel,...",
            "score": 0.8219096660614014,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has launched all of the GeForce RTX 50-series GPUs that it announced at CES, at least technically\u2014whether you're buying from Nvidia, AMD, or Intel, it's nearly impossible to find any of these new cards at their advertised prices right now.\n\nBut hope springs eternal, and newly leaked specs for GeForce RTX 5060 and 5050-series cards suggest that Nvidia may be announcing these lower-end cards soon. These kinds of cards are rarely exciting, but Steam Hardware Survey data shows that these xx60 and xx50 cards are what the overwhelming majority of PC gamers are putting in their systems.\n\nThe specs, posted by a reliable leaker named Kopite and reported by Tom's Hardware and others, suggest a refresh that's in line with what Nvidia has done with most of the 50-series so far. Along with a move to the next-generation Blackwell architecture, the 5060 GPUs each come with a small increase to the number of CUDA cores, a jump from GDDR6 to GDDR7, and an increase in power consumption, but no changes to the amount of memory or the width of the memory bus. The 8GB versions, in particular, will probably continue to be marketed primarily as 1080p cards.\n\nRTX 5060 Ti (leaked) RTX 4060 Ti RTX 5060 (leaked) RTX 4060 RTX 5050 (leaked) RTX 3050 CUDA Cores 4,608 4,352 3,840 3,072 2,560 2,560 Boost Clock Unknown 2,535 MHz Unknown 2,460 MHz Unknown 1,777 MHz Memory Bus Width 128-bit 128-bit 128-bit 128-bit 128-bit 128-bit Memory bandwidth Unknown 288 GB/s Unknown 272 GB/s Unknown 224 GB/s Memory size 8GB or 16GB GDDR7 8GB or 16GB GDDR6 8GB GDDR7 8GB GDDR6 8GB GDDR6 8GB GDDR6 TGP 180 W 160 W 150 W 115 W 130 W 130 W\n\nAs with the 4060 Ti, the 5060 Ti is said to come in two versions, one with 8GB of RAM and one with 16GB. One of the 4060 Ti's problems was that its relatively narrow 128-bit memory bus limited its performance at 1440p and 4K resolutions even with 16GB of RAM\u2014the bandwidth increase from GDDR7 could help with this, but we'll need to test to see for sure.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Tesla, Nvidia, Oracle, Delta, Southwest: Stocks to watch today",
            "link": "https://qz.com/stocks-to-watch-march-11-tesla-nvidia-musk-trump-1851769028",
            "snippet": "The Dow, the S&P 500, and the Nasdaq Composite waver in early trading after yesterday's bloodbath.",
            "score": 0.9201425313949585,
            "sentiment": null,
            "probability": null,
            "content": "The S&P 500, Nasdaq Composite and Dow Jones indexes were little changed in early trading Tuesday, a day after Monday\u2019s market plunge.\n\nDraftKings CEO on March Madness, AI, and the future of sports gambling CC Share Subtitles Off\n\nEnglish view video DraftKings CEO on March Madness, AI, and the future of sports gambling\n\nDraftKings CEO on March Madness, AI, and the future of sports gambling CC Share Subtitles Off\n\nEnglish DraftKings CEO on March Madness, AI, and the future of sports gambling\n\nTesla (TSLA+3.19% ) gained about 3.8% after Monday\u2019s 15% plummet, as President Donald Trump said he\u2019d buy one of the company\u2019s cars to help out Elon Musk. Nvidia (NVDA+4.51% ) advanced while Apple (AAPL+1.48% ) fell and Google (GOOGL+1.35% ) was little changed.\n\nAdvertisement\n\nU.S. job openings numbers are due at 10 a.m., with consensus for 7.6 million, the same as in December. Trump is scheduled to meet with the Business Roundtable later on Tuesday.\n\nAdvertisement\n\nThese stocks may be active today:\n\nBank of America\n\nBank of America (BAC+3.05% ) has eliminated some investment banking roles, Reuters reported. The stock gained 0.5%\n\nAdvertisement\n\nDelta Air Lines\n\nDelta Air Lines (DAL+5.33% ) shares fell 4.3% after the carrier cut its first-quarter profit and sales forecasts on weaker domestic travel demand. It retained its full-year outlook. Expedia (EXPE+3.17% ), Hilton (HLT+3.28% ) and Airbnb (ABNB+2.82% ) shares also declined.\n\nAdvertisement\n\nDick\u2019s Sporting Goods\n\nDick\u2019s Sporting goods fell 0.6% after a disappointing earnings projection outweighed better-than-expected fiscal fourth-quarter results, Barron\u2019s reported.\n\nAdvertisement\n\nOracle\n\nOracle (ORCL+0.94% ) dropped 5.3% after its fiscal third-quarter earnings fell short of analysts\u2019 expectations. Company leaders instead focused on what CTO Larry Ellison called \u201chypergrowth\u201d in its cloud and AI projects, saying they expect double-digit growth to continue.\n\nAdvertisement\n\nSouthwest Airlines\n\nSouthwest Airlines (LUV+2.15% ) stock jumped about 9% after the carrier said it\u2019ll abandon its free-bags policy for most passengers, which analysts said will boost revenue but may harm customer loyalty.\n\nAdvertisement\n\nViking Holdings\n\nViking Holdings fell about 5.5% after reporting earnings. The company said it sees no sign of travel demand abating, with record-breaking bookings for its luxury river cruises, Bloomberg reported.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "History shows Nvidia's conference next week could be a catalyst to turn the stock around",
            "link": "https://www.cnbc.com/2025/03/11/history-shows-nvidias-conference-next-week-could-be-a-catalyst-to-turn-the-stock-around.html",
            "snippet": "Nvidia typically outperforms peers in the week of and period directly following the GTC.",
            "score": 0.666651725769043,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia could see a recovery rally next week if history repeats itself, according to Wells Fargo. The chipmaker is hosting its GPU Technology Conference, known as the GTC, next week. Shares have typically outperformed those of peers in the timeframe including this conference, Wells Fargo data shows. That's especially important as the artificial intelligence darling and bull market leader has pulled back following back-to-back years of monster gains. Shares of the retail investor favorite have tumbled 20% so far in 2025 after surging more than 238% in 2023 and 171% 2024. Nvidia is also a relative underperformer against the iShares Semiconductor ETF (SOXX) , which has shed 10% year to date. That could change with the conference. Historical data from Wells Fargo found that Nvidia has outperformed the SOXX by 6.5 percentage points on average during the week of the GTC over the past five years. Nvidia has also historically been able to keep up that outperformance following the conference. In the two weeks following the conference, Nvidia has eclipsed the exchange-traded fund by 3.8 percentage points on average going back half a decade. For Nvidia, that equates to absolute average returns of 7% during conference week and 5.5% in the two weeks after. Those gains didn't always hold up when looking one month out, Wells Fargo data shows. The average return for Nvidia a month after the conference is -1.6%. The stock also lags the SOXX fund by 2.9 percentage points after 30 days. NVDA SOXX 1Y mountain NVDA vs. SOXX, 1-year Analyst Aaron Rakers also pointed out that the stock is entering GTC week with shares at a valuation discount of around 35% relative to its median, forward price-to-earnings multiple over three years and enterprise value-to-EBIT ratio. \"We maintain our positive views on NVDA's broadening platform strategy and would be buying NVDA ahead of next week,\" he wrote to clients.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-10": {
        "0": {
            "title": "Utah to Advance AI Education, Training",
            "link": "https://blogs.nvidia.com/blog/utah-ai-education/",
            "snippet": "A new AI education initiative in the State of Utah, developed with NVIDIA, is set to advance the state's commitment to workforce training and economic...",
            "score": 0.863713800907135,
            "sentiment": null,
            "probability": null,
            "content": "A new AI education initiative in the State of Utah, developed in collaboration with NVIDIA, is set to advance the state\u2019s commitment to workforce training and economic growth.\n\nThe public-private partnership aims to equip universities, community colleges and adult education programs across Utah with the resources to develop skills in generative AI.\n\n\u201cAI will continue to grow in importance, affecting every sector of Utah\u2019s economy,\u201d said Spencer Cox, governor of Utah. \u201cWe need to prepare our students and faculty for this revolution. Working with NVIDIA is an ideal path to help ensure that Utah is positioned for AI growth in the near and long term.\u201d\n\nAs part of the new initiative, Utah\u2019s educators can gain certification through the NVIDIA Deep Learning Institute University Ambassador Program. The program offers high-quality teaching kits, extensive workshop content and access to NVIDIA GPU-accelerated workstations in the cloud.\n\nBy empowering educators with the latest AI skills and technologies, the initiative seeks to create a competitive advantage for Utah\u2019s entire higher education system.\n\n\u201cWe believe that AI education is more than a pathway to innovation \u2014 it\u2019s a foundation for solving some of the world\u2019s most pressing challenges,\u201d said Manish Parashar, director of the University of Utah Scientific Computing and Imaging (SCI) Institute, which leads the One-U Responsible AI Initiative. \u201cBy equipping students and researchers with the tools to explore, understand and create with AI, we empower them to be able to drive advancements in medicine, engineering and beyond.\u201d\n\nThe initiative will begin with the Utah System of Higher Education (USHE) and several other universities in the state, including the University of Utah, Utah State University, Utah Valley University, Weber State University, Utah Tech University, Southern Utah University, Snow College and Salt Lake Community College.\n\nSetting Up Students and Professionals for Success\n\nThe Utah AI education initiative will benefit students entering the job market and working professionals by helping them expand their skill sets beyond community college or adult education courses.\n\nUtah state agencies are exploring how internship and apprenticeship programs can offer students hands-on experience with AI skills, helping bridge the gap between education and industry needs. This initiative aligns with Utah\u2019s broader goals of fostering a tech-savvy workforce and positioning the state as a leader in AI innovation and application.\n\nAs AI continues to evolve and gain prevalence across industries, Utah\u2019s proactive approach to equipping educators and students with resources and training will help prepare its workforce for the future of technology, sharpening its competitive edge.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Price Target Slashed As AI Stocks Swoon. 'No One Knows What's Going On,' Says Analyst.",
            "link": "https://www.investors.com/news/technology/nvidia-stock-price-target-cut-ai-stocks-swoon/",
            "snippet": "A Wall Street analyst cut his price target on Nvidia stock on Monday, citing \"a multitude of factors (that) have clouded the near term.\"",
            "score": 0.9549975991249084,
            "sentiment": null,
            "probability": null,
            "content": "A Wall Street analyst cut his price target on Nvidia (NVDA) stock on Monday, citing \"a multitude of factors (that) have clouded the near term.\"\n\nMelius Research analyst Ben Reitzes reiterated his buy rating on Nvidia stock but slashed his two-year price target to 170 from 195.\n\nOn the stock market today, Nvidia stock fell 5.1% to close at 106.98.\n\nNvidia and other AI stocks have been pressured by \"potential tariffs, regulations and bans (and) innovations that make computing cheaper,\" Reitzes said in a client note. \"As a result, AI semis and hardware stocks are trading like no one knows what's going on \u2014 including shares of Nvidia.\"\n\nThe next potential catalyst for Nvidia stock will be the company's GTC conference next week in San Jose, Calif. Nvidia Chief Executive Jensen Huang will give a keynote speech on March 18.\n\nHuang might be able to calm investors by focusing on the continued potential for artificial intelligence, Reitzes said. Areas of great promise include AI agents that could replace \"overpriced seat-based SaaS\" (software as a service), autonomous vehicles, and robots to augment the workforce. AI also could find cures for diseases and cancer, he said.\n\nNvidia Stock, Others 'On Sale'\n\nAt GTC, Nvidia is likely to detail its upcoming Blackwell Ultra graphics processing unit (GPU) and GB300 system, due out later this year. It also could preview its Rubin GPU and Arm (ARM)-based central processing unit (CPU) called Vera, which are scheduled for 2026.\n\n\"At this point, we believe Nvidia and several others in the AI semis and hardware space are on sale and good buys right now,\" Reitzes said. \"It doesn't mean that the stocks will work in the very near-term since there may not be visibility on key issues regarding regulations and geopolitics including tariffs.\"\n\nReitzes on Monday also trimmed his price targets on buy-rated Apple (AAPL), Arista Networks (ANET), Cisco (CSCO), Dell Technologies (DELL) and Marvell Technology (MRVL).\n\nNvidia stock is on the IBD Tech Leaders list along with Apple, Arista and Marvell.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nBroadcom Rallies As Chipmaker Shows AI Demand Remains Healthy\n\nNvidia's CEO Said The 'Robotics Era' Is Coming. This Company Looks To Lead It.\n\nSee Stocks On The List Of Leaders Near A Buy Point\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Don\u2019t Go All In on Nvidia Stock. Here\u2019s How to Play the AI Trade Now.",
            "link": "https://www.barrons.com/articles/ai-stocks-selloff-nvidia-buy-cc990671",
            "snippet": "The best strategy for buying the dip in artificial-intelligence stocks isn't going all in on Nvidia. It's spreading your bets among a bunch of AI-exposed...",
            "score": 0.9171596169471741,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Why Nvidia (NVDA) Stock Is Nosediving",
            "link": "https://finance.yahoo.com/news/why-nvidia-nvda-stock-nosediving-180752636.html",
            "snippet": "Shares of leading designer of graphics chips Nvidia (NASDAQ:NVDA) fell 5.4% in the morning session after markets tumbled, extending the weakness from the...",
            "score": 0.7170435786247253,
            "sentiment": null,
            "probability": null,
            "content": "Why Nvidia (NVDA) Stock Is Nosediving\n\nWhat Happened?\n\nShares of leading designer of graphics chips Nvidia (NASDAQ:NVDA) fell 5.4% in the morning session after markets tumbled, extending the weakness from the previous week as concerns over the ongoing trade war continued to spread. On Sunday, March 9, 2025, President Trump fielded questions regarding recession worries on FOX News, calling the market struggle \"a period of transition,\" but that didn't do much to calm investors. The sell-off was particularly pronounced in the tech sector, with the Nasdaq falling 3.5% into correction territory, while the S&P 500 also posted a 2.7% decline.\n\nThe shares closed the day at $107.02, down 5.1% from previous close.\n\nThe stock market overreacts to news, and big price drops can present good opportunities to buy high-quality stocks. Is now the time to buy Nvidia? Access our full analysis report here, it\u2019s free.\n\nWhat The Market Is Telling Us\n\nNvidia\u2019s shares are very volatile and have had 28 moves greater than 5% over the last year. In that context, today\u2019s move indicates the market considers this news meaningful but not something that would fundamentally change its perception of the business.\n\nThe previous big move we wrote about was 7 days ago when the stock dropped 7.9% as market volatility continued, with the stock seemingly affected by the broader downturn as the Nasdaq declined 1.3% in another negative session. Tepid economic manufacturing and construction data sparked another wall of worry about the US economy.\n\nAdditionally, investors might have been concerned about the company's business in China, which hadbeen tangled up in the trade war drama. Notably, the Wall Street Journal reported that some Chinese buyers were finding ways around export restrictions to get their hands on Nvidia chips. That kind of news could get regulators thinking about tighter measures, which could further limit Nvidia's business in China.\n\nNvidia is down 22.2% since the beginning of the year, and at $107.62 per share, it is trading 28% below its 52-week high of $149.43 from January 2025. Investors who bought $1,000 worth of Nvidia\u2019s shares 5 years ago would now be looking at an investment worth $16,488.\n\nToday\u2019s young investors likely haven\u2019t read the timeless lessons in Gorilla Game: Picking Winners In High Technology because it was written more than 20 years ago when Microsoft and Apple were first establishing their supremacy. But if we apply the same principles, then enterprise software stocks leveraging their own generative AI capabilities may well be the Gorillas of the future. So, in that spirit, we are excited to present our Special Free Report on a profitable, fast-growing enterprise software stock that is already riding the automation wave and looking to catch the generative AI next.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia's 30% sell-off makes it cheaper than when ChatGPT launched \u2014 at least by one metric",
            "link": "https://www.businessinsider.com/nvidia-stock-price-correction-plunges-valuation-metric-levels-before-chatgpt-2025-3",
            "snippet": "A closely watched valuation indicator for Nvidia has fallen to levels not seen since before the launch of ChatGPT. Nvidia's trailing 12-month...",
            "score": 0.6347585916519165,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's price-to-earnings ratio is now lower than when ChatGPT launched in 2022.\n\nThe decline comes amid a broad decline in semiconductor stocks. Nvidia shares are down 30% from their 52-week high.\n\nMelius Research remains optimistic on Nvidia ahead of its conference next week.\n\nA closely watched valuation indicator for Nvidia has fallen to levels not seen since before the launch of ChatGPT.\n\nNvidia's trailing 12-month price-to-earnings ratio dropped to 36.4 times on Monday, according to data from YCharts. The decline has been driven by a sharp price decline in Nvidia's stock, which finished Monday down 30% from its intraday record high reached in early January.\n\nThe valuation multiple decline makes Nvidia cheaper today than it was when ChatGPT was first released on November 30, 2022. The stock is trading at its cheapest level since August 2019.\n\nBen Reitzes, managing director at Melius Research, said the valuation dynamic for Nvidia also holds true on a forward P/E ratio, which incorporates Wall Street's year-ahead earnings estimates for the company.\n\nAccording to Reitzes, with a forward P/E ratio of 24 times, Nvidia stock is 41% cheaper than it was when ChatGPT launched.\n\nMelius Research\n\nIt's an impressive feat given that Nvidia shares have soared 583% since OpenAI released ChatGPT to the masses.\n\nIn this case, the stock-price surge was backed up by profits, as Nvidia's GPU business boomed due to strong demand from AI companies. Nvidia's net income in the fiscal year 2025 soared 788% compared to the fiscal year 2023, which ended in January 2023.\n\nReitzes believes there's a valuation disconnect and said in a note on Monday that the risks have largely priced into semiconductor stocks like Nvidia.\n\n\"We remain very optimistic,\" Reitzes said.\n\nPart of the optimism stems from Nvidia's GPU Technology Conference next week, where CEO Jensen Huang is expected to assure investors that the AI boom remains alive and well and unveil its product roadmap into 2027, according to Reitzes.\n\nAnother reason Reitzes is staying bullish on Nvidia is because he's seen this valuation compression play out before in shares of a technology stalwart: Apple.\n\n\"A similar thing happened to Apple actually a long time ago, when its forward multiple went from 33 times on the day it announced the iPhone in 2008 and shrank to 15 times by the end of 2008 in the crisis,\" Reitzes said.\n\n\"As you know, the mobile trend didn't end then \u2014 and Apple now trades at 31x earnings on much larger numbers. If Nvidia duplicates its own version of this industry stewardship, we could look back at this period of uncertainty and have a good chuckle,\" he added.\n\nMelius Research rates Nvidia at \"Buy\" with a $170 price target, representing potential upside of 60% from current levels.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Utah, NVIDIA Team Up for AI Upskilling in Education, Workforce",
            "link": "https://www.govtech.com/education/higher-ed/utah-nvidia-team-up-for-ai-upskilling-in-education-workforce",
            "snippet": "NVIDIA is lending teaching materials and upskilling opportunities in artificial intelligence to Utah's higher education students and state workers in a new...",
            "score": 0.6168021559715271,
            "sentiment": null,
            "probability": null,
            "content": "Utah higher education institutions and state agencies will work with NVIDIA on workforce training and economic development opportunities around artificial intelligence, partners in the initiative announced today. NVIDIA , the Governor\u2019s Office of Economic Opportunity (GOEO), the Utah System of Higher Education (UHSE) and legislative leaders, among others, signed a memorandum of understanding with plans to upskill educators, integrate new technologies in the classroom and create AI-related apprenticeships in state agencies.USHE and several universities \u2014 the University of Utah, Utah State University, Utah Tech University, Southern Utah University and Salt Lake Community College, to name a few \u2014 will be involved in the partnership to start, according to NVIDIA. Their instructors will have access to NVIDIA\u2019s Deep Learning Institute, where they can earn certifications in areas like generative AI, data science and accelerated computing. NVIDIA will also provide teaching materials, some of which are fully developed courses housed on Canvas, to help university teachers incorporate graphics computing into their instruction.\u201cInstructors will be able to adapt their teaching with real-world industry insights, and students will work with AI in practical ways, from analyzing data to automating tasks and developing new technology,\u201d Geoffrey Landward, commissioner of higher education at UHSE, said in a public statement . \u201cBeyond the classroom, this partnership will equip Utah graduates with the skills, certifications and experience needed to excel in high-demand careers in an evolving job market full of opportunities.\u201dBoth Utah Gov. Spencer Cox and NVIDIA leader Greg Estes called AI revolutionary and emphasized the need to prepare Utahns for that sea change, especially in the workforce. State agencies will develop internships and apprenticeships for students to gain hands-on experience, according to news releases from participating institutions.This is not Utah's first statewide initiative around AI. Last year, the Department of Commerce established an Office of AI Policy , and the Division of Technology Service has an AI program to improve government operations, led by Chief Information Officer Alan Fuller As an early adopter of AI initiatives, Utah and its higher education system followed California, which formed a similar partnership with NVIDIA in August 2024.\u201cThis collaboration with NVIDIA reinforces Utah\u2019s leadership in the AI and technology space, unlocking new opportunities that will influence the next generation of tech leaders,\u201d Ryan Starks, executive director of the GOEO, said in a public statement . \u201cInvesting in technology is investing in Utah\u2019s future \u2014 our students, workforce and continued prosperity.\u201d",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia RTX 5050, RTX 5060, and RTX 5060 Ti specs leak \u2014 expect 8GB/16GB flavors and higher TGPs",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-rtx-5050-rtx-5060-and-rtx-5060-ti-specs-leak-expect-8gb-16gb-flavors-and-higher-tgps",
            "snippet": "Specifications revealed by Kopite suggest that Nvidia's budget RTX 5060 and RTX 5050 GPUs will be limited to just 8GB of memory.",
            "score": 0.928987979888916,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia GeForce RTX 5050 and RTX 5060 series GPUs leaks, via renowned industry insider Kopite, reveal minimal changes across the board, especially in the VRAM department. Earlier today, the leaker spilled the beans surrounding Nvidia's upcoming budget GPU offerings in a series of tweets. From the looks of it, after three long years we're finally getting another 50-class GPU from Nvidia, but there isn't much to show for it.\n\nAs shown by the Steam Hardware Survey, Nvidia's 60-class GPUs are extremely popular among gamers, with flagship-grade SKUs nowhere in sight. While it's hard to estimate how much Nvidia rakes in from the budget segment, it undoubtedly plays a key role in shaping mind share and consumer sentiment. The cheapest GPU in the top-five list goes for $300. This speaks volumes about how much the average user is willing to spend on their graphics card.\n\nAs per their usual leaks, Kopite has detailed specifications of Nvidia's soon-to-be-announced budget GPUs, likely at GTC in a few days. The RTX 5060 Ti reportedly carries the GB206-300-A1 GPU core, with 4,608 CUDA cores (36 SMs) and a 128-bit memory interface. Going by rumored data from the same leaker, this should be a fully enabled GB206 die, coming in 8GB and 16GB (clamshell) flavors with a 180W TGP (Total Graphics Power). With 75W from the PCIe slot, the 5060 Ti falls comfortably within range of a single 8-pin connector (150W), even for some custom variants.\n\nSwipe to scroll horizontally GPU Name RTX 5060 Ti RTX 4060 Ti RTX 5060 RTX 4060 RTX 5050 Family Blackwell Ada Lovelace Blackwell Ada Lovelace Blackwell Board Name PG152-SKU10/15 PG190-SKU361 PG152-SKU25 PG173-SKU371 PG152-SKU50 GPU Core GB206-300-A1 AD106-350-A1 GB206-250-A1 AD107-400-A1 GB207-300-A1 CUDA Cores 4,608 4,352 3,840 3,072 2,560 SMs 36 34 30 24 20 Bus Width 128-bit 128-bit 128-bit 128-bit 128-bit Memory 16GB/8GB GDDR7 16GB/8GB GDDR6 8GB GDDR7 8GB GDDR6 8GB GDDR6 TGP 180W 165W/160W 150W 115W 130W\n\nThe vanilla RTX 5060 only purportedly offers an 8GB configuration, but that's expected since all these GPUs are limited to a 128-bit bus. With 3,840 CUDA cores (30 SMs), it is expected to be built using binned GB206 dies (GB206-250-A1) that otherwise did not qualify for the more powerful RTX 5060 Ti. The TGP has been upped to 150W from 115W, though you could tune that with a pinch of undervolting if needed. Nvidia might be able to extract a decent uplift versus the RTX 4060 considering the 25% more CUDA cores, 30% higher TGP, and other architectural refinements.\n\nLastly, the budget RTX 5050 drops to the entry-level GB207, with a fully functional die provided Kopite's data is solid. The GB207 design limits it to just 20 SMs or 2,560 CUDA cores. Assuming a 10% architectural uplift from Ada Lovelace to Blackwell, normalized for core counts and frequencies, the RTX 5050 might struggle against the RTX 4060 in certain scenarios. In any case, we'd love to see this GPU hit shelves for under $200, but let's not get ahead of ourselves.\n\nNvidia's GTC runs from March 17 to 21, where we can expect to learn more about budget Blackwell among other AI, data science, and robotics related developments.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Apple and Nvidia led Monday\u2019s $750 billion tech wreck. How to play the aftermath.",
            "link": "https://www.marketwatch.com/story/apple-and-nvidia-led-mondays-750-billion-tech-wreck-how-to-play-the-aftermath-66604244",
            "snippet": "Trump's policy moves are causing angst. But one analyst says the broader AI thesis is still intact, and he names Tesla and Microsoft among big potential...",
            "score": 0.9299689531326294,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nasdaq Correction: Time to Buy the Dip on Nvidia?",
            "link": "https://www.fool.com/investing/2025/03/10/nasdaq-correction-time-to-buy-the-dip-on-nvidia/",
            "snippet": "The Nasdaq Composite (^IXIC 2.61%) has moved into correction territory (down at least 10% from all-time highs). A significant contributor to that drop has...",
            "score": 0.7745527625083923,
            "sentiment": null,
            "probability": null,
            "content": "The Nasdaq Composite (^IXIC 2.61%) has moved into correction territory (down at least 10% from all-time highs). A significant contributor to that drop has been Nvidia (NVDA 5.27%) stock, which is down about 20% year to date, as of this writing. The chipmaker reported outstanding results recently, but investor concerns about tariffs and the U.S. suddenly looking like it could be headed toward a recession have rattled the market in the near term.\n\nFor investors interested in Nvidia who have a long-term mindset, this price correction presents a great opportunity to scoop up Nvidia shares on the cheap. Let's look at three reasons why the stock is a must-buy for the long term on this dip.\n\n1. Nvidia is the artificial intelligence infrastructure leader\n\nWith an approximate 90% market share for graphics processing units (GPUs), Nvidia is the dominant leader among the chip designers that are powering the artificial intelligence (AI) infrastructure buildout. While originally designed to speed up graphics rendering in video games, Nvidia's GPUs' fast processing times have made them ideal for helping train large language models (LLMs) and running inference for AI.\n\nIn addition, the company's CUDA (Compute Unified Device Architecture) platform has helped create a wide moat for the company. Nvidia became the first company to allow GPUs to be programmed for tasks outside their original purpose back in 2006 through CUDA. As such, developers learned to program GPUs using Nvidia's software platform.\n\nRival Advanced Micro Devices (AMD 2.92%) didn't introduce its ROCm (Radeon Open Compute) software platform until about 10 years later in 2016. Meanwhile, through CUDA-X, which was built on top of CUDA, Nvidia now has a full software stack comprised of libraries, microservices, and tools designed to accelerate applications in the areas of AI and high-performance computing.\n\nCUDA and CUDA-X continue to be the primary reason for Nvidia's dominance, particularly in AI model training. In a recent study, semiconductor research outfit Semianalysis found AMD's latest GPUs unusable for AI training out of the box due to software bugs, while praising Nvidia's chips. As such, Nvidia remains the company best positioned to continue to benefit from AI infrastructure growth.\n\n2. AI data center infrastructure continues to grow\n\nDespite Chinese AI company DeepSeek's claims of building an effective AI model cheaply (how effective it actually is is in dispute), the best-known way to advance AI models currently is through brute compute-power force. This means building out systems with more and more AI chip clusters.\n\nRecent AI model iterations have needed exponentially more GPU chips to be trained on than their predecessors. For example, Meta Platform's Llama 4 LLM needed 10x as many GPUs to be trained on than Llama 3. Elon Musk-backed xAI, meanwhile, originally used 5x the GPUs (100,000) to build out its Grok 3 model before bumping it up to 200,000 GPUs.\n\nMeanwhile, cloud computing companies, along with other tech companies, are pouring money into building AI data centers. The big three cloud computing companies plan to spend a combined $255 billion building out AI data centers this year to help keep up with demand.\n\nCloud computing is an infrastructure-as-a-service platform, and customers have been using these services to help customize and build their own AI models and applications. Meanwhile, Meta plans to spend up to $65 billion in capital expenditures (capex) this year largely aimed at expanding its AI infrastructure, while a consortium led by OpenAI and Softbank have pledged to spend $500 billion over the next few years building out AI data centers in the U.S. through Project Stargate.\n\nThis all points to a lot of continued future spending on AI infrastructure in the years ahead. As such, Nvidia remains well-positioned to continue to grow.\n\n3. Nvidia stock is inexpensive\n\nThe third big reason to own Nvidia is that its stock remains attractively priced. The stock currently trades at a forward price-to-earnings ratio (P/E) of 24 times 2025 analyst estimates and a price/earnings-to-growth ratio (PEG) of below 0.5. PEG ratios under 1 typically indicate a stock is undervalued, and growth stocks will often trade at PEGs well above 1.\n\nNvidia isn't a software-as-a-service (SaaS) company with a recurring and predictable revenue stream, so it isn't going to command the same type of valuation multiple as these types of companies -- nor should it. However, its current valuation is inexpensive, given that we still appear to be in the early days of AI and that AI infrastructure spending will continue to rise. As such, the recent pullback looks like a solid buying opportunity in the stock long term.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Investigating NVIDIA\u2019s Defective GPUs: RTX 5080 Missing ROPs Benchmarks",
            "link": "https://gamersnexus.net/gpus/investigating-nvidias-defective-gpus-rtx-5080-missing-rops-benchmarks",
            "snippet": "GPUs Investigating NVIDIA's Defective GPUs: RTX 5080 Missing ROPs Benchmarks March 10, 2025 Last Updated: 2025-03-10 We take a look at an RTX 5080 with...",
            "score": 0.9153763055801392,
            "sentiment": null,
            "probability": null,
            "content": "We traded a functional Zotac 5080 (read our review) to our viewer, Mason, for his defective 5080. Thanks for the trade, Mason.\n\nWe have confirmed and validated that this GPU is missing 8 of its ROPs, down at 104 ROPs from the expected 112 ROPs. That means the proper card has nearly 8% more ROPs than the defect.\n\nThis is a huge problem. There are no obvious signs that this issue is present without knowing to look for it, which screws mainstream owners. There will absolutely be defective units out there without people knowing, and we don\u2019t think NVIDIA has done enough to draw attention to this issue. You\u2019d have to know to launch GPU-Z, then know to check the ROPs, then recognize that the count is wrong. That knowledge and skill will be way more common for this audience than most, but even then, most people aren\u2019t going to feel a need to validate the GPU they bought has each individual fixed function unit present.\n\nNVIDIA needs to do better about notifying customers. NVIDIA may claim 0.5%, but from first-hand experience in our inbox alone, we have a hard time believing the count is that low -- especially since it didn\u2019t name the 5080. Either NVIDIA didn\u2019t know about the 5080, in which case it\u2019s wrong about the defect rate, or it did know and it was disingenuous at best by leaving it out. We\u2019re not sure which is worse. We\u2019ve received dozens of emails and messages about units deficient in ROPs count, which seems awfully high for a focused audience with seemingly low distribution of the card so far.\n\nThat\u2019s the backstory. Let\u2019s get into the testing.\n\nA \u201cROP\u201d is a raster operations pipeline (or render output unit) and is a core part of the GPU.\n\nWe already explained this in-depth in a video about this topic. Here\u2019s the basics: The NVIDIA Blackwell architecture for gaming GPUs looks like this block diagram at most.\n\nEach GPC has 8 TPCs, with two groups of 8 ROPs assigned separately to groupings of 4 TPCs. In some 5090, 5070 Ti, and now 5080s, one of these banks of 8 ROPs appears to be disabled, or at least not functioning.\n\nAs we said before, the weird thing is that this shouldn\u2019t have been possible on the 5080. The 5080 should be a full GB203 die, and so there are no disabled SMs on a GB203 RTX 5080 unlike a 5090 or a 5070 Ti where some of the stuff is turned off and there could be collateral damage under traditional understanding.\n\n\n\n\n\nMaybe some poor TSMC or NVIDIA employee knocked over a coffee mug and hit the KILLROPS.EXE key on the 5080 production run or something. Whatever the case is, it has an impact.\n\nOver the years, we\u2019ve seen GPUs with a ROPs advantage and iso other conditions typically show their benefit in higher resolution scenarios or in heavily anti-aliased testing. ROPs perform some of the final stages in the rendering pipeline. Of the 3 resolutions we test, 4K will show the biggest impact, but scenes with a lot of blending or some types of anti-aliasing will also reflect the change.\n\nNVIDIA acknowledged this issue shortly after it emerged, but only for the 5070 Ti (read our review) and 5090 (read our review). It almost immediately somehow knew exactly how many units were affected, which further reinforces our belief that NVIDIA would have known about this, with the company pinning it to 0.5% of units. The company did not name the 5080 -- it also didn\u2019t really apologize and we felt it downplayed the performance impact to the lowest number, which would be 4% on the 5090. It didn\u2019t mention that the 5070 Ti and 5080 impact would be greater.\n\nAfter Mason\u2019s card showed up on Reddit, NVIDIA issued a second statement that we\u2019re going to call their \u201coopsies\u201d statement, where they confirmed the 5080 was also affected.\n\nRTX 5080 Missing ROPs Benchmarks\n\nGrab a GN15 Large Anti-Static Modmat to celebrate our 15th Anniversary and for a high-quality PC building work surface. The Modmat features useful PC building diagrams and is anti-static conductive. Purchases directly fund our work! (or consider a direct donation or a Patreon contribution!)\n\nLet\u2019s get into some simple performance numbers. We only really need two basics:\n\n1 - The impact to performance against the normal 5080, which can be done in a simple A/B chart\n\n2 - The change in relative positioning versus nearby alternatives\n\nWe\u2019re going to keep the charts really focused and simple because we don\u2019t need much to show the evidence of performance impact.\n\nPerformance Recap: 4K Raster\n\nThis chart shows the head-to-head in average FPS for the two 5080 cards. Some games are almost exactly identical: Baldur\u2019s Gate 3 predictably is CPU-bound, but it\u2019s nice to know that a CPU-bound scenario didn\u2019t force a gap. Black Myth: Wukong was remarkably consistent and Final Fantasy 14 was within 1 FPS for this testing, but there are some differences.\n\nTotal War: Warhammer 3 is the most concerning of these. This one has always rooted-out the most erratic behaviors in testing and that\u2019s why we keep it around. Across all 3 resolutions, we saw major swings. At 4K, we observed an 11% improvement with the actual RTX 5080 rather than the deficient one. That is a difference as big as the gap between some of NVIDIA\u2019s models entirely.\n\nDying Light 2 also consistently showed a gap: The full 5080 ran 8.7% higher framerate for average FPS than the deficient one. F1 24 showed a 3.3% improvement with all ROPs, with Resident Evil 4 at 1.6%, which is outside our run-to-run variance and makes it a real result, and Starfield at 2.3%.\n\nPerformance Recap: 1440p Raster\n\nAt 1440p, we saw an 8.8% improvement with all ROPs in Dying Light 2, which matches our 4K results. Final Fantasy is at about 2%, Dragon\u2019s Dogma 2 is at 2.5%, and F1 24 is at 0.8%.\n\nComparative Charts\n\nLet\u2019s look at how this impacts the relative ranking versus other cards. We\u2019ll look at only the games with the largest impact for a worst-case scenario.\n\nTotal War: Warhammer - 4K\n\nHere\u2019s Total Warhammer III result at 4K. The 5080 \u201cReduction of Performance\u201d variant ran at 82 FPS AVG, a significant reduction from the correct result of 91 FPS AVG. Before, the 5080 was tied with the 7900 XTX and within error. Now, the 7900 XTX outperforms the 5080 by 12%. That is a huge swing and makes the 7900 XTX significantly better value. Sure, the partners might help you replace a defective model; however, that\u2019d require noticing it.\n\nThe gap over the not-ROPs-deficient 5070 Ti is also reduced to nothing. This particular title and the way we test it is highly reactive to this defect.\n\nDragon\u2019s Dogma 2 - 4K\n\nIn Dragon\u2019s Dogma 2, the 5080 Special Edition landed between the stock model and the 7900 XTX, cutting the gap in half. This significantly harms the value of the RTX 5080. The lead is reduced from 10% to 5%. Literally halved. The lead over the 5070 Ti is also cut, now 9.4% from 15%.\n\nDragon\u2019s Dogma 2 - 1080p\n\n1080p shows the 5080 Regression of Performance edition at 157.1 FPS AVG from 165, which reduces it to equal the 4080 (watch our review). Before, they were functionally equal. Now, they\u2019re literally equal. The 5080\u2019s lead over the 5070 Ti was 9%. Now it\u2019s 3.8%. It was cut into a third of the benefit, basically. The 7900 XTX now is nearly within run-to-run variance of the 5080 defect.\n\nDying Light 2 - 4K\n\nWe\u2019ll just look at one more. In Dying Light 2 at 4K, the RTX 5080 normal card ran at 81 FPS AVG, with the 5080 ROPs defect card at 74.5 FPS AVG. The 5080 was 11.7% ahead of the 7900 XTX, but is now only 2.8% ahead.\n\nConclusion\n\nVisit our Patreon page to contribute a few dollars toward this website's operation (or consider a direct donation or buying something from our GN Store!) Additionally, when you purchase through links to retailers on our site, we may earn a small affiliate commission.\n\nIf you had bought the 5080 instead of the 7900 XTX because of the expectation and ended up with a defect, this is a big problem because now the ranking shuffles. If you don\u2019t notice and you keep using the defective device, you\u2019re going to get screwed. You\u2019ll be stuck with something worse than you thought.\n\nChecking for this is really easy. When you buy a 50 series card, the first thing you should do is check for all of the ROPs to make sure that your card is not affected.\n\nYou have to do a clean install of the drivers. If you check GPU-Z without the drivers installed, it will reference a look-up table and tell you the correct amount even if they\u2019re not present. So you need to install the drivers first and then install the latest version of GPU-Z and look for ROPs.\n\nThen look at the image above to see how many ROPs should be present.\n\nIf what you have differs from what it should be, you absolutely need to seek a refund or replacement, but we\u2019d encourage a refund as it\u2019s the fastest path.\n\nThere is absolutely a performance impact and NVIDIA\u2019s approach of \u201cjust reach out and we\u2019ll make it right\u201d is completely unacceptable. They also left out the 5080. The company deserves to get raked over the coals for this. Most users will not notice this.\n\nNVIDIA originally said the problem was with 1 ROP instead of 8 ROPs.\n\nWe did take apart the card and observed no physical difference to the die with the text looking the same. It also has the same branding.\n\nThis puts a cap on what has been an utter disaster of a launch for NVIDIA. One or two mistakes is understandable, but the totality of these mistakes is insane, especially for the prices they are going for.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-09": {
        "0": {
            "title": "This Nvidia-Backed IPO Grew 737% Last Year and Is About to Go Public: What Investors Should Know About CoreWeave",
            "link": "https://www.fool.com/investing/2025/03/09/this-nvidia-backed-ipo-grew-737-last-year-and-is-a/",
            "snippet": "There haven't been a ton of initial public offerings (IPOs) in recent years, let alone interesting, prominent companies with the potential to take on the...",
            "score": 0.9043609499931335,
            "sentiment": null,
            "probability": null,
            "content": "There haven't been a ton of initial public offerings (IPOs) in recent years, let alone interesting, prominent companies with the potential to take on the top tech stocks in the market. However, one company right in the middle of the artificial intelligence (AI) revolution may be about to make its public debut. Not only is this stock growing at eye-popping levels, but it also counts Nvidia (NVDA 5.27%) as a big investor and Microsoft (MSFT 2.58%) as a huge customer.\n\nHere's what you need to know about CoreWeave ahead of its IPO.\n\nA \"Neocloud\" backed by Nvidia\n\nCoreWeave, which plans to list on the Nasdaq under the symbol CRWV, finds itself at the center of the AI revolution. However, the company didn't actually start out that way. In 2017, CoreWeave was founded by three executives at Hudson Ridge Asset Management, a natural-gas-focused hedge fund, with the original mission of mining cryptocurrencies.\n\nThe experience with cryptocurrencies honed CoreWeave's skills in deploying Nvidia graphics processing units (GPUs) -- which were used to mine crypto -- and managing energy-intensive computing clusters. Those skill sets also turned out to be incredibly important in AI computing.\n\nIn 2020, the company pivoted to build the CoreWeave Cloud Platform, and in April 2023, Nvidia invested in the company. Today, the AI chip giant owns just over 5% of the stock. In addition, Nvidia is a customer and likely uses CoreWeave to run its software offerings and perhaps test AI applications.\n\nThe Nvidia investment came at a very interesting time. As one may recall, May 2023 was the first blowout Nvidia earnings report, ushering in the hypergrowth stage of the AI buildout.\n\nCoreWeave's competitive advantages\n\nSome may wonder what exactly CoreWeave delivers that sets it apart from other major cloud infrastructure platforms. Reading through the Form S-1, it appears the company does a few things very well.\n\nOne key point CoreWeave emphasizes is that its clusters are built from the ground up as AI-optimized GPU clusters. This is in contrast to generalized cloud platforms that have to build both AI and traditional cloud infrastructures across their footprints.\n\nCoreWeave believes it has a key advantage running these clusters through its proprietary orchestration and observability software, which enable more efficient utilization of its GPUs. Since AI GPU workloads are massive, complex, and computationally intense, it is difficult to orchestrate an entire data center for multiple clients efficiently.\n\nTo put some numbers to it, CoreWeave describes a metric called the model FLOPS (MFLOPS) utilization rate, which basically measures the utilization rate of an AI cluster relative to its total theoretical compute capacity. The industry average may surprise you. Due to the complexities of AI, CoreWeave says a typical MFLOPS rate is just 35% to 45% industrywide. That gap between actual and theoretical performance is the opportunity CoreWeave believes it can narrow, largely through some novel software innovations.\n\nOne such software innovation is SUNK, a software platform that combines Kubernetes and Slurm open-source software platforms. Kubernetes is a platform for containerized workloads in traditional cloud environments, which AI customers also use to serve their models. Meanwhile, Slurm is a popular open-source software that orchestrates massive parallel computing workloads for AI training.\n\nCustomers have traditionally had to choose between one or the other for each compute cluster. However, CoreWeave's SUNK platform allows Slurm to run inside Kubernetes, enabling developers to use the best of both. That boosts the efficiency of compute utilization.\n\nA second software innovation is CoreWeave's Tensorizer, which is an optimization software for inference and training. For inference, Tensorizer can route a model in storage to the optimally close GPU node for the client. According to data cited in the S-1, this results in a faster load time than rivals HuggingFace and SafeTensors. For training, the Tensorizer can reduce training times through similar efficiency optimizations.\n\nIn addition to software, CoreWeave likely has a time-to-market advantage over other clouds. Because of Nvidia's investment, CoreWeave is likely at the front of the line, or very close to it, for the latest and greatest Nvidia GPUs. In its S-1, CoreWeave noted it was among the first to market with Nvidia H100 and H200 systems and was actually the first cloud to have Nvidia GB200 NVL72-based instances generally available more recently.\n\nFinancials show hypergrowth\n\nOf course, nothing speaks more to the positive aspects of CoreWeave than its financials. As one can see, the company has seen explosive growth over the past two years:\n\nCoreWeave 2022 2023 2024 Revenue $15.8 million $228.9 million $1,915.4 million Operating income ($22.9 million) ($14.5 million) $324.4 million Operating margin (145%) (6%) 17%\n\nThe big 2024 jump amounts to 737% growth, an incredibly high rate even for a so-called start-up. The flip to operating profitability is certainly promising as well. Of note, the past year's results were based on 32 deployed data centers hosting about 250,000 GPUs.\n\nWhile it's unclear at which valuation CoreWeave decides to go public, certain analysts estimate the company will seek to raise $3.5 billion to $4 billion at a $32 billion market cap. So, the stock will trade at roughly 16 times trailing revenue and 100 times operating income. But before the IPO, CoreWeave has $7.9 billion in debt and $1.4 billion in cash, making it a bit more expensive on an enterprise value basis.\n\nRisks may be high enough to keep some away\n\nAt first, CoreWeave may seem like the next big AI juggernaut. While its valuation is high on the surface, the stock valuation doesn't look so expensive, given the company's current growth rates and the long-term growth potential of generative AI.\n\nHowever, the composition of that growth may raise questions. In 2024, 62% of CoreWeave's revenue came from just one company: Microsoft. Microsoft has been renting spare GPU capacity from CoreWeave to supplement its Azure cloud, despite spending tens of billions of dollars annually on its own cloud infrastructure as well.\n\nOne might ask why Microsoft is such a big customer when the other major clouds, Amazon and Alphabet, aren't listed as major CoreWeave customers. This could be because Amazon and Alphabet have fairly mature custom ASIC programs themselves. Alphabet designed its own Tensor Processing Unit chips in 2015, and Amazon unveiled its Inferentia chip in 2019 and its Trainium AI chip in 2021.\n\nMicrosoft was late to the custom AI chip game but unveiled its Maia AI chip in November 2023, just a little over a year ago. It's unclear whether the lack of a custom ASIC is the total reason for Microsoft's high use of CoreWeave. After all, Microsoft may appreciate CoreWeave's ability to run data centers for a different reason. But that could also be a significant part of it.\n\nTherefore, if Microsoft ups its game and Maia matures to the level of Google TPUs or Amazon's AI chips, Microsoft may have less use for CoreWeave's infrastructure.\n\nMake no mistake, Nvidia GPUs are still in demand today and will likely be in the future. However, since Alphabet and Google can supplement certain workloads with their own chips, Microsoft's scaling of its own chips may free up a lot of dollars to buy Nvidia chips directly for its own data centers.\n\nRemember, cloud companies can buy custom ASICs at foundry prices, but Nvidia has gross margins in the mid-70% range. That means it's basically three to five times more expensive to buy an Nvidia GPU than to design one's own chip and buy it directly from a foundry.\n\nIn addition, thanks to Nvidia's investment, part of CoreWeave's appeal is likely early access to the most advanced Nvidia chips. Therefore, CoreWeave's destiny seems tied very tightly to Nvidia's going forward.\n\nOf course, that's a great place to be right now. But should the AI buildout slow or something happen with Nvidia's competitive position, that would affect CoreWeave significantly, too.\n\nCoreWeave is a fascinating company\n\nIn a volatile market, CoreWeave will likely prove to be a volatile and controversial stock when it becomes public. The filings show valid reasons for investors to buy into the IPO but also several big risks that will probably keep this investor on the sidelines, at least upon the IPO's unveiling.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Palantir Vs. Nvidia: Which Is The Better AI Stock Buy?",
            "link": "https://www.forbes.com/sites/investor-hub/article/palantir-vs-nvidia-which-is-the-better-ai-stock-buy/",
            "snippet": "Palantir vs. Nvidia: Which AI stock is the better buy? Compare growth potential, financials, and market positions to decide which tech giant fits your...",
            "score": 0.9170833230018616,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "NVIDIA (NVDA) Supplies 64,000 AI Chips for OpenAI-Oracle\u2019s Texas Data Center",
            "link": "https://finance.yahoo.com/news/nvidia-nvda-supplies-64-000-065248237.html",
            "snippet": "We recently published a list of Top 10 AI Stocks Dominating the Market Right Now. In this article, we are going to take a look at where NVIDIA Corporation...",
            "score": 0.7911457419395447,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of Top 10 AI Stocks Dominating the Market Right Now. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against other top AI stocks dominating the market right now.\n\nAttention is slowly shifting from US chip stocks to what analysts consider as the next big AI play: software. Investor enthusiasm has begun waning for semiconductor stocks due to tariff-driven volatility and the emergence of lower cost AI models from China\u2019s DeepSeek. The AI chips transition is now paving the way for the second stage of the innovation cycle where software companies will begin to monetize AI breakthroughs.\n\nREAD ALSO: 10 Hot AI News Updates Analysts are Monitoring and 10 High-Potential AI Stocks to Watch Right Now\n\n\u201cThe second stage of the innovation cycle is when people start utilizing products and that\u2019s when the software companies start getting paid \u2026 we\u2019re now starting to see the ascendancy of the software part of the equation.\u201d\n\nPreviously, software startups such as Harvey were dismissed by Silicon Valley investors. Investors used to take these software firms lightly, stating them as mere wrappers around OpenAI\u2019s models. However, now the narrative is shifting and these AI wrappers are becoming quite popular today.\n\n\u201cThe market\u2019s perception of companies like us\u2026 was that they\u2019re GPT wrappers, referencing a derisive term used to suggest the repackaging of OpenAI\u2019s models. If investors \u201cwere going to put money into something,\u201d he added, \u201cit needed to be into OpenAI or Anthropic.\u201d\n\nHere is what another investor has to say:\n\n\u201cJust like after the iPhone launched, there were millions of new mobile apps,\u201d said Mignano, an investor in the AI notetaking service Granola, which uses technology from OpenAI and Anthropic. \u201cNow with AI and LLMs, there will be millions of new AI products.\u201d\n\nDeepSeek has also played its due part in leading the shift from chip stocks to software and similar AI plays.\n\n\u201cInvestors are looking for the next three-to-five-year stories \u2026 those companies that are going to benefit from what Nvidia has already done.\u201d\n\nFor this article, we selected AI stocks by going through news articles, stock analysis, and press releases. These stocks are also popular among hedge funds. The hedge fund data is as of Q4 2024.\n\nWhy are we interested in the stocks that hedge funds pile into? The reason is simple: our research has shown that we can outperform the market by imitating the top stock picks of the best hedge funds. Our quarterly newsletter\u2019s strategy selects 14 small-cap and large-cap stocks every quarter and has returned 373.4% since May 2014, beating its benchmark by 218 percentage points (see more details here).",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia RTX 50 owners get another Hotfix, with 572.75 addressing crashes and clock speeds",
            "link": "https://www.tomshardware.com/pc-components/gpu-drivers/nvidia-rtx-50-owners-get-another-hotfix-with-572-75-addressing-crashes-and-clock-speeds",
            "snippet": "The launch of Nvidia's RTX 50-series GPUs has been marred with several issues, including shortages, overheating power connectors, and driver instability.",
            "score": 0.7886305451393127,
            "sentiment": null,
            "probability": null,
            "content": "The launch of Nvidia\u2019s RTX 50-series GPUs has been marred with several issues, including shortages, overheating power connectors, and driver instability. While the first two issues are harder to sort, the last one should be fixable via downloadable software updates. In fact, the company has already released several Hotfix versions, with the last one \u2014 version 572.65 \u2014 being released March 2. However, it seems that some issues (very similar) remain, so we just received another Hotfix update from Team Green. The GeForce Hotfix Driver Version 572.75 tackles two specific problems: the first one is some overclocked RTX 5090 and 5080 GPUs refuse to run at maximum frequency after a system reboot, while the second one fixes black screen crashes.\n\n(Image credit: Future)\n\nNvidia usually releases driver updates monthly; these Hotfixes are released out of schedule for problems that require priority solutions, like an unstable driver that causes BSODs and black screens across the wider community of RTX users. Note that these Hotfixes aren\u2019t usually automatically installed, so you\u2019ll have to find, download, and install them yourself. The company says, \u201cTo be sure, these Hotfix drivers are beta, optional and provided as-is. They are run through a much abbreviated QA process. The sole reason they exist is to get fixes out to you more quickly.\u201d Nevertheless, they will still be included with the next drop of Game Ready drivers in the Nvidia app. Even if you don\u2019t know that there\u2019s an issue with your RTX GPU (or are not affected), you\u2019ll still get the fix within the next few weeks.\n\nHardware drivers are inherently complicated pieces of software, especially as Nvidia must consider the huge number of configurations that its hardware will encounter in the field. Aside from that, it must also work flawlessly with thousands of apps and game titles, which means that it is next to impossible to test every possible permutation of hardware and software before it releases a driver. The company says, \u201cA GeForce driver is an incredibly complex piece of software. We have an army of software engineers constantly adding features and fixing bugs.\u201d\n\nIf you\u2019re experiencing a problem with your newly bought RTX 50-series GPU, maybe downloading a Hotfix would be enough to solve it. But since Hotfixes are essentially Beta versions of what\u2019s coming out in the regular driver update, you might run into another bug here and there. If that happens, you should report it to Nvidia\u2019s customer service \u2014 that way, it would have a chance of fixing what you\u2019re experiencing before the Hotfix gets a wider release as part of Nvidia\u2019s Game Ready drivers.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "[News] NVIDIA to Unveil GB300 at GTC, with Shipment Reportedly to Begin in May, Driving Cooling Demands",
            "link": "https://www.trendforce.com/news/2025/03/10/news-nvidia-to-unveil-gb300-at-gtc-with-shipment-reportedly-to-begin-in-may-driving-cooling-demands/",
            "snippet": "With NVIDIA's GTC set for mid-March, supply chain sources cited by the Economic Daily News suggest the GB300 AI chip will be unveiled, making it one o...",
            "score": 0.7031247019767761,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "NVIDIA Releases Another Hotfix Driver to Resolve Black Screen Issues, Marking the Firm\u2019s Fifth Attempt to Address The Problem",
            "link": "https://wccftech.com/nvidia-releases-another-hotfix-driver-to-resolve-black-screen-issues/",
            "snippet": "NVIDIA has released another \"hotfix driver\" to address issues with the RTX 50 GPUs, including the troublesome black-screen problem.",
            "score": 0.3855471909046173,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA has released another \"hotfix driver\" to address issues with the RTX 50 GPUs, including the troublesome black-screen problem.\n\nNVIDIA's Latest v572.75 Driver To Hopefully Eliminate Black Screen Crashing With RTX 50 GPUs\n\nThe RTX Blackwell lineup has been one of the most troublesome launches for NVIDIA, given that, despite being in the retail market for around a month, the GPUs have experienced all kinds of issues, ranging from low availability to performance degradation due to missing ROPs. It surely has been a rollercoaster ride; however, Team Green is trying to address all the problems, and now, with a new hotfix driver, the firm has solved two different issues with RTX 50 GPUs, one of which is apparently a new one.\n\nGeForce Hotfix Display Driver version 572.75 is based on our latest Game Ready Driver 572.70. [GeForce RTX 5080/5090] Graphics cards may not run at full speeds on system reboot when overclocked [5088034]\n\n[GeForce RTX 50 series] GeForce RTX 50 series GPUs crashes with black screen [5120886]\n\nThe first issue is specific to the GeForce RTX 5080/RTX 5090, where the overclocked GPUs are said not to perform at full potential when a system reboot occurs. We haven't seen reports of users facing this problem, so it is likely something NVIDIA has identified internally, or there might be cases that haven't surfaced in the media. The second problem which is said to be addressed is the black screen issue, and it seems like NVIDIA's previous solutions haven't worked out.\n\nA few days ago, we reported that despite the new driver released by NVIDIA, a user was still facing black screen problems, especially on Frame-Gen titles. Hence, this hotfix might have solved the issue, but it is uncertain for now. We recommend users update to the latest hotfix display driver, given that it may solve the black screen crashing for many users out there.\n\nNews Source: Videocardz",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "The New Rust-Written NVIDIA \"NOVA\" Driver Submitted Ahead Of Linux 6.15",
            "link": "https://www.phoronix.com/news/NOVA-Driver-For-Linux-6.15",
            "snippet": "Written by Michael Larabel in NVIDIA on 10 March 2025 at 12:00 AM EDT. 123 Comments. NVIDIA. For quite a while Red Hat engineers have been developing the...",
            "score": 0.8588220477104187,
            "sentiment": null,
            "probability": null,
            "content": "\"This is the inital PR for Nova (nova-core).\n\n\n\nBesides the nova-core skeleton driver and the initial project documentation, I picked up two firmware patches and one Rust patch (no conflicts expected) as dependency of nova-core.\"\n\nFor quite a while Red Hat engineers have been developing the open-source, Rust-written NOVA driver to in effect serve as the successor to the reverse-engineered Nouveau driver that isn't too actively developed in more recent times. But unlike Nouveau's extensive range of NVIDIA GPU support, the NOVA driver is intentionally limited to the RTX 20 \"Turing\" GPUs and newer where there is the NVIDIA GPU System Processor (GSP) with the firmware support to leverage for an easier driver-writing experience. The very initial NOVA driver code was sent out on Sunday for DRM-Next ahead of the Linux 6.15 merge window.If this pull request is honored and Linus Torvalds or any other prominent developers raise objections in the coming days, Linux 6.15 is likely to be the first kernel version with this NOVA driver and also as the first Rust-written Direct Rendering Manager driver to go mainline. But before getting too excited, what's being upstreamed for Linux 6.15 is just the very initial skeleton driver and isn't yet in any way practical for end-users... That is part of their plan to build the driver piece-by-piece within the mainline kernel as opposed to waiting and having a massive review burden for upstreaming any (semi)completed driver. You'll still need to be running either the Nouveau driver or NVIDIA's official out-of-tree drivers for the near-term. At least with the GSP firmware doing much of the heavy lifting and only catering to more recent GPU generations, the bring-up hopefully won't take as long as how long it took Nouveau to become somewhat practical for open-source NVIDIA GPU support.Danilo Krummrich of Red Hat sent out the pull request for NOVA on Sunday and commented:This very early code push amounts to just around 1,207 lines of which around 700 some lines is actual Rust code and then the rest the early documentation (400+ lines of which is the TODO list). Over the next number of Linux kernel cycles, the NOVA driver will continue to be built out until ultimately it becomes a useful open-source NVIDIA GPU driver when paired with the NVIDIA GSP firmware binaries.\n\nIn any event it's exciting to see the very early NOVA code likely to be mainlined for the Linux 6.15 kernel cycle for this new and modern open-source NVIDIA driver that with time will hopefully prove competitive to the official NVIDIA Linux driver while being more maintainable and modern engineering compared to the Nouveau code.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia: Enough Pain Already (NASDAQ:NVDA)",
            "link": "https://seekingalpha.com/article/4766021-nvidia-enough-pain-already",
            "snippet": "Nvidia posts stellar earnings with $39B revenue and $43B forecast, fueled by AI demand. Click here to read more about NVDA stock and why it is a Buy.",
            "score": 0.8406335115432739,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "NVIDIA GTC 2025: AI Breakthroughs, Quantum Leap, and Big Market Moves",
            "link": "https://www.tipranks.com/news/nvidia-gtc-2025-ai-breakthroughs-quantum-leap-and-big-market-moves",
            "snippet": "NVIDIA ($NVDA) has had an underwhelming start to the year, declining 16.08% year-to-date. However, the company is dusting off and ironing its suits for next...",
            "score": 0.7986563444137573,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA (NVDA) has had an underwhelming start to the year, declining 16.08% year-to-date. However, the company is dusting off and ironing its suits for next week\u2019s grand technological occasion: the annual GPU Technology Conference (GTC) 2025, which will be held from March 17 to 21. The peak moment will occur when CEO Jensen Huang, the keynote speaker on March 18, is expected to unveil major AI and computing advancements.\n\nWith over 25,000 in-person attendees and 300,000 virtual viewers, the event is shaping up as the largest AI-focused conference of the year. Investors and industry experts will closely watch for product launches, strategic moves, and the company\u2019s evolving role in AI, cloud computing, and quantum technology.\n\nWhat Is Expected at the GTC?\n\nThere\u2019s great anticipation for significant reveals of the next-generation AI hardware, including potential updates to NVIDIA\u2019s Hopper and Blackwell architectures, which power AI models and data centers. The company is also making a notable push into quantum computing, hosting its first-ever Quantum Day on March 20, signaling interest in hybrid AI-quantum solutions. Additionally, major advancements are expected in CUDA, AI supercomputing, and new cloud partnerships with Microsoft (MSFT) and Amazon (AMZN).\n\nThe conference will feature other key players from the tech industry, such as Google (GOOGL), AMD (AMD), Intel (INTC), Tesla (TSLA), and OpenAI in attendance. AI startups and enterprise tech leaders will showcase innovations across industries, from self-driving cars to robotics and healthcare.\n\nAlthough Nvidia\u2019s stock has been experiencing a rough start to 2025, showcasing new and advanced products or strategic shifts at GTC could drive NVDA shares higher. If Nvidia also proves it has a genuine foothold in quantum computing, it could solidify its leadership against rivals such as IBM (IBM) or Google. With the AI revolution accelerating, GTC 2025 is becoming a pivotal event for investors, researchers, and tech enthusiasts alike.\n\nIs NVDA a Buy, or a Sell?\n\nTurning to Wall Street, Nvidia is considered a Strong Buy, based on 42 analysts ratings. The average price target for NVDA stock is $177.41, suggesting a 57.43% upside potential.\n\nSee more NVDA analyst ratings\n\nQuestions or Comments about the article? Write to editor@tipranks.com",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NVIDIA GeForce RTX 5060 Ti spec leak: 4608 CUDA and 16/8GB GDDR7, RTX 5050 with 2560 CUDA and 8GB GDDR6",
            "link": "https://videocardz.com/newz/nvidia-geforce-rtx-5060-ti-spec-leak-4608-cuda-and-16-8gb-gddr7-rtx-5050-with-2560-cuda-and-8gb-gddr6",
            "snippet": "Kopite7kimi has more details. What is this? Report Ad. It appears that the rumors are true: NVIDIA is indeed planning to release the RTX 5050 after all.",
            "score": 0.9443740248680115,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-08": {
        "0": {
            "title": "NVIDIA Corp. (NVDA): Among Stocks That Will Go to the Moon According to Reddit",
            "link": "https://finance.yahoo.com/news/nvidia-corp-nvda-among-stocks-204730786.html",
            "snippet": "We recently published a list of 11 Stocks That Will Go to the Moon According to Reddit. In this article, we are going to take a look at where NVIDIA Corp.",
            "score": 0.9216814637184143,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of 11 Stocks That Will Go to the Moon According to Reddit. In this article, we are going to take a look at where NVIDIA Corp. (NASDAQ:NVDA) stands against other stocks that will go to the moon according to Reddit.\n\nSaira Malik, Nuveen\u2019s Head of Equities and Fixed Income, recently joined CNBC\u2019s \u2018Closing Bell\u2019 on February 18 to discuss opportunities outside the tech sector and her outlook for the markets. She began by addressing the two primary factors driving markets this year: technology and tariffs. Malik highlighted that while tech stocks have dominated the narrative, recent developments, such as DeepSeek\u2019s announcement, have raised concerns about the immense spending in AI and the uncertainty surrounding its monetization and returns. Malik emphasized looking beyond tech to areas like infrastructure. She mentioned utility companies as a promising investment. When asked about momentum stocks, Malik explained that the momentum trade has propelled markets over the past few years, with the S&P 500 delivering over 20% annual gains during that period. However, this growth was largely concentrated in the MAG7, which resulted in an S&P 500 valuation premium of 20% above historical averages entering this year. She noted that the momentum trade is now unwinding, partly due to inflated expectations around AI and the lack of productivity gains despite tens of billions of dollars spent in the space.\n\nMalik pointed out that international markets are outperforming US markets this year. European markets, in particular, entered 2025 with a 40% valuation discount compared to US markets and have a more cyclical bias. She stated that year-to-date returns suggest investors would benefit from owning international equities. While she expects the S&P 500 to post about 7% earnings growth for 2025 (following a strong Q4 with 12% year-over-year earnings growth), she believes international markets may continue to outperform due to their discounted valuations and cyclical exposure. Malik concluded by emphasizing the importance of being selective in this market environment. She recommended focusing on smaller mid-cap companies with profits, lower leverage, reduced refinancing risks, or economic sensitivity. Sectors like financials could thrive if deregulation or increased mergers and acquisitions activity materialize. She advised investing in stocks with upward earnings estimate revisions rather than those facing downward adjustments to capitalize on current market conditions.\n\nMethodology\n\nWe first sifted through threads and posts on WSB and similar subreddits to compile a list of the top trending stocks among retail investors. We then selected the 11 stocks that analysts were bullish on and had an average upside potential over 40% as of March 3. We also added the hedge fund sentiment for each stock, as of Q4 2024, which was sourced from Insider Monkey\u2019s database. The stocks are ranked in ascending order of the number of hedge funds that have stakes in them.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Could Nvidia Stock Help You Retire a Millionaire?",
            "link": "https://www.fool.com/investing/2025/03/08/could-nvidia-stock-help-you-retire-a-millionaire/",
            "snippet": "Everyone wants to retire a millionaire. But many will never realize that vision. How do you make sure this future happens to you?",
            "score": 0.8575630784034729,
            "sentiment": null,
            "probability": null,
            "content": "Everyone wants to retire a millionaire. But many will never realize that vision. How do you make sure this future happens to you? Find great companies to invest in for decades at a time.\n\nRight now, the artificial intelligence (AI) revolution is creating some of the biggest growth opportunities in history. Could Nvidia (NVDA 5.27%) stock be your secret to a wealthy retirement? You might be surprised by the answer.\n\nNvidia's growth journey has just begun\n\nOver the past few years, Nvidia's revenues have exploded higher, and not from a small base, either. In 2023, the company was bringing in less than $40 billion in sales. Today, annual sales have exceeded $130 billion, with the end of this growth trajectory nowhere in sight.\n\nThat's because, in many ways, the AI revolution has just begun. And that's great news for Nvidia, considering its GPUs -- critical components for training and running AI models -- are widely considered best in class right now.\n\nAccording to global consultancy McKinsey & Co., investments into new AI software and services has gone gangbusters in recent years. \"Equity investments in generative AI jumped from $5 billion in 2022 to $36 billion in 2023,\" one of the firm's recent reports revealed. The numbers for 2024 are likely significantly higher.\n\nThe firm's low estimate has AI Software and Services revenue growing from $85 billion in 2022 to $1.5 trillion in 2040. Its high estimate sees industry revenue surpassing to $4.6 trillion by 2040. In other words, the pace of revenue growth for the AI sector from here on out is expected to be like nothing we've ever seen before.\n\nI've written before about how Nvidia's CUDA developer suite has created a vendor lock-in effect that could help it sustain dominant market shares for AI GPUs for years, if not decades, to come. All of this put together means that Nvidia will be selling into a rapidly growing, truly gigantic market with an industry-leading product that developers need for their innovation to become reality. It really is an incredible position to be in -- a major reason why Nvidia's valuation has soared into the trillions of dollars.\n\nTo be fair, Nvidia stock is quite expensive on paper. It's surprising to see a trillion-dollar business garner a price-to-sales ratio of 21.6. That's typically been considered a lofty multiple, even for significantly smaller companies.\n\nWill this stock make you a millionaire?\n\nThis begs the question: Is Nvidia stock still a buy today? You may be surprised by the answer.\n\nOver the short term, anything is possible with Nvidia stock. High multiple market darlings like this are prone to erratic shifts in market sentiment. In early 2025, we experienced such a swing, with hundreds of billions of dollars wiped off Nvidia's valuation over a matter of days, with Chinese start-up DeepSeek announcing a chatbot created with cheaper chips.\n\nBut here's the thing: Retiring rich doesn't typically happen overnight. It also doesn't happen by finding a single worthy investment. Retirement investing is a long game, where you stack the magic of compound interest in your favor. That is, you must fill your portfolio with companies that can grow your money consistently over time. That requires long holding periods and a level of patience few possess.\n\nNvidia stock is undoubtedly expensive. But long stretches of growth can make almost any multiple look attractive in hindsight. The company has a durable competitive advantage due to vendor lock-in, and its end market growth will be incredible to watch, even at the low end of estimates.\n\nThose looking to retire rich should consider adding Nvidia shares to their portfolio today. But don't forget that it will be time, and likely a well-balanced portfolio, that will lead to the best results come retirement.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Is the chip sector having a meltdown? By this measure, it\u2019s mostly just Nvidia.",
            "link": "https://www.marketwatch.com/story/is-the-chip-sector-having-a-meltdown-by-this-measure-its-mostly-just-nvidia-810ace94",
            "snippet": "Broadcom and Marvell shares have been hit recently. But Nvidia alone is responsible for about three quarters of the chip sector's lost market cap in the...",
            "score": 0.7487882375717163,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia RTX 5090's 16-pin power connector hits 150C in reviewer's thermal camera shots",
            "link": "https://www.tomshardware.com/pc-components/power-supplies/nvidia-rtx-5090s-16-pin-power-connector-hits-150c-in-reviewers-thermal-camera-shots",
            "snippet": "Some thermal imagery shared on Twitter/X underlines how toasty-hot power connectors servicing Nvidia GeForce RTX 5090 graphics cards can get.",
            "score": 0.6864402890205383,
            "sentiment": null,
            "probability": null,
            "content": "Some thermal imagery shared on Twitter/X underlines how toasty-hot power connectors servicing Nvidia GeForce RTX 5090 graphics cards can get. Veteran hardware reviewer Andreas Schilling, an editor at Germany's Hardware Luxx, took thermal photos of his water-cooled graphics card.\n\nWhile the GPU barely broke a sweat, the power connectors could be seen \"cooking at 150+ degrees.\" That's Celsius, and for those unfamiliar with metric units, 150 degrees Celsius equates to just over 300 degrees Fahrenheit.\n\nThough reviewers at Tom's Hardware haven't experienced dangerously hot 12V-2x6 cable connectors first-hand, reports indicate this issue can affect anyone\u2014from seasoned hardware veterans to budding enthusiasts. In previous generations and with earlier iterations of the 16-pin connector, there was often a nagging doubt about 'user error, ' but we seem to be fully past that notion now. We must face up to the problem that RTX 5090 power cables may be doomed to burn.\n\nWhile doing some thermal imaging of a graphics card and reviewing the images, I noticed, that the camera picked up a hotspot at the PSU. The 12V-2x6 cable was cooking at 150+ degrees. This is no joke and will forever remain a weak point of this generation(s). pic.twitter.com/qTMfgTwUr0March 7, 2025\n\nSchilling provided some background information on the thermal imagery he shared. The graphics card he was testing was a liquid-cooled Inno3D RTX 5090 Frostbite, and one of the images clearly shows the pipe fittings. The PSU in this PC build was a be quiet! Dark Power 13 and Schilling confirmed that the GPU pulled 600W during tests.\n\nThe Hardware Luxx editor sounds like he has run out of patience with Nvidia's graphics card power connector(s) choice. \"The 12V-2x6 cable was cooking at 150+ degrees,\" he observed, backed up by the thermography. \"This is no joke and will forever remain a weak point of this generation(s).\"\n\nThis power connection \"will forever remain a weak point\"\n\nPrompted by social media interest in the visuals, Schilling added some extra details. He said the cabling looked like it was still for now, despite the high temperatures seen. \"But you can see that they have been subjected to thermal stress. I have zero trust in that solution of any kind,\" blasted the reviewer.\n\nLater, Schilling recalled that the PSU-to-cabling mating cycles were still very low, \"a handful.\" However, the connector might have been plugged and unplugged \"several hundred\" times on the GPU side. That second figure seems well beyond the 12V-2x6 connector's \"mating cycle life of 30 cycles,\" mentioned by Corsair. The images show the unexpectedly high temperatures on both sides\u2014the graphics card and PSU connections.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nBefore we go, it is important to remember that thermal cameras measure surface temperatures so that things could be far hotter inside the plastic connectors. Schilling didn't see actual melting during his later inspections but said there was evidence of some thermal stress. We expect the 12V-2x6 connector to use the same Nylon 66 and LCP housing as per 12VHPWR specs. The former has a melting point of 255 degrees Celsius (491 degrees Fahrenheit), and the latter melts above 335 degrees Celsius (635 degrees Fahrenheit).\n\nThankfully, Schilling's power connectors must not have quite reached these thresholds inside, but extended testing and use of this build \u2013 without changes \u2013 sounds like it could be hazardous.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "I have an RTX 50-series GPU, and I've barely touched DLSS 4",
            "link": "https://www.xda-developers.com/why-i-barely-use-dlss-4/",
            "snippet": "Nvidia's latest RTX 50-series GPUs use DLSS 4 as a key selling point, but I rarely find myself actually using it.",
            "score": 0.8805175423622131,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA\u2019s GeForce RTX 5050 To Be The First RTX Blackwell GPU To Feature GDDR6 Memory; Likely Being a Budget-Friendly Model",
            "link": "https://wccftech.com/nvidia-geforce-rtx-5050-to-be-the-first-rtx-blackwell-gpu-to-feature-gddr6-memory/",
            "snippet": "NVIDIA's RTX 5050 is said to be the first RTX Blackwell GPU to debut with GDDR6 memory, likely being a more value-for-money option.",
            "score": 0.818356454372406,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's \"mid-range\" GeForce RTX 5050 is said to be the first RTX Blackwell GPU to debut with GDDR6 memory, likely being a more value-for-money option.\n\nNVIDIA Is Expected To Tackle Intel's Arc B580 & AMD's RX 9050 GPUs With The RTX 5050; GDDR6 Inclusion Will Make It Cheaper\n\nYesterday, we exclusively reported on the specifications of the GeForce RTX 5050 and the RTX 5060, two of NVIDIA's main \"budget contenders\" for the GPU market. With the release of AMD's RX 9070 series, it won't be wrong to say that Team Green has lost ground in the mainstream GPU segment, given the value Team Red's RDNA 4 series offers and its availability across global retailers.\n\nHowever, NVIDIA's 50-class and 60-class GPUs will decide how market shares evolve further, and now, according to Benchlife, the RTX 5050 will be the first RTX 50 GPU to offer GDDR6 memory.\n\nFor those still living under a rock, a big change with NVIDIA's RTX 50 lineup was the use of the advanced GDDR7 memory, which offered enhanced performance, but with a cost of higher pricing. Despite being a newer standard, AMD decided not to employ the memory type with its RX 9070 GPUs, in an attempt to keep prices under control, and apparently, this has worked out for them pretty well, as the perf/$ values on the Radeon RX 9070 XT are simply phenomenal. With the RTX 5050, NVIDIA might target something similar, but that is yet to be seen.\n\nDiving into the RTX 5050's rumored specifications, it will feature 8 GB GDDR6 of memory and be rated at a TBP of 135W. Moreover, the GPU is said to retail somewhere between the $199-$249 US price tag and will be positioned against the Intel Arc B580. On AMD's side, we could expect the GPU to tackle the rumored RX 9050 lineup, and given that NVIDIA had skipped the 50-class GPU with Ada Lovelace, we expect the release to be a decent one.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia stock nosedives, Bitcoin bleeds, and the Trump bump is over: Markets news roundup",
            "link": "https://qz.com/nvidia-stock-bitcoin-price-trump-crypto-stagflation-1851768468",
            "snippet": "Nvidia stock nosedives, Bitcoin bleeds, and the Trump bump is over: Markets news roundup. Plus, stagflation fears rise as a brutal jobs report follows new...",
            "score": 0.7239334583282471,
            "sentiment": null,
            "probability": null,
            "content": "President Donald Trump has announced plans to create a strategic crypto reserve, shifting gears from his earlier promise to stockpile Bitcoin. Trump\u2019s cryptocurrency reserve will consist of Bitcoin, often referred to as \u201cdigital gold\u201d for its ability to hedge against inflation, along with Ether, the second-largest cryptocurrency by market cap, XRP, Solana, and Cardano.\n\nAdvertisement\n\nRead More",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NVIDIA RTX PRO 6000 Blackwell GPU leaks: 24064 CUDA cores, 96GB GDDR7, nearly a new TITAN GPU",
            "link": "https://www.tweaktown.com/news/103781/nvidia-rtx-pro-6000-blackwell-gpu-leaks-24064-cuda-cores-96gb-gddr7-nearly-new-titan/index.html",
            "snippet": "NVIDIA's new RTX PRO 6000 Blackwell workstation GPU details: GB202 GPU, 24064 CUDA cores, 752 Tensor Cores, 188 RT Cores, 96GB GDDR7 with ECC, 600W power.",
            "score": 0.8172896504402161,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia stock analyzed amid recent selloff (NVDA:NASDAQ)",
            "link": "https://seekingalpha.com/news/4418720-nvidia-after-selloff-undervalued-or-further-downside-ahead",
            "snippet": "NVIDIA (NVDA) stock becomes the subject of analyst comments as the company drops below the coveted $3T market cap amid trade tensions and AI competition.",
            "score": 0.910409688949585,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "NVIDIA RTX PRO 6000 Blackwell leaked: 24064 cores, 96GB G7 memory and 600W Double Flow Through cooler",
            "link": "https://videocardz.com/newz/nvidia-rtx-pro-6000-blackwell-leaked-24064-cores-96gb-g7-memory-and-600w-double-flow-through-cooler",
            "snippet": "Following the disclosure of the card's name, a simple web search reveals the specs of the upcoming workstation card from NVIDIA.",
            "score": 0.913780689239502,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-07": {
        "0": {
            "title": "Nvidia GeForce RTX 5070 review: an RTX 4070 Super with a DLSS 4 badge",
            "link": "https://www.rockpapershotgun.com/nvidia-geforce-rtx-5070-review",
            "snippet": "The RTX 5070 isn't a bad graphics card, but unless a game support DLSS 4's Multi Frame Generation, it barely improves on the RTX 4070 Super's performance.",
            "score": 0.8459906578063965,
            "sentiment": null,
            "probability": null,
            "content": "PNY GeForce RTX 5070 OC specs: CUDA Cores: 6144\n\n6144 Base Clock Speed: 2.16GHz\n\n2.16GHz Boost Clock Speed: 2.51GHz\n\n2.51GHz VRAM: 12GB GDDR7\n\n12GB GDDR7 Power: 250W\n\n250W Recommended System Power: 650W\n\n650W Price: From \u00a3540 / $550\n\nI knew the RTX 5070 was tricking me. Parked next to the extravagant silliness of the two-grand RTX 5090, this \u00a3539 / $549 graphics card looked like a very agreeable deal, offering all the same DLSS 4 and Multi Frame Generation as its bigger, pricier brothers. Also, an upgrade to the RTX 4070 Super, a GPU that could handle 4K without looking too out of place in a premium 1080p rig. Tragically, though, the RTX 5070 breaks a sacred covenant, a mutual understanding between PC owners and parts makers that\u2019s held strong for decades: if you buy a new version of a thing, it should be faster than the old version of that thing. Look past the MFG illusion, and far too often, it isn\u2019t.\n\nRifling through the specs doesn\u2019t turn up any clear reasons for this lack of performance gain; the RTX 5070\u2019s CUDA core count comes in at 6144, putting it merely in-between the RTX 4070 (5888) and the RTX 4070 Super (7168), but both base and boost clock speeds have been raised to compensate. VRAM, meanwhile, remains at 12GB, but with a switch from GDDR6X to GDDR7 that confers significantly more bandwidth. Then there\u2019s the power rating, which both the delayed Founders Edition and this here triple-fan PNY GeForce RTX 5070 OC set at 250W \u2013 50W more than an entry-level RTX 4070 Super. It looks, and sounds, like a decent generational update.\n\nAnd yet...\n\nNvidia GeForce RTX 5070 review: 4K benchmarks\n\nWhile the RTX 5070 does make the cut as a 4K contender, surpassing 60fps in most Ultra-quality games without upscaling, the only time it produced a visibly faster framerate was in Cyberpunk 2077. Elsewhere, it only outperforms the RTX 4070 Super by a scant handful of single-digit frames per second, and repeatedly loses out to the non-Super RTX 4070 Ti in the process.\n\nClick to embiggen! | Image credit: Rock Paper Shotgun\n\nThe RPS test PC: CPU: AMD Ryzen 7 9800X3D\n\nAMD Ryzen 7 9800X3D RAM: 32GB Trident Z5 Neo RGB DDR5\n\n32GB Trident Z5 Neo RGB DDR5 Motherboard: MSI MPG X870E Carbon WiFi\n\nMSI MPG X870E Carbon WiFi PSU: NZXT C1000 Gold\n\nObviously, very few potential buyers are going to be upgrading directly from last year\u2019s RTX 4070 Super to the RTX 5070, and the latest model does at least make for a meaty improvement on the RTX 3070. Trouble is, that improvement has already been made in the last generation, and for prices that \u2013 even if you\u2019re lucky enough to find an RTX 5070 in stock below \u00a3600 \u2013 are likely drop much faster, especially where the secondhand market is concerned.\n\nIt\u2019s also worth addressing a specific claim, made by Nvidia during their original RTX 50 series announcement, that the RTX 5070 delivers \"RTX 4090 performance\" with MFG. This is, depending on how generous you want to be, either lacking in context or complete tosh. While it\u2019s true that, for instance, 4x MFG can drag the RTX 5070 to 71fps in a fully path-traced Alan Wake 2, that\u2019s only equal or faster than the RTX 4090 if you deny the older GPU its own tools. Sure enough, it only needs DLSS 3\u2019s old-timey, 2x frame generation to average 82fps on otherwise identical settings. In short, the only way the RTX 5070 is truly moving at the same pace as the RTX 4090 is if they\u2019re in the same delivery van.\n\nImage credit: Rock Paper Shotgun\n\nYou could still consider MFG to be the RTX 5070\u2019s party trick, though at 4K specifically, it\u2019s not as much of a difference-maker as it is on on burlier cards like the RTX 5070 Ti and RTX 5080. In both Cyberpunk and Alan Wake 2, the RTX 4070 joins its 40-series rivals in being unable to churn out playable path tracing performance on DLSS Quality; in fact, in the latter, it\u2019s narrowly the slowest of the lot. As a result, when 4x MFG kicks in, it\u2019s producing a higher number in a framerate counter but isn\u2019t actually helping the games to run and smoother or more responsively than on the DLSS 3-limited 40 series models. As far as your PC knows, it\u2019s still only running at 26/21fps, and all the AI-generated frames in the world won\u2019t get rid of the sluggish feel that aiming and camera control will suffer in those conditions.\n\nImage credit: Rock Paper Shotgun\n\nNvidia GeForce RTX 5070 review: 1440p benchmarks\n\n1440p is a more comfortable environment for the RTX 5070\u2019s frame gen tech, though its problems with overfamiliar performance remain. If anything, it\u2019s more bothersome here, as the higher framerates make it even harder to tell a naked-eye difference between this and its predecessors.\n\nImage credit: Rock Paper Shotgun\n\nIn Assassin\u2019s Creed Mirage, the RTX 4070 Super even finishes ahead, if only be a single frame, while even the games that the RTX 5070 does relatively well in (Cyberpunk 2077, F1 24) only see an uptick of 10% or so. There is a simultaneous gap-narrowing with the RTX 5070 Ti, suggesting you don\u2019t need to stretch to the Ti version for quality Quad HD, and again, anyone upgrading from the RTX 3070 will enjoy a sizeable speed boost. Still, these benefits could apply to the RTX 4070 Super just as much as the RTX 5070.\n\nThe same can\u2019t be said, in fairness, for frame gen performance. Although these tests suggest the RTX 5070 is in fact slightly worse at dealing with path tracing/full ray tracing than those pesky RTX 40 GPUs, unlike at 4K, it is able to produce enough pre-generation frames to support a workable application of 4x MFG.\n\nImage credit: Rock Paper Shotgun\n\nI\u2019m not saying this is an adequate replacement for traditionally rendered frames, but at least at this resolution, you can kind of see where Nvidia is coming from. Spared from the need to compensate for sub-30fps framerates, frame gen is free to simply shine up the visual smoothness of already adequately-running games, as it should be.\n\nStill, it\u2019s also hard not to peer over at those towering RTX 5070 Ti and RTX 5080 bars, wondering why the standard RTX 5070 couldn\u2019t just do a little more in the conventional rendering department.\n\nImage credit: Rock Paper Shotgun\n\nNvidia GeForce RTX 5070 review: 1080p benchmarks\n\nThe drop to an even less demanding screen rez does little to raise the RTX 5070\u2019s value proposition. Yet again it\u2019s haunted by the Ghosts of XX70 GPUs Past, especially in Assassin\u2019s Creed Mirage, where it repeats its embarrassing loss. It does gain a few extra frames over the RTX 4070 Super in Shadow of the Tomb Raider, but then that\u2019s already running so fast that you\u2019d need a 300Hz monitor and downright inhuman observation skills to perceive them in action.\n\nImage credit: Rock Paper Shotgun\n\nThat all means we must return to the world of maxed-out lighting effects and AI frame generation in search of a consolation prize. Like at 1440p, the RTX 5070 starts off inexplicably worse than the RTX 4070 Super when path tracing in involved, though in both games it still forms itself a solid enough base to apply 4x MFG without input lag coating your PC innards in treacle.\n\nImage credit: Rock Paper Shotgun\n\nYou\u2019ll have to forgive me for not sharing these results with the kind of bubbly enthusiasm that multicoloured bar charts deserve. I know it\u2019s not like the RTX 5070 has regressed to the point that it\u2019s a bad GPU in the general sense \u2013 get it at RRP, like this PNY model costs, and it\u2019s still a fine multi-discipline graphics card that will comfortably play anything outside of the most brutalising 4K settings. And what I was saying earlier, about the RTX 4070 Super going cheaper faster? Reader, I\u2019m afraid that was conjecture. Right now, these are still on sale in the \u00a3600-\u00a3700 range, and while plenty of board partner RTX 5070s will surely fill the same space, if these two GPUs cost the same then there\u2019s no compelling reason to stick with the older one. There\u2019s maybe an argument in favour of the RTX 4070 Super\u2019s power efficiency \u2013 I measured it peaking at 219W during my tests, versus 251W on the RTX 5070 \u2013 but for all its shortcomings, I would rather have MFG than a 30W saving.\n\nImage credit: Rock Paper Shotgun\n\nEven so, it\u2019s also completely fair and reasonable to expect some kind of meaningful, not wholly AI-reliant improvement from each new generation. The RTX 5070 simply does not deliver, and I\u2019m honestly a little worried that if we simply shrug and accept that, frame generation really will be the only way that future GPUs bother to upgrade \"performance\". Even when, as this very card\u2019s 4K path tracing results show, such an approach is inadequate when the games side of the industry continues to ask more and more of your hardware.\n\nThis review is based on a retail unit provided by the manufacturer.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "As NVIDIA\u2019s Quantum Day Nears, Analysts Suggest Event is More Than a Gesture",
            "link": "https://thequantuminsider.com/2025/03/07/as-nvidias-quantum-day-nears-analysts-suggest-event-is-more-than-a-gesture/",
            "snippet": "Insider Brief. NVIDIA's Quantum Day at GTC 2025 is bringing together industry leaders to discuss the technology's current capabilities and future potential.",
            "score": 0.7996925115585327,
            "sentiment": null,
            "probability": null,
            "content": "Insider Brief\n\nNVIDIA\u2019s Quantum Day at GTC 2025 is bringing together industry leaders to discuss the technology\u2019s current capabilities and future potential.\n\nCEO Jensen Huang\u2019s earlier skepticism about quantum computing\u2019s timeline caused stock declines for several quantum companies, prompting NVIDIA to engage more directly with the quantum industry, analysts suggest.\n\nThe event will feature discussions on quantum computing\u2019s role in AI, commercialization challenges, and potential breakthroughs, with executives from leading quantum firms participating.\n\nImage: Google Quantum AI\n\nNVIDIA\u2019s inaugural Quantum Day at GTC 2025 \u2014 scheduled for March 20 \u2014 is closing in and anticipation is growing for the event, which will bring together industry leaders and researchers to discuss quantum computing\u2019s present advances and future potential.\n\nInitially, some in the quantum industry \u2014 and perhaps in the larger tech industry \u2014 viewed NVIDIA\u2019s decision to host its first Quantum Day as more of an apology and slightly an apology. However, the event is now seen as more than a simple industry gathering and more than a gesture to the quantum computing community \u2014 it\u2019s a statement, according to Investor\u2019s Business Daily, who asked analysts and key figures to break down the events that led to the Quantum Day announcement.\n\nAfter CEO Jensen Huang\u2019s remarks \u2014 that quantum computing would not be \u201cvery useful\u201d for 15 to 30 years \u2014 in January sent quantum computing stocks plummeting, the tech giant is now bringing industry leaders together to discuss the technology\u2019s future. The move suggests NVIDIA is recalibrating its stance and recognizing quantum computing\u2019s potential sooner rather than later, according to IBD.\n\n\u201cIf NVIDIA really thought that quantum was 15 to 30 years out, would they have an event today? Probably not,\u201d Gil Luria, an analyst at D.A. Davidson and head of the firm\u2019s technology research, told IBD.\n\nThe GTC event in Silicon Valley \u2014 a highlight on the tech conference calendar \u2014 will feature quantum executives, including D-Wave CEO Alan Baratz, who was personally invited by Huang shortly after the controversy erupted.\n\nThe Fallout From Huang\u2019s Comments\n\nHuang\u2019s comments at CES had an immediate impact. Investors panicked, causing shares of Rigetti Computing, IonQ, D-Wave, and Quantum Computing Inc. to drop sharply.\n\nLuria, who had just given a quantum company a buy rating, knew the quote would have impact.\n\n\u201cWhen I heard that, I rolled my eyes a little bit,\u201d Luria told IBD. \u201cBut I realized that that was going to have an impact.\u201d\n\nBaratz was among the most vocal in pushing back.\n\n\u201cThe quantum computing industry is still young, and the investor market is still maturing,\u201d Baratz said, according to IBD. \u201cEverybody\u2019s looking for any information that they can use to make decisions. Sometimes misinformation can get out there and drive decisions.\u201d\n\nQuantum computing companies argue that real-world applications already exist, with businesses exploring the technology for supply chain optimization, drug discovery and financial modeling. Others, including veteran Silicon Valley investors, argue that large-scale commercial viability is still years away.\n\nA steady stream of predictions have caused waves of predictions about quantum from CEOs and former CEOs to scientists and physicists. Those predictions \u2014 not coincidentally \u2014 have caused turbulence in the burgeoning quantum stock market.\n\n\u201cAnd the whole thing doubled again, which is to say our expectations went from five years to 15 years to 10 years back to five years over a period of two weeks,\u201d Luria told IBD. \u201cThat\u2019s how you can explain the stock moves: When discounting cash flows, it matters a lot if all my cash flows are five years out or 15 years out.\u201d\n\nDiscounted cash flow (DCF) is a financial valuation method that estimates the present value of an investment by projecting future cash flows and adjusting them for risk and time value using a discount rate. DCF analysis shows that even slight delays in expected cash flow timelines can cause drastic valuation swings for quantum companies compared to mature firms, The Quantum Insider reported at the time.\n\nNVIDIA is investing strategically in quantum computing.\n\nNVIDIA\u2019s Position and the AI Factor\n\nThe debate over quantum computing is particularly relevant for NVIDIA, whose dominance in artificial intelligence chips is undisputed. Quantum computing could eventually pose a challenge to NVIDIA\u2019s core business by offering an alternative way to process complex computations more efficiently than GPUs.\n\n\u201cQuantum computing isn\u2019t what\u2019s next to AI,\u201d Luria said. \u201cThere are two parallel paths that are intersecting. Quantum will at some point be helpful toward creating really sophisticated AI. A quantum computer may replace the data center GPUs \u2013 but it\u2019s not replacing AI.\u201d\n\nBaratz agrees that quantum\u2019s impact on AI could arrive sooner than expected. D-Wave is already collaborating with Japan Tobacco\u2019s pharmaceutical division on AI-powered drug discovery, while IonQ is working with AstraZeneca on similar initiatives.\n\nQuantum Computing\u2019s Challenges\n\nWhile the technology holds promise, major hurdles remain, IBD reports. Quantum computers rely on subatomic particles to perform calculations that classical computers struggle with. However, they require highly specialized conditions \u2014 such as extreme cold \u2014 and remain expensive to build and maintain.\n\n\u201cD.A. Davidson analyst Alex Platt said quantum computing comes with \u201can expensive price tag,\u201d estimating costs of a single system in the tens of millions.\n\nBaratz estimated that a D-Wave quantum computer costs under $2 million to build and install.\n\nThese challenges, combined with uncertainty over when quantum systems will surpass classical computers in practical applications, fuel skepticism. Meta CEO Mark Zuckerberg recently added to doubts, stating on an interview for the Joe Rogan Show podcast that quantum computing is \u201cstill quite a ways off.\u201d\n\nLuria dismissed the remark, telling IBD that Zuckerberg admitted he was not an expert. \u201cHe said, \u2018Look, I\u2019m not really an expert, but it seems like it\u2019s decades away.\u2019 He is not an expert\u2014but he\u2019s Mark Zuckerberg.\u201d\n\nRebuilding Confidence in Quantum Computing\n\nFollowing the backlash and stock drops, NVIDIA\u2019s announcement of Quantum Day signals an effort to reshape the conversation. The company described quantum computing as \u201cone of the most exciting areas in computer science, promising progress in accelerated computing beyond what\u2019s considered possible today.\u201d\n\nBaratz, who will be speaking at the event, confirmed that Huang personally reached out to invite him. \u201cThat\u2019s between me and him,\u201d he said when asked about their exchange.\n\nRigetti CEO Subodh Kulkarni sees the event as an opportunity for constructive discussion.\n\n\u201cThis was a nice way to say, \u2018Let\u2019s have a good dialogue,\u2019\u201d he told IBD. \u201cThese are all uncertain things.\u201d\n\nQuantum\u2019s \u2018ChatGPT Moment\u2019?\n\nFor now, NVIDIA\u2019s primary focus remains AI, with quantum computing still an emerging sector. Some analysts compare quantum\u2019s development timeline to AI\u2019s evolution, arguing that it may take a major breakthrough \u2014 akin to the release of ChatGPT in 2022 \u2014 to shift perceptions.\n\n\u201cIf that was to happen, two, three, five years down the road, that would be the equivalent of a ChatGPT moment,\u201d Luria told IBD, pointing to quantum-assisted drug discovery as a potential turning point.\n\nEven critics acknowledge quantum\u2019s long-term potential.\n\n\u201cNobody is shipping quantum computers in volume, on any kind of scale in the next five to seven years,\u201d said Rob Siegel, a Stanford lecturer and investor, as reported by IBD. \u201cBut that doesn\u2019t mean the technology isn\u2019t important.\u201d\n\nWhat Comes Next?\n\nThe global quantum computing market could add a total of more than $1 trillion to the global economy between 2025 and 2035, according to a report from The Quantum Insider. Vendors are expected to capture $50 billion of revenue over this period, the report added.\n\nHowever, investor confidence in the sector remains volatile, with shifting perceptions about when real-world applications will take off.\n\nFor industry leaders, NVIDIA\u2019s Quantum Day represents a crucial moment to reset expectations and bring clarity to the conversation. The event will also include a chat between Huang and executives from industry leaders, including: Alice & Bob, Atom Computing, D-Wave, Infleqtion, IonQ, Pasqal, PsiQuantum, Quantinuum, Quantum Circuits, QuEra Computing, Rigetti and SEEQC.\n\nKulkarni believes the event\u2019s significance extends beyond NVIDIA.\n\n\u201cSomeone like him taking personal interest in hosting the event, I think carries a lot of credibility,\u201d Kulkarni said. \u201cI wish we didn\u2019t go through such movements of the stock prices. But that\u2019s our life, I suppose.\u201d\n\nYou can register for GTC here.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia GTC to bring thousands to downtown San Jose",
            "link": "https://www.nbcbayarea.com/news/tech/nvidia-gtc-downtown-san-jose/3812368/",
            "snippet": "Nvidia's flagship GPU Technology Conference is making its way back to downtown San Jose. In 2024, the giant tech conference could be felt all over downtown,...",
            "score": 0.7972331047058105,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's flagship GPU Technology Conference is making its way back to downtown San Jose.\n\nIn 2024, the giant tech conference could be felt all over downtown, bringing more than $15 million to the area.\n\nDan Phan, owner of EOS & NYX, said he's excited for the buzz the conference will bring and the business it will generate.\n\n\"We decided to open this restaurant. It's our biggest project yet this year, and a big part of it was that we knew this momentum was gonna come back; we knew people were gonna come to the valley,\" he said.\n\nTens of thousands of techies will roam the conference floor on March 18, looking for jobs and new opportunities.\n\n\"And I hope that inspires other large conference organizers and employers to consider bringing back in-person events like this that offer networking opportunities,\" said Mayor Matt Mahan.\n\nNvidia said there will be networking and training sessions to help job seekers get trained in using artificial intelligence.\n\n\"We take the training courses we give to our own engineers, and we make teaching kits, and we have course curriculum, then we work to train and certify educators, and then we can drive that down into the educational system,\" said Greg Estes of Nvidia.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia sheds $1 trillion from record high market cap as market sell-off intensifies",
            "link": "https://finance.yahoo.com/news/nvidia-sheds-1-trillion-from-record-high-market-cap-as-market-sell-off-intensifies-174818089.html",
            "snippet": "Nvidia (NVDA) stock briefly extended its decline on Friday as the AI chip giant's market cap losses from its record high in January reached $1 trillion.",
            "score": 0.8836283087730408,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock briefly extended its decline on Friday as the AI chip giant's market cap losses from its record high in January reached $1 trillion.\n\nA broader market sell-off coupled with fears of an overvaluation in the AI trade has sent the stock tumbling more than 23% over the past two months.\n\nOn Friday shares temporarily fell to hover near $107 each, down from their record close of $149.43 on Jan. 6, when the company's valuation sat just north of $3.66 trillion.\n\nNvidia's market cap stood at $2.6 trillion during Friday's session as selling on Wall Street intensified.\n\nOn Thursday renewed fears of an overextended AI trade surfaced after chipmaker Marvell Technology's (MRVL) revenue outlook failed to impress investors and semiconductor stocks fell.\n\n\"It\u2019s been a rough year for NVDA so far. ... The stock (along with many of its AI-semi peers) has suffered, battered by a storm of growth fears, supply chain noise, and tariff and regulatory risks,\" Bernstein analyst Stacy Rasgon wrote in a note to investors earlier this week. \"Sentiment has clearly pivoted for now on the AI group.\"\n\nRead more: How does Nvidia make money?\n\nTech stocks have led a broader market sell-off recently as investors weigh the impact of the Trump administration's tariff policy on the economy.\n\nIn late February Nvidia stock fell 8.5% in one session, sending the company's market cap below $3 trillion after the company's fourth quarter earnings topped Wall Street's expectations but its outlook for first quarter gross margin came in lower than estimates.\n\nThe stock also took a hit in late January after a new AI model released by Chinese firm DeepSeek called into question the mammoth spending from Big Tech on artificial intelligence infrastructure.\n\nNvidia fell 17% in a single day on the news, shaving $589 billion off the AI chipmaker's market cap \u2014 the largest single-day loss in stock market history.\n\nNvidia founder and CEO Jensen Huang speaks during a Nvidia news conference ahead of the CES tech show on Jan. 6, 2025, in Las Vegas. (AP Photo/Abbie Parr, File) \u00b7 ASSOCIATED PRESS\n\nInes Ferre is a senior business reporter for Yahoo Finance. Follow her on X at @ines_ferre.\n\nClick here for in-depth analysis of the latest stock market news and events moving stock prices\n\nRead the latest financial and business news from Yahoo Finance",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "What You Need To Know About CoreWeave, the Nvidia-Backed Firm Targeting a $4B IPO",
            "link": "https://www.investopedia.com/what-you-need-to-know-about-coreweave-the-nvidia-backed-firm-targeting-a-usd4b-ipo-11693233",
            "snippet": "CoreWeave, a cloud computing company backed by Nvidia, could soon go public in what is expected to be one of the biggest IPOs in recent years.",
            "score": 0.9159089922904968,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways CoreWeave could go public as soon as next week in what is expected to be one of the biggest IPOs in recent years.\n\nThe cloud computing company is backed by Nvidia and counts Microsoft among its clients.\n\nCoreWeave provides its customers with access to data centers equipped with highly coveted Nvidia chips for training AI models.\n\nCoreWeave, a cloud computing company backed by Nvidia (NVDA), could go public as soon as next week in what is expected to be one of the biggest IPOs in recent years.\n\nThe company is expected to aim for $4 billion in funds raised through the IPO, which would value the company around $35 billion, according to IPO research firm Renaissance Capital. That would make it one of the largest public offerings in recent years.\n\nCoreWeave makes money by providing its clients with access to data centers, which are used to develop artificial intelligence models. The company\u2019s data centers are equipped with highly coveted chips from Nvidia, which holds a roughly 5% stake in CoreWeave.\n\nCoreWeave\u2019s biggest client is Microsoft (MSFT), which accounted for 62% of its $1.9 billion in revenue last year, according to its prospectus. Its reliance on Microsoft has been a source of some concern for would-be investors, particularly after a Financial Times report this week said Microsoft \u201cwalked away from some of its commitments\u201d with the company.\n\nCoreWeave disputed the report, telling Investopedia \"there have been no contract cancellations or walking away from commitments. Any claim to the contrary is false and misleading.\"\n\nCoreWeave reported a net loss of $863 million on revenue of $1.9 billion in 2024, which the company attributed to investments in its business.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA Planning Entry-Level GeForce RTX 5050 8 GB GPU, RTX 5060 8 GB Hits Retail In April",
            "link": "https://wccftech.com/nvidia-geforce-rtx-5060-rtx-5050-8-gb-gpus-april-launch/",
            "snippet": "NVIDIA is preparing entry-level & mainstream options for gamers with its upcoming GeForce RTX 5060 and RTX 5050 series graphics cards.",
            "score": 0.8972627520561218,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA is preparing entry-level & mainstream options for gamers with its upcoming GeForce RTX 5060 and RTX 5050 series graphics cards.\n\nNVIDIA GeForce RTX 5060 & RTX 5050 8 GB GPUs To Enter The Entry-Level Gaming Markets\n\nA few days ago, we reported the launch plan of the NVIDIA GeForce RTX 5060 Ti, which will come in 16 GB and 8 GB flavors. Now, we have some new information on more entry-level products such as the GeForce RTX 5060 and the GeForce RTX 5050.\n\nAs per our information, the NVIDIA GeForce RTX 5060 will use the \"PG152 SKU 25\" PCB, and will come equipped with a maximum of 8 GB memory. The RTX 5050 will have a rated TBP of 145W and is said to launch in April. Do keep in mind that the RTX 5060 Ti 8 GB models will also launch in April, but during the first half, while the RTX 5060 8 GB cards will most likely arrive in the second half of April.\n\nBut there's more: NVIDIA is also said to be preparing the GeForce RTX 5050, which is going to be positioned in the entry-level price segment. If the RTX 5060 retails for around $299 US, then we can expect the RTX 5050 to end up somewhere between the $199-$249 US price tag. This card will be positioned against the Intel Arc B580, which is a strong contender in this segment, while future AMD offerings may also spice things up in the sub-$300 US market.\n\nBased on what we have seen, the NVIDIA GeForce RTX 5050 features 8 GB of memory and will be rated at a TBP of 145W. This information is still a bit early, but the card is said to be rated at a TBP of 135W, which is 10W lower than the RTX 5060 8 GB. There's no information on the specific GPU or PCB yet, but we will make sure to keep you updated. As for the launch, the RTX 5050 is also aiming for an April release plan and may come in either Non-Ti or Ti branding.\n\nWell-known insider, MEGAsizeGPU, also posted about the launch plan of the GeForce RTX 5060 series and states that while the announcement will take place in about 10 days, the actual launch will be scheduled for next month.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "3 Reasons Why I'm Buying Nvidia's Stock Like There's No Tomorrow",
            "link": "https://www.fool.com/investing/2025/03/07/3-reasons-why-im-buying-nvidias-stock-like-theres/",
            "snippet": "Nvidia (NVDA 4.33%) has been the artificial intelligence (AI) stock to own since 2023. Its performance over this time frame has been incredible,...",
            "score": 0.9293981194496155,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) has been the artificial intelligence (AI) stock to own since 2023. Its performance over this time frame has been incredible, and its latest results were no exception. However, Wall Street seems to be getting bored with the stock.\n\nSimilar to how a sports franchise may be loved at the start of a dynasty run, then hated by the end of it, Nvidia seems not to capture the attention of the market anymore. It reported fantastic fourth-quarter results for its fiscal 2025 year (ended Jan. 26), and yet the stock sold off.\n\nSo is this an excellent time to load up on the stock? I've got three reasons why investors should be bullish on Nvidia's stock, not bearish.\n\n1. Blackwell growth\n\nNvidia's graphics processing units (GPUs) have powered the AI revolution because they can compete in parallel. This makes them perfectly suited to take on complex computing tasks like training AI models. Most of the AI training that we've seen to date has occurred on Nvidia's Hopper architecture, but that's being replaced by its latest design: Blackwell.\n\nBlackwell GPUs provide significant performance boosts over the previous Hopper design, including four times faster AI training. Furthermore, AI inference (which is when an AI model is given an input and a user expects an output) is 20 times cheaper than the Hopper 100 (H100) GPU. Blackwell GPUs will unlock a new phase of AI innovation that we haven't experienced yet, and this will continue to be a boost for Nvidia throughout the year.\n\nAlthough Blackwell chips made up $11 billion of Nvidia's $35.6 billion in data center revenue, they're still ramping up production on this game-changing product. However, this ramp-up also caused the one negative thing Wall Street focused on during Q4: falling gross margins. Nvidia's management was aware of this, as its gross margins fell from the mid-to high-70% range to 73% in Q4. This pattern should persist through Q1 but recover by the year's end as they become more efficient in producing Blackwell GPUs.\n\nThe big item here is to know that these margins will recover, and Nvidia is putting the customer first by getting out as many of its innovative Blackwell chips as possible. This ramp-up will continue to cause Nvidia's revenue to rise, and investors should be very bullish about that, even if its gross margins take a short-term dip.\n\n2. Sustained growth rates\n\nThere has never been a company of Nvidia's size that has sustained growth rates as strong as it has during its recent run. In Q4, revenue was up 78% year over year and up 12% over Q3. For Q1, management expects $43 billion in revenue, indicating 65% year over year growth and 9% quarter over quarter growth. However, Nvidia's management has a track record of under-guiding and overdelivering, so the real figures may be a few percentage points higher.\n\nThat's stunning growth for a company of Nvidia's size, and Wall Street expects that growth to persist throughout this year and next. For the current fiscal year and next year, Wall Street analysts expect 57% revenue growth and 22% growth, far exceeding what many of Nvidia's big tech peers are putting up.\n\nThere is still plenty of growth left in the AI space and other industries that Nvidia supports. The growth is far from over for Nvidia, and it's another bullish reason to purchase the stock.\n\n3. Nvidia's stock is borderline cheap right now\n\nNvidia shares have sometimes looked rather expensive throughout their run-up over the past few years, but I'd say they're starting to look rather cheap.\n\nNVDA PE Ratio data by YCharts.\n\nNvidia now trades for about 43 times trailing earnings and 28 times forward earnings and is posting phenomenal growth figures. Let's contrast that with other common big tech companies (like some of the Magnificent Seven).\n\nNVDA PE Ratio data by YCharts.\n\nWhile Nvidia is still the most expensive by its trailing P/E ratio, it's not by much. When forward earnings are considered, Nvidia is the cheapest of these other big tech companies.\n\nSo, why would you buy any of these other three when you can buy the company with the most to benefit from the most important innovation since the Internet? That's exactly why Nvidia is a screaming buy right now, and investors should be loading up on the stock after the weakness from its latest earnings report.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Labor Department Investigating Nvidia, Amazon-Backed Startup Scale AI",
            "link": "https://www.insurancejournal.com/news/national/2025/03/07/814690.htm",
            "snippet": "The U.S. Department of Labor is investigating Scale AI, a data labeling startup backed by tech giants Nvidia, Amazon and Meta, for its compliance with the...",
            "score": 0.5487301349639893,
            "sentiment": null,
            "probability": null,
            "content": "The U.S. Department of Labor is investigating Scale AI, a data labeling startup backed by tech giants Nvidia, Amazon and Meta, for its compliance with the Fair Labor Standards Act, the California-based firm said on Thursday.\n\nThe investigation is looking into Scale AI\u2019s compliance with fair pay practices and working conditions and was initiated nearly an year ago under the former President Joe Biden\u2019s administration, the company said.\n\nThe startup said over the last year it has worked with the department to explain its business model and the emerging artificial intelligence industry.\n\nFounded in 2016, Scale AI provides vast amounts of accurately labeled data, which is pivotal for training sophisticated tools like OpenAI\u2019s ChatGPT.\n\nIt also provides platform for researchers to exchange AI-related information, with contributors in more than 9,000 cities and towns.\n\n\u201cThe feedback we get from contributors is overwhelmingly positive, and we have dedicated teams to ensure people are paid fairly and feel supported,\u201d a spokesperson said.\n\nNearly all contributor payments are made on time and the company resolves 90% of payment-related inquiries within three days, the company said.\n\nThe startup was valued at $14 billion in a late-stage funding round last year and counts AI firms OpenAI and Cohere, as well as Microsoft MSFT.O Morgan Stanley MS.N among its clients.\n\nTopics InsurTech Data Driven Artificial Intelligence",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia's RTX 5060 and 5060 Ti rumored launch in 'ten days' \u2014 but don't expect any stock until April",
            "link": "https://www.tomshardware.com/pc-components/nvidias-rtx-5060-and-5060-ti-rumored-launch-in-ten-days-but-dont-expect-any-stock-until-april",
            "snippet": "A hardware leaker claims that Nvidia's mainstream RTX 5060 and RTX 5060 Ti GPUs could \"launch\" in just 10 days, with stock landing on shelves a month later.",
            "score": 0.9472140073776245,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's GeForce RTX 5060 and RTX 5060 Ti Blackwell GPUs are rumored to be releasing in just 10 days, according to hardware leaker @Zed__Wang. The RTX 5060 family of cards were previously rumored to launch in March, alongside a higher asking price than previous generation 60-class GPUs.\n\n\n\nMost likely, this is a guess based on Nvidia's GPU Technology Conference (GTC) that takes place from March 17\u201321 in San Jose, CA. It would be a prime showcase to announce the RTX 5060 class GPUs, which of course is an entirely different story than \"launching\" said GPUs.\n\n\n\nApply the usually dash of salt, but the timing does seem entirely appropriate, considering Nvidia also announced the RTX 5090, 5080, 5070 Ti, and 5070 at CES 2025 but didn't finish actually launching those GPUs until just this week with the RTX 5070 \u2014 not that most people were able to buy one, as graphics cards continue to sell out almost instantly.\n\nThe RTX5060 family will be released in about 10 days but will be on the shelf a month later.March 7, 2025\n\nThe leaker says that while the GPUs would be \"released in about 10 days\", stock on shelves won't arrive until April. Given how rare it is to find an RTX 50-series graphics card on a shelf, it's confounding to think Nvidia would want to paper launch a product without any real stock. Again, releasing details about the specifications and pricing ahead of an April launch seems more likely.\n\n\n\nThe 5060 Ti is expected to release with two VRAM configurations: 8GB and 16GB, according to ECC filings. It's also likely that Nvidia will stick with 8GB for the standard RTX 5060 model, which may hinder the GPU's performance, despite faster GDDR7 memory. But, since Nvidia has not formally announced the RTX 5060 or 5060 Ti yet, official specifications are still up in the air.\n\n\n\nGiven current trends when looking at the RTX 50-series stack, you can hazard a good guess at where Nvidia's mainstream-focused cards will land. The RTX 5060 Ti is likely to utilize the GB206 die, the same chip that's used in the RTX 5070 Laptop GPU. It could feature 36 SMs, or around 4,608 CUDA cores, alongside a 128-bit memory interface and GDDR7 VRAM. The RTX 5060 is much more uncertain. It could theoretically use the same GB206 die as the 5060 Ti, but it's also possible that Nvidia could use the further cutdown GB207.\n\n\n\nThe GB207 die is believed to feature up to 20 SMs and 2,560 CUDA cores, and a 128-bit memory interface. It would likely also use a PCIe 5.0 x8 interface rather than the full x16 interface, and that could apply to the GB206 as well. 20 SMs would be a notable step down from the RTX 5060 Ti, but until Nvidia lifts the lid on official specifications, we're still left waiting on exactly how their mainstream offerings might perform.\n\n\n\nIf the rumor surrounding the RTX 5060 and 5060 Ti's release (announcement) hold true, we shouldn't have to wait too long to hear official specifications, alongside seeing a handful of Nvidia-approved benchmarks.\n\n\n\nIt's expected that the duo of 60-class GPUs will be targeting 1080p resolutions for gaming workloads. And if Nvidia sticks with 8GB of VRAM on the base models, neural rendering or not it's going to be a tough sell. Demanding modern titles like Indiana Jones and the Great Circle, with all of its fancy ray-traced bells and whistles applied, can exceed 12GB of VRAM use, never mind a paltry 8GB \u2014 and it's not alone.\n\n\n\nPricing also remains unknown, and that goes for both MSRPs as well as real retail prices. MSRPs have basically been a sad joke for the RTX 50-series so far, with many custom AIB (add-in board) models selling at 50% or more above Nvidia's \"recommended\" price. Even then, scalpers are stepping in to push prices even higher. 184 RTX 5090 graphics cards were sold on eBay in the past month, with an average going rate of $4,664. RTX 5080 isn't much better, with 464 cards sold at an average price of $1,850. The RTX 5070 Ti has only been available since Feb. 20, but it has also seen 127 sales on eBay with an average price of $1,242.\n\n\n\nAnyone hoping to purchase an RTX 5060 class graphics card may be out of luck, and given the situation around GPU demand and stock constraints at retailers worldwide, we're not optimistic. Stock landing in April for an actual launch seems reasonable, but will Nvidia and its AIB partners have hundreds of thousands of cards ready, or will we see another case of perhaps thousands of cards that sell out in seconds? The portents aren't looking good for anyone hoping to get their hands on a new Nvidia GPU at MSRP this year.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NVIDIA reportedly planning GeForce RTX 5050 graphics card to tackle Arc B580",
            "link": "https://videocardz.com/newz/nvidia-reportedly-planning-geforce-rtx-5050-graphics-card-to-tackle-arc-b580",
            "snippet": "NVIDIA finally realizes there's an entry-level segment. What is this? Report Ad. If this rumor holds true, we may see an RTX 50 card that could cost eight...",
            "score": 0.8068779110908508,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-03-06": {
        "0": {
            "title": "\u2018Monster Hunter Wilds\u2019 Charges Onto GeForce NOW",
            "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-monster-hunter-wilds/",
            "snippet": "Capcom's wildly popular 'Monster Hunter Wilds' and EA's 'Split Fiction' are part of 8 games joining the GeForce NOW cloud this week.",
            "score": 0.7545431852340698,
            "sentiment": null,
            "probability": null,
            "content": "Time for a roaring-good time with Capcom\u2019s hit Monster Hunter Wilds. GeForce NOW members can hunt even the largest, most daunting monsters with the sharpest clarity, armed with a GeForce RTX 4080-class gaming rig in the cloud.\n\nPlus, jump into mind-bending adventures with Split Fiction from Hazelight Studios, an action-adventure experience that will keep players on the edges of their seats with plenty of unexpected twists.\n\nIt\u2019s all part of the eight games available to stream in the cloud this week.\n\nThe Hunt Begins\n\nHappy hunting in the cloud. The unbridled force of nature runs wild and relentless in Monster Hunter Wilds, with environments transforming drastically from one moment to the next. This is a story of monsters and humans and their struggles to live in harmony in a world of duality. Members can fulfill their duties as a Hunter by tracking and defeating powerful monsters and forging strong, new weapons and armor from materials harvested from the hunt, all while uncovering the connection between the people of the Forbidden Lands and the locales they inhabit.\n\nGeForce NOW members can join the ultimate hunting experience without waiting for game downloads or worrying about hardware space. Stream the title across devices, from underpowered PCs and Macs to the Steam Deck and virtual-reality devices. Performance members get six-hour gaming sessions, and Ultimate members get eight-hour sessions. Performance and Ultimate members can also stream with NVIDIA DLSS and ray-tracing technologies for the highest frame rates. This game has system requirements that require a GeForce NOW Performance or Ultimate membership \u2014 free members can upgrade today to join in on the action.\n\nJoin the Writer\u2019s Block Party\n\nSplit Fiction from Hazelight Studios, creators of the award-winning It Takes Two, is now available to stream in the cloud. Split Fiction is a cooperative adventure where science fiction and fantasy authors Mio and Zoe are trapped in a simulation that\u2019s stealing their stories.\n\nPlayers must work together using unique abilities in ever-changing worlds, ranging from cyberpunk cities to enchanted forests, to overcome diverse challenges like taming dragons, mastering laser swords and solving gravity puzzles. The game also features innovative split-screen mechanics and a Friend\u2019s Pass feature that enables one player to host the full game while their partner joins for free.\n\nSplit Fiction emphasizes teamwork and communication for a genre-bending, chaotic and imaginative co-op experience. Stream in the cloud today across devices with a GeForce NOW membership.\n\nHit the Gas on New Games\n\nMembers can now stream the newest season of The Crew Motorfest. Season six brings significant updates, including a full series of challenges, activities and surprises. Discover a new playground, striking new vehicles, world improvements and two new Playlists including \u201dRed Bull Speed Clash\u201d at the game\u2019s launch. The enhanced player vs. player experience offers weekly themed Grand Races, vehicle handling improvements and new features like Photo Quest fast travel. Enjoy an even more immersive and enjoyable open-world driving experience across the Hawaiian islands of O\u2019ahu and Maui with the wings of Red Bull, streaming on GeForce NOW.\n\nLook for the following games available to stream in the cloud this week:\n\nWhat are you planning to play this weekend? Let us know on X or in the comments below.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "What it\u2019s like to use the RTX 5070 Ti, before and after a much-needed PC overhaul",
            "link": "https://www.polygon.com/review/533860/nvidia-rtx-5070-ti-review-3070-upgrade-worth-it",
            "snippet": "Nvidia's RTX 5070 Ti is a great GPU in the right desktop. However, it alone can't bring the performance upgrade you may expect to outdated systems.",
            "score": 0.9229544997215271,
            "sentiment": null,
            "probability": null,
            "content": "Trying to figure out what\u2019s causing slow frame rates in a PC is sort of like solving for a mysterious leak in your home. It could be the graphics card, or it could be slow storage, outdated RAM, the CPU \u2014 one, or all of the above. While doing a complete overhaul is a surefire way to solve the problem, it\u2019s expensive and not financially savvy in the slightest. Ideally, you isolate the issue, then methodically make upgrades as needed.\n\nSo, yeah, my PC is getting old. The Monster Hunter Wilds graphics benchmark made me realize I would need some upgrades sooner than later (ideally, before Grand Theft Auto 6 and The Witcher 4), as it chugged and textures were slow to load in. My RTX 3070 Ti is an unlikely suspect, being the newest component in my setup, and otherwise not having issues running most games pretty well at 1440p resolution. On the other hand, my 2018 Intel Core i5-9600K is probably the culprit, having fallen below the minimum required specs in many new, graphically demanding games coming out.\n\nA slow, pre-pandemic CPU simply won\u2019t do with today\u2019s games. Photo: Cameron Faulkner/Polygon\n\nWith the RTX 5070 Ti showing up at my doorstep, I didn\u2019t feel like I could fairly assess it with my preexisting hardware. So, this review took me on a journey during which I explored multiple hardware scenarios, ultimately ending up with a near-total rebuild of my PC. My testing illustrates how a new GPU can inject life into an outdated build, and how sometimes that\u2019s not the case. Additionally, I explored how much just refreshing the hardware surrounding the GPU can improve gaming performance, even if you have an old-ish graphics card. Plus, unlike fixing a leak, these are issues that I actually had fun solving.\n\nI compared anecdotal performance in Avowed and Indiana Jones and the Great Circle, then checked out controlled benchmarks in Cyberpunk 2077 and Monster Hunter Wilds \u2014 all in 1440p and on the highest possible settings, including ray tracing when possible.\n\nGiving my old PC a $750 implant\n\nHooking up the $749.99 Asus Prime RTX 5070 Ti to my preexisting rig required just a bit of tweaking; I needed to plug in an additional PCIe power cable into my 750 W power supply to connect to the GPU. My 3070 Ti uses just two PCIe cables from the power supply, while the RTX 50-series cards released so far require three. Once I powered it on, I reinstalled the Nvidia GPU driver and got to gaming. It was illuminating in ways I didn\u2019t expect to run games with the RTX 5070 Ti installed, as some titles performed much better than before, while others didn\u2019t improve enough over my RTX 3070 Ti\u2019s performance to warrant the 5070 Ti\u2019s high cost. It simply depends on what you play.\n\nIn Avowed, which is a very CPU-intensive game (my 2018 CPU just made the cut for minimum spec requirements), the RTX 5070 Ti made it possible to get about 10 more frames per second on average than I was getting before. It went from a relatively stable 40-50 frames per second (with dips to about 35) in the 3070 Ti to a more stable 50-60 in the RTX 5070 Ti. Performance improved more once ray tracing was turned off, of course. Based on the miniscule performance gains in this title, I\u2019d be kicking myself for having paid for the RTX 5070 Ti.\n\nImage: Obsidian Entertainment/Xbox Game Studios\n\nCyberpunk 2077 is more reliant on the GPU than other open-world games I\u2019ve tried. And so the 5070 Ti delivers considerably improved performance despite being installed in my older system. With Frame Generation (FG, a feature exclusive to RTX 40- and newer GPUs) turned off, the 5070 Ti achieved an average of 57 frames per second in the game\u2019s benchmark with ray tracing set to \u201cPsycho\u201d mode and the very intensive path-tracing graphical mode on (which does an even better job than ray tracing in terms of showing how light bounces realistically) \u2014 more than doubling what my 3070 Ti could do. With FG turned on with the 5070 Ti, the frame rate shot up to 100. Having FG at my disposal is an incredible asset (especially in this aging PC), letting me keep the quality cranked up without sacrificing frame rate. But big graphically demanding games that are primarily taxing to GPUs, and not to CPUs as well, aren\u2019t so common.\n\nTo my surprise, Indiana Jones and the Great Circle ran well enough on my old rig with the 3070 Ti that I could have gotten through it without too much trouble. Despite bypassing errors about not having enough VRAM to run high graphical settings, it ran comfortably between 60-70 frames per second. With the RTX 5070 Ti, primed to take advantage of the new DLSS 4 update, there were plenty of new options to turn on, including path tracing, quality toggles for various ray-tracing effects, and Multi Frame Generation (MFG, exclusive to RTX 50-series cards, can display up to three AI-created frames for every one frame rendered traditionally). Performance shot up to about 130 frames per second without path tracing, and about 75 with it on.\n\nImage: Capcom via Polygon\n\nThe most brutal test for my old system, as mentioned before, was none other than Monster Hunter Wilds. No matter the GPU, this game and its hardware benchmarks did not want to play nicely with my machine. The Intel i5 9600K is one generation below Wilds\u2019 minimum specifications, and the downsides of not meeting it were less forgiving than I expected. Textures often loaded in slowly, and frame rates were anything but stable \u2014 even with the RTX 5070 Ti installed. Neither GPU could achieve a stable 60 frames per second with all settings turned up, with the newer GPU adding just a handful of frames onto what the 3070 Ti could do. Frame Generation in the 5070 Ti greatly improved the performance, going up to 94 frames per second.\n\nIn most cases, the RTX 5070 Ti\u2019s improvements were noticeable in my old machine. But if I didn\u2019t have a near-total PC rebuild on the horizon, I\u2019d definitely regret buying it. As previously mentioned, I (responsibly for this review, but irresponsibly for my budget) lined myself up with some key upgrades to get the most out of the new GPU. I got an AMD Ryzen 7 9700X processor, 32 GB of much faster DDR5 RAM, and a modern motherboard totaling $399.99. I kept my old case, the power supply, and my PCIe 3.0 M.2 SSDs (full setup here).\n\nNaturally, the first thing I did with this brand-new setup was slot in the older RTX 3070 Ti. I wanted to see just how much a new CPU could uplift performance in the games I was playing, or not.\n\nNew system, old GPU\n\nAvowed on this new system was a much smoother experience with the RTX 3070 Ti installed. The frame rate hung around in the 70s no matter the location (to reiterate, this is better than the performance I got with the 5070 Ti installed in my old system). If I was keen on keeping my GPU, or having a difficult time finding an RTX 5070 Ti in stock, I\u2019d be ecstatic with this improvement, to the point that I might not upgrade to this 50-series generation.\n\nThat enthusiasm wore off quickly because, in Cyberpunk 2077, my upgrades made next to no difference to the frame rate. Good to know, but not surprising. Same goes for Indiana Jones; there were minor improvements, but nothing to write home about.\n\nWith this new foundation of hardware, some games yield better performance \u2014 even with the older RTX 3070 Ti. Photo: Cameron Faulkner/Polygon\n\nMonster Hunter Wilds yielded some interesting improvements with the combo of the faster processor and memory. The jittery performance and texture pop-in was eliminated, and I was thrilled to wave goodbye to that mess. That said, it averaged out to a similar, but smoother-appearing frames rate as my older PC configuration. Whereas I\u2019d be miserable enduring the choppy gameplay I noted earlier with either GPU in my old build, I think I could tolerate this experience with the RTX 3070 Ti.\n\nProbably the part you\u2019ve been waiting to read\n\nI finally let the RTX 5070 Ti get to work in the new configuration, and it felt like such a long time coming, what with all of the permutations I\u2019d been testing (and the fun but stressful weekend of PC building).\n\nThere were monumental frame rate improvements in all of the games I\u2019d been testing. Avowed soared above 100 frames per second. It feels wonderful to play without staring at the frames per second counter. The 5070 Ti made short work of Cyberpunk 2077\u2019s most intense graphical settings, running it with path tracing, \u201cPsycho\u201d ray-tracing settings, and Multi Frame Generation (set to two frames generated by AI) at 124 frames per second. Doubling MFG to four frames resulted in performance going up to a whopping 219 frames per second. If you have a high-refresh-rate monitor that goes above 144 Hz, the 5070 Ti will let you squeeze more value out of your tech.\n\nPhoto: Cameron Faulkner/Polygon\n\nIndiana Jones\u2019 many ray-tracing features include extensive path tracing along with MFG, making it possible to experience the best visuals and a high frame rate. With all settings switched to their maximum values, I was able to get about 100 frames per second. Some people may be sensitive to the latency introduced by MFG, but so long as I kept it at 2x instead of 4x, things felt fine for me. Without MFG on, the 5070 Ti ran the game at a smooth 70 frames per second \u2014 not bad.\n\nIn the end, I solved my PC problems, and it runs better \u2014 with or without the RTX 5070 Ti. Nvidia is positioning the RTX 5070 Ti as being a great card to update to from the RTX 3070 Ti, and The Verge\u2019s review considers it to be on the level of an RTX 4080, but cheaper. I, too, can attest that it\u2019s a great upgrade, and that it\u2019ll be a good component to keep around for the next five years or so. But, for anyone out there whose PC components are already five or more years old, you might have bigger problems than just a GPU. And even a powerful GPU like this one can\u2019t magically make them go away.\n\nThe Asus Prime RTX 5070 Ti is available to purchase from multiple retailers. It was tested using a retail unit provided by Nvidia. Vox Media has affiliate partnerships. These do not influence editorial content, though Vox Media may earn commissions for products purchased via affiliate links. You can find additional information about Polygon\u2019s ethics policy here.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "OpenAI, Oracle Eye Nvidia Chips Worth Billions for Stargate Site",
            "link": "https://www.bloomberg.com/news/articles/2025-03-06/openai-oracle-eye-nvidia-chips-worth-billions-for-stargate-site",
            "snippet": "OpenAI and Oracle Corp. plan to begin filling a massive new data center in Texas with tens of thousands of powerful AI chips from Nvidia Corp. in the coming...",
            "score": 0.7953503131866455,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia leads chipmaker stocks lower as investor fears over AI demand continue to weigh",
            "link": "https://finance.yahoo.com/news/nvidia-leads-chipmaker-stocks-lower-as-investor-fears-over-ai-demand-continue-to-weigh-151656986.html",
            "snippet": "Nvidia stock dropped nearly 6% Thursday, leading other chipmakers down as fears over AI demand continued to weigh on the stocks.",
            "score": 0.9663645625114441,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock fell 5.7% Thursday, leading other chipmakers down as fears over AI demand continued to weigh on the sector.\n\nThe drop puts Nvidia shares down nearly 18% year to date, with the AI chipmaking giant seeing its worst monthly performance in February since June 2022.\n\n\"It\u2019s been a rough year for NVDA so far. ... The stock (along with many of its AI-semi peers) has suffered, battered by a storm of growth fears, supply chain noise, and tariff and regulatory risks,\" Bernstein analyst Stacy Rasgon wrote in a note to investors earlier this week.\n\n\"Sentiment has clearly pivoted for now on the AI group.\"\n\nFuturum Group analyst David Nicholson added in his own analysis shared with Yahoo Finance: \"Wall Street is catching up to the reality that Nvidia will not create a decades-long dynasty like Intel once did. Competition is hitting them from dozens of directions.\"\n\nIndustry news late Wednesday and early Thursday did little to quell investor anxieties.\n\nThe Financial Times reported Wednesday night that CoreWeave \u2014 a private cloud services company prepping to launch a $35 billion IPO \u2014 has lost business from Microsoft (MSFT) over delivery issues and missed deadlines. CoreWeave is a large Nvidia customer.\n\nSeparately, custom AI chipmaker Marvell Technology (MRVL) \u2014 which supplies semiconductors to Amazon and Microsoft \u2014 reported quarterly financial results after the bell Wednesday that failed to impress investors looking for an AI payoff. Also on Thursday, Chinese tech giant Alibaba (BABA) unveiled an AI model that it said rivals DeepSeek's R-1, whose introduction spurred market fears over a reduction in AI hardware spending as models become more cost-efficient.\n\nRaymond James analysts said Marvell's fourth quarter earnings and guidance were \"more modest than we anticipated, which is a surprise given the strong results from peers.\"\n\nMarvell stock plunged almost 20% Thursday.\n\nOther artificial intelligence chipmakers and AI-adjacent stocks also suffered early Thursday.\n\nCustom AI semiconductor maker Broadcom (AVGO) fell over 6%, and GPU maker Advanced Micro Devices (AMD) fell nearly 3%. British chip architecture designer Arm (ARM) dropped 5.5%, Micron (MU) sank 5.4%, and Qualcomm (QCOM) declined 1%.\n\nBroadcom and AMD are seen as rivals to Nvidia, while Arm and Micron are partners whose products are used in Nvidia's server designs. Broadcom is set to report earnings after the bell on Thursday.\n\nThe declines come as investors size up the hype over artificial intelligence and digest Big Tech's stated AI investments.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Here's what I'm going to do instead of upgrading to an Nvidia RTX 50 GPU",
            "link": "https://www.xda-developers.com/not-upgrading-nvidia-rtx-50-gpu/",
            "snippet": "It's not really news to anyone that gamers aren't exactly lining up for Nvidia's RTX 50 series (not that they're in stock anyway).",
            "score": 0.9103489518165588,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Why Nvidia Stock Is Sinking Today",
            "link": "https://www.fool.com/investing/2025/03/06/why-nvidia-stock-is-sinking-today/",
            "snippet": "Nvidia (NVDA 4.40%) stock is getting hit with sell-offs in Thursday's trading. The company's share price was down 5.1% as of 2 p.m. ET amid a 2% decline for...",
            "score": 0.8193390369415283,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) stock is getting hit with sell-offs in Thursday's trading. The company's share price was down 5.1% as of 2 p.m. ET amid a 2% decline for the S&P 500 and a 2.5% decline for the Nasdaq Composite.\n\nInvestors are selling out of artificial intelligence (AI) and semiconductor stocks today after Marvell Technology saw a big valuation pullback despite posting better-than-expected Q4 results and guidance. The negative reaction to Marvell's earnings report has spurred sell-offs for AI stocks, and Nvidia's share price is taking a hit as investors signal the need for higher growth to support valuation levels amid macroeconomic and geopolitical risk factors.\n\nNvidia stock falls after Marvell plummets despite a solid Q4 report\n\nMarvell's non-GAAP (adjusted) earnings per share of $0.60 in the fourth quarter topped Wall Street's call for per-share earnings of $0.59, and sales of $1.82 billion exceeded the average analyst forecast by $20 million. But investors weren't satisfied with annual growth of approximately 27% for sales and 33% for adjusted earnings.\n\nMarvell's guidance also wasn't hot enough to support bullish sentiment despite beating the average Wall Street forecast. The chip specialist targeted revenue of approximately $1.875 billion, which beat the consensus forecast for sales of $1.87 billion. Management's guidance for an adjusted gross margin of 60% suggests that earnings should be strong in the period, but the outlook wasn't enough to prevent big sell-offs for Marvell -- and the pullback is extending to Nvidia and other AI stocks.\n\nWhy is the market's reaction to Marvell's results bearish for Nvidia?\n\nMarvell is a leading provider of connectivity chips and other solutions that have been seeing demand tailwinds in conjunction with the rise of AI. As a result, the company's performance has come to be viewed as an indicator for overall demand trends in the artificial intelligence space.\n\nWhile the company's recent fourth-quarter report points to a strong near-term outlook, the market's reaction to its Q4 performance and forward guidance suggests that investors are feeling more cautious about valuations for AI stocks -- and that's weighing on Nvidia's share price.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Inside The Quantum Computing Crash Triggered By Nvidia CEO \u2014 And What His Upcoming 'Quantum Day' May Bring",
            "link": "https://www.investors.com/news/technology/quantum-computing-jensen-huang-nvidia-ai/",
            "snippet": "Nvidia (NVDA) Chief Executive Jensen Huang's controversial remarks about quantum computing reached Rigetti Computing CEO Subodh Kulkarni via text.",
            "score": 0.7348942756652832,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) Chief Executive Jensen Huang's controversial remarks about quantum computing reached Rigetti Computing CEO Subodh Kulkarni via text. A frantic acquaintance in the investment community wanted to know, \"What's going on? Why is Jensen so negative about quantum computing?\"\n\nGil Luria, an analyst with D.A. Davidson, had just given another quantum computing company, IonQ, a buy rating. He was following the CES event in January where the Nvidia CEO declared that quantum computing won't be \"very useful\" for 15-30 years.\n\nQuantum Computing: Nvidia CEO Triggers Crash\n\n\"When I heard that, I rolled my eyes a little bit,\" Luria, the firm's head of technology research, told Investor's Business Daily. \"But I realized that that was going to have an impact.\"\n\nThe impact was stunning. Huang's remarks sent quantum stocks crashing as Rigetti Computing (RGTI) plunged 45% in one day, while D-Wave Quantum (QBTS) shed 36%, IonQ (IONQ) tumbled 39% and Quantum Computing Inc. (QUBT) dropped 43%.\n\nThe slump turned a harsh spotlight on a technology that's expected to transform computing. But a debate rages on when it will actually take off and how big the quantum market will be in the near term.\n\nHuang's comments also highlighted questions, even worries, about quantum computing's impact on artificial intelligence, the technology now in the Wall Street limelight and dominated by Nvidia.\n\nNvidia's First 'Quantum Day'\n\nThe debate will be rekindled this month when industry leaders gather in Silicon Valley for Nvidia's first Quantum Day. The March 20 event highlights a quirky twist in the controversy: Huang himself will be the host. Nvidia announced the gathering a week after its CEO's comments sent quantum computing stocks tumbling.\n\nTop quantum industry executives are scheduled to speak. They include D-Wave Quantum CEO Alan Baratz, who said Huang reached out via email to invite him to Quantum Day shortly after his comments roiled the market. Asked how that conversation went, he said, \"That's between me and him.\" Nvidia declined to comment for this story.\n\nA Quantum Computing CEO Pushes Back\n\nBaratz has been the most outspoken of the industry CEOs in hitting back at the Nvidia CEO's comments. He blasted Huang's remarks in a LinkedIn post as quantum computing stocks were plummeting. \"I have not held back,\" he told IBD.\n\n\"The quantum computing industry is still young, and the investor market is still maturing,\" Baratz said. \"Everybody's looking for any information that they can use to make decisions. Sometimes misinformation can get out there and drive decisions. And that's what happened with Jensen's comment. He's respected as somebody who has significant technology prowess. So when he says something stupid, people don't think it's stupid.\"\n\nIt certainly didn't help that Huang was talking about a very complex technology that has baffled even veteran technologists like Bill Gates.\n\nWhat Is Quantum Computing?\n\nQuantum computing is widely considered a powerful and exciting technology poised to outperform what the industry refers to as classical computing. The technology operates based on the principles of subatomic physics. A quantum computer uses subatomic particles, such as protons and electrons, and can perform computations exponentially faster than traditional computers.\n\nBut unlike classical computing, some quantum computing systems require specialized environments, including the need to store the machines at extremely low temperatures. They are also known to be very expensive, although quantum computing players offer a range of options.\n\nCost Of A Quantum Computer\n\nBaratz said it costs D-Wave \"less than $2 million to build, install and calibrate a quantum computer.\" D.A. Davidson analyst Alex Platt estimated that the typical cost is \"in the tens of millions for a single system.\"\n\n\"It's an expensive price tag right now for sure,\" he told IBD.\n\nThat's a major hurdle for the technology, said Silicon Valley investor Rob Siegel, a management lecturer at the Stanford Graduate School of Business.\n\n\"Not only is the technology still very nascent, but to commercialize it requires an entire system-level design, and to put the systems together is a nontrivial task,\" he told IBD.\n\n\"The whole thing is a complex, expensive, big, power-hungry unit that needs to be put together. Nobody is shipping quantum computers in volume, on any kind of scale in the next five to seven years. No one, zero, zilch.\"\n\nThe Year of Quantum Computing\n\nWhat made the turmoil triggered by Huang's comments frustrating for industry leaders is that it hit as they were basking in the glow of upbeat quantum computing news in late 2024.\n\nShares of IonQ rose in September after the company announced a $54.5 million contract with the U.S. Air Force Research Lab. Quantum computing stocks also rallied after Amazon announced its Quantum Embark Program, aimed at helping customers get ready for the technology.\n\nThe year closed with even bigger news. On Dec. 9, Google unveiled its latest quantum chip, called Willow. A week later, Quantum Computing Inc. announced that the National Aeronautics and Space Administration's Goddard Space Flight Center had awarded the company a quantum computing contract. The news sparked a rally not just in shares of Quantum Computing Inc., but also D-Wave, Rigetti and IonQ.\n\n\"We saw the excitement rise,\" Baratz said. \"There was discussion that 2025 is the year for quantum. Jensen put some cold water on that.\"\n\nQuantum Computing: A Question Of Timing\n\nBaratz maintains that the perception that the technology is not yet ready for prime time is inaccurate and unfair. Quantum computing, he argues, is already having a meaningful impact.\n\n\"We have companies that are seeing benefits today, that are actually using our system today as part of our operation,\" he said. \"We are delivering value today.\"\n\nThe view was echoed by Christian Klein, CEO of enterprise software giant SAP (SAP), which has been testing the use of quantum computing in running supply chains. The results have been promising and impressive, Klein said.\n\nWhat would \"normally, today, take a week we now can do in an hour,\" he told IBD. \"Give this technology a few more years, probably we will talk hours or minutes at a certain point in time. We see quantum, not in 10 or 15 years, but certainly in three to four years.\"\n\nQuantum Computing And AI\n\nFor Wall Street, a key question is what quantum computing means for the hot tech trend of the day: AI.\n\nSilicon Valley veteran John Chambers, the former CEO of Cisco Systems (CSCO), called the 2020s \"the decade of AI.\" He downplayed the near-term impact of quantum computing on this trend.\n\n\"I think it is a very important technology, but it is further out,\" he told IBD. \"If you're looking for economic returns in terms of products and the next five years, you're probably too far ahead of your headlights.\"\n\nSiegel of Stanford downplayed quantum computing's potential impact on Nvidia.\n\n\"So for all the competitive things that should keep Jensen up at night for the next five to seven years, quantum should not be it,\" he said.\n\nOthers disagree.\n\nIn fact, quantum computing's impact on AI and Nvidia will be huge, D.A. Davidson analysts argue.\n\nLuria said the technology poses \"an existential threat\" to the chip behemoth. He also stressed that quantum computing and AI are not competing trends.\n\n\"Quantum computing isn't what's next to AI,\" Luria said. \"There are two parallel paths that are intersecting. Quantum will at some point be helpful toward creating really sophisticated AI. A quantum computer may replace the data center GPUs \u2013 but it's not replacing AI.\"\n\nAI Impact: 'Sooner Than You Might Expect'\n\nBaratz of D-Wave acknowledged that \"nobody in the quantum industry today is having a significant impact on AI.\" But it could happen \"sooner than you might expect,\" he said.\n\nHe pointed to the work that D-Wave is doing with the pharmaceutical division of Japanese Tobacco Inc. that is focused on using AI for drug discovery.\n\nIonQ is also using its technology for drug discovery with the biopharma giant AstraZeneca (AZN). If IonQ succeeds, that work could be \"a very very big deal,\" Luria said.\n\n\"If that was to happen, two, three, five years down the road, that would be the equivalent of a ChatGPT moment because then everyone would say, 'Oh wow, if we didn't have a quantum computer, we wouldn't have this new drug that's going to generate billions of dollars for AstraZeneca,'\" he said.\n\nQuantum Vs. Nvidia Chips\n\n\"Within a five-year time frame, it's going to get to a point where we're going to use a different type of architecture for the most complicated problems,\" Luria said.\n\nTo be sure, Nvidia chips, known as GPUs, will remain important in AI, he stressed.\n\nBut \"we're not going to need Nvidia GPUs for everything,\" Luria said. \"We're not going to need to build these increasingly large arrays of Nvidia GPUs, because we're going to be able to replace those with a quantum computer. And that's where their runway starts to end.\"\n\nOn the other hand, Huang's comments altered investors' perception of how much runway quantum computing stocks have before the segment takes off.\n\nHow Big Is Quantum Computing Market?\n\nQuantum computing is projected to become a $90 billion to $170 billion market by 2040, according to a July 2024 Boston Consulting Group report.\n\nHuang's comments spooked quantum computing investors who were banking on the technology taking off sooner than later.\n\n\"When your expected cash flows are far in the future, even a little change in how far in the future makes a lot of change to what the value is today,\" Luria said. \"If I believe it's five years out, I think quantum computing is worth $10 billion today. If I believe it's 15 years out, it's only worth $2 billion. That's what happened broadly. People changed their minds about how far out it is.\"\n\n\"Investor behavior is often volatile when things are less concrete and more pie in the sky,\" he added.\n\nBig Tech Companies Weigh In\n\nBut what was also striking was how that volatility was triggered, not by an industry or academic report or an earth-shattering disclosure by a major quantum computing player, but by the comments of prominent technology figures.\n\nShortly after Huang's remarks, Meta CEO Mark Zuckerberg echoed the Nvidia CEO's view. He told commentator Joe Rogan that quantum computing is \"still quite a ways off from being a very useful paradigm.\"\n\nWhile Huang is considered a tech executive who \"is in the know,\" Zuckerberg offered what was essentially an offhand comment, Luria said.\n\n\"I listened to the Joe Rogan podcast. It was offhand,\" Luria said of Zuckerberg's comment. \"He said, 'Look, I'm not really an expert, but it seems like it's decades away.' He is not an expert \u2014 but he's Mark Zuckerberg.\"\n\nBaratz hammered the same point on what Zuckerberg said. \"Seriously, once he said, 'I don't know anything about quantum,' everybody should just have ignored everything.\"\n\nWhen Big Tech CEOs Speak\n\nThe biggest issue confronting the quantum computing industry, Baratz argued, is \"around Big Tech CEOs who are misinformed, saying things either publicly or to customers that could stall our work with them.\"\n\nHuang's remarks, he said, suggested that \"either he didn't understand that there were different approaches to quantum or he did, and he just simply was sloppy in the language that he was using.\"\n\nKulkarni of Rigetti Computing was less critical of Huang, telling IBD: \"Honestly, I didn't think it was as negative as what the media portrayed it to be. Because there was a context before and after. But unfortunately, that one sentence got a lot of coverage \u2014 and that sentence definitely was very negative.\"\n\nSiegel of Stanford described Huang and Zuckerberg as \"two very smart, powerful people, so when they talk, you listen.\"\n\n\"But I think anybody who's an analyst and is covering the quantum space should be talking to all the public and private companies, building a holistic picture,\" he said. \"One or two comments by a couple of CEOs should not move the market 30%. But then again, I don't understand meme stocks.\"\n\nAre Quantum Computing Stocks A Buy Now?\n\nQuantum computing stocks stabilized for a couple weeks or more after the January crash, but they're down sharply from where they started the year. Shares of Rigetti, D-Wave and Quantum Computing Inc. have each maintained the highest-possible Relative Strength Rating of 99, according to MarketSurge. IonQ has an RS Rating of 98. But those ratings reflect the stocks' sharp gains last year. The relative strength lines on their stock charts show they have been underperforming the market recently.\n\nEach stock's Composite Rating, which provides an overall score covering a combination of fundamental and technical metrics, also suggests caution. While Nvidia holds an impressive Composite Rating of 90 on a 1-99 scale, IonQ's is at 50, D-Wave at 73, Rigetti at 65 and Quantum Computing Inc. at 72.\n\nA Wild Ride\n\n\"What was phenomenally interesting was how quickly this played out,\" Luria said, noting how the \"quantum space declined by about half\" after Huang and Zuckerberg said the technology will take a long time to have an impact. Then Nvidia announced its first Quantum Day, calling the technology \"one of the most exciting areas in computer science, promising progress in accelerated computing beyond what's considered possible today.\" That same day, Microsoft unveiled its Quantum Ready program to help businesses prepare for the technology.\n\n\"And the whole thing doubled again, which is to say our expectations went from five years to 15 years to 10 years back to five years over a period of two weeks,\" Luria said. \"That's how you can explain the stock moves: When discounting cash flows, it matters a lot if all my cash flows are five years out or 15 years out.\"\n\nAnd clearly, quantum computing is a big deal for Big Tech. This was underscored last month when Microsoft unveiled its first quantum chip. Amazon made a similar quantum chip announcement a week later.\n\n\"These large technology companies want to convey that they are planning for the future of technology and the next big platform,\" Luria said. \"Once Google released its Willow announcement, Microsoft and Amazon felt a need to make sure the technology community did not think of them as falling behind on this front.\"\n\nQuantum Computing Powwow\n\nIn a way, the upcoming Nvidia Quantum Day comes across as the AI chip giant's way of making amends for the turmoil its CEO caused \u2014 and an affirmation of quantum computing's importance.\n\n\"Nvidia having a quantum day is very tangible,\" Luria said. \"This is a result of them planning their future. If Nvidia really thought that quantum was 15 to 30 years out, would they have an event today? Probably not.\"\n\nKulkarni, Rigetti's CEO, said the Nvidia event \"gives all of us a chance to be on the stage at the same time.\"\n\n\"This was a nice way to say, 'Let's have a good dialogue,'\" he said. \"These are all uncertain things.\"\n\nGrappling With Uncertainty\n\nIn fact, the debate over quantum computing underscores how the tech industry routinely grapples with uncertainty. The recent tech sell-off sparked by the launch of DeepSeek's AI agent underscored this.\n\nFor decades, AI was viewed as a \"pie in the sky\" sci-fi concept, the stuff of \"Star Wars\" and \"The Terminator.\" That changed in 2016 when AlphaGo, an AI program created by Google's DeepMind, became an international sensation when it defeated the world champion of Go, the board game that computers found difficult to master. Six years later, with the 2022 release of ChatGPT, AI exploded as a pop culture phenomenon now driving a powerful tech trend.\n\nWhen will quantum computing have its AlphaGo or ChatGPT moment?\n\n\"No one's exactly, precisely going to be right,\" Kulkarni said. \"We are all engineers by training. We all understand the risks with emerging technologies. It's good to have healthy dialogue with industry leaders.\"\n\nKulkarni even gives Huang some credit for making dialogue possible, even though he triggered the turmoil.\n\n\"Someone like him taking personal interest in hosting the event, I think carries a lot of credibility,\" he said. \"I'm glad we have reached this outcome. I wish we didn't go through such movements of the stock prices. But that's our life I suppose.\"\n\nYOU MAY ALSO LIKE:\n\nIPO Market Slump Drags On As Unicorns Multiply. Why There's Hope For More IPOs In 2025.\n\nNvidia-Backed CoreWeave's IPO Filing Shows Big Revenue Jump, Helped By Microsoft\n\nIBD Live: A New Tool For Daily Stock Market Analysis",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia denies allegations that laptop RTX 50 GPUs are also missing ROPs",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-denies-allegations-that-laptop-rtx-50-gpus-are-also-missing-rops",
            "snippet": "RTX 50 series laptop GPUs are reportedly missing ROPs, just like the RTX 5090, RTX 5080, and RTX 5070 Ti. However, Nvidia has countered these reports,...",
            "score": 0.8983741402626038,
            "sentiment": null,
            "probability": null,
            "content": "New reports have surfaced that RTX 50-series laptop GPUs are facing the same missing ROPs problem as all of Nvidia's desktop counterparts. Heise Online (via HardwareLuxx) reports that Nvidia has asked notebook manufacturers to catch bad batches of laptop devices sporting faulty RTX 50-series GPUs with missing ROPs.\n\nHowever, Nvidia has responded to these claims, stating the exact opposite. Nvidia told The Verge that laptop GPUs are unaffected by the issue of missing ROPs. The Verge asked Nvidia for additional clarification, to which Nvidia stated: \"Correct, no further issues.\"\n\nThe original report from Heise Online states that several notebook manufacturers are allegedly working extra shifts in the Far East to catch buggy laptops with GPUs sporting missing ROPs before they get sold to the public. As previously stated, Nvidia (ironically) was allegedly the one to ask these companies to look for these faulty laptop GPUs.\n\nNvidia's response to these reports does not fully confirm if the issue was ever present or if it was only recently fixed. According to Nvidia's wording, there's a chance RTX 50 series laptop GPUs were, in fact, missing ROPs only to be fixed by the time reports came out stating there were problems.\n\nRegardless, if the reports are true, Heise Online did not disclose how many GPUs are affected. There's not even an indication if certain GPUs or all RTX 50 series laptop GPUs are missing ROPs (after all, most existing desktop Blackwell GPU models launched so far are known to suffer from this issue).\n\nHowever, the problem (again, if it did actually happen) appeared to have only affected a fraction of laptop devices (just like Nvidia's desktop counterparts). One manufacturer Heise Online spoke to allegedly only mentioned that its first batch of laptop devices had to be fixed. The manufacturer neglected to share if more batches were affected.\n\nAgain, if Nvidia's claim is true, it is good to hear it has finally plugged the missing ROPs issue on its latest RTX 50 series GPUs. However, the RTX 50 series laptop GPUs are still suffering from supply chain issues just like their desktop counterparts, forcing manufacturers to postpone their device launches.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nHigh-end devices have reportedly been pushed back from January to March, and mid-range and lower devices have been pushed back from March to April.\n\nThe missing ROPs anomaly first started on the RTX 5090 when reports of defective RTX 5090 AIB partner models sporting fewer ROPS than advertised began to spring up in late February. Soon afterward, Nvidia issued an official statement confirming that the issue was real, affecting not only the RTX 5090 but the RTX 5070 Ti as well.\n\n\n\nHowever, after Nvidia's statement, reports of defective RTX 5080s also began to appear, forcing Nvidia to publish another statement confirming RTX 5080s were also affected.\n\nMissing ROPS on a GPU is bad for performance as ROPs (Render Output Units) are part of the render pipeline. Third-party testing confirmed that the RTX 5080, with its missing ROPs, loses up to 11% of its performance compared to its advertised performance. The RTX 5090 likewise also allegedly loses up to 11% of performance, at least in TimeSpy.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia, Broadcom, and Other Chip Stocks Drop Thursday as AI Trade Falters",
            "link": "https://www.investopedia.com/nvidia-broadcom-and-other-chip-stocks-drop-thursday-as-ai-trade-falters-11692103",
            "snippet": "Nvidia and other chip stocks fell in early trading Thursday as the artificial intelligence trade faltered.",
            "score": 0.9695955514907837,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) and other chip stocks fell in early trading Thursday as the artificial intelligence trade faltered.\n\nShares of the AI chipmaker were down more than 2% in recent trading. Advanced Micro Devices (AMD), Qualcomm (QCOM), and TSMC (TSM) were also lower, along with shares of Nvidia partners such as Supermicro (SMCI), Dell (DELL), and Micron Technology (MU).\n\nShares of Broadcom (AVGO), which is set to report earnings after the bell, were down more than 3%, pulling the PHLX Semiconductor Index down 2%, while shares of chipmaker Marvell Technology (MRVL) plunged 17% after its outlook failed to impress. The tech-heavy Nasdaq fell 1%.\n\nUncertainty about the impact of new U.S. tariffs weighed on stocks, along with worries about tightening chip export curbs, and competition from China as Alibaba unveiled its latest AI reasoning model, which it said rivals offerings from DeepSeek and ChatGPT maker OpenAI. Alibaba shares climbed about 1%.\n\nMeanwhile, shares of AI play favorite Palantir (PLTR) dropped about 3.5%, nearly wiping away Wednesday's gains on expectations the company's software could be strongly positioned to support the Trump administration's efforts to trim government spending. Jefferies analysts highlighted insider sales, with Palantir CEO Alex Karp moving to sell shares worth another $45 million in the last two weeks after dumping over $2 billion worth in 2024.\n\nThe rest of the Magnificent Seven aside from Nvidia\u2014Apple (AAPL), Microsoft (MSFT), Alphabet (GOOG), Amazon (AMZN), Tesla (TSLA), and Meta (META)\u2014were little changed to slightly lower after a volatile stretch for big tech that has seen the major indexes give back all of their gains since November's presidential election.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia\u2019s stock selloff deepens after Marvell earnings: \u2018Boy, sentiment is rough.\u2019",
            "link": "https://www.marketwatch.com/story/nvidias-stock-selloff-deepens-after-marvell-earnings-boy-sentiment-is-rough-7b4965a0",
            "snippet": "Nvidia Corp.'s stock was among the many semiconductor stocks moving lower in Thursday morning action, in the wake of Marvell Technology Inc.'s earnings...",
            "score": 0.8337005376815796,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-03-05": {
        "0": {
            "title": "NVIDIA CEO Jensen Huang and Industry Visionaries to Unveil What\u2019s Next in AI at GTC 2025",
            "link": "https://nvidianews.nvidia.com/news/nvidia-ceo-jensen-huang-and-industry-visionaries-to-unveil-whats-next-in-ai-at-gtc-2025",
            "snippet": "NVIDIA today announced GTC 2025, the world's premier AI conference, will return March 17-21 to San Jose, Calif. \u2014 bringing together the brightest minds in...",
            "score": 0.9239816069602966,
            "sentiment": null,
            "probability": null,
            "content": "GTC 2025 to Feature Huang\u2019s Keynote and More Than 1,000 Sessions, Providing a Glimpse Into the Future of AI Infrastructure, Scientific Computing, AI and Robotics\n\nNVIDIA today announced GTC 2025 , the world\u2019s premier AI conference, will return March 17-21 to San Jose, Calif. \u2014 bringing together the brightest minds in AI to showcase breakthroughs happening now in physical AI, agentic AI and scientific discovery. GTC will bring together 25,000 attendees in person \u2014 and 300,000 attendees virtually \u2014 for an in-depth look at the technologies shaping the future.\n\nNVIDIA founder and CEO Jensen Huang will deliver the keynote from SAP Center on Tuesday, March 18, at 10 a.m. PT focused on AI and accelerated computing technologies changing the world. It will be livestreamed and available on demand at nvidia.com . Registration is not required to view the keynote online .\n\nOnsite attendees can arrive at SAP Center early to enjoy a live pregame show hosted by the \u201cAcquired\u201d podcast and other surprise festivities. Virtual attendees can catch the pregame show live online .\n\n\u201cAI is pushing the limits of what\u2019s possible \u2014 turning yesterday\u2019s dreams into today\u2019s reality,\u201d Huang said. \u201cGTC brings together the brightest scientists, engineers, developers and creators to imagine and build a better future. Come and be first to see the new advances in NVIDIA computing and breakthroughs in AI, robotics, science and the arts that will transform industries and society.\u201d\n\nAI is here, and it\u2019s mainstream \u2014 powering the everyday brands that shape people\u2019s lives. At GTC, some of the world\u2019s largest companies, groundbreaking startups and leading academic minds will convene to explore the transformative impact of AI across industries.\n\nWith over 1,000 sessions, 2,000 speakers and nearly 400 exhibitors, GTC will showcase how NVIDIA\u2019s AI and accelerated computing platforms tackle the world\u2019s biggest and toughest challenges \u2014 spanning climate research to healthcare, cybersecurity, humanoid robotics, autonomous vehicles and more. From large language models and physical AI to cloud computing and scientific discovery, NVIDIA\u2019s full-stack platform is driving the next industrial revolution.\n\nAt the conference, attendees can also look forward to curated experiences, including dozens of demos spanning every industry, hands-on training, autonomous vehicle exhibits and rides, and a new GTC Night Market featuring street food and wares from 20 local vendors and artisans.\n\nNotable speakers include:\n\nPieter Abbeel, director of the UC Berkeley Robot Learning Lab and co-director of the UC Berkeley Artificial Intelligence Lab\n\nDrago Anguelov, vice president and head of research, Waymo\n\nFrances Arnold, Nobel Laureate in chemistry and Linus Pauling Professor of chemical engineering, bioengineering and biochemistry, California Institute of Technology\n\nG\u00fclen Bengi, chief marketing officer, Mars Snacking\n\nEsi Eggleston Bracey, chief growth and marketing officer, Unilever\n\nNoam Brown, research scientist, OpenAI\n\nNadia Carlsten, CEO, Danish Centre for AI Innovation, Novo Nordisk Foundation\n\nMax Jaderberg, chief AI officer, and Sergei Yakneen, chief technology officer, Isomorphic Labs\n\nAthina Kanioura, executive vice president and chief strategy and transformation officer, PepsiCo\n\nJeffrey Katzenberg, founding partner, WndrCo\n\nThe Rt Hon Peter Kyle MP, secretary of state for science, innovation and technology, United Kingdom\n\nYann LeCun, vice president and chief AI scientist, Meta; professor, New York University\n\nArthur Mensch, CEO, Mistral AI\n\nJoe Park, chief digital and technology officer, Yum! Brands; president, Byte by Yum!\n\nRajendra \u201cRP\u201d Prasad, chief information and asset engineering officer, Accenture\n\nRaji Rajagopalan, vice president, Azure AI Foundry, Microsoft\n\nAaron Saunders, chief technology officer, Boston Dynamics\n\nRJ Scaringe, founder and CEO, Rivian\n\nClara Shih, head of business AI, Meta\n\nAlicia Tillman, chief marketing officer, Delta Air Lines\n\nPras Velagapudi, chief technology officer, Agility Robotics\n\nMore than 900 organizations will participate, including Accenture, Adobe, Arm, Airbnb, Amazon Web Services (AWS), BMW Group, The Coca-Cola Company, CoreWeave, Dell Technologies, Disney Research, Field AI, Ford, Foxconn, Google Cloud, Kroger, Lowe\u2019s, Mercedes-Benz, Meta, Microsoft, MLB, NFL, OpenAI, Oracle Cloud Infrastructure, Pfizer, Rockwell Automation, Salesforce, Samsung, ServiceNow, SoftBank, TSMC, Uber, Volvo, Volkswagen, Wayve and Zoox.\n\nQuantum Day Arrives\n\nNVIDIA will host its first Quantum Day at GTC on March 20. The event will bring together the global quantum computing community and key industry figures.\n\nLeaders from the quantum computing industry will join a panel with Huang from 10 a.m. to 12 p.m. PT, shedding light on the current state and future of quantum computing. The panel will be livestreamed and available on demand, and feature pioneers in quantum computing, including:\n\nAlan Baratz, CEO, D-Wave\n\nBen Bloom, CEO, Atom Computing\n\nPeter Chapman, executive chair, IonQ\n\nRajeeb Hazra, CEO, Quantinuum\n\nLo\u00efc Henriet, co-CEO, Pasqal\n\nMatthew Kinsella, CEO, Infleqtion\n\nSubodh Kulkarni, CEO, Rigetti\n\nJohn Levy, CEO, SEEQC\n\nAndrew Ory, CEO, QuEra Computing\n\nTh\u00e9au Peronnin, CEO, Alice & Bob\n\nRob Schoelkopf, chief scientist, Quantum Circuits\n\nSimone Severini, general manager, quantum technologies, AWS\n\nPete Shadbolt, chief scientific officer, PsiQuantum\n\nKrysta Svore, technical fellow, Microsoft\n\nQuantum Day will also feature technical sessions with partners, NVIDIA researchers and more.\n\nAI Training and Certification for Developers\n\nNVIDIA is training the workforce of the future to equip them with critical skills for navigating and leading in an AI-driven future.\n\nGTC attendees can participate in more than 80 hands-on instructor-led workshops and training labs provided by NVIDIA Training .\n\nFor the first time, onsite attendees can take certification exams for free \u2014 gaining a tremendous opportunity to validate their AI and accelerated computing skills and advance their careers.\n\nIn addition, new professional certifications will be available in accelerated data science and AI networking, as well as workshops in generative AI, agentic AI and accelerated computing with CUDA\u00ae C++.\n\nLearn more about training offerings at GTC on the event webpage .\n\nStartup and Venture Capital Ecosystem\n\nFor startups and VCs, GTC will feature an AI Day with expert panels, live demos from top startups, session tracks designed for investors, a VC reverse pitch session and exclusive networking opportunities with investors.\n\nThe NVIDIA Inception Pavilion will spotlight cutting-edge innovation from the NVIDIA Inception program, home to more than 22,000 startups. Nearly 250 Inception members will showcase their breakthroughs with demos, exhibitions and sessions spanning areas such as healthcare, climate science and robotics.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "After a run of RTX 50-series launches with seemingly little availability and mega price tags, I'm left wondering 'is that it?'",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/after-a-run-of-rtx-50-series-launches-with-seemingly-little-availability-and-mega-price-tags-im-left-wondering-is-that-it/",
            "snippet": "If this is what a GeForce GPU launch is meant to look like from here on out, it's tough to get too excited about new products.",
            "score": 0.4833514094352722,
            "sentiment": null,
            "probability": null,
            "content": "The embers are still hot from Nvidia's RTX 5070 graphics card launch, but looking around, you'd be hard-pressed to tell anything happened at all.\n\nI've covered a good few GPU launches in my time, including the crypto/covid-driven shortages, and I've been a customer trying to buy a GPU in the past. So I get that it's pretty much guaranteed to be a horrible time to secure a graphics card, or really any tech, on release day. But there's something about the RTX 50-series launch that feels completely futile.\n\nOver our lunch today, my colleagues and I were searching for the RTX 5070. We hit the main places: in the US that's Best Buy, Newegg, B&H, Micro Center; in the UK that's Scan, Overclockers, Ebuyer, and even high street chain Currys. We even went further afield to the more niche retailers, such as MSI's own webstore and Novatech (remember them?).\n\nAnd what did we find?\n\nNothing much at all, really. A handful of overpriced RTX 5070s, like the one below, and none at MSRP, bar one noted as entered into Newegg's lottery system, Newegg Shuffle. The ones we did find were getting up to the MSRP of the RTX 5070 Ti of $749, though that was about as ephemeral a price tag for the Ti as the $549 price tag of the RTX 5070.\n\nBlink and they're gone, too. Even the overpriced ones.\n\n(Image credit: Asus, Newegg)\n\nIt was all pretty bleak, leading us on the team to say things such as \"I honestly didn't think it would be this bad\" and \"I haven't seen the FE model go up at all\".\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nYeah, the Founders Edition model, the saving grace for gamers everywhere as a card that's actually sold at MSRP\u2026 it was noted as \"Coming soon\" at Best Buy and, with multiple staff members watching, turned to \"Out of stock\" without a single one of us seeing it ever noted as available. Nvidia's website in the UK was the official carrier for the FE, supposedly, and again, we never saw it arrive.\n\n(Image credit: Nvidia, Best Buy)\n\nThe worst thing is, this wasn't an isolated event. I also watched the RTX 5070 Ti launch closely, and the RTX 5070 launch was basically just a re-run of the Ti card launch. There were few to no MSRP cards, some overpriced third-party units, and it was all largely gone in a flash and left a foul smell in the air afterwards.\n\nThis feels a little different to previous periods of catastrophic supply issues. During the double whammy of Covid-19 and cryptocurrency mining on GPUs at its peak, you couldn't buy a graphics card without treating searching for one like a new, all-encompassing hobby. It sucked. But where that was huge demand for a product combined with global shipping and production issues to make one awful experience for the average gamer, this launch feels like it was doomed from the start.\n\nThe lack of MSRP cards has to sting the worst. Whatever the reasoning behind it, manufacturers are not prepared to sell many cards close to Nvidia's MSRP. Is that Nvidia's MSRP that's the problem? Manufacturers? Are retailers making it worse to cover their bottom line? Whatever the reason, the problem has seen to sting gamers the most, as we're looking at real-world prices often so far in excess of MSRP for Nvidia's cards, they're practically the price for the next tier of GPU up.\n\nPaying $1,000 for a graphics card stings. Paying $1,000 for a graphics card that should cost $750 is worse.\n\nI'm not offering any sort of solution here, but I'd be keen to hear one. Something, anything, from Nvidia about how it might help ease this problem. Same goes for AMD if it cannot keep a lid on third-party prices (for its lack of a reference card) with its RX 9070-series launch tomorrow, led by the RX 9070 XT. If this is the new reality of graphics card prices, why?\n\n(Image credit: Future)\n\nGoing back to Nvidia, its latest GPUs are built on a similar process node to their RTX 40-series predecessors. Bar the genuinely huge RTX 5090 chip, they're no larger than last gen, and in the case of the RTX 5070, its GPU is smaller than the AD104 die used on the RTX 4070. The memory might cost a little more, and maybe there's a degree of demand (for the 5070?), but this feels like a weirdly paper launch for a graphics card generation a long time coming.\n\nIt's just been a tough watch, from my point of view. These last few launches have been brutal and unforgiving. I've mentioned it with my colleagues, but systems put in place to reward people through Discord participation just for the chance to buy a full-price graphics card feel extremely bleak to me. Some colleagues disagree with that, saying it's a good shot at a card for loyal customers, but I don't understand why there's the need for it right now. The world isn't a great place with wars and tariffs, but the direct impact of Covid and cryptocurrency on graphics card supply was at least easier to understand.\n\nI offer no answers here\u2014sorry if you wanted some\u2014but this is more of just an exasperated sigh of a story. As far as paper launches go, of which there have been many, this one feels like one of the worst I've covered. And I'm sure there are some stats out there that make it sound like Nvidia sold a bajillion cards, which might be true, but I certainly haven't seen much proof of that on the ground. And the prices\u2026 don't get me started on those again.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Where to buy Nvidia RTX 5070 at launch",
            "link": "https://mashable.com/article/march-5-nvidia-rtx-5070-where-to-buy",
            "snippet": "As of March 5, the Nvidia RTX 5070 is available from $549 at Amazon and Best Buy. Here's how to get one before stock runs out.",
            "score": 0.9462196230888367,
            "sentiment": null,
            "probability": null,
            "content": "Deal pricing and availability subject to change after time of publication. Learn more about how we select deals\n\nWHERE TO BUY: The Nvidia GeForce RTX 5070 launches on March 5 with a starting price of $549. It will be available at Amazon and Best Buy, though third-party models may cost more.\n\nNvidia GeForce RTX 5070 is here, and while it is not the most exciting upgrade, it is still the latest mid-range option in the 50-series lineup.\n\nPerformance is nearly identical to RTX 4070 Super, with only a slight lead in some games and even some losses in others. The only real improvement is the inclusion of DLSS 4, which allows Multi-Frame Generation to artificially boost frame rates. That comes with added latency, making it more useful for single-player games than competitive ones.\n\nShop Nvidia RTX 5070 from Amazon (pricing TBC)\n\nShop Nvidia RTX 5070 from Best Buy\n\nNvidia RTX 5070 vs. RTX 4070 Super\n\nAt launch, RTX 5070 is priced at $549. That makes it $50 cheaper than the 4070 Super at launch, but that may not be enough to make up for the lack of a real generational jump. It is a reasonable choice for those who skipped the 40-series and are upgrading from something older. But it may be better to buy the 4070 Super if you can find stock.\n\nFor those who already have a RTX 4070 Super, I've noticed around a 15% bump in general performance from my MSI model by using a program called DLSS Swapper. This allows users to use DLSS 4 on 4000 series cards (usually locked to 3.5) and is managed by games that use the AI tech.\n\nHere's a spec comparison for RTX 5070 to RTX 4070 Super:\n\nCUDA Cores: 6,144 / 7,168\n\nVRAM: 12GB GDDR7 / 12GB GDDR6X\n\nMemory Bus: 192-bit / 192-bit\n\nPower Draw: 250W / 220W\n\nDLSS Version: DLSS 4 / DLSS 3.5\n\nFor those set on getting RTX 5070, Amazon and Best Buy are the best places to look. Amazon will have a range of third-party models from brands like ASUS, MSI, and Gigabyte. Prices may vary, especially if resellers get involved. Best Buy is the most reliable place to find Nvidia\u2019s Founders Edition cards, which tend to sell out quickly. Both stores will have stock at launch, but availability may change within hours.\n\nThe card itself is built on Nvidia\u2019s Blackwell architecture and features 6,144 CUDA cores, 12GB of GDDR7 memory, and a 192-bit memory bus. The power draw is slightly higher than the previous generation, reaching 250W compared to the 220W of 4070 Super. That should not be an issue for most modern power supplies, but it is something to keep in mind for those with lower-wattage setups.\n\nMashable Deals Want more hand-picked deals from our shopping experts? Sign up for the Mashable Deals newsletter. Loading... Sign Me Up By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy Thanks for signing up!\n\nBuying a GPU at launch always comes with some risk. Prices may fluctuate, stock may disappear, and better deals may come up later. If RTX 5070 drops below $549, it becomes a more attractive option. If the 4070 Super stays in stock at a lower price, it may be worth choosing instead.\n\nFor those who do not want to wait, checking Amazon and Best Buy early on launch day is the best bet.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia Stock: Are Tariff Fears Overblown?",
            "link": "https://www.forbes.com/sites/greatspeculations/2025/03/05/nvidia-stock-are-tariff-fears-overblown/",
            "snippet": "Nvidia's chips are primarily manufactured by TSMC in Taiwan; however, some systems and computers utilizing these chips are produced in other regions,...",
            "score": 0.518903374671936,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "This Key Metric Shows Why Nvidia Stock Is Too Cheap to Ignore",
            "link": "https://www.fool.com/investing/2025/03/05/nvidia-growth-stock-too-cheap-valuation-buy/",
            "snippet": "Nvidia's (NVDA 6.43%) fourth-quarter fiscal 2025 (ended Jan. 26, 2025) earnings report rocked markets last week, with a sell-off in the major indexes on...",
            "score": 0.8063563108444214,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's (NVDA 5.27%) fourth-quarter fiscal 2025 (ended Jan. 26, 2025) earnings report rocked markets last week, with a sell-off in the major indexes on Thursday followed by a rebound on Friday.\n\nAs of March 2, Nvidia is down 7% year to date, underperforming the S&P 500 index's (^GSPC 2.13%) 1.2% gain. Given Nvidia's over 800% increase between 2023 and the end of 2024, investors may feel a pullback in the growth stock is only natural. But there are plenty of reasons why Nvidia is surprisingly cheaper than a chart of its stock price would suggest.\n\nNvidia's valuation has come down\n\nThe price-to-earnings (P/E) ratio is a well-known financial metric that divides a stock's price by earnings per share (EPS). The P/E ratio shows the multiple investors are paying for a company relative to its earnings. The higher the P/E ratio, the more expensive the stock based on trailing earnings.\n\nThe P/E ratio is helpful when analyzing companies with consistent business models and rangebound growth rates, especially blue chip companies like Apple, Microsoft, Home Depot, Coca-Cola, etc. But the metric can be a little less useful for companies that aren't yet profitable, are inconsistently profitable, or are undergoing a major transformation -- like Nvidia.\n\nNvidia's stock price has risen significantly in recent years, but so have its earnings. The company is growing so quickly that its valuation has actually gotten cheaper. This is especially true over the last nine months or so because Nvidia's stock price has stayed roughly the same while its earnings continue to soar higher.\n\nNvidia's P/E ratio has come down to 42.5 -- which may seem high given the P/E ratio of the S&P 500 is 29.8. But since Nvidia is still growing earnings rapidly, its forward P/E ratio, which is based on analyst consensus estimates for the next 12 months, is just 27.8 -- meaning that if Nvidia's stock price didn't move for a year, and it met estimates, it would have a valuation around the same as the S&P 500.\n\nExplosive growth\n\nNvidia heavily depends on graphics processing unit (GPU) sales to hyperscalers like Amazon, Microsoft, Alphabet, and Meta Platforms. This recent development has led to exponential growth in Nvidia sales, operating income, and profit margins.\n\nAs you can see in the following table, Nvidia's revenue is up nearly eight-fold in five years, but operating income has increased over 18-fold.\n\nMetric Fiscal 2021 Fiscal 2022 Fiscal 2023 Fiscal 2024 Fiscal 2025 Revenue (billions) $16.7 $26.9 $27 $60.9 $130.5 Operating income (billions) $4.5 $10 $5.6 $33 $81.5 Net income (billions) $4.3 $9.8 $4.4 $29.8 $72.9 Profit margin 26% 36.2% 16.2% 48.9% 55.9% Diluted EPS $0.17 $0.39 $0.17 $1.19 $2.94\n\nIn fiscal 2025, Nvidia converted over 55 cents of every dollar in revenue into net income -- an unbelievable profit margin that is simply unheard of for a company of its size.\n\nVirtually any stock is cheap if the company keeps doubling or tripling revenue and earnings yearly. But what happens when there's a cyclical slowdown in the semiconductor industry? Or if some of Nvidia's largest customers pull back on capital expenditures? Or if less computing power is needed to run complex artificial intelligence (AI) models, leading to lower GPU demand? Or if a competitor comes up with a formidable solution at a lower cost that erodes Nvidia's margins?\n\nEvery business faces uncertainties. And Nvidia certainly has its fair share of risk. But it also has plenty of opportunities, too. To quote Nvidia CEO Jensen Huang on the recent earnings call:\n\nModels like OpenAI, Grok 3, DeepSeek-R1 are reasoning models that apply inference time scaling. Reasoning models can consume 100x more compute. Future reasoning models can consume much more compute. DeepSeek-R1 has ignited global enthusiasm. It's an excellent innovation. But even more importantly, it has open-sourced a world-class reasoning AI model.\n\nIn sum, Nvidia is encouraged, not threatened, by newer reasoning models like DeepSeek. Nvidia is also very optimistic about the potential for AI to expand beyond the digital world into robotics, physical AI development, autonomous vehicles, and more. All told, there's potential for Nvidia to be less dependent on hyperscalers in the future.\n\nThe impact of earnings growth on valuation over time\n\nBuying Nvidia stock is a bet on continued AI adoption across different end markets and use cases. Over time, Nvidia's growth rate and margins will probably come down as AI becomes a more mature industry and because it is harder to grow a massive business as quickly as a smaller one.\n\nBut even if Nvidia's earnings growth slows to a compound annual growth rate of, say, 15% over the next 10 years, the stock is still a phenomenal buy. For context, consensus analyst estimates have Nvidia growing EPS to $4.49 in fiscal 2026 and then $5.72 in fiscal 2027 -- growth rates of 52.7% and 27.3%, respectively. So, earnings growth isn't expected to slow down to 20% per year anytime soon.\n\nStill, let's pencil in a 20% earnings growth rate over 10 years. And let's say you only want to pay 25 times earnings for the stock. Here's what the stock price would look like under those assumptions. For context, Nvidia is $124.81 per share at the time of this writing.\n\nMetric Fiscal 2025 Fiscal 2026 Fiscal 2027 Fiscal 2028 Fiscal 2029 Fiscal 2030 Fiscal 2031 Fiscal 2032 Fiscal 2033 Fiscal 2034 EPS $2.94 $3.53 $4.23 $5.08 $6.10 $7.32 $8.78 $10.53 $12.64 $15.17 Stock price $73.50 $88.25 $105.75 $127 $152.50 $183 $219.50 $263.25 $316 $379.22\n\nNow, let's assume analyst consensus estimates are correct for fiscal 2026 and fiscal 2027, but then earnings grow at just a 15% compound annual growth rate for eight years after that. And that an investor only wants to pay 20 times Nvidia's earnings.\n\nMetric Fiscal 2025 Fiscal 2026 Fiscal 2027 Fiscal 2028 Fiscal 2029 Fiscal 2030 Fiscal 2031 Fiscal 2032 Fiscal 2033 Fiscal 2034 EPS $2.94 $4.49 $5.72 $6.58 $7.56 $8.70 $10 $11.51 $13.24 $15.22 Stock price $58.80 $89.80 $114.40 $131.60 $151.20 $174 $200 $230.20 $264.80 $304.40\n\nEven with a steep slowdown in growth and a multiple contraction to just 20 times earnings, Nvidia would still be a $300 stock in 10 years.\n\nThis exercise shows the power of earnings growth and how a premium-priced stock can be a better value than it appears at first glance.\n\nNvidia has the makings of an ultra-long-term holding\n\nToo often, investors get bogged down by near-term factors, like whether a company will hit its quarterly projections or how the economy is doing.\n\nBut with Nvidia, the growth is so explosive that even a slowdown and a multiple contraction could still mean the stock will produce sizable gains over the long term. If hyperscalers pull back on spending, Nvidia still has tons of potential in other end markets like robotics, automation, and more.\n\nAdd it all up, and Nvidia's balance between risk and potential reward makes it simply too cheap to ignore.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Why Nvidia (NVDA) Stock Is Slipping Today",
            "link": "https://finance.yahoo.com/news/why-nvidia-nvda-stock-slipping-174940982.html",
            "snippet": "Nvidia (NVDA) is pulling back 1% today after Morningstar called the stock \u201cfairly valued,\u201d and two technical analysts made cautious comments about the...",
            "score": 0.9468594193458557,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) is pulling back 1% today after Morningstar called the stock \"fairly valued,\" and two technical analysts made cautious comments about the shares in statements to Yahoo Finance.\n\nJim Cramer NVIDIA (NVDA)\u2019s Jensen Huang \u2018Should Be Able\u2019 to Speak More on DeepSeek Situation\n\nMorningstar Says NVDA Is \"Fairly Valued\"\n\nIn an article published yesterday afternoon, research firm Morningstar kept a $130 fair value estimate on NVDA in the wake of its fourth-quarter earnings. However, it also wrote that the \"shares appear fairly valued to us.\"\n\nMorningstar believes that NVDA's current level accurately reflects the huge potential of the company's AI chips and the possibility that the demand for those semiconductors will slow starting in 2026.\n\nMorningstar expects the company's revenue growth to slow to 21% during its next fiscal year.\n\nTechnical Analysts Are Fairly Bearish on NVDA\n\nNoting that the shares had fallen below their 200-day moving average, Todd Sohn, senior ETF and technical strategist at Strategas Securities LLC, told Yahoo Finance that \"On a tactical basis, it\u2019s hard to remain super bullish on a name once the 200-day starts to crest and slope downward.\u201d\n\n\u201cI\u2019m inclined to think there\u2019s more to go on the downside. We could see support anywhere from here to $110, but below that my minimum downside target is in the $107-$103 range,\" added Rick Bensignor, CEO of Bensignor Investment Strategies and a former Morgan Stanley strategist.\n\nWhile we acknowledge the potential of NVDA, our conviction lies in the belief that some AI stocks hold greater promise for delivering higher returns, and doing so within a shorter time frame. If you are looking for an AI stock that is more promising than NVDA but that trades at less than 5 times its earnings, check out our report about the cheapest AI stock.\n\nREAD ALSO 8 Best Wide Moat Stocks to Buy Now and 30 Most Important AI Stocks According to BlackRock\n\nDisclosure: None. This article is originally published at Insider Monkey.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Nvidia Stock Wavers. Does the Market See a Bargain?",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-today-ai-chips-e06e7f23",
            "snippet": "Nvidia stock was wavering Wednesday as investors gauge whether there's a potential bargain in the chip maker's shares. Evidence of more money flowing into...",
            "score": 0.6052308082580566,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Nvidia\u2019s GTC 2025 will feature AI\u2019s brightest minds and draw 25K attendees in person",
            "link": "https://venturebeat.com/games/nvidias-gtc-2025-will-feature-ais-brightest-minds-and-draw-25k-attendees-in-person/",
            "snippet": "Nvidia today announced GTC 2025, the world's premier AI conference, will return March 17 to March 21 to San Jose, California.",
            "score": 0.8243873715400696,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Will Tariffs Really Hurt Nvidia Stock?",
            "link": "https://www.nasdaq.com/articles/will-tariffs-really-hurt-nvidia-stock",
            "snippet": "Nvidia stock (NASDAQ:NVDA) tumbled by close to 9% in Monday's trading, with the broader Nasdaq index sliding by a little over 2%. While the broader market...",
            "score": 0.8638360500335693,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock (NASDAQ:NVDA) tumbled by close to 9% in Monday\u2019s trading, with the broader Nasdaq index sliding by a little over 2%. While the broader market sell-off comes as U.S. President Donald Trump confirmed that his administration will enforce tariffs of 25% on imports from Canada and Mexico starting from Tuesday, Nvidia\u2019s decline was compounded by investigations into Chinese buyers allegedly bypassing U.S. export controls on advanced semiconductor chips. So how do these developments impact Nvidia stock?\n\nTariffs Will Likely Have a Limited Impact\n\nWe don\u2019t see the Trump Administration\u2019s initial round of tariffs materially impacting Nvidia. Nvidia\u2019s chips are largely fabricated by TSMC in Taiwan, although the company has some systems and computers that use its chips, which are produced in other regions, including Mexico. It\u2019s possible that these parts of the business could be affected to some extent. However, the core of Nvidia\u2019s business, its high-margin GPU unit, should remain largely unaffected. However, last month, President Trump also hinted at a \u201c25% or higher\u201d tariff on all semiconductor chips the United States imports. This is something investors will need to watch more closely, although we don\u2019t think this will also have a very sharp impact on Nvidia\u2019s bottom line. Nvidia had an adjusted gross margin of about 75.5% as of 2024, meaning that the cost of the products it imports is likely less than a quarter of its revenues. Moreover, Nvidia derives just about 47% of its sales from the U.S., meaning that the margin impact is likely even more limited. Additionally, TSMC, Nvidia\u2019s primary contract manufacturer for its GPUs, has outlined plans to invest around $100 billion into new chip-making facilities in the United States. Nvidia officials have indicated that they would manufacture chips at these new facilities, which might help Nvidia stave off any threats of tariffs in the longer run. However, if you seek upside with less volatility than a single stock, consider the High-Quality portfolio, which has outperformed the S&P 500 and achieved returns greater than 91% since inception.\n\nChina\u2019s AI Loophole A Concern\n\nThe U.S. has imposed export control curbs on most of Nvidia\u2019s latest AI chipset offerings to China, except the H20 chips, due to national security concerns. However, there have been reports that resellers in the gray market are using entities registered outside of China to buy servers that use Nvidia\u2019s latest offerings from companies located in various countries, including Singapore, Malaysia, Taiwan, and Vietnam. This is a valid concern, given that Singapore has become Nvidia\u2019s second-biggest market, accounting for about $23 billion in sales in FY\u201925, or about 18% of revenue, up from a mere $2.3 billion, or 8% of revenue, back in FY\u201923. Now, Singapore has announced an investigation into these potential loopholes. The rise of China\u2019s DeepSeek open-source AI model also indicates that the country is making considerable progress in AI, with leading startups as well as major players such as Alibaba and Baidu investing heavily in AI infrastructure. If Chinese companies are indeed bypassing sanctions, and if stricter enforcement closes them, this could impact Nvidia\u2019s revenues.\n\nNow, the increase in NVDA stock over the last 4-year period has been far from consistent, with annual returns being considerably more volatile than the S&P 500. Returns for the stock were 125% in 2021, -50% in 2022, 239% in 2023, and 171% in 2024. The Trefis High Quality (HQ) Portfolio, with a collection of 30 stocks, is considerably less volatile. And it has comfortably outperformed the S&P 500 over the last 4-year period. Why is that? As a group, HQ Portfolio stocks provided better returns with less risk versus the benchmark index, less of a roller-coaster ride as evident in HQ Portfolio performance metrics. Given the current uncertain macroeconomic environment around rate cuts and multiple wars, could NVDA face a similar situation as it did in 2022 and underperform the S&P over the next 12 months \u2013 or will it see a strong jump?\n\nWe value Nvidia stock at about $101 per share, roughly 20% below the current market price. See our analysis of Nvidia valuation: Expensive or Cheap. There are a couple of reasons why we are negative on the stock at the moment. We see a possibility that the \u201cfear-of-missing-out\u201d driven AI wave seen over the last two years could ease off due to diminishing incremental performance gains from larger models and also as the availability of high-quality training data becomes a bottleneck. This shift toward more efficient models could compound the impact of a potential slowdown for GPU makers such as Nvidia. Moreover, Nvidia also faces mounting competition from the likes of AMD as well as its own customers such as Amazon, who have been focusing on developing and deploying their own AI chips. While Nvidia does have a comprehensive software ecosystem around its AI processors, including programming languages that should help it better lock customers into its products, the company could face pressure. Nvidia\u2019s premium valuation may not fully reflect these risks at the moment.\n\nReturns Mar 2025\n\nMTD [1] 2025\n\nYTD [1] 2017-25\n\nTotal [2] NVDA Return -9% -15% 4238% S&P 500 Return -2% -1% 161% Trefis Reinforced Value Portfolio -2% -4% 658%\n\n[1] Returns as of 3/4/2025\n\n[2] Cumulative total returns since the end of 2016\n\nInvest with Trefis Market-Beating Portfolios\n\nSee all Trefis Price Estimates\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia-backed cloud firm CoreWeave to acquire AI developer platform Weights & Biases",
            "link": "https://www.reuters.com/markets/deals/nvidia-backed-cloud-firm-coreweave-acquire-ai-developer-platform-weights-biases-2025-03-05/",
            "snippet": "Nvidia-backed CoreWeave said on Tuesday that it is acquiring AI developer platform Weights & Biases as the company seeks to extend its cloud platform ahead...",
            "score": 0.8885528445243835,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-04": {
        "0": {
            "title": "Nvidia R&D expenses FY2025",
            "link": "https://www.statista.com/statistics/988048/nvidia-research-and-development-expenses/",
            "snippet": "Nvidia research and development expenses worldwide FY2017-2025 ... In its 2025 fiscal year, Nvidia spent 12.9 billion U.S. dollars on research and development (...",
            "score": 0.9331296682357788,
            "sentiment": null,
            "probability": null,
            "content": "Currently, you are using a shared account. To use individual functions (e.g., mark statistics as favourites, set statistic alerts) please log in with your personal account. If you are an admin, please authenticate by logging in again.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Leads AI Stock Rebound After Shaking Off Trump Tariff Fears",
            "link": "https://www.investopedia.com/nvidia-leads-ai-stock-rebound-after-shaking-off-trump-tariff-fears-11690801",
            "snippet": "Nvidia stock rebounded from an early morning slump on Tuesday as AI stocks shook off the tariff shock to lead Nasdaq gainers.",
            "score": 0.47310689091682434,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock rebounded from an early-morning slump on Tuesday as AI stocks shook off the tariff shock that sent the broader market reeling.\n\nNvidia shares rose 1.7% Tuesday after tumbling more than 8% yesterday and trading as much as 4% lower in premarket trading today. AI server maker Super Micro Computer (SMCI), which tumbled 13% yesterday, also rebounded, climbing 8.5%. The two led a broad AI rally, with Wall Street darling Palantir (PLTR) and Vistra (VST) also advancing.\n\nAI stocks have been on a wild ride for the past month. Concerns about stubborn inflation and the consequences of President Donald Trump's tariffs have cast a fog over the market's outlook for both the U.S. economy and interest rates. Headlines out of Washington have tempered the risk appetite that last year powered triple-digit gains for AI beneficiaries like Palantir, Vistra, and Applovin (APP).\n\nNvidia's stock has also been pressured over the last month by Chinese start-up DeepSeek's claim it developed a top-tier reasoning model without Nvidia\u2019s most powerful chips. That revelation initially sparked concern that U.S. cloud providers could rein in their spending on chips and servers to focus on creating more efficient AI, but big tech companies have since stood by their plans to invest hundreds of billions in AI infrastructure.\n\nDeepSeek has led the U.S. and other countries to investigate whether Chinese developers are acquiring Nvidia\u2019s chips illegally. Singapore is reportedly investigating whether servers containing advanced Nvidia chips were illegally routed to DeepSeek after being sold to local firms. The investigations could compel the Trump Administration to tighten export restrictions even further, weighing on Nvidia\u2019s international sales.\n\nDespite all the concerns, analysts are still mostly bullish on Nvidia's stock. Wedbush analysts have even argued that Nvidia will benefit from DeepSeek; they argue that demand for AI and Nvidia's chips will increase as models become more efficient and less expensive.\n\n",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Nvidia shares plunge 9% as Trump\u2019s tariff crackdown and Singapore AI server smuggling arrests shake chip giant",
            "link": "https://finance.yahoo.com/news/nvidia-shares-plunge-9-trump-114557045.html",
            "snippet": "Nvidia's stock has taken a hit as Donald Trump's new tariffs rattled investors. Nvidia's stock tumbled nearly 9% on Monday, while concerns over unauthorized...",
            "score": 0.9522889852523804,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia\u2019s stock has taken a hit as Donald Trump\u2019s new tariffs rattled investors. Nvidia\u2019s stock tumbled nearly 9% on Monday, while concerns over unauthorized exports to China also rattled investors.\n\nNvidia's stock price plunged nearly 9% following Donald Trump's tough tariff announcements.\n\nThe U.S. president announced that tariffs on imports from Canada and Mexico will go into effect on Tuesday, dashing hopes of a last-minute reprieve. The aggressive tariff plans prompted a broader market pullback, as concerns over trade restrictions sent stocks reeling and the Dow plunging more than 600 points.\n\nAsian tech stocks also slid in response to Trump stating he would impose an additional 10% tariff on imports from China. Shares of Advantest, a Japanese semiconductor company, South Korea's SK Hynix, and Chinese AI companies fell following the tariff announcement.\n\nNvidia is now valued at approximately $2.79 trillion\u2014down from its previous $3 trillion valuation\u2014and is trading at its lowest levels since September, before the U.S. presidential election. The tech giant's stock has fallen just over 13% since the company reported earnings on Wednesday.\n\nWhile Nvidia beat estimates in its latest earnings report, with revenue rising 78% from a year ago to $39.3 billion\u2014just edging past Wall Street's projection of $38.3 billion\u2014the growth didn't appear to be enough for investors, who have grown accustomed to blowout numbers from the AI chipmaker.\n\nNvidia primarily produces its AI chips in Taiwan, which is not currently subject to U.S. tariffs. However, many of its products are assembled in other locations, including Mexico, where higher import costs could drive up expenses.\n\n\u201cThe tariffs on Mexico and Canada announced by President Trump may impact Nvidia as they have some manufacturing in Mexico. This raises concerns about additional tariffs that have been threatened against chips made in Taiwan, where Nvidia has the majority of their chips produced currently,\" Alvin Nguyen, a senior analyst at Forrester, told Fortune.\n\n\"Although Nvidia has committed to having chips produced in the U.S. by TSMC after its announcement of a $100 billion investment in U.S. facilities, this type of build-out takes years,\" he added.\n\nNvidia's challenges beyond tariffs\n\nConcerns around unauthorized exports of Nvidia chips to China may have also affected investor confidence in the company.\n\nOn Monday, Singaporean authorities arrested three people for allegedly misrepresenting the final destination of U.S.-made servers, likely containing Nvidia\u2019s AI chips. The arrests highlight an ongoing shadow trade network, despite export restrictions, with analysts warning that a full ban on Nvidia chip exports to China could cost the company up to $5 billion in revenue.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia\u2019s stock is doing things not seen in nearly a decade \u2014 and that\u2019s good news",
            "link": "https://www.marketwatch.com/story/nvidias-stock-is-doing-things-not-seen-in-nearly-a-decade-and-thats-good-news-75a5182b",
            "snippet": "A Bernstein analyst looks at several valuation metrics that are near 10-year lows and deemed Nvidia's stock \u201cincreasingly attractive.\u201d",
            "score": 0.931512713432312,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "I really wanted to like the Nvidia GeForce RTX 5070, but it broke my heart and it shouldn't have to break yours, too",
            "link": "https://www.techradar.com/computing/gpu/nvidia-geforce-rtx-5070",
            "snippet": "The Nvidia GeForce RTX 5070 is an overall disappointment, with the small consolation of Multi Frame Generation to soften the sting.",
            "score": 0.8207467794418335,
            "sentiment": null,
            "probability": null,
            "content": "The Nvidia GeForce RTX 5070 is as close to the mainstream gamer's graphics card as you're going to get, and so I really had high hopes for this card. Unfortunately, it is a disappointment that doesn't give buyers much of a reason to buy it over an RTX 4070 Super or competing AMD cards other than its Multi Frame Generation capabilities. And for this card, that just isn't enough to justify an upgrade, even from two generation-old cards. Go for the RTX 5070 Ti or get a cheaper RTX 40 series card to hold you over for a couple of years.\n\nWhy you can trust TechRadar We spend hours testing every product or service we review, so you can be sure you\u2019re buying the best. Find out more about how we test.\n\nNvidia GeForce RTX 5070: Two-minute review\n\nA lot of promises were made about the Nvidia GeForce RTX 5070, and in some narrow sense, those promises are fulfilled with Nvidia's mainstream GPU. But the gulf between what was expected and what the RTX 5070 actually delivers is simply too wide a gap to bridge for me and the legion of gamers and enthusiasts out there who won't be able to afford\u2014or even find, frankly\u2014Nvidia's best graphics cards from this generation.\n\nLaunching on March 5, 2025, at an MSRP of $549 / \u00a3549 / AU$1,109 in the US, UK, and Australia, respectively, this might be one of the few Nvidia Blackwell GPUs you'll find at MSRP (along with available stock), but only for lack of substantial demand. As the middle-tier GPU in Nvidia's lineup, the RTX 5070 is meant to have broader appeal and more accessible pricing and specs than the enthusiast-grade Nvidia GeForce RTX 5090, Nvidia GeForce RTX 5080, and Nvidia GeForce RTX 5070 Ti, but of all the cards this generation, this is the one that seems to have the least to offer prospective buyers over what's already on the market at this price point.\n\nThat's not to say there is nothing to commend this card. The RTX 5070 does get up to native Nvidia GeForce RTX 4090 performance in some games thanks to Nvidia Blackwell's exclusive Multi-Frame Generation (MFG) technology. And, to be fair, the RTX 5070 is a substantial improvement over the Nvidia GeForce RTX 4070, so at least in direct gen-on-gen uplift, there is a roughly 20-25% performance gain.\n\nBut this card is a far, far cry from the promise of RTX 4090 performance that Nvidia CEO Jensen Huang presented on stage at CES 2025, even with the qualifier that such an achievement would be \"impossible without artificial intelligence,\" which implies a heavy reliance on DLSS 4 and MFG to get this card over the line.\n\nIf we're just talking framerates, then in some very narrow cases this card can do that, but at 4K with ray tracing and cranked-up settings, the input latency for the RTX 5070 with MFG can be noticeable depending on your settings, and it can become distracting. Nvidia Reflex helps, but if you take RTX 4090 performance to mean the same experience as the RTX 4090, you simply won't get that with MFG, even in the 80 or so games that support it currently.\n\n(Image credit: Future / John Loeffler)\n\nAdd to all this the fact that the RTX 5070 barely outpaces the Nvidia GeForce RTX 4070 Super when you take MFG off the table (which will be the case for the vast majority of games played on this card) and you really don't have anything to show for the extra 30W of power this card pulls down over the RTX 4070 Super.\n\nWith the RTX 5070 coming in at less than four percent faster in gaming without MFG than the non-OC RTX 4070 Super, and roughly 5% faster overall, that means that the RTX 5070 is essentially a stock-overclocked RTX 4070 Super, performance-wise, with the added feature of MFG. An overclocked RTX 4070 Super might even match or exceed the RTX 5070's overall performance in all but a handful of games, and that doesn't even touch upon AMD's various offerings in this price range, like the AMD Radeon RX 7900 GRE or AMD's upcoming RX 9070 XT and RX 9070 cards.\n\nGiven that the RTX 4070 Super is still generally available on the market (at least for the time being) at a price where you're likely to find it for less than available RTX 5070 cards, and competing AMD cards are often available for less, easier to find, and offer roughly the same level of performance, I really struggle to find any reason to recommend this card, even without the questionable-at-best marketing for this card to sour my feelings about it.\n\nI caught a lot of flack from enthusiasts for praising the RTX 5080 despite its 8-10% performance uplift over the Nvidia GeForce RTX 4080 Super, but at the level of the RTX 5080, there is no real competition and you're still getting the third-best graphics card on the market with a noticeable performance boost over the RTX 4080 Super for the same MSRP. Was it what enthusiasts wanted? No, but it's still a fantastic card with few peers, and the base performance of the RTX 5080 was so good that the latency problem of MFG just wasn't an issue, making it a strong value-add for the card.\n\nYou just can't claim that for the RTX 5070. There are simply too many other options for gamers to consider at this price point, and MFG just isn't a strong enough selling point at this performance level to move the needle. If the RTX 5070 is the only card you have available to you for purchase and you need a great 1440p graphics card and can't wait for something better (and you're only paying MSRP), then you'll ultimately be happy with this card. But the Nvidia GeForce RTX 5070 could have and should have been so much better than it ultimately is.\n\nNvidia GeForce RTX 5070: Price & availability\n\n(Image credit: Future / John Loeffler)\n\nHow much is it? MSRP/RRP starting at $549 / \u00a3549 / AU$1,109\n\nMSRP/RRP starting at $549 / \u00a3549 / AU$1,109 When can you get it? The RTX 5070 goes on sale on March 5, 2025\n\nThe RTX 5070 goes on sale on March 5, 2025 Where is it available? The RTX 5070 will be available in the US, UK, and Australia at launch\n\nThe Nvidia GeForce RTX 5070 is available starting March 5, 2025, with an MSRP of $549 / \u00a3549 / AU$1,109 in the US, UK, and Australia, respectively.\n\nThis puts it at the same price as the current RTX 4070 MSRP, and slightly less than that of the RTX 4070 Super. It's also the same MSRP as the AMD's RX 7900 GRE and upcoming RX 9070, and slightly cheaper than the AMD RX 9070 XT's MSRP.\n\nThe relatively low MSRP for the RTX 5070 is one of the bright spots for this card, as well as the existence of the RTX 5070 Founders Edition card, which Nvidia will sell directly at MSRP. This will at least put something of an anchor on the card's price in the face of scalping and general price inflation.\n\nValue: 4 / 5\n\nNvidia GeForce RTX 5070: Specs\n\nGDDR7 VRAM and PCIe 5.0\n\nHigher power consumption\n\nStill just 12GB VRAM, and fewer compute units\n\nSwipe to scroll horizontally Nvidia GeForce RTX 5070 vs RTX 4070 Super vs RTX 4070 Specs Header Cell - Column 0 RTX 5070 RTX 4070 Super RTX 4070 Process Node TSMC N4P TSMC N4 TSMC N4 Transistor Count (Billion) 31.1 35.8 35.8 Compute Units 48 56 46 Shaders 6,144 7,168 5,888 RT Cores 48 56 46 Tensor Cores 192 224 184 Render Output Units 80 80 64 Cache (MB) 48 48 36 Base Clock (MHz) 2,325 1,980 1,920 Boost Clock (MHz) 2,512 2,475 2,475 Memory Clock (MHz) 1,750 1,313 1,313 Memory Type GDDR7 GDDR6X GDDR6X Memory Pool (GB) 12 12 12 Memory Interface (bits) 192 192 192 Effective Memory Speed (Gbps) 28 21 21 Memory Bandwidth (GB/s) 672.0 504.2 504.2 PCIe Interface 5.0 4.0 4.0 TGP (W) 250 220 200 Recommended PSU (W) 600 550 550 Power Connector 1 x 16-pin 1 x 16-pin 1 x 16-pin\n\nThe Nvidia GeForce RTX 5070 is a mixed bag when it comes to specs. On the one hand, you have advanced technology like the new PCIe 5.0 interface and new GDDR7 VRAM, both of which appear great on paper.\n\nOn the other hand, it feels like every other spec was configured and tweaked to make sure that it compensated for any performance benefit these technologies would impart to keep the overall package more or less the same as the previous generation GPUs.\n\nFor instance, while the RTX 5070 sports faster GDDR7 memory, it doesn't expand the VRAM pool beyond 12GB, unlike its competitors. If Nvidia was hoping that the faster memory would make up for keeping the amount of VRAM the same, it only makes a modest increase in the number of compute units in the GPU (48 compared to the RTX 4070's 46), and a noticeable decrease from the RTX 4070 Super's (56).\n\nWhatever performance gains the RTX 5070 makes with its faster memory, then, is completely neutralized by the larger number of compute units (along with the requisite number of CUDA cores, RT cores, and Tensor cores) in the RTX 4070 Super.\n\n(Image credit: Future / John Loeffler)\n\nThe base clock on the RTX 5070 is notably higher, but its boost clock is only slightly increased, which is ultimately where it counts while playing games or running intensive workloads.\n\nLikewise, whatever gains the more advanced TSMC N4P node offers the RTX 5070's GPU over the TSMC N4 node of its predecessors seems to be eaten up by the cutting down of the die. If there was a power or cost reason for this, I have no idea, but I think that this decision is what ultimately sinks the RTX 5070.\n\nIt seems like every decision was made to keep things right where they are rather than move things forward. That would be acceptable, honestly, if there was some other major benefit like a greatly reduced power draw or much lower price (I've argued for both rather than pushing for more performance every gen), but somehow the RTX 5070 manages to pull down an extra 30W of power over the RTX 4070 Super and a full 50W over the RTX 4070, and the price is only slightly lower than the RTX 4070 was at launch.\n\nFinally, this is a PCIe 5.0 x16 GPU, which means that if you have a motherboard with 16 PCIe lanes or less, and you're using a PCIe 5.0 SSD, one of these two components is going to get nerfed down to PCIe 4.0, and most motherboards default to prioritizing the GPU.\n\nYou might be able to set your PCIe 5.0 priority to your SSD in your motherboard's BIOS settings and put the RTX 5070 into PCIe 4.0, but I haven't tested how this would affect the performance of the RTX 5070, so be mindful that this might be an issue with this card.\n\nSpecs: 2.5 / 5\n\nNvidia GeForce RTX 5070: Design\n\n(Image credit: Future / John Loeffler)\n\nNo dual-pass-through cooling\n\nFE card is the same size as the RTX 4070 and RTX 4070 Super FE cards\n\nThe Nvidia GeForce RTX 5070 Founders Edition looks identical to the RTX 5090 and RTX 5080 that preceeded it, but with some very key differences, both inside and out.\n\nOne of the best things about the RTX 5090 and RTX 5080 FE cards was the innovative dual pass-through cooling solution on those cards, which improved thermals so much that Nvidia was able to shrink the size of those cards from the gargantuan bricks of the last generation to something far more manageable and practical.\n\n(Image credit: Future / John Loeffler)\n\nIt would have been nice to see what such a solution could have done for the RTX 5070, but maybe it just wasn't possible to engineer it so it made any sense. Regardless, it's unfortunate that it wasn't an option here, even though the RTX 5070 is hardly unwieldy (at least for the Founders Edition card).\n\nOtherwise, it sports the same 16-pin power connector placement as the RTX 5090 and RTX 5080, so 90-degree power connectors won't fit the Founders Edition, though you will have better luck with most, if not all, AIB partner cards which will likely stick to the same power connector placement of the RTX 40 series.\n\nThe RTX 5070 FE will easily fit inside even a SFF case with ease, and its lighter power draw means that even if you have to rely on the included two-to-one cable adapter to plug in two free 8-pin cables from your power supply, it will still be a fairly manageable affair.\n\nLastly, like all the Founders Edition cards before it, the RTX 5070 has no RGB, with only the white backlight GeForce RTX logo on the top edge of the card to provide any 'flair' of that sort.\n\nDesign: 3.5 / 5\n\nNvidia GeForce RTX 5070: Performance\n\n(Image credit: Future / John Loeffler)\n\nAlmost no difference in performance over the RTX 4070 Super without MFG\n\nUsing MFG can get you native RTX 4090 framerates in some games\n\nSignificantly faster performance over the RTX 4070\n\nA note on my data The charts shown below offer the most recent data I have for the cards tested for this review. They may change over time as more card results are added and cards are retested. The 'average of all cards tested' includes cards not shown in these charts for readability purposes.\n\nBoy howdy, here we go.\n\nThe best thing I can say about the performance of this card is that it is just barely the best 1440p graphics card on the market as of this review, and that DLSS 4's Multi Frame Generation can deliver the kind of framerates Nvidia promises in those games where the technology is available, either natively or through the Nvidia App's DLSS override feature.\n\nBoth of those statements come with a lot of caveats, though, and the RTX 5070 doesn't make enough progress from the last gen to make a compelling case for itself performance-wise, especially since its signature feature is only available in a smattering of games at the moment.\n\nOn the synthetic side of things, the RTX 5070 looks strong against the card it's replacing, the RTX 4070, and generally offers about 25% better performance on synthetic benchmarks like 3DMark Steel Nomad or Speed Way. It also has higher compute performance in Geekbench 6 than its direct predecessor, though not be as drastic a margin (about 10% better).\n\nCompared to the RTX 4070 Super, however, the RTX 5070's performance is only about 6% better overall, and only about 12% better than the AMD RX 7900 GRE's overall synthetic performance.\n\nAgain, a win is a win, but it's much closer than it should be gen-on-gen.\n\nThe RTX 5070 runs into similar issues on the creative side, where it only outperforms the RTX 4070 Super by about 3% overall, with its best performance coming in PugetBench for Creators' Adobe Premiere benchmark (~13% better than the RTX 4070 Super), but faltering somewhat with Blender Benchmark 4.3.0.\n\nThis isn't too surprising, as the RTX 5070 hasn't been released yet and GPUs tend to perform better in Blender several weeks or months after the card's release when the devs can better optimize things for new releases.\n\nAll in all, for this class of cards, the RTX 5070 is a solid choice for those who might want to dabble in creative work without much of a financial commitment, but real pros are better off with the Nvidia GeForce RTX 5070 Ti if you're looking to upgrade without spending a fortune.\n\nIt's with gaming, though, where the real heartbreak comes with this card.\n\nTechnically, with just 12GB VRAM, this isn't a 4K graphics card, but both the RTX 4070 Super and RTX 5070 are strong enough cards that you can get playable native 4K in pretty much every game so long as you never, ever touch ray tracing, global illumination, or the like. Unfortunately, both cards perform roughly the same under these conditions at 4K, with the RTX 5070 pulling into a slight >5 fps lead in a few games like Returnal and Dying Light 2.\n\nHowever, in some titles like F1 2024, the RTX 4070 Super actually outperforms the RTX 5070 when ray tracing is turned on, or when DLSS is set to balanced and without any Frame Generation. Overall and across different setting configurations, the RTX 5070 only musters a roughly 4.5% better average FPS at 4K than the RTX 4070 Super.\n\nIt's pretty much the same story at 1440p, as well, with the RTX 5070 outperforming the RTX 4070 Super by about 2.7% across configurations at 1440p. We're really in the realm of what a good overclock can get you on an RTX 4070 Super rather than a generational leap, despite all the next-gen specs that the RTX 5070 brings to bear.\n\nOK, but what about the RTX 4090? Can the RTX 5070 with DLSS 4 Multi Frame Generation match the native 4K performance of the RTX 4090?\n\nYes, it can, at least if you're only concerned with average FPS. The only game with an in-game benchmark that I can use to measure the RTX 5070's MFG performance is Cyberpunk 2077, and I've included those results here, but in Indiana Jones and the Great Circle and Dragon Age: Veilguard (using the Nvidia App's override function) I pretty much found MFG to perform consistently as promised, delivering substantially faster FPS than DLSS 4 alone and landing in the ballpark of where the RTX 4090's native 4K performance ends up.\n\nAnd so long as you stay far away from ray tracing, the base framerate at 4K will be high enough on the RTX 5070 that you won't notice too much, if any, latency in many games. But when you turn ray tracing on, even the RTX 5090's native frame rate tanks, and it's those baseline rendered frames that handle changes based on your input, and the three AI-generated frames based on that initial rendered frame don't factor in whatever input changes you've made at all.\n\nAs such, even though you can get up to 129 FPS at 4K with Psycho RT and Ultra preset in Cyberpunk 2077 on the RTX 5070 (blowing way past the RTX 5090's native 51 average FPS on the Ultra preset with Psycho RT), only 44 of the RTX 5070's 129 frames per second are reflecting active input. This leads to a situation where your game looks like its flying by at 129 FPS, but feels like it's still a sluggish 44 FPS.\n\nFor most games, this isn't going to be a deal breaker. While I haven't tried the RTX 5070 with 4x MFG on Satisfactory, I'm absolutely positive I will not feel the difference, as it's not the kind of game where you need fast reflexes (other than dealing with the effing Stingers), but Marvel Rivals? You're going to feel it.\n\nNvidia Reflex definitely helps take the edge off MFG's latency, but it doesn't completely eliminate it, and for some games (and gamers) that is going to matter, leaving the RTX 5070's MFG experience too much of a mixed bag to be a categorical selling point. I think the hate directed at 'fake frames' is wildly overblown, but in the case of the RTX 5070, it's not entirely without merit.\n\nSo where does that leave the RTX 5070? Overall, it's the best 1440p card on the market right now, and it's relatively low MSRP makes it the best value proposition in its class. It's also much more likely that you'll actually be able to find this card at MSRP, making the question of value more than just academic.\n\nFor most gamers out there, Multi Frame Generation is going to be great, and so long as you go easy on the ray tracing, you'll probably never run into any practical latency in your games, so in those instances, the RTX 5070 might feel like black magic in a circuit board.\n\nBut my problem with the RTX 5070 is that it is absolutely not the RTX 4090, and for the vast majority of the games you're going to be playing, it never will be, and that's essentially what was promised when the RTX 5070 was announced. Instead, the RTX 5070 is an RTX 4070 Super with a few games running MFG slapped to its side that look like they're playing on an RTX 4090, but may or may not feel like they are, and that's just not good enough.\n\nIt's not what we were promised, not by a long shot.\n\nPerformance: 3 / 5\n\nShould you buy the Nvidia GeForce RTX 5070?\n\n(Image credit: Future / John Loeffler)\n\nSwipe to scroll horizontally Nvidia GeForce RTX 5070 Ti Scorecard Category Notes Score Value The cheapest of the Blackwell GPUs, you might actually be able to find this one at MSRP. 4 / 5 Specs GDDR7 and PCIe 5.0 look great on paper, but the entire card seems designed to make sure those two features are a complete non-factor. Also, please stop making 12GB RTX 4070s. Memory isn't that expensive. 2.5 / 5 Design There's no dual pass-through cooling on this card and it's pretty much inoffensive on every front, but doesn't break the mold in any way either. 3.5 / 5 Performance Without DLSS 4 Multi Frame Generation, this card offers next to no real performance gains over the RTX 4070 Super, and very few games have MFG. Those that do, it works great, but it's not nearly enough. 2.5 / 5 Final score The RTX 5070 is a very hard card to recommend, especially when there are so many competing cards at this price that offer similar performance. 3.13 / 5\n\nBuy the Nvidia GeForce RTX 5070 if...\n\nYou don't have the money for (or cannot find) an RTX 5070 Ti or RTX 4070 Super\n\nThis isn't a bad graphics card, but there are so many better cards that offer better value or better performance within its price range.\n\nYou want to dabble in creative or AI work without investing a lot of money\n\nThe creative and AI performance of this card is great for the price.\n\nDon't buy it if...\n\nYou can afford to wait for better\n\nWhether it's this generation or the next, this card offers very little that you won't be able to find elsewhere within the next two years.\n\nAlso consider\n\nNvidia GeForce RTX 5070 Ti\n\nThe RTX 5070 Ti is a good bit more expensive, especially with price inflation, but if you can get it at a reasonable price, it is a much better card than the RTX 5070. Read the full Nvidia GeForce RTX 5070 Ti review\n\nNvidia GeForce RTX 4070 Super\n\nWith Nvidia RTX 50 series cards getting scalped to heck, if you can find an RTX 4070 Super for a good price, it offers pretty much identical performance to the RTX 5070, minus the Multi Frame Generation. Read the full Nvidia GeForce RTX 4070 Super review\n\nHow I tested the Nvidia GeForce RTX 5070\n\nI spent about a week with the RTX 5070\n\nI used my complete GPU testing suite to analyze the card's performance\n\nI tested the card in everyday, gaming, creative, and AI workload usage\n\nTest System Specs Here are the specs on the system I used for testing: Motherboard: ASRock Z790i Lightning WiFi\n\nCPU: Intel Core i9-14900K\n\nCPU Cooler: Gigabyte Auros Waterforce II 360 ICE\n\nRAM: Corsair Dominator DDR5-6600 (2 x 16GB)\n\nSSD: Crucial T705\n\nPSU: Thermaltake Toughpower PF3 1050W Platinum\n\nCase: Praxis Wetbench\n\n\n\nI spent about a week testing the Nvidia GeForce RTX 5070, using it as my main workstation GPU for creative content work, gaming, and other testing.\n\nI used my updated testing suite including industry standard tools like 3DMark and PugetBench for Creators, as well as built-in game benchmarks like Cyberpunk 2077, Civilization VII, and others.\n\nI've reviewed more than 30 graphics cards for TechRadar in the last two and a half years, as well as extensively testing and retesting graphics cards throughout the year for features, analysis, and other content, so you can trust that my reviews are based on experience and data, as well as my desire to make sure you get the best GPU for your hard earned money.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Why Nvidia Stock Jumped Today",
            "link": "https://www.nasdaq.com/articles/why-nvidia-stock-jumped-today-1",
            "snippet": "Despite initially opening the day's trading in the red, Nvidia (NASDAQ: NVDA) stock wound up posting gains in Tuesday's trading.",
            "score": 0.6610116362571716,
            "sentiment": null,
            "probability": null,
            "content": "Despite initially opening the day's trading in the red, Nvidia (NASDAQ: NVDA) stock wound up posting gains in Tuesday's trading. The artificial intelligence (AI) hardware leader's share price ended the session up 1.7% and had been up as much as 4.6% earlier in the session.\n\nNvidia stock saw a modest recovery today as investors digested the effects of tariffs and other macroeconomic pressures and geopolitical dynamics that involve the company's chips. The AI leader's share price also got a boost from Taiwan Semiconductor Manufacturing's recent announcement that it will invest $100 billion to expand its U.S. chip-fabrication operations.\n\nWhere to invest $1,000 right now? Our analyst team just revealed what they believe are the 10 best stocks to buy right now. Learn More \u00bb\n\nInvestors bought back into Nvidia stock after yesterday's rout\n\nNvidia stock fell 8.7% in yesterday's trading as investors reacted to the impending implementation of new tariffs on Canada and Mexico. Expectations that export restrictions and enforcement initiatives could intensify following new reports that China is gaining access to prohibited Nvidia chips through third-party sellers also added to sell-offs.\n\nWhile these risk factors remain, investors bought back into stocks as the day progressed and helped drive a moderate shift in bullish sentiment for the day. Despite today's gain, Nvidia stock is still down 13.8% across 2025's trading.\n\nTSMC's $100 billion U.S. investment could be good for Nvidia\n\nTSMC plans to spend $100 billion to build new chip foundries in Arizona. The Taiwan-based chip fabrication leader manufactures Nvidia's semiconductor designs and is the world's largest and best-performing contract foundry.\n\nRising tensions between the U.S. and China and concerns that the latter country could move to invade or exert greater control over Taiwan have been a source of valuation volatility for the tech sector. TSMC has enormous importance in global supply chains and AI technologies in particular, and disruption of its operations would have huge adverse effects. With the fabrication giant expanding its operational footprint in the U.S., a major geopolitical risk factor could be mitigated -- but hitting its new expansion target in the country will take a long time.\n\nShould you invest $1,000 in Nvidia right now?\n\nBefore you buy stock in Nvidia, consider this:\n\nThe Motley Fool Stock Advisor analyst team just identified what they believe are the 10 best stocks for investors to buy now\u2026 and Nvidia wasn\u2019t one of them. The 10 stocks that made the cut could produce monster returns in the coming years.\n\nConsider when Nvidia made this list on April 15, 2005... if you invested $1,000 at the time of our recommendation, you\u2019d have $655,630!*\n\nStock Advisor provides investors with an easy-to-follow blueprint for success, including guidance on building a portfolio, regular updates from analysts, and two new stock picks each month. The Stock Advisor service has more than quadrupled the return of S&P 500 since 2002*. Don\u2019t miss out on the latest top 10 list, available when you join Stock Advisor.\n\nSee the 10 stocks \u00bb\n\n*Stock Advisor returns as of March 10, 2025\n\nKeith Noonan has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Nvidia and Taiwan Semiconductor Manufacturing. The Motley Fool has a disclosure policy.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia GeForce RTX 5070 review: $549 price and performance look decent on paper",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-5070-review-founders-edition",
            "snippet": "The Nvidia GeForce RTX 5070 lands at the upper end of what used to be the mainstream price segment, but we strongly doubt the $549 MSRP will be in effect...",
            "score": 0.9324355721473694,
            "sentiment": null,
            "probability": null,
            "content": "Why you can trust Tom's Hardware Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.\n\nIntroducing the Nvidia GeForce RTX 5070 Founders Edition\n\nThe Nvidia GeForce RTX 5070 Founders Edition has a big hole to fill in the graphics card market. As the first true mainstream offering for the Blackwell RTX 50-series GPUs, it takes over from the discounted RTX 4070 Founders Edition with the same nominal $549 base MSRP. It also has the same 12GB of VRAM and nearly the same number of streaming multiprocessors (SMs) \u2014 48 versus 46 \u2014 but with the new Blackwell features. On paper, getting a faster GPU for less money with new features should make this one of the best graphics cards, but we have some concerns.\n\n\n\nThe biggest problem will no doubt be retail availability and pricing, and we've seen every GPU launch of the past few months sell out almost instantly. From Intel's $249 Arc B580 to the $1,999 RTX 5090, with the RTX 5080 and RTX 5070 Ti filling in the middle, MSRPs have been effectively non-existent. We don't expect the 5070 to buck that trend, and it's all starting to feel a lot like 2021 \u2014 just with AI-induced GPU shortages rather than cryptocurrency mining shortages. When will it end? That's a difficult question to answer.\n\n\n\nNvidia posted record earning of $130 billion for the 2025 fiscal year that just ended, more than double its 2024 earnings. Nearly all of the gains came from its AI and data center business, which accounted for 88% of gross revenue. Gaming was a very distant second place at just 8.7% of the total revenue. Nvidia has been saying it's no longer primarily a gaming company for a while now, and nowhere is that more apparent than in the financials.\n\n\n\nWith massive demand coming from the AI sector, and with limited 5nm-class wafers from TSMC, the simple economics show that it's far more profitable to make data center and AI products right now rather than consumer GPUs. It's not that Nvidia won't order any consumer GPUs, but it's unlikely to be anywhere near sufficient to meet the demand. And in fact, right now virtually every graphics card of the past two years is either sold out or severely overpriced relative to the launch MSRP \u2014 with the only exceptions being the RTX 4060, AMD's RX 7600 (the RX 7600 XT currently starts at $430, $100 more than its original MSRP), and Intel's Arc B570.\n\n\n\nThe prospects for reasonably priced GPUs look grim, in other words. It could be many months before anything gets close to MSRP \u2014 and that goes for AMD's RX 9070 XT and RX 9070 that are slated for review tomorrow. We expect those to be just as hard to acquire at MSRP as the RTX 5070, which will officially go on sale tomorrow. But maybe our pessimism will prove misplaced! For now, all we can do is look at the performance and features on tap, and hope that supply will catch up to demand sooner rather than later.\n\nWe've been kept busy during the past two months testing and retesting graphics cards. The fourth Nvidia GPU launch of the year and sixth new graphics card since December hasn't given us time to catch our collective breath, never mind getting all the other prior generation GPUs we'd like to test filed through our new test suite.\n\n\n\nLast month we also took a closer look at DLSS 4 and MFG, using the 5080 and 5090, which will have to suffice for now \u2014 time constraints didn't allow us to cover the same tests on the RTX 5070 Ti or the 5070, or the 9070 XT and 9070 for that matter. But we'll get around to those hopefully by next week and update the appropriate review pages.\n\n\n\nUntil then, the TLDR remains the same: MFG is a great way to inflate benchmark scores, and in the right scenarios it can feel better than framegen or non-framegen even if it has slightly higher input latencies. But the benchmark numbers tend to be much higher compared to how games actually feel. It's not bad as such, but subjectively MFG4X might feel more like 30~40 percent faster than the non-MFG performance, rather than the 200% improvement benchmarks can show. It will look smoother even while typically delivering the same or lower levels of responsiveness.\n\n\n\nFor additional information about Nvidia's Blackwell RTX GPUs, check the links in the boxout. The RTX 5070 Founders Edition represents the reference clocks and design from Nvidia, which will likely be just as fast as most of the non-reference card models from AIB partners. It might also be slightly more affordable, assuming you can find any in stock. But as usual, let's start with the specs table to see how it compares to the prior generation.\n\nSwipe to scroll horizontally Graphics Card RTX 5070 RTX 4070 RTX 3070 RX 9070 Architecture GB205 AD104 GA104 Navi 48 Process Technology TSMC 4N TSMC 4N Samsung 8N TSMC N4P Transistors (Billion) 31 32 17.4 53.9 Die size (mm^2) 263 294.5 392.5 356.5 SMs / CUs 48 46 46 56 GPU Shaders (ALUs) 6144 5888 5888 3584 Tensor / AI Cores 192 184 184 112 Ray Tracing Cores 48 46 46 56 Boost Clock (MHz) 2512 2475 1725 2520 VRAM Speed (Gbps) 28 21 14 20 VRAM (GB) 12 12 8 16 VRAM Bus Width 192 192 256 256 L2 / Infinity Cache 48 36 4 64 Render Output Units 80 64 96 128 Texture Mapping Units 192 184 184 224 TFLOPS FP32 (Boost) 30.9 29.1 20.3 36.1 TFLOPS FP16 (FP4/FP8/INT4 TOPS) 247 (988) 233 (466) 163 289 (1156) Bandwidth (GB/s) 672 504 448 640 TBP (watts) 250 200 220 220 Launch Date Feb 2025 Apr 2023 Oct 2020 Mar 2025 Launch Price $549 $599 $499 $549\n\nThe paper specifications don't necessarily tell the full story. For example, the Blackwell architecture doubles the ray/triangle intersections per clock for the RT cores, the tensor cores support new number formats like FP4, and the CUDA cores all support FP32 and INT32 operations (only half of the CUDA cores in the RTX 40- and 30-series GPUs supported INT32 operations). That leads to what might appear at first to be little to no change in performance potential.\n\n\n\nRTX 5070 has peak theoretical FP32 compute of 30.9 TeraFLOPS, compared to 29.1 TeraFLOPS on the RTX 4070 \u2014 a mere 6.2% increase. TGP (Total Graphics Power) has increased from 200W to 250W, however, along with memory getting a sizeable 33% bump in bandwidth thanks to the move to GDDR7 memory. So in theory, the 5070 should be somewhere between 6% and 33% faster than its direct predecessor for 'normal' workloads (i.e. things that don't leverage the FP4 support or MFG). In practice, the gains are on the higher end of that range for most games.\n\n\n\nDie size and transistor counts are interesting as well, mostly because the previous generation AD104 GPU was used in the RTX 4070 Ti and had up to 60 SMs available, even though only 46 were enabled in the 4070. The GB205 die only has up to 50 SMs, however, with 48 enabled in the RTX 5070. That's what makes the new chip smaller and also gives it fewer transistors \u2014 both chips are made on the same TSMC 4N node.\n\n\n\nAI compute does potentially favor the RTX 5070 a lot, but only if we include FP4 support. It has up to 988 TFLOPS of FP4 compute (which Nvidia classifies as \"TOPS\" even though that's normally only used for integer calculations), more than double the 4070's 466 TFLOPS of FP8. But for FP8 compute, it's the same 6.2% difference as the graphics FP32 compute. Clock speeds on paper are only slightly higher with the 5070 compared to the 4070, but we'll need to look at real-world clocks as Blackwell and Ada GPUs tend to run at much higher clocks than the stated boost clocks.\n\n\n\nThe RTX 5070 offers a much bigger improvement over the older RTX 3070, naturally, with about 50% more theoretical compute and up to 6X more AI compute (comparing FP4 to FP16, with sparsity in both cases). But AMD's upcoming RX 9070, which we'll review tomorrow, looks set to deliver some serious competition. Check back in 24 hours and we'll have the full review for AMD's 9070 and 9070 XT.\n\nNvidia includes far more flexible 16-pin to 8-pin adapters with its 50-series Founders Edition cards, though most people should use a direct 16-pin 12V-2x6 connection if possible. (Image credit: Tom's Hardware)\n\nAgain, before we even get to the benchmarks, there are a couple of elephants in the corner.\n\n\n\nFirst is retail availability and pricing. We have every reason to expect the RTX 5070 cards will sell out quickly tomorrow when they go on sale, and that many models will end up at significantly higher prices than the ostensible $549 MSRP. After all, the cheapest graphics cards are pretty much stupidly expensive \u2014 and that goes for used cards on places like eBay as well, where the RTX 4070 price in the past 30 days has averaged over $650. Will a card that's newer, faster, and has more features cost less than the previous generation? Not a chance.\n\n\n\nThe other item to remember is the impending AMD Radeon RX 9070 and RX 9070 XT launch, which will be one day after the RTX 5070 \u2014 meaning, MSRP-priced reviews go up tomorrow, and the cards go on sale starting March 6. The RX 9070 competes directly with the RTX 5070 on price, or at least MSRP. Traditionally, AMD GPUs also don't command quite as much demand as Nvidia GPUs. But the RX 9070 XT for $50 more looks like it will potentially compete with the RTX 5070 Ti, or alternatively it should easily beat the RTX 5070 for a relatively minor price increase.\n\n\n\nBut AMD GPU availability right now isn't any better than Nvidia GPUs. Everything from the RX 7600 XT and above is horribly overpriced, and the previous generation RX 7900 GRE that was intended to compete with the RTX 5070 at the $549 price point now sells for over $900, with the average eBay price for used GPUs over the past 30 days sitting at $711. Newer, faster, and better RX 9070-class GPUs will inevitably sell out and end up going for much more than $549 or $599.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Nvidia GeForce RTX 5070 review: definitely not a $549 RTX 4090",
            "link": "https://www.theverge.com/gpu-reviews/623441/nvidia-rtx-5070-review-test-benchmark",
            "snippet": "Nvidia's new RTX 5070 improves on the existing RTX 4070 and RTX 4070 Super. It launches on March 5th, just ahead of AMD's RX 9070 series.",
            "score": 0.8948202729225159,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years.\n\nThe RTX 50-series launch hasn\u2019t gone smoothly so far. Severely limited stock, high prices, and manufacturing issues have left PC gamers frustrated. Now, Nvidia is trying to get things heading in the right direction with the RTX 5070, a GPU that it promised would deliver $1,599 RTX 4090-like performance for $549.\n\nIt doesn\u2019t, and it was never going to. It\u2019s about 20 percent faster than the RTX 4070 and a bare 4 percent faster than last year\u2019s RTX 4070 Super. It\u2019s a solid card for 1440p gaming, just like the RTX 4070, but as with the rest of the 50-series cards, Nvidia\u2019s performance claims rely on Frame Generation rather than meaningful improvements to rendering.\n\nAt $549, the RTX 5070 will also come up against AMD\u2019s new $549 Radeon RX 9070 and $599 Radeon RX 9070 XT when they launch on March 6th. If AMD manages to beat Nvidia\u2019s RTX 5070 or come close to the $749 RTX 5070 Ti, that\u2019s going to put a lot of much-needed pressure on Nvidia\u2019s pricing.\n\nYou might want to wait a couple of days to purchase anything until we can talk about those AMD cards.\n\n1440p and 4K benchmarks\n\n\n\nTest machine: CPU: AMD Ryzen 9 9800X3D\n\nCPU cooler: Corsair H150i Elite LCD\n\nMotherboard: Gigabyte Aorus Master\n\nRAM: 32GB G.Skill DDR5-6000\n\nStorage: Samsung 970 Evo Plus 2TB\n\nPSU: Corsair HX1000W\n\nCase: Streacom BC1 V2 open benchtable\n\nThe RTX 5070 Founders Edition card I\u2019ve been testing looks like a miniature RTX 5090 with a slightly darker paint job and no LEDs. It\u2019s still a two-slot card, and it retains the two fans at the bottom that direct cooler air over the card and exhaust it out of the top and rear, as well as the slightly angled 12V-2x6 power connector that makes it easier to fit into more cases.\n\nI really like the size. It feels like the ideal companion for a small form factor PC, and I don\u2019t think I\u2019d be too worried about the heat this little GPU would generate or the power draw (more on that later).\n\nI\u2019ve been testing the RTX 5070 card with AMD\u2019s Ryzen 9 9800X3D processor and Asus\u2019 32-inch 4K OLED PG32UCDP. I\u2019ve put it up against the previous RTX 4070 and RTX 4070 Super cards, as well as the more expensive RTX 5070 Ti and AMD\u2019s RX 7900 XT, which you can see in the benchmark charts below.\n\nPrevious Next\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 / 9\n\nI\u2019ve tested a variety of games at both 1440p and 4K, including more demanding titles like Black Myth: Wukong and benchmarking favorites like Cyberpunk 2077 and Shadow of the Tomb Raider. All games have been tested at very high or Ultra settings, so you\u2019ll get a good idea of the RTX 5070\u2019s true capabilities, though you\u2019ll probably want to drop the settings for better frame rates.\n\nAs with every other 50-series card, the 5070 isn\u2019t much of an upgrade in raw performance over the last generation. Without DLSS or RT enabled, the RTX 5070 is 18 percent faster than an RTX 4070 at 1440p and 22 percent faster at 4K; compared to the 4070 Super, it\u2019s only 3 percent faster at 1440p and 4 percent at 4K. You\u2019ll see a solid boost if you\u2019re coming from a 30-series card or older, though; it\u2019s nearly 55 percent faster than the RTX 3070 Ti.\n\nThe RTX 5070 is still largely a 1440p card, which has been the sweet spot for Nvidia\u2019s 70 lineup for a few generations. While it delivers playable frame rates for most games in my test suite at 4K resolution, you\u2019ll need to enable DLSS in some titles if you want to be over 60fps consistently with all the settings maxed out. If you\u2019re willing to drop some quality settings and enable DLSS, the RTX 5070 becomes a lot more capable.\n\nPrevious Next\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 / 9\n\nIf you\u2019re looking for better 4K performance, stepping up to an RTX 5070 Ti gets you about 29 percent higher frame rates, but you\u2019ll pay at least 35 percent more, assuming either card stays anywhere near MSRP.\n\nMy main concern about the RTX 5070 for 4K gaming is its 12GB of VRAM. I\u2019ve already run into performance issues testing entry-level 4K gaming with the 8GB RTX 3070 Ti in certain games, and I don\u2019t think 12GB of VRAM is going to fare well in the future, either. This is particularly relevant when AMD is about to ship its RX 9070 and RX 9070 XT with 16GB of VRAM. I wish Nvidia had done the same.\n\nDLSS 4 and Multi Frame Gen\n\nIt was obvious from the start that Nvidia\u2019s claims that the RTX 5070 would match the RTX 4090 relied entirely on DLSS 4\u2019s Multi Frame Generation. The 50-series cards have up to 4x frame generation with DLSS 4, so they can generate up to three additional frames per frame rendered traditionally. The RTX 4090 can use DLSS 4, including Nvidia\u2019s updated transformer models, but only has 2x frame generation.\n\nIt\u2019s only the fact that the 5070 can generate three times as many interstitial frames as the RTX 4090 that lets it appear anywhere near as fast. In Cyberpunk 2077 at 4K with Ultra settings, ray tracing enabled, and 4x Frame Generation, the RTX 5070 averaged 122fps. The older RTX 4090, with 2x Frame Generation, managed 128fps, and it\u2019s rendering far more of them the old-fashioned way.\n\nIf you take Nvidia at face value and assume performance equals frame rate, the RTX 4090 is still 5 to 10 percent faster than the RTX 5070 when frame generation is cranked up all the way on both cards. In traditional rendering, the RTX 4090 is 75 percent faster on average. Without ray tracing or frame generation, the RTX 4090 got 76fps in Cyberpunk 2077, and the RTX 4070 got 48fps.\n\nAs I\u2019ve discussed in previous reviews, and as even Nvidia\u2019s materials show, multi-frame generation makes gameplay look smoother, but the game will still feel sluggish if the base frame rate is low. You can really feel it on the RTX 5070, especially in 4K. With DLSS Super Resolution and x4 Multi Frame Generation, Cyberpunk 2077 hit 77fps at Ultra settings with full path tracing, but it still feels like the 24fps it is before Multi Frame Gen is applied.\n\nMulti Frame Gen makes more sense at 1440p because the base frame rates are a lot higher. I wouldn\u2019t buy the RTX 5070 because of Multi Frame Generation alone, but the DLSS 4 improvements and the new transformer model have certainly improved image quality for me in a variety of games, and you can even force DLSS 4 in unsupported games through Nvidia\u2019s new app. But you don\u2019t need an RTX 5070 for DLSS 4, so if you don\u2019t care about Multi Frame Generation and you can somehow find an RTX 4070 Super at a reasonable price, it\u2019s still a good choice.\n\nBeyond gaming, the RTX 5070 can also deliver better performance in video editing or for AI workloads. In Procyon\u2019s AI XL (FP16) test, the RTX 5070 is nearly 28 percent faster than the RTX 4070, or nearly 4 percent faster than the RTX 4070 Super. For video editing, I tested the RTX 5070 with PugetBench\u2019s DaVinci Resolve test and found it\u2019s 11 percent faster than the RTX 4070 and nearly 10 percent faster than the RTX 4070 Super.\n\nPower draw and heat\n\nNvidia recommends a 650-watt power supply for the RTX 5070, which is exactly the same as both the RTX 4070 and RTX 4070 Super. The total graphics power has increased to 250 watts in total, up by 50 watts over the RTX 4070 and just 30 watts over the RTX 4070 Super.\n\nAt 4K resolution, the RTX 5070 averaged 210 watts across the nine games tested \u2014 25 watts more than the RTX 4070 averaged (185 watts) and just four watts more than the RTX 4070 Super (206 watts). The RTX 5070 hit its max power draw of 250 watts during the Metro Exodus Extreme benchmark, and it hit a maximum temperature of 77 degrees Celsius in that same test on my open bench, compared to 67C on both the RTX 4070 and 4070 Super.\n\nI\u2019m once again impressed that Nvidia hasn\u2019t massively increased the power requirements here, but it looks like the RTX 5070 will run hotter than previous cards. While the RTX 5090 is certainly power-hungry, both the RTX 5070 and RTX 5070 Ti are a lot more efficient, while delivering gains over the previous-generation cards.\n\nThe RTX 5070 on top of an RTX 5080.\n\nI feel like a broken record saying that the RTX 5070 continues the trend of modest generational improvements with the RTX 50 series, but unfortunately, that\u2019s the case here again. This is really an ideal 1440p card thanks to the high frame rates at that resolution, or an entry-level 4K card if you\u2019re willing to drop settings down and enable DLSS.\n\nBut it\u2019s hard to recommend the RTX 5070 until the question hanging over it has been answered: AMD\u2019s RX 9070-series cards. AMD surprised everyone with a $549 price tag for its RX 9070 and $599 for its RX 9070 XT last week. Both cards are challenging Nvidia\u2019s pricing for its RTX 5070 and RTX 5070 Ti, and if they\u2019re good enough, they might put pressure on Nvidia to adjust its pricing.\n\nUntil we know how all four cards compare, I would hold off on buying an RTX 5070 on day one. AMD might just surprise us with its performance as well as its pricing.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "GeForce RTX 5070 FE Review: NVIDIA\u2019s More Affordable Blackwell Tested",
            "link": "https://hothardware.com/reviews/nvidia-geforce-rtx-5070-fe-review-and-benchmarks",
            "snippet": "The final member of the initial GeForce RTX 50 series launches today, the GeForce RTX 5070, and we've got the full scoop.",
            "score": 0.6490355730056763,
            "sentiment": null,
            "probability": null,
            "content": "\n\nNVIDIA GeForce RTX 5070 Founders Edition: MSRP $549\n\nThe final member of the initial GeForce RTX 50 series launches today, the mid-range GeForce RTX 5070, and we've got the full scoop with copius benchmarks and overclocking data.\n\n\n\n\n\nLatest Architecture And Features\n\nRelatively Low Power\n\nSmall Form Factor\n\nModest Temps And Noise\n\nGood Overclocker\n\nDLSS 4 + RTX Neural Rendering Support\n\nNew Media Engine\n\nSmall Upgrade Over 40-Series Cousin\n\n\n\n\n\nNVIDIA GeForce RTX 5070 Founders Edition Specifications\n\nChecking Out The NVIDIA GeForce RTX 5070 FE\n\nNVIDIA is launching the final member of the initial batch GeForce RTX 50 series cards today, the GeForce RTX 5070. First announced at CES in early July, the GeForce RTX 5070 will be the most affordable Blackwell-based GPU for the time being, or at least until NVIDIA releases information regarding potential GeForce RTX 5060 and RTX 5050 class products.NVIDIA CEO Jensen Huang famously proclaimed the GeForce RTX 5070 was \u2018faster than a 4090\u2019 during his keynote at CES , but as most of you know by now, that statement requires some qualification. In terms of pure raster performance, the GeForce RTX 5070 represents a similar upgrade over the older RTX 4070, that its higher-end siblings offer over their previous-gen counterparts. Factor in DLSS 4 multi-frame generations, however, and the GeForce RTX 5070 can smooth out in-game animation at a much higher rate.Alas, we\u2019re getting ahead of ourselves. Let\u2019s get to know the GeForce RTX 5070 Founders Edition, before we dive into some benchmarks. On with it...The GeForce RTX 5070 is based on yet another GPU in NVIDIA\u2019s Blackwell-based portfolio, the GB205. If you recall, the GeForce RTX 5070 Ti and RTX 5080 are based on the larger GB203, and the flagship GeForce RTX 5090 on the larger-still GB202.In its fully-enabled form, the GB205 GPU features 5 GPCs, 25 TPCs, 50 SMs, and six 32-bit memory controllers, for an aggregate 192-bit memory interface. Tunneling a little deeper, the GB203 packs a total of 6,400 CUDA Cores, 50 RT Cores, 200 Tensor Cores, 200 Texture Units, and 80 ROPS. And its cache configuration includes 6,400KB L1 Cache, a 12,800KB Register File, and 49,152KB of L2 Cache.All told, the chip is comprised of approximately 31B transistors and it\u2019s the smallest xx70-class GPU in a few generations at only 263mm, despite being manufactured on the same process as the previous-gen AD203.The GeForce RTX 5070, however, doesn\u2019t feature a full implementation of the GB203. On the GeForce RTX 5070, one TPC and two SMs are disabled, which brings the core counts down to the 6,144 (CUDA), 48 (RT) and 192 (Tensor), and the L1 and register file down to 6,144KB and 12,288KB, respectively.The GB203 on the RTX 5070 is linked to 12GB of GDDR7 memory operating at a speedy 28Gbps, for peak memory bandwidth in the 672GB/s range, and it features one of NVIDIA\u2019s latest encoders and decoders. And total board power is rated for 250 watts.When compared to the last couple of generations of xx70-class GPU, the GeForce RTX 5070 is an across-the-board upgrade in every category. We\u2019ll find out how all of those specs translate in the real-world soon enough, but we should also remind you that Blackwell introduces a host of updates and new features as well. You can read more about them in our initial architecture coverage, here The GeForce RTX 5070\u2019s industrial design and overall aesthetics, resemble the higher-end GeForce RTX 5080 and 5090 Founders Edition cards, but the RTX 5070 is significantly smaller and more compact.The card measures only 9.5\u201d long, two-slots wide, and about 4.5\u201d high \u2013 the shroud is barely higher than the case bracket. It's a dense package that uses premium materials thoughout, and presents as a \u201cmini-5090\u201d.Unlike its larger siblings, the GeForce RTX 5070 doesn\u2019t feature a \u201cDouble Flow Through\u201d cooler design, which allows both cooling fans to blow air straight through the heatsink. It\u2019s a Single Flow through design, similar to the previous-gen GeForce RTX 4070. NVIDIA\u2019s newly designed cooling solutions on the RTX 50 series cards, however, are more capable than previous-gen offerings, while also emitting less noise.At the top of the card, you'll find the familiar 12VHPWR 16-pin connector, also used on the RTX 40 series. On the RTX 5070 though, the connector is angled off the back on the PCB, and recessed slightly in the shroud. This configuration should allow for easier cable management and minimize the need to bend the power feed in thinner PC cases. The included power adapter requires two PCIe 8-pin feeds, but the cabling is longer and far more flexible than the adapters included in older RTX 40-series cards. The connector on the adapter is also beefier and emits a solid \"click\" when pushed fully into place.The GeForce RTX 5070 features a cold plate, with a dense array of heatsink thin-fins, linked via heat pipes, similar to the RTX 5080\u2019s \u2013 just smaller. NVIDIA also uses a more traditional phase-change TIM on the RTX 5070, so none of the complexities associated with liquid metal are at play.Outputs in the GeForce RTX 5070 (and other RTX 50 series cards) include a trio of DisplayPorts (2.1b) and a single HDMI port (2.1b). Their orientations have been reversed versus previous-gen cards though, and the case bracket features a solid front bezel with an anti-fingerprint coating.And with that you've gotten a tour of the GeForce RTX 5070 Founders Edition's design, so let's get to some benchmarks...",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "NVIDIA GeForce RTX 5070 Founders Edition Review",
            "link": "https://pcper.com/2025/03/nvidia-geforce-rtx-5070-founders-edition-review/",
            "snippet": "NVIDIA GeForce RTX 5070 Founders Edition Review It has been just over a year since the last xx70 card from NVIDIA, as the GeForce RTX 4070 Super was...",
            "score": 0.940584659576416,
            "sentiment": null,
            "probability": null,
            "content": "With AMD lurking, did NVIDIA deliver?\n\nIt has been just over a year since the last xx70 card from NVIDIA, as the GeForce RTX 4070 Super was launched in January of 2024, and while that was a mid-cycle refresh it does seem to be the logical point of comparison with the latest in the RTX 50 Series. But that may not be the best point of reference, as we may discover here.\n\nWait a minute, you may ask, does this mean that we have reached the point where meaningful gains from an RTX 50 Series card over an RTX 40 Series card sometimes need to look past the \u201cSuper\u201d variants of their precursors, and back instead to the vanilla launch versions? I don\u2019t want to spoil anything, but\u2026yes.\n\nEnter the GeForce RTX 5070.\n\nForgetting all about architecture for a moment, and Blackwell definitely has improvements over Ada Lovelace (insert ROP joke here), a high-level look at the specs might make one concerned about the performance of an RTX 5070 over the RTX 4070 Super from last year (plus, we already told you that might be the best point of reference anyway):",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-03": {
        "0": {
            "title": "US servers in Singapore fraud case may contain Nvidia chips, minister says",
            "link": "https://www.reuters.com/technology/servers-used-singapore-fraud-case-may-contain-nvidia-chips-minister-says-2025-03-03/",
            "snippet": "Servers used in a fraud case that Singapore announced last week were supplied by U.S. firms and may have contained Nvidia's advanced chips, a government...",
            "score": 0.7323338985443115,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "The NVIDIA RTX 5080 is shockingly small for its performance",
            "link": "https://www.windowscentral.com/hardware/the-nvidia-rtx-5080-is-all-kinds-of-incredible-but-i-wasnt-expecting-it-to-be-in-one-particular-way",
            "snippet": "NVIDIA's new RTX 5080 has massive performance within, but I was surprised by just how small it is compared to other graphics cards.",
            "score": 0.8532660007476807,
            "sentiment": null,
            "probability": null,
            "content": "You may not be able to buy an NVIDIA RTX 5080 right now, which is poor, but that doesn't change just how good it is. I've just installed our RTX 5080 Founders Edition into my gaming PC, replacing a much loved, but struggling (in some titles) ASRock Intel Arc A770.\n\nI've read our review of the RTX 5080, of course, but from the first second, I was amazed by something I wasn't expecting; The size.\n\nIt's a complete reversal of my thoughts when the RTX 40 series came out, and I first opened the obscenely large RTX 4090. I am actually shocked. Probably more shocked that it's smaller, even, than my outgoing Arc A770.\n\nWait, it's smaller than an Arc A770?!\n\nI was not expecting the RTX 5080 to be smaller than my existing Intel graphics card. (Image credit: Windows Central)\n\nI should point out that the ASRock Phantom Gaming Arc A770 to give the full title is not the same as the Intel-released Arc A770 Limited Edition. While the latter was a sleek, compact design, ASRock went bananas and stacked an absurdly large heatsink and three fans on top of their version. It's still a two slot card, at least, but it's HUGE. Especially considering the mediocre performance compared to what's replacing it.\n\nI simply couldn't believe that the RTX 5080 Founders Edition is a hair shorter and a decent amount thinner than this significantly less powerful graphics card. But it is, and while it's still a snug fit in my fairly small case, it fits well, and there's even a little more clearance for the other PCIe cards I have in there compared to before.\n\nI'm also pleasantly surprised by how rigid it is. It's a heavy graphics card, but there's absolutely no sign of sag. I'm not brave enough yet to remove my GPU support, but I don't think it's needed.\n\nIt's a shame that the OEM cards won't be as small, with gigantic heatsinks and extra fans and let's not forget about the RGB! Smaller PCs are increasingly desirable, and I'm impressed that with the 50 Series NVIDIA has made positive strides. I couldn't even put the side on my PC case when I had to review the RTX 4090, and the case I had then was larger than the one I use now.\n\nGet the Windows Central Newsletter All the latest news, reviews, and guides for Windows and Xbox diehards. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nI also want to shout out the decision to flip the power connector from horizontal to vertical. It has made it so much easier to connect without worrying whether the power cable is bent too much, or under stress, because, you know, I don't want it to catch fire.\n\nNVIDIA needs to get these things back on store shelves\n\nAs good as it is, nobody can buy one. That's the biggest shame. (Image credit: Windows Central | Ben Wilson)\n\nI've only had 24 hours with the RTX 5080 so far, but it's only taken that time to know that NVIDIA has really screwed up by not having a healthy supply of these things. I jokingly sent a message to a friend earlier saying that I'd figured out how to make Monster Hunter: Wilds look better \u2014 using an RTX 5080.\n\nI still don't love that game developers are relying on upscaling technology way too much, but the truth is that the RTX 5080 is an absolute beast. Not only that, but it's fairly quiet while doing it.\n\nDLSS is still the best of the upscalers out there, but at least with the horsepower on tap here, the quality setting can be used and still get some crazy high frame rates and great looking games. The new Dune: Awakening benchmark looks phenomenal, giving me a nice 140 FPS average maxed out at 1440p, and I can finally play Call of Duty: Black Ops 6 and Warzone without turning all the settings down to preserve those valuable frames.\n\nIt's almost like looking at a different game with all the graphics settings turned back up. I think I'm now finally equipped to play Spider-Man 2 and Indiana Jones, as well, because I'd been holding off convinced that my Arc A770 would start to choke, and I want to enjoy those titles at their best.\n\nNVIDIA also needs to worry about AMD, with the newest Radeon cards finally becoming available soon, and at some pretty eyebrow raising prices. For all I love about the RTX 5080, I do not like the price. Sure, it's crazy powerful, but it's a little insulting how expensive these graphics cards are getting.\n\nBut the Founders Edition at least gets a big thumbs up. It's an outlier, but it's the best design of them all in my eyes, and a marvel that this much performance has been squeezed inside a normal-sized graphics card.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia-Backed CoreWeave Files for IPO, Shows Growing Revenue",
            "link": "https://finance.yahoo.com/news/nvidia-backed-coreweave-files-ipo-221314619.html",
            "snippet": "(Bloomberg) -- CoreWeave, a cloud-computing provider that's one of the hottest startups in artificial intelligence, filed for an initial public offering...",
            "score": 0.9481883645057678,
            "sentiment": null,
            "probability": null,
            "content": "(Bloomberg) -- CoreWeave, a cloud-computing provider that\u2019s one of the hottest startups in artificial intelligence, filed for an initial public offering disclosing rapidly growing revenue.\n\nMost Read from Bloomberg\n\nThe Nvidia Corp.-backed company had revenue of $1.9 billion in 2024, resulting in a net loss of $863 million, versus revenue of $229 million and a net loss of $594 million in the previous year, according to a filing with the US Securities and Exchange Commission on Monday.\n\nThe company and its investors could raise about $4 billion and is expected to target a valuation greater than $35 billion, Bloomberg News has reported. CoreWeave and some of its shareholders are set to sell shares in the IPO.\n\nCoreWeave joins a wave of companies preparing for potentially sizable listings this year, amid expectations that the US IPO market could return to its pre-pandemic average.\n\nAbout 77% of CoreWeave\u2019s revenue came from its top two customers in 2024, one of which was Microsoft Corp., which accounted for nearly two thirds of overall sales, the filing shows.\n\nThe company, led by co-founder and Chief Executive Officer Michael Intrator, was started in 2017 as a crypto mining firm. Along with Nvidia, the firm counts Magnetar Capital, Coatue Management, Jane Street, JPMorgan Asset Management, Fidelity and Lykos Global Management among its investors.\n\nCoreWeave has also drawn backing from Cisco Systems Inc., which agreed to invest in CoreWeave as part of a transaction valuing it at $23 billion, Bloomberg News reported in October. Also that month, the company closed a $650 million credit facility led by JPMorgan Chase & Co., Goldman Sachs Group Inc. and Morgan Stanley.\n\nThe AI cloud company has material weaknesses in its internal controls over financial reporting, according to the section of the filing where pre-IPO companies disclose the risks to their businesses. The issues cited included insufficient IT controls over applications supporting financial reporting and a lack of qualified personnel in related roles, the filing shows.\n\nIntrator currently controls 2.4% of the company\u2019s Class A shares and almost half the company\u2019s Class B shares, giving him 38% of the shareholder voting power, according to the filing. Magnetar has 7.2% of the voting power, while Nvidia controls 1.2%.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia shares fall 9% on tariff fears",
            "link": "https://www.cnbc.com/2025/03/03/nvidia-shares-fall-9percent-on-tariff-fears.html",
            "snippet": "Nvidia shares fell nearly 9% on Monday after President Donald Trump confirmed that tariffs from Canada and Mexico will go into effect on Tuesday.",
            "score": 0.9697946310043335,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang gives a keynote address at CES 2025, an annual consumer electronics trade show, in Las Vegas, Nevada, on Jan. 6, 2025.\n\nNvidia shares fell nearly 9% on Monday after President Donald Trump confirmed that tariffs from Canada and Mexico will go into effect on Tuesday.\n\nThe chipmaker's shares retreated on a bad day for the market. The Dow, of which Nvidia is a component, tumbled 800 points, or 1.8%, and the Nasdaq Composite slid more than 3%.\n\nNvidia shares are now trading at the same price they were in September, before the U.S. presidential election. The company, having shed its $3 trillion market cap, is worth $2.79 trillion after Monday's slide knocked another $265 billion off Nvidia's valuation.\n\nNvidia is down over 13% since Wednesday, when the company reported earnings that topped analysts' estimates across the board. The company's revenue jumped 78% from a year earlier to $39.33 billion.\n\nDuring Nvidia's earnings report, analysts asked about the company's response to U.S. tariffs.\n\n\"Tariffs at this point, it's an unknown until we understand further what the U.S. government's plan is,\" Nvidia Chief Financial Officer Colette Kress told investors.\n\nNvidia's chips are mostly made in Taiwan, but some of its sophisticated systems and full computers surrounding the chips are manufactured in other regions, including Mexico and the U.S. They could be affected by Trump's 25% duties on imports from Mexico and Canada that go into effect Tuesday.\n\nSeparately, Nvidia was scrutinized on Monday for its exports to Singapore, which some analysts see as a waypoint to ship the company's chips to China and bypass the U.S. export controls. Late last week, Singapore officials detained three people for lying about where U.S.-manufactured servers would end up.\n\nNvidia officials did say they would manufacture chips in the new $100 billion expansion of Taiwan Semiconductor Manufacturing facilities in the U.S. that was announced by Trump on Monday.\n\nLast week, investors were eager to hear what the company's outlook was for continued AI growth from a handful of massive cloud companies, which comprise about half of Nvidia's data center revenue. CEO Jensen Huang said the company had ironed out issues with its latest chips, Blackwell.\n\n\"We're going to have a good quarter next quarter,\" Huang told CNBC last week. \"And we've got a fairly good pipeline of demand for Blackwell.\"",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia Stock Leads AI Selloff as Trump Tariffs Shake Wall Street",
            "link": "https://www.investopedia.com/nvidia-stock-leads-ai-selloff-as-trump-tariffs-shake-wall-street-markets-11689936",
            "snippet": "The AI trade continued to falter on Monday, with Nvidia stock slumping into bear territory. Equities nosedived after President Trump reiterated plans for a...",
            "score": 0.9073159694671631,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Shares of Nvidia fell more than 8% Monday, slipping into bear market territory.\n\nThe company's latest quarterly financial results raised questions about what was next for the stock\u2014and the AI trade\u2014and uncertainty driven by US trade policy has further concerned investors.\n\nAll told, markets are falling broadly, with few AI stocks spared Monday afternoon.\n\n\n\nThe AI trade continues to falter.\n\nNvidia (NVDA) stock slumped into bear market territory Monday as equities nosedived after President Trump confirmed plans to impose a 25% tariff on Canadian and Mexican imports starting tomorrow.\n\nShares of Nvidia lost 8.7% on Monday, extending a volatile stretch that followed last Wednesday\u2019s earnings report. With Monday\u2019s losses, Nvidia is trading more than 20% below its January all-time high.\n\nNvidia topped earnings estimates last week, but its meteoric rise and $3 trillion valuation left the chipmaker little room to disappoint. Narrowing profit margins spooked investors, prompting semiconductor and AI stocks to sell off last week.\n\nThe contagion continued on Monday, with fellow chipmaker Broadcom (AVGO), slated to report earnings on Thursday, following Nvidia stock lower. Shares of AI server maker Super Micro Computer (SMCI) fell 13%, and nuclear power providers Constellation Energy (CEG) and Vistra (VST), both lost more than 7%.\n\nVery few AI stocks were spared. AppLovin (APP), which soared more than 700% last year on AI-fueled revenue growth, rose 3.6%, bouncing back a bit from short-seller reports that tanked the stock last week.\n\nWall Street\u2019s bullishness has been tempered lately by uncertainty about the outlook for the U.S. economy and the business of AI. Inflation has appeared stickier than expected, and consumer confidence has declined amid concerns about the consequences of Trump\u2019s tariff policies.\n\nThe January release of Chinese start-up DeepSeek\u2019s R1 reasoning model, which its developers said could compete with the most advanced U.S. models at a fraction of the cost, injected fresh uncertainty into the AI trade. AI infrastructure stocks\u2014the chipmakers, server makers, and energy providers who have benefited from the build-out of AI data center capacity\u2014plummeted on concerns that DeepSeek\u2019s efficiency would undercut spending.\n\nSince then, cloud providers have stood by their plans to aggressively invest in AI. Those commitments, however, have failed to reignite an AI rally.\n\nUpdate\u2014Monday, March 3: This story has been updated with closing stock price data.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Nvidia Falls 9% As Trump\u2019s Tariffs Rout Stocks",
            "link": "https://www.forbes.com/sites/dereksaul/2025/03/03/nvidia-falls-9-as-trumps-tariffs-pledge-routs-stocks-on-monday/",
            "snippet": "Nvidia shares fell Monday, diving further into correction territory despite reporting stronger-than-expected headline earnings last Wednesday,...",
            "score": 0.9509982466697693,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Goldman Sachs sounds the alarm on Nvidia and the Magnificent 7",
            "link": "https://sherwood.news/markets/goldman-sachs-sounds-the-alarm-on-nvidia-and-the-magnificent-7/",
            "snippet": "Nasdaq 100 rallies are to be sold, according to strategists at Goldman Sachs....",
            "score": 0.7331879138946533,
            "sentiment": null,
            "probability": null,
            "content": "Nasdaq 100 rallies are to be sold, according to strategists at Goldman Sachs.\n\nEver since DeepSeek roiled markets, the AI trade vibes have oscillated between \u201cit\u2019s so over\u201d and \u201cwe\u2019re so back.\u201d\n\nPut Goldman Sachs firmly in the former camp amid another dreadful session in which Nvidia is down as much as 7%.\n\nIn a flurry of notes released late last week and over the weekend, strategists at the bank basically said that the party\u2019s over for the megacap US tech trade. To sum up their case:\n\nWhen a stock fails to respond to good news, that\u2019s bad news.\n\nThe earnings growth that made the Magnificent 7 so magnificent isn\u2019t as magnificent any more.\n\nHedge funds are dumping the cohort and other AI-linked positions.\n\nPopular stocks could see a lot more of a valuation reset lower.\n\nThe strategists, in their own words:\n\nPaolo Schiavone:\n\n\u201c The NVDA print was a clearing event \u2014 the reality is that from here the AI theme is for sale. In AI, investors are worried about 2026 growth not 25. Nasdaq 100 rallies will be used as liquidity events.\u201d\n\nTony Pasquariello:\n\n\u201c We all knew it was coming, but the immense earning premium that you had earned in US mega cap tech vs everything else is narrowing. DeepSeek triggered a shift in the flow [of] capital away from the US plays. In a few ways, NVDA earnings are an illustration of what\u2019s going on here: they didn\u2019t pull a hamstring as the cyclical impulse to spend on compute is still clearly intact, but price action told a certain story (i.e. -$320 billion of market cap in one day). Bigger picture, the stock has been range bound for the past eight months \u2014 coming off a 24,000% cumulative return in the prior ten years, if nothing else that\u2019s anti-climactic.\u201d\n\nJohn Flood:\n\n\u201c February\u2019s notional de-grossing in US TMT [technology, media, and telecom] is tracking to be the second largest on our record (behind January 2021 amid the meme stocks rally). Net exposure to Mag7 names has continued to fall and is now at the lowest level since April 2023, and aggregate long-short ratio across our US TMT AI basket constituents remains well below the highs seen around the middle of last year.\u201d\n\nSource: Goldman Sachs\n\nMark Wilson:",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Why Nvidia Stock Is Plummeting Today",
            "link": "https://www.nasdaq.com/articles/why-nvidia-stock-plummeting-today-1",
            "snippet": "Shares of Nvidia (NASDAQ: NVDA) are falling on Monday. The company's stock lost 4.5% as of noon ET, but was down as much as 5.3% earlier in the day.",
            "score": 0.9204849004745483,
            "sentiment": null,
            "probability": null,
            "content": "Shares of Nvidia (NASDAQ: NVDA) are falling on Monday. The company's stock lost 4.5% as of noon ET, but was down as much as 5.3% earlier in the day. The drop comes as the S&P 500 (SNPINDEX: ^GSPC) slipped 0.3% and the Nasdaq Composite (NASDAQINDEX: ^IXIC) lost 0.4%.\n\nCircumventing controls\n\nA report released Sunday by The Wall Street Journal revealed that despite strict U.S. export restrictions, Nvidia's latest chips are finding their way into China.\n\nStart Your Mornings Smarter! Wake up with Breakfast news in your inbox every market day. Sign Up For Free \u00bb\n\nSince 2022, the U.S. has sought to keep the best artificial intelligence (AI)-powering chips from entering the Chinese market. Nvidia, the undisputed market leader, is barred from selling the latest versions of its chips to Chinese companies.\n\nHowever, a gray market has emerged, allowing Chinese firms to purchase servers filled with Nvidia's latest Blackwell chips. The chips are sold to legitimate partners in the region and are then being routed into the Chinese companies, unbeknownst to Nvidia. The company said it would \"investigate every report of possible diversion and take appropriate action.\"\n\nMore action may be required\n\nThe Trump administration is currently weighing its options in escalating trade tensions with China. In contrast to Canada and Mexico, China did not make overt concessions after the initial round of tariffs were imposed earlier this year. It's entirely possible that the administration intends to raise tariffs already, but news that export controls on critical AI-enabling Nvidia chips are being circumvented could lead the administration to up the ante. Additional controls or tariffs could have a direct effect on Nvidia's bottom line.\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nEver feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia: if you invested $1,000 when we doubled down in 2009, you\u2019d have $304,161 !*\n\nif you invested $1,000 when we doubled down in 2009, !* Apple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $44,694 !*\n\nif you invested $1,000 when we doubled down in 2008, !* Netflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $534,395!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nContinue \u00bb\n\n*Stock Advisor returns as of March 3, 2025\n\nJohnny Rice has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Nvidia. The Motley Fool has a disclosure policy.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia Passes Its Latest Test. Here's What It Means for Investors.",
            "link": "https://www.fool.com/investing/2025/03/03/nvidia-passes-its-latest-test-heres-what-it-means/",
            "snippet": "All eyes were on Nvidia (NVDA -0.14%) once again when it reported fourth-quarter earnings last week. However, Nvidia's stock isn't in the same position as...",
            "score": 0.9063358902931213,
            "sentiment": null,
            "probability": null,
            "content": "All eyes were on Nvidia (NVDA 5.27%) once again when it reported fourth-quarter earnings last week. However, Nvidia's stock isn't in the same position as it was a year or two ago when it was delighting investors with skyrocketing growth and a seemingly endless opportunity in artificial intelligence (AI).\n\nThe company is still delivering blistering growth, but investor expectations now seem to be aligned with the company's trajectory, and there is less room for surprises. Despite ample anticipation for the fourth-quarter report after hours on Wednesday, the stock was trading down 3% this past Thursday even as the company beat estimates.\n\nNvidia didn't disappoint with its fourth-quarter results. Revenue jumped 78% from the quarter a year ago to $39.3 billion, which was ahead of the consensus at $38.2 billion. Growth was again paced by the data center segment, where AI computing is taking place, with data center revenue up 93% to $35.6 billion.\n\nThe gross margin fell from 76% to 73%, reflecting increased spending on the production ramp-up for the new Blackwell platform. However, it gained leverage in operating expenses, and earnings per share jumped 82% to $0.89, ahead of the consensus at $0.85.\n\nLooking ahead to the first quarter, the company sees revenue of around $43 billion, representing 9% growth on a sequential basis and 65% year over year.\n\nThe AI boom is priced in\n\nNvidia's earnings report, and the market's response, seemed to underscore that the surprise factor in its earnings report is now gone.\n\nWhile 78% revenue growth is phenomenal, investors have come to expect such growth from Nvidia as demand for its Blackwell components continues to outstrip supply. Now that expectations have adjusted to Nvidia's growth, and the massive opportunity in AI, it will be harder for Nvidia to deliver the kind of breakout performance we saw from the stock in 2023 and 2024.\n\nOn the earnings call, CEO Jensen Huang seemed to quell lingering concerns that DeepSeek's low-cost AI model would sap the company's growth as investors had feared when the new model triggered a massive one-day sell-off in the AI sector a month ago.\n\nDespite the DeepSeek innovation, AI computing needs are only growing. Huang noted that post-training demands more computing that pre-training. He also explained that the company has good visibility into AI expansions and its own demand as it works closely with its customers. He noted the large number of AI start-ups that continue to come online as well.\n\nIs Nvidia a buy?\n\nDespite the DeepSeek scare, Nvidia for now seems more likely to be range-bound in the coming months as the benefit of the massive AI infrastructure build-out is understood. At this point, it's unclear what could be the catalyst for Nvidia to take another leg up. The company might need to see a ramp-up in another area of AI such as autonomous vehicles or agentic AI, which would lift the stock.\n\nAI is still in its early stages as big tech companies race toward artificial general intelligence (AGI), and Nvidia seems likely to dominate AI computing infrastructure in the future as well. At one point, investors believed challengers like Intel and Advanced Micro Devices would take share in the GPU market, but Nvidia has fended them off with new advances in its Blackwell platform, and it's likely to maintain that leadership in data center GPUs down the road.\n\nCurrently, the stock trades at a price-to-earnings ratio of 43, which looks like a great price for a company growing revenue by 78%, though that growth rate is set to steadily slow. Over the long term, Nvidia still looks like a must-own stock. It's leading the AI revolution. Huang has proven himself as a visionary, and the company is likely to benefit from future technologies that aren't yet seen.\n\nHowever, over the near term, investors should temper their expectations as Nvidia's current growth trajectory is well understood. For the AI chip leader, 2025 could be about digesting the gains of the last two years, rather than making new ones.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia Stock Plummets. 5 Reasons Shares Are at Their Lowest Price Since September.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-ai-chips-china-64db8502",
            "snippet": "Nvidia stock faces continued concerns about potential restrictions on exports of its AI hardware amid reports that advanced chips are reaching China despite...",
            "score": 0.9560881853103638,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-03-02": {
        "0": {
            "title": "Chinese Buyers Are Ordering Nvidia\u2019s Newest AI Chips, Defying U.S. Curbs",
            "link": "https://www.wsj.com/tech/china-nvidia-blackwell-chips-ai-531fed0c",
            "snippet": "Traders offer servers containing the company's Blackwell chips by routing them through third parties in nearby regions.",
            "score": 0.9240824580192566,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "The dust settles on Nvidia: Morning Brief",
            "link": "https://finance.yahoo.com/news/the-dust-settles-on-nvidia-morning-brief-110036550.html",
            "snippet": "The opinions on Nvidia's stock will stay varied in the near term.",
            "score": 0.8334164023399353,
            "sentiment": null,
            "probability": null,
            "content": "This is The Takeaway from today's Morning Brief, which you can sign up to receive in your inbox every morning along with:\n\nThe chart of the day\n\nWhat we're watching\n\nWhat we're reading\n\nEconomic data releases and earnings\n\nThe dust has settled on the latest Nvidia (NVDA) earnings week frenzy.\n\nAnd I think it's important to take stock of where things stand for the world's most important stock (sorry, Apple (AAPL)). Why? Because you should be thinking about whether the pullback in Nvidia is a buying opportunity or the start of a deeper sell-off as expectations are reset.\n\nWe know Nvidia's margins in the first half of the year will be below their usual robust levels as Blackwell AI chips ramp up. I would argue the Street knew this ahead of the results, so they got flustered over nothing.\n\nOn Nvidia's earnings call, execs sought to push back on the bears, who have put forth a narrative that there will be a digestion period for AI investments by hyperscalers such as Amazon (AMZN) and that Nvidia's margins may have peaked.\n\n\"Once our Blackwell fully rounds, we can improve our cost and our gross margin,\" Nvidia CFO Colette Kress said. \"So, we expect to probably be in the mid-70s later this year.\"\n\nWe also know that, fundamentally, Nvidia's business is strong and likely to stay strong.\n\nFourth quarter revenue rose 12% sequentially and 78% from the prior year. Data center sales more than doubled from the prior year. Earnings handily beat analyst estimates.\n\n\"We're going to have to continue to scale as demand is quite high, and customers are anxious and impatient to get their Blackwell systems,\" Nvidia founder and CEO Jensen Huang said.\n\nListen: Nvidia could be unstoppable\n\nNowhere in the company's 2025 guidance or commentary from Huang did I sense that AMD (AMD) is taking Nvidia's market share; ditto custom chips from Amazon. I heard no hint that hyperscalers are sending AI chips back to Nvidia or have stopped fawning over Jensen to get more of these chips at any cost.\n\nPut together, I would argue what we heard from Nvidia in terms of demand and margins was all well known going into the results. So, the sell-off could prove to be an overreaction, a function of investors aiming to model out mixed first quarter guidance for the next two years.\n\nBut there are a couple of things we don't yet know about Nvidia that warrant greater attention. These play into the long-term bull thesis.\n\nA quote from Wedbush analyst Dan Ives says,\n\nFor starters, there's Huang's point about DeepSeek's R1 requiring 100x more compute resources compared to pre-training models due to inference time scaling. Look, most of us have no clue what this even means. But the casual observer could read it as the market has it strong on DeepSeek, and there could be a lot of upside to Nvidia estimates as DeepSeek and other reasoning models gain hold.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Continues to See Unstoppable Growth, but Is the Stock Still a Buy?",
            "link": "https://www.fool.com/investing/2025/03/02/nvidia-unstoppable-growth-stock-buy/",
            "snippet": "A glance at its fiscal 2025 fourth-quarter report shows Nvidia (NVDA 1.66%) once again showed remarkable growth. The demand for its graphics processing...",
            "score": 0.7858761548995972,
            "sentiment": null,
            "probability": null,
            "content": "A glance at its fiscal 2025 fourth-quarter report shows Nvidia (NVDA 5.27%) once again showed remarkable growth. The demand for its graphics processing units (GPUs) remains insatiable. The company's semiconductor chips have become the backbone of the artificial intelligence (AI) infrastructure build-out, which continues to show no signs of slowing down.\n\nDespite the strong report, the stock was unable to get a nice bounce and it trades down about 10.5% year to date. Still, the stock has been on a tremendous run the past five years, up an astounding 1,810% over the past five years, as of this writing.\n\nLet's take a closer look at Nvidia's results and see if the stock can regain its momentum.\n\nRapid growth continues\n\nFor the second consecutive year, Nvidia was able to more than double its revenue, which is an astonishing feat for a company its size. Revenue rose 114% to $130.5 billion in its fiscal 2025 (which ended Jan. 26) on top of 126% revenue growth the year before.\n\nFor fiscal Q4, revenue soared 78% year over year to $39.3 billion, once again powered by AI demand. Its adjusted earnings per share (EPS) climbed 71% to $0.89. Adjusted EPS grew slower than revenue as its gross margin slipped 300 basis points to 73%. Nonetheless, the results surpassed analyst expectations for adjusted EPS of $0.84 on revenue of $38.1 billion.\n\nIts data center business once again led the way, with revenue surging 93% year over year to $35.6 billion. The company said the growth was led by its H200 Hopper chip, while its next-generation Blackwell GPU architecture exceeded expectations with revenue of about $11 billion. While Nvidia is known for its chips' superior performance in training AI models, it said its inference demand was accelerating, while noting that Blackwell was designed for reasoning AI inference.\n\nMuch of Nvidia's revenue continues to come from large cloud computing providers, which made up about half of Nvidia's data center revenue in the quarter; however, it said regional cloud providers increased as a percentage of data center revenue. Meanwhile, its consumer internet revenue, which includes customers such as Meta Platforms and Elon Musk's xAI, tripled. Enterprise revenue doubled, with Nvidia saying that its platform is being adopted by organizations across industries. It said its solutions are being used for such things as autonomous driving, fraud detention, and drug discovery.\n\nNvidia's other segments were mixed. Gaming revenue sank 11% to $2.5 billion, as Q4 shipments were impacted by supply constraints. Professional visualization revenue increased 10% to $511 million, while automotive and robotics revenue more than doubled to $570 million.\n\nThe company continues to be a cash-flow machine, generating operating cash flow of $16.6 billion and free cash flow of $15.6 billion in the quarter. Nvidia ended the year with net cash and marketable securities of $43.2 billion and $8.5 billion in debt. It announced it will begin paying a modest $0.01 quarterly dividend next quarter.\n\nNvidia projected fiscal Q1 revenue to be around $43 billion, which would represent growth of approximately 65% year over year. The growth will be led by its new Blackwell GPU architecture. However, it is looking for its gross margin to come in at 70.6%, down from 78.4% a year ago. It expects the gross margin to improve throughout the year as Blackwell scales and efficiencies are gained, reaching the mid-70% range.\n\nMeanwhile, the company is looking to launch its new Ultra GPU architecture in the second half of 2025.\n\nIs Nvidia stock a buy?\n\nNvidia's stock remains inexpensive given its growth, with a forward price-to-earnings (P/E) ratio of just over 28 times this year's analyst estimates and a price/earnings-to-growth (PEG) ratio of under 0.5. PEG ratios below 1 typically indicate a stock is undervalued.\n\nMeanwhile, Nvidia continues to be the biggest winner of AI infrastructure growth. Companies continue to pour money into AI infrastructure as evidenced by the announced capital expenditure (capex) budgets from the big three cloud computing companies (over $250 million combined) as well as Meta Platforms, which is looking to spend up to $65 billion in AI-related capex. In addition, a consortium of companies led by Japan's SoftBank and OpenAI are set to spend $500 billion on AI infrastructure in the next few years in the U.S. as part of Project Stargate.\n\nThat's a lot of AI infrastructure spending that Nvidia is set to benefit from this year. That said, Nvidia doesn't have a recurring-revenue business model like a software company and the semiconductor business can be cyclical. Where AI infrastructure spending trends over the next five to 10 years will go a long way in determining whether the stock is a good long-term investment.\n\nRight now, though, Nvidia looks to be in a very good position for this year while trading at an attractive valuation, making it a buy at current levels.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia's post-earnings slide highlights key risk",
            "link": "https://www.thestreet.com/investing/nvidias-post-earnings-slide-highlights-key-risk",
            "snippet": "Nvidia shares have lost more than $220 billion in value since its fourth quarter earnings report on Wednesday.",
            "score": 0.9424944519996643,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "NVIDIA\u2019s Latest Driver Update Is Apparently Unable To Resolve \u201cBlack Screen\u201d Issues For Many Users; MFG-Supported Titles Are Now Crashing",
            "link": "https://wccftech.com/nvidia-latest-driver-update-is-apparently-unable-to-resolve-black-screen-issues-for-many-users/",
            "snippet": "NVIDIA's latest driver update, which was released to fix the black screen issue with RTX 50 GPUs, has not worked out for many users.",
            "score": 0.9581897258758545,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's latest driver update, which was released to fix the \"troublesome\" black screen issue with RTX 50 GPUs, has apparently not worked out for many users and even has made the problem worse for some.\n\nNVIDIA's Driver Update Seems To Cause More Trouble For Gamers, As It Doesn't Sort Out Crashing Issue, Rather Makes It Worse\n\nWell, it seems like Team Green isn't having a great time with its RTX 50 Blackwell GPU launch since the troubles around it just started to keep growing. Out of all the issues, a major one was users experiencing crashes and black screen issues while gaming, and to address this, NVIDIA recently released their newest Game Ready 572.60 driver, which was said to sort out problems with the DisplayPort connection, along with the BIOS as well, however, according to @mpr_reviews, the update has made the issue even more troublesome, since games support Multi-Frame Generation (MFG) are now crashing out.\n\nUsing the latest Nvidia driver 572.60 causes every game that supports multi frame generation to black screen crash and restart my PC on the RTX 5080 when MFG 3x or 4x is used. Either at game startup or when exiting the game. The latest 572.65 hotfix driver also exhibits the same\u2026 \u2014 Mostly Positive Reviews (@mpr_reviews) March 2, 2025\n\nIt is claimed that all titles supporting MFG are now crashing out for the user, and even after applying the 572.65 \"hotfix,\" the issue seems to be persistent. However, the black-screen isn't just limited to MFG titles; it is more frequent with them, suggesting that NVIDIA's newest driver hasn't worked out for users out there. Since the update was released a few days ago, we have not seen masses reports on crashing issues. However, the initial reports indeed indicate that something is wrong. The problem could lie with how MFG works for RTX 50 Blackwell GPUs, but we are not sure for now.\n\nIt happened to me today playing Death Stranding on my 5080 with DLSS and frame gen turned off, it was weird. It didn't happen in any driver and I thought it could be an overheating, but I was monitoring the temps and both CPU and GPU was below 65. Gonna rollback the driver. \u2014 Timebringer |: \ud83c\udfaeExpedition 33 waiting room (@Timebringer) March 2, 2025\n\nFor users still experiencing crashing issues after NVIDIA's latest driver update, we urge them to let us know about the problem so that we can contact Team Green for a response. Many users, including @mpr_reviews, are rolling back the driver update, since, according to them, with the previous version, the crashes were less frequent. The problem is still an evolving issue, hence we will wait for more incidents to surface before we can conclude on the actual reason.\n\nWith AMD's RX 9070 series launch right around the corner, it is certainly disappointing to see NVIDIA's RTX 50 GPUs showing a \"sloppy\" user experience. This will ultimately give Team Red an edge in the mainstream GPU market unless NVIDIA doesn't resort to the problems.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Arrcus Announces Telco Grade AI Ethernet Switch (TGAX) Based on NVIDIA Spectrum Platform",
            "link": "https://www.businesswire.com/news/home/20250302420646/en/Arrcus-Announces-Telco-Grade-AI-Ethernet-Switch-TGAX-Based-on-NVIDIA-Spectrum-Platform",
            "snippet": "Arrcus today announced TGAX - a new Telco Grade Network Switch targeting telecom service providers globally, empowering Network Operators to unlock ne.",
            "score": 0.81333327293396,
            "sentiment": null,
            "probability": null,
            "content": "BARCELONA, Spain--(BUSINESS WIRE)--Arrcus today announced TGAX - a new Telco Grade Network Switch targeting telecom service providers globally, empowering Network Operators to unlock new networking and AI revenue streams. Building on networking services like multicloud networking, VPN and datacenter interconnect services, telcos can seamlessly add AI capabilities and then converge AI and RAN, all on the same TGAX infrastructure.\n\nThe TGAX switch is based on the NVIDIA Spectrum-4 high performance ethernet switch and ArcOS NOS, the foundation of Arrcus\u2019 ACE networking software stack. TGAX is highly programmable and offers flexibility and agility in deploying new services - Telcos can begin immediate monetization with the introduction of new networking services for distributed AI workloads, leveraging existing in-house networking expertise. This same platform can then be leveraged to add new AI services as well.\n\nKey revenue generating services include:\n\nVPN and datacenter interconnect services\n\nMulti-Cloud Networking (MCN)\n\nConverged AI and RAN, unlocking new revenue streams through AI-driven network services and optimized RAN performance\n\nTGAX is a key component of the broader AI-RAN architecture, for converging AI and RAN workloads on a common shared accelerated infrastructure and deploying AI and Telco applications from the edge to core. As part of this solution, TGAX enables seamless scalability from a few nodes to thousands, from cell sites to large data centers, allowing Network Operators to deploy AI and other Telco applications including RAN wherever they are needed: at the edge, in the core or in the cloud. By leveraging the same hardware and software platform across multiple deployment scenarios, Network Operators can reduce operational complexity and total cost of ownership.\n\n\u201cThe TGAX solution represents the next generation of flexible, future-proof platforms for Telcos, enabling the delivery of new revenue streams while driving operational efficiencies in a phased approach,\u201d said Shekar Ayyar, Chairman, and CEO of Arrcus. \u201cWe are thrilled to work with NVIDIA to deliver this best-in-class AI-ready switch for Telcos.\u201d\n\n\u201cBuilding a distributed AI infrastructure that can be upgraded to AI-RAN is a strategic imperative for network operators for enabling new AI services and maximizing RAN infrastructure utilization,\u201d said Soma Velayutham, Vice President, Telecoms at NVIDIA. \u201cArrcus TGAX combines the best of AI-ready hardware and flexible telco grade software features to deliver this vision for telcos worldwide.\u201d\n\nKey capabilities of TGAX include:\n\nSRv6-based Traffic engineering innovation in Arrcus ACE platform to deliver low-latency, high bandwidth with prioritization for real-time AI applications at the edge\n\nCloud bandwidth cost management through Egress Cost Control\n\nAdvanced Quality of Service (QoS) mechanisms to prioritize and manage traffic\n\nOptimized performance through NVIDIA's AI-optimized hardware and software\n\nTGAX Telco-Grade AI Ethernet switch brings together best-in-class switching hardware and switching software for Telco and AI-RAN applications. This combination enables unparalleled performance, scalability, and reliability, making TGAX the ideal solution for Network Operators seeking to unlock new revenue streams through AI adoption.\n\nThe solution will be showcased at Mobile World Congress 2025 in Barcelona in Arrcus\u2019 Hall 2 booth (2D41). Interested visitors can submit a request here.\n\nAdditional Resources\n\nAbout Arrcus\n\nArrcus is a leading provider of networking software solutions that empower businesses to achieve unparalleled scalability, performance, and reliability in their infrastructure. Arrcus is disrupting the industry with disaggregated solutions that deliver innovative, agile, and cost-effective networking, allowing enterprises to break free from traditional, monolithic systems and embrace a more flexible, efficient, and scalable approach to modern networking. The Arrcus team consists of world-class technologists who have an unparalleled record in shipping industry-leading networking products, complemented by industry thought leaders, operating executives, strategic partners, and top-tier VCs. The company is headquartered in San Jose, Calif. For more information, go to www.arrcus.com or follow Arrcus on LinkedIn and Twitter/X.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "If You\u2019d Invested One Stimulus Check in Nvidia During the Pandemic, Here\u2019s How Much Money You\u2019d Have Now",
            "link": "https://www.nasdaq.com/articles/if-youd-invested-one-stimulus-check-nvidia-during-pandemic-heres-how-much-money-youd-have",
            "snippet": "In 2020 and early 2021, three rounds of stimulus checks went out to American families to help cover daily expenses while the world was locked down.",
            "score": 0.9208521842956543,
            "sentiment": null,
            "probability": null,
            "content": "In 2020 and early 2021, three rounds of stimulus checks went out to American families to help cover daily expenses while the world was locked down. Between furloughs and rising prices of everyday goods, stimulus checks helped households get through the worst part of the COVID-19 pandemic.\n\nRead Next: I\u2019m a Self-Made Millionaire: 5 Stocks You Shouldn\u2019t Sell\n\nCheck Out: 5 Subtly Genius Moves All Wealthy People Make With Their Money\n\nBut if you were in the fortunate position to be able to save your stimulus checks instead of spending the money, you could have invested that money in the stock market and made a pretty penny.\n\nIf you happened to invest your stimulus checks in Nvidia at the time, you\u2019d have quite a bit of money right now.\n\nNvidia Stock Price in 2020\n\nIn early 2020, Nvidia stock was hovering around $6 per share, split-adjusted. This stock had been on the rise after losing half its value during 2018 and 2019 due to rising interest rates.\n\nBut in March of 2020, the pandemic locked down the U.S. \u2014 and the world. Nvidia stock dropped down below $5 per share before starting its rapid rise as stimulus checks started to get sent out.\n\nTry This: How To Get a 10% Return on Investment (ROI): 10 Proven Ways\n\nInvesting the First Stimulus Check in Nvidia Stock\n\nThe first round of stimulus checks were paid to each individual taxpayer and their families. Here\u2019s how much you\u2019d receive based on your family size:\n\nSingle: $1,200\n\n$1,200 Married: $2,400\n\n$2,400 Married with one kid: $2,900\n\n$2,900 Married with two kids: $3,400\n\n$3,400 Married with three kids: $3,900\n\n$3,900 Married with four kids: $4,300\n\nIf you received your stimulus check in March of 2020 and decided to put it all in Nvidia stock, you would have quickly started to make a return on your investment. After just a few months, Nvidia stock rose from around $5 per share to over $9 per share.\n\nAfter one year, Nvidia stock was worth around $12 per share. Fast-forward 2 years from the first stimulus checks were issued, Nvidia stock was worth around $22 per share. This is nearly four times the price when you invested in March 2020.\n\nBut fast-forward to today, and Nvidia stock has skyrocketed to around $120 per share! This is 24 times the price you would have paid in March of 2020!\n\nHere\u2019s how much you\u2019d have if you invested your whole first stimulus check into Nvidia stock in March 2020 at $5 per share:\n\nHousehold Stimulus Check Amount Nvidia Stock Value Today Single $1,200 $28,800 Married $2,400 $57,600 Married with 1 kid $2,900 $69,600 Married with 2 kids $3,400 $81,600 Married with 3 kids $3,900 $93,600 Married with 4 kids $4,300 $103,200\n\nIf you just received a $1,200 stimulus check and dumped the entire thing into Nvidia stock, you\u2019d have over $28,000 today.\n\nBut if you\u2019re married with a few kids and invested $3,400 into Nvidia stock, you\u2019d have over $90,000 today! This is an incredible return on investment in just five years.\n\nInvesting the Second Stimulus Check in Nvidia Stock\n\nThe second round of stimulus checks were paid out in December of 2020. Here\u2019s how much you\u2019d receive based on your family size:\n\nSingle: $600\n\n$600 Married: $1,200\n\n$1,200 Married with one kid: $1,800\n\n$1,800 Married with two kids: $2,400\n\n$2,400 Married with three kids: $3,000\n\n$3,000 Married with four kids: $3,600\n\nIf you received your stimulus check in December of 2020 and decided to put it all in Nvidia stock, you would have bought it for around $13 per share.\n\nAfter one year, Nvidia stock was worth around $28 per share. Another year after that, Nvidia stock was worth around $15 per share, due to rising interest rates. This is still higher than when you bought it in December 2020.\n\nIf you held those stocks until today, you would have nearly 10 times return on your investment.\n\nHere\u2019s how much you\u2019d have if you invested your whole second stimulus check into Nvidia stock in December 2020 at $13 per share:\n\nHousehold Stimulus Check Amount Nvidia Stock Value Today Single $600 $5,538 Married $1,200 $11,076 Married with 1 kid $1,800 $16,614 Married with 2 kids $2,400 $22,152 Married with 3 kids $3,000 $27,690 Married with 4 kids $3,600 $33,228\n\nIf you just received a $600 stimulus check and invested it in Nvidia stock, you\u2019d still have over $5,000 today. Not bad!\n\nBut if you\u2019re married with four kids and invested $3,600 into Nvidia stock, you\u2019d have over $33,000 today!\n\nInvesting the Third Stimulus Check in Nvidia Stock\n\nThe third round of stimulus checks were the largest, and were sent out in March 2021. Here\u2019s how much you\u2019d receive based on your family size:\n\nSingle: $1,400\n\n$1,400 Married: $2,800\n\n$2,800 Married with one kid: $4,200\n\n$4,200 Married with two kids: $5,600\n\n$5,600 Married with three kids: $7,000\n\n$7,000 Married with four kids: $8,400\n\nIf you invested your entire stimulus check into Nvidia stock in March 2021, you would have bought Nvidia shares for around $13 per share \u2014 the same price as when the second stimulus check came out.\n\nIf you held onto that investment, Nvidia stock has risen to nearly $120 per share \u2014 over over 9 times your original investment!\n\nHere\u2019s how much you\u2019d have if you invested your whole third stimulus check into Nvidia stock in March 2021 at $13 per share:\n\nHousehold Stimulus Check Amount Nvidia Stock Value Today Single $1,400 $12,922 Married $2,800 $25,844 Married with 1 kid $4,200 $38,766 Married with 2 kids $5,600 $51,688 Married with 3 kids $7,000 $64,610 Married with 4 kids $8,400 $77,532\n\nIf you received a $1,400 stimulus check and put all of it into Nvidia stock, you\u2019d have nearly $13,000 today.\n\nBut if you\u2019re married with three kids and invested $7,000 into Nvidia stock in March 2021, you\u2019d have nearly $65,000 today!\n\nInvesting ALL Stimulus Checks Into Nvidia Stock\n\nIf you decided to go all in with your stimulus checks and invest every penny of all three stimulus checks into Nvidia stock the moment you received the money, here\u2019s how much you\u2019d have today:\n\nHousehold Stimulus Check Amount Nvidia Stock Value Today Single $3,200 $47,260 Married $6,400 $94,520 Married with 1 kid $8,900 $124,980 Married with 2 kids $11,400 $155,440 Married with 3 kids $13,900 $185,900 Married with 4 kids $16,300 $213,960\n\nThe numbers are crazy! If you received the minimum amount of stimulus checks \u2014 $3,200 \u2014 and put it all into Nvidia immediately, you\u2019d have over $47,000 today.\n\nBut if you had a big family with four kids and put all your stimulus money into Nvidia stock, you\u2019d have over $210,000 today!\n\nEditor\u2019s note: Stock prices were sourced from TradingView and are accurate as of Feb. 28, 2025.\n\nMore From GOBankingRates\n\nThis article originally appeared on GOBankingRates.com: If You\u2019d Invested One Stimulus Check in Nvidia During the Pandemic, Here\u2019s How Much Money You\u2019d Have Now\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Defective RTX 5080 takes up to 11% performance hit in gaming \u2014 Larger impact at higher resolutions",
            "link": "https://www.tomshardware.com/pc-components/gpus/defective-rtx-5080-takes-up-to-11-percent-performance-hit-in-gaming-larger-impact-at-higher-resolutions",
            "snippet": "We finally have some reputable data about the expected performance penalty for Nvidia's defective RTX 50 series GPUs, thanks to Gamers Nexus.",
            "score": 0.8649339079856873,
            "sentiment": null,
            "probability": null,
            "content": "We finally have some reputable data about the expected performance penalty for Nvidia's defective RTX 50 series GPUs, thanks to Gamers Nexus. The YouTube channel's Steve Burke put up a bounty for Blackwell cards with missing ROPs. Burke offered a $500 bonus on top of the price on the receipt while covering both shipping and taxes. He successfully managed to procure a defective RTX 5080 Founders Edition in exchange for a Zotac equivalent with all its ROPs operational.\n\nInvestigating NVIDIA\u2019s Defective GPUs: RTX 5080 Missing ROPs Benchmarks - YouTube Watch On\n\nIn its original statement, Nvidia confirmed that only 0.5% of produced RTX 5090s and RTX 5070 Tis had been affected by the missing ROPs issue. This assertion would be short-lived, as just two days later, we found an RTX 5080 with a similar defect; it had 104 ROPs instead of the advertised 112. In synthetics, based on user-testing, the nerfed RTX 5080 scored 12% lower than what you'd normally expect. TechPowerUp's results were the only estimate of the performance delta in gaming benchmarks before these Gamers Nexus tests. The RTX 5070 should not be subject to similar problems. Even if defective chips were produced, let's hope AIBs and Nvidia can manage to keep them out of consumer hands.\n\nNvidia reported an on-average 4% difference in graphical performance if a GPU had missing ROPs. In the handful of games Burke tested, simple bad luck could land you up-to 11% behind other RTX 5080 users, as shown in Total War: Warhammer 3 at 4K. Moving over to the 1440p results, the gap closes somewhat as most games are within 3-4%, with a maximum variation of 8.8% in Dying Light 2 Stay Human. The data indicates that higher resolutions are generally more taxing on the ROP units.\n\nWhen compared against other GPUs in the same ballpark, in the worst-case scenario, the defective RTX 5080 falls to RTX 5070 Ti levels, which invalidates the $250 price gap between the two. Of course, these results are game-dependent, so these outcomes aren't universal.\n\nThe best resolution from Nvidia is to create a driver-level identifier to alarm users if their RTX 50 GPU is missing ROP units. This is because the average consumer likely doesn't use third-party hardware monitoring utilities like GPU-Z and HWiNFO. Even if partners are offering replacements and refunds, supply constraints could keep you waiting for days, if not weeks for a new graphics card.\n\nOn the bright side, faulty RTX 5080s, before PCB assembly, could be repurposed by Nvidia (through software) as RTX 5070 Tis, in the future. A BIOS reflash could potentially restore these special units to RTX 5080 specs, unless Nvidia scraps these dies.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Singapore probes suspected fraud in sales of US-controlled Nvidia chips",
            "link": "https://www.ft.com/content/d940249a-29a8-4d0e-b324-a5e65c1ed1a9",
            "snippet": "Singapore has charged three men in a case suspected to involve Nvidia semiconductor sales that potentially breach US export controls, the government...",
            "score": 0.9248955249786377,
            "sentiment": null,
            "probability": null,
            "content": "Try unlimited access Only Skr10 for 4 weeks\n\nThen Skr739 per month. Complete digital access to quality FT journalism on any device. Cancel anytime during your trial.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia's Main Threat Isn't DeepSeek But Something Else",
            "link": "https://finance.yahoo.com/news/nvidias-main-threat-isnt-deepseek-145553080.html",
            "snippet": "Investment Thesis. Nvidia Corporation(NASDAQ: NVDA), the Santa Clara-based chip maker that became the third-ever American company to hit the $3 trillion...",
            "score": 0.8508934378623962,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has a staggering TTM EBIT margin of nearly 62%. However, such incredible margin figures come at a hefty price for enterprises and start-ups. Any company intending to buy H100 GPU should expect to spend 30 grand a piece upfront. And to lease those GPUs, the total bill comes at almost 48 grand a year for a single piece for 24/7, 365 days. Such astronomic ballpark figures may be trivial for startups with piles of cash at their disposal. However, companies who do not have such a privilege but want to incorporate state-of-the-art generative AI in their products and services have begun a simmering search for alternative options. Additionally, a material design flaw in B200 had caused major disruptions in the supply chain. Hence, diversifying chip suppliers would mitigate such supply chain interruptions and also help enterprises bring the costs down. AMD stands as the most credible alternative to Nvidia, having spent more than two decades competing against its consumer-grade GPU business. AMD's latest device for AI, the Instinct MI325X, was launched in October 2024 and has started shipping to customers, ahead of Nvidia's delayed B100 and B200 GPUs. AMD is seemingly adopting a strategy that has worked before while it cannot match Nvidia's products in terms of absolute performance, it aims to offer a more attractive price-to-performance ratio. Unlike Nvidia, AMD does not dabble in server and rack designs or mandate specific levels of rack power its reference designs call for anything from one to sixteen compute nodes per rack. The MI325X became the fastest-ramping product in AMD's history, adopted by customers including OpenAI, Meta, Microsoft, and Google. Its successor, the Instinct MI350, is expected to ship in the second half of 2025.Furthermore, Hyperscalers have already produced several generations of AI silicon for both training and inference, positioning their chips as low-cost alternatives to GPUs. Notable examples include Trainium2 from AWS, Trillium from Google Cloud, and Maia 100 from Microsoft Azure all launched in 2024. Furthermore, Beatriz Valle of Global Data has noted in her report that numerous startups including Cerebras, Groq, Mythic, Graphcore, Cambricon, and Horizon Robotics are focused on creating custom AI chips that operate faster, consume less power, and can be optimized for training neural nets and making inferences more effectively. For instance, Cerebras, a US-based hardware design startup, provides an example of a radically different, non-GPU architecture for AI. Its wafer-scale engine is an integrated processor that includes compute, memory, and interconnect fabric on a single piece of silicon that measures eight by nine inches. The third generation of the platform set a world record for inference performance in November 2024 while consuming just 15 kW per system. Cerebras' customers include four national laboratories in the US and several pharmaceutical companies.\n\nNvidia Corporation(NASDAQ: NVDA), the Santa Clara-based chip maker that became the third-ever American company to hit the $3 trillion mark now faces some rather grave questions as the dynamics of AI integration evolve. With AI applications beginning to proliferate, former Principal Engineer of IBM Frank Palermo expects the future of AI to revolve around inference-oriented workloads. This casts doubts over Nvidia's high-octane, power-hungry GPUs, as inference generally does not require such GPUs. Additionally, the development of cost-effective options better suited for inference workloads by hyperscalers and startups has picked up momentum lately.\n\nStory Continues\n\nNvidia's Main Threat Isn't DeepSeek But Something Else\n\nArtificial Analysis\n\nThe Paradigm of Inference Management\n\nNvidia's Main Threat Isn't DeepSeek But Something Else\n\nNVIDIA\n\nThe difference between inference and training is explained in simple words by Donald Farmer, First, in the training phase, the model looks at an existing data set to discover patterns and relationships within it. Next, in the inference phase, the trained model applies these learned patterns to create predictions, generate content, or make decisions when it encounters new, previously unseen data.The future battleground for any aspiring AI chip designer is inference. While training AI models can be compared to software development an expensive process requiring specialist skills and infrastructure inference resembles software delivery, requiring little skill and priced at a fraction of the development cost. Inference is going to be the largest component of the AI workload landscape as per Adam Clark of Barron's.Lower-performance, less power-hungry GPUs are growing in demand as does not require cutting-edge manufacturing, making them more affordable. Furthermore, startups including Groq, Enflame, SambaNova, Untether, Blaize, FuriosaAI, Recogni, Etched, and many others have developed low-cost products for inference. Additionally, both Intel and AMD have enhanced their processors to support the delivery of inference without requiring additional hardware, hence catering server CPUs to enterprises with lower budget and power needs. This essentially paints a rather dim and gloom picture of Nvidia's growth prospects, something that would worry Nvidia's shareholders\n\nValuation\n\nNvidia's Main Threat Isn't DeepSeek But Something Else\n\nSeeking Alpha\n\nIn the Valuation factor, Seeking Alpha has graded Nvidia's stock an F, the lowest possible grade. This signifies how overvalued Nvidia's stock is fundamentally and has little to offer to portfolio managers in terms of valuations. However, as a minor sign of relief, the forward PEG of Nvidia is currently at almost 33% discount compared to the sector median, highlighting the formidable future growth expectations investors have in Nvidia albeit the forward PEG ratio is 1.22, above the border mark of 1, which is usually a signal of an overvalued stock.\n\nInvestment Upsides\n\nGB300 GPUs\n\nTaiwanese media outlet Economic Daily News leaked in December 2024 that NVIDIA is expected to unveil its new version of GPU GB300 at the GTC (GPU Technology Conference) in March 2025. SemiAnalysis, the data center-focused news outlet has reported the new line of GB300 to have 200W additional power with TDP going to 1.4KW from 1.2KW for GB200. The B300 GPU is a brand-new tape out on the TSMC 4NP process node, i.e. it is a tweaked design, for the compute die. This enables the GPU to deliver 50% higher FLOPS versus the B200 on the product level. Some of this performance gain will come from 200W additional power with TDP going to 1.4KW and 1.2KW for the GB300 and B300 HGX respectively (compared to 1.2KW and 1KW for GB200 and B200). GB300 will once again prove Nvidia's might in delivering GPUs with surreal computational power and speed. However, the question remains whether enterprises would be able to afford it and perhaps a bigger question is whether enterprises need such high power-hungry GPUs.\n\nNvidia's Main Threat Isn't DeepSeek But Something Else\n\nNVIDIA\n\nRTX 50 Series GPUs\n\nThe RTX 50 Series launched at the CES 2025 was the biggest highlight of the entire event. RTX 50 series is the most updated generation of Nvidia's consumer GPUs which is built on the architecture of Blackwell. Blackwell architecture harnesses TSMC's custom 4NP process node and achieves 30% more power efficiency over its predecessor. represents Nvidia's latest generation of consumer GPUs, built on Blackwell architecture, and was a highlight at CES 2025, held from January 7-10, 2025. Models like the RTX 5090, 5080, 5070 Ti, and 5070, have already been launched and RTX 5060 is expected to be launched in the latter half of 2025.\n\nNvidia's Main Threat Isn't DeepSeek But Something Else\n\nNVIDIA\n\nRTX 50 series represent an incredible opportunity for Nvidia in the gaming industry. The advent of DLSS 4 with Multi Frame Generation can generate up to three frames per rendered frame, and this ramps up performance by eight times. The boost in performance in gaming, content creation activities like video editing even at a grand scale would help Nvidia to diversify its revenue channels. The market intelligence firm Mordor forecasts the gaming market to grow at a CAGR of 10.17% in the next five years through 2030. Additionally, Boston Consulting Group in their report on the gaming industry had highlighted the fact game budget across board\n\nNvidia's Main Threat Isn't DeepSeek But Something Else\n\nAdditionally, Boston Consulting Group in their report on the gaming industry highlighted the fact that game development budget across board is projected to outpace gaming revenue by 5% through 2028. Either of these projections depict that the gaming industry braced itself for rapid expansion in the next few years and Nvidia would benefit the most from it.\n\nNvidia's Main Threat Isn't DeepSeek But Something Else\n\nBCG\n\nPortfolio Management\n\nAlthough there is not the slightest doubt about Nvidia's ability to come up with GPUs with record-level power and speed, based on the simmering shift in the AI workload landscape, where a focus on cheaper and less power-consuming alternatives is paramount for nascent enterprises, along with an F grade for valuation factor, Nvidia's stock is believed to have little to offer to portfolio managers who tend to prioritize economic moat and undervaluation in their investment decisions.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-03-01": {
        "0": {
            "title": "Nvidia: A complete guide to the hardware company behind the AI boom and its chips, financials, and leadership",
            "link": "https://www.businessinsider.com/nvidia",
            "snippet": "Nvidia is leading the AI boom with its GPUs and revolutionary new H100 chips. Read about the company's history, leadership, and financials.",
            "score": 0.9330199360847473,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia is one of the most valuable companies in the world.\n\nThe chipmaker is leading the AI boom, and its stock price surged by more than 800% in two years.\n\nCEO Jensen Huang's leadership and innovative products are part of Nvidia's success.\n\nNvidia has been around for over three decades, but the chipmaker became a household name only in the past couple of years.\n\nNvidia was founded in 1993 by Jensen Huang, Chris Malachowsky, and Curtis Priem \"with a vision to bring 3D graphics to the gaming and multimedia markets.\"\n\nThe boom in AI technology has made it one of the most valuable companies in the world as companies scramble to buy its graphics processing units. Here's what you need to know about Nvidia.\n\nHistory\n\nNvidia's origin story began at a Denny's during a meeting between Huang \u2014 who once worked for the chain \u2014 Malachowsky and Priem.\n\nPersonal computing was on the cusp of taking off, and the trio sought a way to capitalize on it. Huang said in a 2010 interview with Stanford University's engineering school that they \"wondered whether starting a graphics company would be a good idea.\"\n\n\"We brainstormed and fantasized about what kind of company it would be and the world we could help,\" he said. \"It was fun.\"\n\nTheir goal was to improve the experience of gaming on a PC.\n\nBy 1999, the year it went public, Nvidia had invented the graphics processing unit, a computer chip that can handle many tasks simultaneously. It had faced challenges; it built two chips that failed, and it nearly went bankrupt. But it had also established itself as a formidable player in the industry, partnering with companies such as Sega, Dell, and Micron and winning rounds of funding from the venture firms Sequoia Capital and Sierra Ventures.\n\nIn 2006, it released CUDA, a general-purpose programming interface that would expand its business far beyond gaming.\n\nOn Sequoia Capital's \"Crucible Moments\" podcast, Andrew Ng, a Stanford professor who founded Google Brain, recalled his students telling him, \"Hey, Andrew, there's this thing called CUDA \u2014 not that easy to program, but it's letting people use GPUs for something different.\"\n\n\"We started to see 10x or even 100x speedups training neural networks on GPUs because we could do 1,000 or 10,000 things in parallel rather than one step after another,\" he added.\n\nNvidia's GPUs were used to train AlexNet, an image classification system unveiled in 2012 that significantly influenced the field of deep learning.\n\nThe launch of ChatGPT in late 2022 ushered Nvidia into a new era. The chipmaker's shares surged by more than 800% from the start of 2023 to the start of 2025.\n\nMuch of that growth came from the success of Nvidia's H100 chip, which it released in March 2022. The $40,000 chip, named for the computer scientist Grace Hopper, has played a crucial role in providing the computing power for large language models.\n\nRelated stories\n\nLeadership\n\nNvidia's success may be best personified by its CEO: Jensen Huang.\n\nA 61-year-old bona fide tech mogul, Huang has a net worth of about $110 billion. While some execs sport chains or Patagonia vests, Huang is often spotted in a leather jacket. BI identified at least six versions he's worn over the years, including a nearly $9,000 lizard-embossed coat from Tom Ford he wore at the company's global AI conference, GTC, in 2024. He commemorated Nvidia's stock price hitting $100 with a tattoo of Nvidia's logo on his arm.\n\nJensen Huang in one of his many leather jackets. MOHD RASFAN\n\nHuang's early years were tumultuous. He was born in Taiwan, and he spent time there and in Thailand before his parents sent him to the United States because of social unrest in the region.\n\nHe attended a reform school in Kentucky. In an interview with NPR in 2012, he recalled that \"the kids were really tough,\" adding: \"They all had pocket knives \u2014 and when they get in fights, it's not pretty. Kids get hurt.\" He later moved to Oregon, where he was reunited with his parents. In high school, he became a nationally ranked table tennis champion.\n\nHuang graduated from Oregon State University with a degree in electrical engineering in 1984.\n\nDuring his freshman year, Huang met Lori Mills, his future wife. In an interview at the Hong Kong University of Science and Technology, he said he won her over by offering to help her with her homework. They married five years after meeting and now have two children: Madison Huang, a marketing director at Nvidia, and Spencer Huang, a senior product manager at the company.\n\nJensen Huang later earned a master's in electrical engineering from Stanford and he worked at the chip companies LSI Logic and Advanced Micro Devices before launching Nvidia.\n\nHuang sold about 1.3 million shares of Nvidia when the company hit a $3 trillion market cap in June 2024, but he retains a more than 3% stake in the company.\n\nProducts\n\nNvidia's next-generation Blackwell chips have attracted customers, including SoftBank, Amazon Web Services, and Microsoft. Nvidia\n\nNvidia's business is built around GPUs, which can handle tasks simultaneously, as opposed to central processing units, or CPUs, which are in standard computers.\n\nNvidia's GPUs have become a mainstay of the AI revolution because they provide the computing power needed to run massive large language models like OpenAI's GPT-4 and Meta's Llama 3.\n\nDemand for Nvidia's H100 chips, built on its Hopper architecture, has been so high in late 2023 and 2024 that tech execs like Mark Zuckerberg and Elon Musk have bragged about how many units they're training new technology on. ByteDance has found workarounds to US export bans on the chips to China. Saudi Arabia and the United Arab Emirates have bought up thousands of units to fuel their AI ambitions, while venture capitalists have bought Nvidia GPUs as backup units for their startups.\n\nLast year, Nvidia unveiled its Blackwell chips, which it says are twice as fast as its Hopper chips and have attracted customers including SoftBank, Amazon Web Services, and Microsoft. The recent frenzy around the Chinese company DeepSeek's models has fueled demand for Nvidia's H200 chips.\n\nBut a key to Nvidia's success is CUDA, a software layer that can link GPUs to almost any AI application a developer wants to run. It's a critical component of the competitive advantage, or moat, that Nvidia has built up over the years.\n\nStill, AMD, Nvidia's main competitor, is quietly catching up. In AMD's third-quarter earnings call in October 2024, CEO Lisa Su said the company had \"closed a good part of\" the gap with Nvidia.\n\nNvidia's other competitors include Intel and IBM. Tech giants like Google, Amazon, Microsoft, and Meta have also released their own AI chips.\n\nIn January 2025, at the Consumer Electronics Show in Las Vegas, Huang unveiled new chips targeting the gaming, robotics, and autonomous vehicle industries, as well as partnerships with Toyota and Microsoft.\n\nFinancials\n\nNvidia has been competing with Apple and Microsoft for the title of the most valuable company in the world. It briefly surpassed Apple to take the top spot numerous times in 2024 and again in January 2025, when its market capitalization reached $3.45 trillion.\n\nFor the fiscal quarter that ended January 26, 2025, Nvidia's earnings call reported record revenue of $39.3 billion, up by 78% from the prior year. Net income for the quarter was more than $22.1 billion, up by 80% from the prior year.\n\nWorking at Nvidia\n\nNvidia is based in Santa Clara, California. Nvidia's headquarters, known as Voyager, was designed by the architectural firm Gensler and is about 750,000 square feet.\n\nIt has parks, \"treehouses\" for gatherings, and places designed to help employees focus. However, the overall design is intended to facilitate Nvidia's flat organizational structure.\n\n\"When you're moving that fast, you want to make sure that that information is flowing through the company as quickly as possible,\" Huang told the Harvard Business Review in 2023.\n\nNvidia's Santa Clara headquarters has parks, treehouses, and gathering places to help employees focus. Jason O'Rear / Gensler San Francisco\n\nIt's also a way to create more harmony between leadership and workers. Huang, who in 2023 oversaw 50 direct reports, has said that CEOs \"by definition\" should have the most direct reports at a company.\n\nHuang has earned a reputation among those who work with him as a demanding boss. Meetings with Huang can get heated, and senior employees have described his tough questions as a \"Jensen grilling.\"\n\nNvidia's top executives include Ian Buck, a vice president of hyperscale and high-performance computing; Colette Kress, the chief financial officer; and Bryan Catanzaro, a vice president of applied deep learning research.\n\nLanding a job at Nvidia isn't easy, but Lindsey Duran, a VP of recruitment, told BI that Nvidia applicants should express an interest in generative AI, tap into their professional network for referrals, and aim to do an internship.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "NVIDIA GeForce RTX 5070 Listed For $549 On Best BUY, Listing Reveals ASUS PRIME Edition GPU",
            "link": "https://wccftech.com/nvidia-geforce-rtx-5070-listed-for-549-on-best-buy-listing-reveals-asus-prime-edition/",
            "snippet": "The NVIDIA GeForce RTX 5070 looks ready to hit the market as one of its custom editions has already been listed on a retailer.",
            "score": 0.9444260001182556,
            "sentiment": null,
            "probability": null,
            "content": "The NVIDIA GeForce RTX 5070 looks ready to hit the market as one of its custom editions has already been listed on a retailer.\n\nASUS's Custom NVIDIA GeForce RTX 5070 12GB GDDR7 Listed on Best Buy for MSRP Ahead of Launch\n\nIt may seem as a surprise to see the NVIDIA GeForce RTX 5070 already being listed on a retailer. Since most RTX 50 series GPUs are already out of stock, seeing an upcoming RTX 50 series GPU and that too being listed for its official MSRP is a bit surprising. It's not yet available for purchase, and won't be till NVIDIA lifts the embargo on the sales.\n\nAs spotted by @momomo_us, the ASUS PRIME GeForce RTX 5070 12GB GDDR7 GPU is currently live on Best Buy for $549.99. The official MSRP for the Founders Edition of the RTX 5070 is $549 and, since the PRIME RTX 5070 from ASUS is the base card, it was expected that it would be priced at $549 as well. However, seeing the recent RTX 50 launches, we can't guarantee that the price will remain the same when the GPU launches.\n\nImage Source: BestBuy\n\nMoreover, it's possible that not only does the price get a hefty increase on the launch date, but the GPU also goes out of stock instantly. At the moment, all the launched RTX 50 series GPUs such as RTX 5090, RTX 5080, and RTX 5070 Ti are out of stock or are incredibly expensive in almost all parts of the world. This scarcity is said to be created by NVIDIA itself, but since AMD is about to launch its RX 9070 GPUs next week, it will create some pressure on NVIDIA as the RTX 5070 is set to launch a day before, i.e., on 5th March.\n\nConsidering that AMD has the Radeon RX 9070 at $549 as a direct competitor to the NVIDIA GeForce RTX 5070, NVIDIA may want to increase the supply drastically, unlike what it did with the previous cards. AMD has promised a wide availability of its RX 9070 and RX 9070 XT GPUs and the pricing will also affect the competition a lot.\n\nThe NVIDIA GeForce RTX 5070 is the fourth-fastest GPU in the Blackwell lineup, featuring 6144 CUDA Cores, 48 RT Cores, 12 GB of GDDR7 VRAM on a 192-bit bus, and a boost clock exceeding 2.5 GHz. The green team was reportedly facing some supply issues for the RTX 5070 and RTX 5060 GPUs, which indicated a delay of several weeks, but the report isn't official.\n\nHopefully, with competitive prices and performance, we can see both NVIDIA and AMD GPUs become widely available for their respective MSRPs.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "ARK Invest's Cathie Wood: Nvidia is still a high-growth stock",
            "link": "https://finance.yahoo.com/news/ark-invests-cathie-wood-nvidia-is-still-a-high-growth-stock-133015198.html",
            "snippet": "Don't give up on Nvidia (NVDA) amid the post-earnings sell-off. At least that's the word from AI investor Cathie Wood. Nvidia is still a high-growth stock,...",
            "score": 0.8428768515586853,
            "sentiment": null,
            "probability": null,
            "content": "Don't give up on Nvidia (NVDA) amid the post-earnings sell-off. At least that's the word from AI investor Cathie Wood.\n\nNvidia is still a high-growth stock, the ARK Invest founder told Yahoo Finance at the Bitcoin Investor Week conference in New York City on Friday. Wood thinks the stock could still be a 20% compound grower, even if the company's margins compress a little due to new AI chip competition from AMD (AMD) and Amazon (AMZN).\n\n\"This market won't be Nvidia's alone, but all praise to Nvidia for the incredible opportunities it has created,\" Wood said. \"It really got the AI revolution going, and we think it's still going to play a mighty role.\"\n\nWood said she continues to hold Nvidia stock.\n\nIn its earnings release on Wednesday evening, Nvidia said it expects gross profit margins of 70.6% to 71% in the first quarter as it contends with the production ramp-up of its new Blackwell chip.\n\nThe margin outlook of 71% is \"a little concerning,\" Benchmark Company managing director and senior research analyst Cody Acree said on Yahoo Finance's Market Domination. \"I think that's indicative of more pricing pressure, more competition from AMD (AMD), and more price sensitivity at their customers as they're investing their own dollars to create their own ASICs [application-specific integrated circuits].\"\n\nAfter toggling between gains and losses on Thursday, Nvidia stock closed down 8.48%. The stock rebounded by 1.5% on Friday.\n\nOn the company's earnings call, Nvidia execs sought to push back against the bears, who have put forth a narrative that there will be a digestion period for AI investments by hyperscalers such as Amazon (AMZN) and that Nvidia's margins may have peaked.\n\n\"We're going to have to continue to scale as demand is quite high, and customers are anxious and impatient to get their Blackwell systems,\" Nvidia founder and CEO Jensen Huang said. Huang teased several new powerful chips set to be unveiled at the company's March 17 GTC conference.\n\nNvidia CFO Colette Kress added: \"Once our Blackwell fully rounds, we can improve our cost and our gross margin. So we expect to probably be in the mid-70s later this year.\"\n\nCathie Wood, CEO and chief investment officer of ARK Invest, speaks during the Bitcoin 2022 Conference at the Miami Beach Convention Center on April 7, 2022. (Marco Bello/Getty Images) \u00b7 Marco Bello via Getty Images\n\nBrian Sozzi is Yahoo Finance's Executive Editor. Follow Sozzi on X @BrianSozzi, Instagram, and LinkedIn. Tips on stories? Email brian.sozzi@yahoofinance.com.\n\nClick here for the latest stock market news and in-depth analysis, including events that move stocks\n\nRead the latest financial and business news from Yahoo Finance",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia\u2019s stock price will jump more than 50% as the chipmaker remains in a 'dominant position,' BofA says",
            "link": "https://fortune.com/2025/03/01/nvidia-stock-price-outlook-nvda-jensen-huang-ai-deepseek-china-blackwell-gtc/",
            "snippet": "Nvidia sales from Blackwell reached $11 billion \u2014 surpassing expectations.",
            "score": 0.9476351141929626,
            "sentiment": null,
            "probability": null,
            "content": "\u00a9 2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information\n\nFORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Should You Buy Nvidia Stock After Its Blowout Q4 Results?",
            "link": "https://www.fool.com/investing/2025/03/01/should-you-buy-nvidia-stock-after-its-blowout-q4-r/",
            "snippet": "Investors no longer have to speculate about how Nvidia (NVDA 6.43%) performed in the fourth quarter of 2024. Now we know. And the news was once again...",
            "score": 0.922866702079773,
            "sentiment": null,
            "probability": null,
            "content": "Investors no longer have to speculate about how Nvidia (NVDA 5.27%) performed in the fourth quarter of 2024. Now we know. And the news was once again positive.\n\nNvidia reported its Q4 results after the market closed on Wednesday. It should come as no shock that the GPU maker handily beat Wall Street estimates. But should you buy Nvidia stock after its blowout Q4 results?\n\nHow good was Nvidia's Q4 performance?\n\nNvidia reported Q4 revenue of $39.33 billion, up 78% year over year and 12% higher than revenue generated in the previous quarter. This result topped the company's previous guidance of $37.5 billion. It also exceeded the average estimate of $38.05 billion among analysts surveyed by LSEG.\n\nThe company posted Q4 adjusted earnings of $0.89 per share, a year-over-year increase of 71% and a 10% jump from the third quarter of 2024. The consensus analysts' estimate was for adjusted earnings of $0.84 per share.\n\nNvidia's big story was its data center business. Data center revenue soared 93% year over year and 16% sequentially to a record $35.6 billion. This made up nearly 91% of the company's total revenue.\n\nThere were a couple of clouds in Nvidia's silver linings, though. One negative with the Q4 results is that the company's growth is slowing somewhat. Nvidia's revenue skyrocketed 94% year over year in Q3 and 122% in Q2. The company's gross margins also slipped 3% year over year in Q4 to 73%.\n\nLooking ahead\n\nWith all of this good news, why did Nvidia's share price slide a little after its Q4 update? Investors focus more on the future than on the past. Nvidia's outlook, while positive, wasn't overly impressive.\n\nThe company's guidance was for revenue in the first quarter of its fiscal 2026 of $43 billion, plus or minus 2%. This figure is below the average estimate of $43.37 billion among the analysts surveyed by LSEG. It also reflects further deterioration in Nvidia's growth rate.\n\nNvidia projects an adjusted gross margin in Q1 of 71%, plus or minus 50 basis points. This continues the downward trend of the company's gross margin in recent quarters.\n\nInvestors would normally be ecstatic about a company delivering 65% revenue growth and gross margins of over 70%. However, when a stock trades at almost 31 times forward earnings as Nvidia does, expectations are understandably higher than for many other companies.\n\nShould you buy Nvidia stock now?\n\nI don't think too much fuss should be made about Nvidia's slowing growth and lower margins. No one should expect the company's growth rate to remain at the dizzying levels seen after generative AI first took off. Nvidia's margins should bounce back into the mid-70s once its launch of the new Blackwell chips is at full speed.\n\nNvidia CEO Jensen Huang noted in the Q4 earnings call that the amount of computing power required for AI inference is already 100 times greater than needed for early large language models (LLMs). He added, \"This is just the beginning.\" Huang thinks future AI models could need computing power that's thousands of times and perhaps even millions of times more than today's models.\n\nI suspect Huang will be proven right. I also expect Nvidia will remain at the forefront of the AI chip market. If these assumptions are correct, growth investors should buy Nvidia stock now. The company could have plenty of blowout quarters ahead.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Fund manager who predicted Nvidia rally revamps stock forecast",
            "link": "https://www.thestreet.com/investing/stocks/fund-manager-who-predicted-nvidia-rally-revamps-stock-forecast",
            "snippet": "Here's what the long-time Wall Street pro thinks is next for Nvidia's shares.",
            "score": 0.7550188302993774,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia Hotfix arrives to address black screen issues remaining after Thursday's driver release",
            "link": "https://www.tomshardware.com/pc-components/gpu-drivers/nvidia-hotfix-arrives-to-address-black-screen-issues-remaining-after-thursdays-driver-release",
            "snippet": "Nvidia released GeForce Game Ready Driver Version 572.60 earlier in the week, but one of the most important bug fixes was ineffectual in some cases,...",
            "score": 0.8610649704933167,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia released GeForce Game Ready Driver Version 572.60 earlier in the week, but one of the most important bug fixes was ineffectual in some cases, according to various tech forum members. Today, Nvidia has sought to plug its GPU black screen issues more tightly with GeForce Hotfix Driver Version 572.65. Fingers crossed it works this time.\n\nThere's just one bullet point in the GeForce Hotfix Display Driver version 572.65 description. \"This hotfix addresses the following issue: PC may boot to a black screen when connected via DisplayPort with certain monitors [5131002],\" writes Nvidia. However, a follow-up paragraph outlines the arduous task facing graphics driver developers in 2025. \"A GeForce driver is an incredibly complex piece of software,\" says Nvidia. \"We have an army of software engineers constantly adding features and fixing bugs. These changes are checked into the main driver branches, which are eventually run through a massive QA process and released.\"\n\nSo, we have to forgive Nvidia (or, indeed, AMD or Intel) for the odd wrinkle in drivers, be it a game or machine-breaking issue or something less drastic like visual artifacts or so on. The good thing about a Hotfix, though, is that it can be delivered pretty quickly, and those who are actually suffering from the bug in question can apply it. Others can simply wait for the update to be rolled into the mainline WHQL driver release in due course \u2013 it might be an important bug fix for them at some later date.\n\nEagle-eyed readers might notice an important difference in black screen bug fixing emphasis between the recently released GeForce Game Ready Driver Version 572.60 and the newly arrived hotfix. While the former was flagged to fix various black screen issues experienced by RTX 50 Series graphics card owners, the hotfix seems to apply to the broader GeForce ownership.\n\nThe wider applicability of this black screen fix, Version 572.65, is welcome if you look at the official Version 572.60 feedback thread on the GeForce forums. Jaunt over to the community, and you will notice that there are owners of a range of cards, including the RTX 3060, RTX 4090, and RTX 5070 Ti, who grouse about Thursday's driver not banishing their black screen problems.\n\nUsually, it is a good idea to hold back from fresh driver updates for a period and monitor forums for other people to discover issues. However, those who have bought one of the new RTX 50 cards and are keen to keep up with the latest software may throw caution to the wind. Also, GeForce Game Ready Driver Version 572.60 was attractive for its support for Naraka: Bladepoint with DLSS 4 and Multi Frame Generation (MFG), plus Monster Hunter Wilds with DLSS and Frame Generation, as well as support for 29 new G-Sync Compatible monitors.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Apple's DEI defense, Nvidia's earnings beat, and Amazon's quantum chip: Tech news roundup",
            "link": "https://qz.com/apple-dei-nvidia-earnings-stock-amazon-quantum-chip-1851767060",
            "snippet": "Apple's DEI defense, Nvidia's earnings beat, and Amazon's quantum chip: Tech news roundup. Plus, Apple says it will invest $500 billion in the U.S. with a...",
            "score": 0.7664199471473694,
            "sentiment": null,
            "probability": null,
            "content": "Apple (AAPL) shareholders on Tuesday voted to keep the iPhone maker\u2019s diversity policies, shutting down a conservative think tank\u2019s proposal.\n\nAdvertisement\n\nThe technology giant had defended its diversity, equity, and inclusion (DEI) practices ahead of its annual meeting. In a prepared statement, Apple cited its established compliance program and a desire to create a \u201cculture of belonging where everyone can do their best work.\u201d\n\nRead More",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia issues a new hotfix for black screens.",
            "link": "https://www.theverge.com/news/622309/nvidia-issues-a-new-hotfix-for",
            "snippet": "An Nvidia support page says the new hotfix (v572.65) drivers \u201care basically the same as the previous released version, with a small number of additional...",
            "score": 0.8765028119087219,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia issues a new hotfix for black screens.\n\nAn Nvidia support page says the new hotfix (v572.65) drivers \u201care basically the same as the previous released version, with a small number of additional targeted fixes,\u201d referring to issues with black screens on RTX 50 GPUs that drivers released earlier this week were also aimed at addressing.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Engadget review recap: iPhone 16e, NVIDIA RTX 5070 Ti, Sony A1 II and more",
            "link": "https://www.engadget.com/engadget-review-recap-iphone-16e-nvidia-rtx-5070-ti-sony-a1-ii-and-more-130054263.html",
            "snippet": "A roundup of the reviews published on Engadget over the last few weeks.",
            "score": 0.9327349662780762,
            "sentiment": null,
            "probability": null,
            "content": "Engadget has been testing and reviewing consumer tech since 2004. Our stories may include affiliate links; if you buy something through a link, we may earn a commission. Read more about how we evaluate products .\n\nAs Engadget celebrates its 21st birthday this weekend, we're rounding up all of the recently published reviews from the last few weeks. It's already a busy time for the reviews team, even though spring is still officially a few weeks away. The latest entries include a new iPhone, the latest from NVIDIA and those Beats workout earbuds the company teased months ago. Pour a couple bevvies for the weekend and make one a double, because we're old enough to drink now.\n\niPhone 16e\n\nApple / Engadget 77 100 Expert Score Apple iPhone 16e The iPhone 16e is the most affordable entry into Apple Intelligence and the rest of the company\u2019s ecosystem, and is a solid option for anyone who doesn\u2019t care very much about smartphone photography. Pros Solid performance\n\nLong battery life\n\nClean, durable design\n\nBright, vibrant screen Cons Only one rear camera, leading to many limitations\n\nExpensive compared to previous iPhone SE\n\nNo MagSafe $599 at Apple\n\nApple's new \"budget\" phone has arrived, providing the most affordable gateway to Apple Intelligence. While the iPhone 16e offers solid performance, expected long battery life and a great display, there are some caveats that you might not be able to live with. \"In my week or so with the iPhone 16e, I felt the drawbacks of the camera the most \u2014 I can live without MagSafe and the Dynamic Island,\" Cherlynn writes. \"But what the iPhone 16e does offer is fast performance, a clean design, long battery life and, most crucially, cheaper access to Apple\u2019s ecosystem.\"\n\nNVIDIA GeForce 5070 Ti\n\nASUS 85 100 Expert Score NVIDIA RTX 5070 Ti NVIDIA's RTX 5070 Ti is a capable GPU for 4K gaming, mostly thanks to DLSS 4. It's a solid upgrade for gamers looking for a bit more power and memory than the 5070. Pros Solid 4K performance\n\nDLSS 4 multi-frame gen makes a huge difference\n\nCool under load Cons Prices fluctuate wildly\n\nStock is a major problem\n\n$750 is still a lot for a mid-range GPU $900 at Best Buy Explore More Buying Options $750 at Newegg\n\nIf you're in the market for a new 4K gaming GPU, the RTX 5070 Ti is a a nice upgrade over the regular 5070. However, finding one, and doing so for a decent price, is another thing entirely. \"I knew it would be a tad faster than the 4070 Ti Super, but with the addition of multi-frame generation, it's also a far more capable 4K card,\" Devindra says. \"And it's definitely more future-proof than the 5070, since it has 16GB of VRAM like the 5080.\"\n\nSony A1 II\n\nSony 88 100 Expert Score Sony A1 II Sony\u2019s A1 II is the company\u2019s best mirrorless camera yet, with improved handling, stabilization and autofocus compared to the original A1. Pros Incredibly fast\n\nSony's best autofocus yet\n\nSharp and color-accurate photos\n\nOutstanding in-body stabilization Cons Lacks the RAW video support its rivals offer\n\nExpensive\n\nNot a huge upgrade from the A1 $6,498 at B&H Photo\n\nWith A1 II, Sony had to live up to its own high standards. And while the camera may be the company's best mirrorless option yet, it's not a huge leap over the A1. \"The innovation that Sony is known for is lacking here, and on top of that, the A1 II is very expensive,\" Steve explains. \"The A1 II is only a mild upgrade over the A1 and when it comes to video, it\u2019s lagging behind the Nikon Z8 and Z9 as well as the Sony R5 II.\"\n\nADVERTISEMENT Advertisement\n\nSteve also took the DJI Flip for a spin. He gives the creator-friendly drone high marks for 4K video quality, battery life and a people-safe design. The drone's obstacle detection isn't the best though, and it struggles in windy conditions.\n\nLenovo Legion Go S\n\nLenovo 75 100 Expert Score A more streamlined take on the original Legion Go Lenovo Legion Go S (Windows version) The Legion Go S takes a lot of the good stuff from its predecessor and distills it into a slightly more compact and portable package. However, the launch model currently costs more than the original Legion Go, while offering worse performance, a smaller screen and no detachable controllers, which makes this a hard handheld to love. Pros Big 120Hz 8-inch screen\n\nHandy little built-in touchpad\n\nSolid design with adjustable triggers\n\nHall effect joysticks\n\nDual USB ports Cons No fingerprint reader\n\nLackluster vibration motor\n\nToo pricey for the performance\n\nLegion Space app is still kind of finicky $730 at Best Buy\n\nWhile Lenovo has already teased a new version of the Legion Go gaming handheld for later this year, the company introduced the more streamlined Legion Go S in the meantime. There are some things to like here, including a more portable design, an 8-inch 120Hz display and handy controls. For now, the price is the main problem.\n\n\"The issue is that Lenovo hasn\u2019t fully rolled out all of its variations,\" Sam writes. \"So even if you aren\u2019t holding out for the SteamOS variant, you\u2019d be silly not to wait for less expensive versions to come out with starting prices closer to $600 (or even $500 for the one with Valve\u2019s platform).\"\n\nBeats Powerbeats Pro 2\n\nBeats 79 100 Expert Score Beats Powerbeats Pro 2 Apple\u2019s first earbuds with heart-rate tracking aren\u2019t AirPods, but they offer a lot of the same smarts via a major design overhaul from Beats. Pros Improved, comfy design\n\nPowerful, balanced bass performance\n\nH2 chip adds smarts\n\nHeart-rate sensors onboard Cons Heart-rate support is limited at launch on iOS\n\nHook design isn\u2019t for everyone\n\nANC is average at best $249 at Amazon Explore More Buying Options $249 at Walmart$250 at Adorama\n\nAfter an initial tease months ago, Beats finally debuted its updated Powerbeats Pro workout earbuds. The hook design remains, but there are significant upgrades both inside and out, including Apple's first dance with heart-rate tracking in an audio product. \"On the whole, the Powerbeats Pro 2 are a substantial upgrade over the original,\" I explain. \"They\u2019re more comfortable and have a host of new features that help it match today\u2019s earbuds.\"\n\nADVERTISEMENT Advertisement\n\nI also reviewed the Noble Audio FoKus Rex5 earbuds in the last month. This set offers the best sound quality I've experienced on wireless earbuds, thanks in part to the company's use of five drivers in each one. However, the Rex5 is expensive at $449 and it's far from a complete package in terms of features and noise-canceling performance.\n\nOther notable reviews and a look ahead\n\nManaging editor Cherylnn Low spent more time living with the Kindle Scribe 2 and updated the review with some long-term observations about the tablet. The Samsung Galaxy Watch 7 was wrapped around the wrist of buying advice senior report Amy Skorheim as she put yet another incremental update from the company through its paces.\n\nMy review of the Technics AZ100 earbuds that one a Best of CES award from Engadget is coming soon. As I suspected during my brief hands-on in Las Vegas, the improved sound is the star of the show here, but I won't spoil the rest of the review with more observations. MWC 2025 is happening next week, so whatever is announced during the show will certainly be on the upcoming review agenda. For now, you can check out our preview here.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-28": {
        "0": {
            "title": "Singapore charges three with fraud that media link to Nvidia chips",
            "link": "https://www.reuters.com/technology/singapore-charges-three-with-fraud-that-media-link-nvidia-chips-2025-02-28/",
            "snippet": "Singapore charged three men with fraud in a case domestic media have linked to the movement of Nvidia's advanced chips from the city state to Chinese...",
            "score": 0.9181886315345764,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "Nvidia Stock Finds Modest Support from Tepid Dip-Buyers",
            "link": "https://www.investopedia.com/nvidia-stock-finds-modest-support-from-tepid-dip-buyers-11688344",
            "snippet": "Nvidia shares were up slightly in early trading Friday, suggesting the stock found some support from dip-buyers after yesterday's sell-off.",
            "score": 0.9215309619903564,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) shares finished Friday higher, suggesting the stock found some support from dip-buyers after yesterday's sell-off.\n\nNvidia stock ended the day up nearly 4% in recent trading after tumbling 8.5% yesterday. Nvidia on Wednesday reported better-than-expected quarterly results, but Wall Street demonstrated on Thursday that\u2019s no longer enough from its favorite AI stock. Nvidia beat revenue estimates by the smallest amount in two years, underwhelming investors who have grown accustomed to gargantuan beats from the AI chip leader.\n\nThe results failed to revive the AI rally. High-flying, richly-priced stocks like Palantir (PLTR), Applovin (APP), and Vistra (VST), which all soared last year on enthusiasm about their AI-fueled growth, had dropped in the recent sessions as investors have grown cautious amid a slew of economic and political concerns.\n\nEven Friday morning, after a promising print of the Federal Reserve\u2019s preferred inflation measure, all three stocks slumped at the open. (They all finished the day higher, however.)\n\nAI stocks have also been weighed down this month by lingering concerns about the impact of Chinese start-up DeepSeek\u2019s R1 reasoning model, which its developers say operates at a far lower cost than comparable U.S. models. R1\u2019s success and efficiency raised concerns among investors that U.S. hyperscalers and other AI developers could scale back their spending on Nvidia\u2019s most advanced technology.\n\nMajor tech companies have since reiterated their commitment to spending hundreds of billions on AI infrastructure in the coming years, but that hasn\u2019t pulled Nvidia and other chip stocks out of their funk.\n\nThis article was updated to reflect closing share-price information.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Investors may be badly underestimating this long-term tailwind for Nvidia",
            "link": "https://finance.yahoo.com/news/investors-may-be-badly-underestimating-this-long-term-tailwind-for-nvidia-151717703.html",
            "snippet": "Nvidia has some big profit tailwinds that the Street may be forgetting.",
            "score": 0.9669690132141113,
            "sentiment": null,
            "probability": null,
            "content": "Listen and subscribe to Opening Bid on Apple Podcasts, Spotify, YouTube, or wherever you find your favorite podcasts.\n\nShort-term-minded Nvidia (NVDA) investors may have forgotten something in their race to dump the stock after earnings this week: the long-term profit tailwind of sovereign artificial intelligence.\n\n\"I think that you're going to hear a lot more about these infrastructure build-outs from nation-states,\" a16z generator partner and Mistral AI board member Anjney Midha said on Yahoo Finance's Opening Bid podcast (see video above or listen below). \"Sometimes these are lagging indicators. It might be underreported on \u2014 it just takes a while for people to realize that nation-state priorities are then turning into enterprise budgets.\"\n\nThis embedded content is not available in your region.\n\nThe build-out of AI infrastructure by major countries is beginning to take form in large numbers.\n\nEarlier this month, the European Union earmarked 200 billion euros for investment in AI, including a new European fund of 20 billion euros for AI gigafactories. OpenAI, Oracle (ORCL), and Softbank (SFTBY) said in late January they would invest up to $500 billion in the coming years in an AI infrastructure project called Stargate.\n\nAt the core of these build-outs will be high-powered chips made by AI leader Nvidia, experts say.\n\nRead more: How does Nvidia make money?\n\nNvidia has said Canada, Denmark, and Indonesia have announced initiatives to develop sovereign AI infrastructure powered by its AI chips.\n\n\"We think AI models are critical national infrastructure,\" Midha added. \"We think these AI factories that turn tokens into intelligence [are] critical national infrastructure. And I see that segment accelerating.\"\n\nWhile sovereign AI represents a long-term profit opportunity for Nvidia, in the near term, the stock could take its cue from a mixed first quarter outlook shared this week.\n\nIn its earnings release on Wednesday evening, Nvidia said it expects gross profit margins of 70.6% to 71% in the first quarter as it contends with the production ramp-up of its new Blackwell chip.\n\nThe margin outlook of 71% is \"a little concerning,\" Benchmark Company managing director and senior research analyst Cody Acree said on Yahoo Finance's Market Domination. \"I think that's indicative of more pricing pressure, more competition from AMD (AMD), and more price sensitivity at their customers as they're investing their own dollars to create their own ASICs [application-specific integrated circuits].\"\n\nAfter toggling between gains and losses on Thursday, Nvidia stock closed down 8.48%. The stock rose slightly in premarket trading on Friday.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia quietly acquires AIOps firm Augtera Networks",
            "link": "https://www.datacenterdynamics.com/en/news/nvidia-quietly-acquires-aiops-firm-augtera-networks/",
            "snippet": "GPU giant Nvidia has quietly acquired AIops firm Augtera Networks. First reported by Rethink Research, the California-based network monitoring company,...",
            "score": 0.8506996631622314,
            "sentiment": null,
            "probability": null,
            "content": "No official statement has been made and terms of the deal haven\u2019t been shared.\n\nFirst reported by Rethink Research , the California-based network monitoring company, was acquired by Nvidia in December 2024.\n\nAugtera is to become a part of Nvidia\u2019s Spectrum-X networking product portfolio. Augtera\u2019s website now redirects to Nvidia.\n\nAugtera founder and CEO Rahul Aggarwal is now listed as a senior director of system software for AI at Nvidia on LinkedIn \u2013 a position he took up in December. Augtera co-founder Bhupesh Kothari is also now at Nvidia as director for system software for AI, after also joining in December.\n\nInvestment bank Bowen advises Augtera on the sale. Nvidia declined to comment further to Rethink.\n\nDCD has contacted Nvidia for more information.\n\nFounded in 2016, Augtera had previously secured around $18 million in seed and Series A funding, with backers including Bain Capital Ventures, Acrew Capital, Intel Capital, and Dell Capital. Its last funding round was in June 2021.\n\nThe company provided AI-based tools to automate network operations \u2013 its network monitoring technology provided anomaly detection, failure prevention, and real-time network visibility.\n\nRethink senior analyst Alex David said Augtera had contracts with the likes of Orange and Vyve Broadband. Colt Technologies was also reportedly a customer. Fintech firm MX Technologies previously selected Augtera to help monitor its data center network.\n\nAIOps \u2013 aka artificial intelligence for IT Operations \u2013 aims to use AI to automate IT operations. By analyzing large volumes of data and applying machine learning, the technology aims to identify operation issues and resolve IT issues.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia CEO Gives $22.5 Million to Save Cash-Starved Arts College",
            "link": "https://www.bloomberg.com/news/articles/2025-02-28/nvidia-nvda-ceo-huang-gives-22-5-million-to-save-unsung-arts-college",
            "snippet": "Jensen Huang, Nvidia Corp.'s leather jacket-clad top executive, is an unlikely savior for a struggling arts school in San Francisco.",
            "score": 0.7172706723213196,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "AMD looks to undercut Nvidia, win gamers' hearts with RX 9070 series",
            "link": "https://www.theregister.com/2025/02/28/amd_rx_9070_series/",
            "snippet": "With the launch of AMD's RX 9070-series graphics cards, AMD is going back to its roots. Rather than trying to compete with Nvidia on raw performance with...",
            "score": 0.9239346981048584,
            "sentiment": null,
            "probability": null,
            "content": "With the launch of AMD's RX 9070-series graphics cards, AMD is going back to its roots. Rather than trying to compete with Nvidia on raw performance with another flagship GPU beyond the means of most gamers, the House of Zen aims to undercut its competitor by delivering more frames per dollar.\n\nAMD has employed various versions of this strategy over the years \u2013 its core-packed Ryzen processors come to mind \u2013 and by focusing on the mid-tier segment, AMD clearly aims to capitalize on growing discontent with Nvidia's sky-high prices this generation.\n\nThe House of Zen is billing its $549 RX 9070 and $599 9070 XT graphics cards as delivering 4K performance at 1440P pricing. But, while the cards are certainly capable of 4K gaming, especially with AMD's AI upscaling and frame gen tech enabled \u2013 more on those in a bit \u2013 it's clear these cards are designed primarily with high-refresh rate 1440p gaming in mind.\n\nThis is reflected in AMD's positioning of the cards, which slot in behind its now two-year-old RX 7900 XT and XTX graphics cards designed for 4K gaming, and ahead of its previous 1440p power house the 7900 GRE.\n\nAs you can see the new chips slot in somewhere between its last-gen flagships and its previous 1440p champion the RX 7900 GRE - click to enlarge\n\nIt also makes sense when you consider just how few folks are actually gaming at 4K. According to Steam's latest hardware survey, more than three-quarters of PC gamers are running monitors at 1440p or lower, and just 4 percent are running at 4k or higher.\n\nIn terms of performance, AMD says the RX 9070 will deliver between 20 and 21 percent higher frame rates on average compared to its prior-gen RX 7900 GRE at 1440p and 4K respectively. The XT variant, meanwhile, promises an average uplift of 38 to 42 percent at those same resolutions.\n\nAs for how they compare to Nvidia's chips, AMD is going after Nvidia's RTX 5070 and 5070 TI graphics cards. During AMD's launch event, David McAfee, VP and GM of AMD's client business claimed the RX 9070 XT would deliver average performance just 2 percent shy of the 5070 TI. But, as always, we recommend taking vendor supplied benchmarks and claims with a hefty dose of salt. Case in point, as you can see from the chart below, Call of Duty Black Ops 6 and Ghost of Tsushima are doing a lot to lift that average.\n\nHere's how AMD says the RX 9070 XT compares to Nvidia's $150 more expensive 5070 TI - click to enlarge\n\nRDNA 4 makes its debut\n\nUnderpinning these performance gains is AMD's all-new RDNA 4 graphics architecture. Without getting too deep in the weeds, it boasts an improved memory subsystem, upgraded compute, RT, and AI compute engines, along with higher overall clock speeds.\n\nAMD is particularly proud of the architecture's new RT cores, an area where it's historically lagged Nvidia by a pretty wide margin. RDNA 4's Gen 3 RT accelerators promise twice the throughput per compute unit versus RDNA 3, which should help as more games begin to list ray-tracing capabilities on their minimum requirements.\n\nAs for the actual silicon powering the 9070-series, AMD has ditched the chiplet architecture used by its 7000-series parts in favor of a more traditional monolithic die this time around.\n\nThe chips are each complemented by 16 GB of GDDR6 memory. The main differentiator between the two is that the non-XT variant has about 13 percent fewer compute, RT, and AI units, and roughly 15 percent lower clock speeds.\n\nSpecs Here's a quick rundown of how they compare for your reference: RX 9070 RX 9070 XT Compute units 56 64 RT accelerators 56 64 AI accelerators 112 128 AI TOPS (Sparse INT4) 1,165 TOPS 1,557 TOPS Boost clock 2.52 GHz 2.97 GHz vRAM 16 GB 16 GB TDP 220 W 304 W Connectivity PCIe 5.0 x16 PCIe 5.0 x16 Display output DisplayPort 2.1a, HDMI 2.1b DisplayPort 2.1a, HDMI 2.1b\n\nNo escaping the AI PC\n\nAlong with gaming, AMD didn't miss an opportunity to talk up the cards in the context of the emerging AI PC segment. However, for those excited about running LLMs, image gen, or some other AI-augmented workflow, the 9070 series is a mixed bag.\n\nOn one hand, the card's second-gen AI accelerators promise significant gains for the kinds of fuzzy math endemic to machine learning workloads. AMD claims up to 1,557 TOPS for the XT and 1,165 for the standard RX 9070. That's sparse INT4 performance, putting it just ahead of Nvidia's recently launched RTX 5070 TI, which boasts 1,406 TOPS.\n\nCompared to the 7900 GRE, AMD expects the 9070 XT to deliver a 12-34 percent uplift in performance for content creation-based machine learning tasks in software like Adobe Lightroom and DaVinci Resolve. For GenAI workloads, AMD suggests the gains will be even greater for image generation in Amuse and Procyon.\n\nHowever, as we've previously discussed, AI compute bottlenecks differ from one workload to another. Image generation benchmarks, for instance, are often compute-bound, while large language models (LLM) like Llama 3 tend to be more constrained by memory bandwidth.\n\nWith a 256-bit memory bus, AMD's 9070-series cards top out at 640 GBps of bandwidth compared to the 7900 XT's 800 GBps and the XTX's 960 GBps. On paper, this means the 9070-series cards should be about 22 to 40 percent slower in memory-bound workloads.\n\nNot even gaming can escape the AI hype\n\nIf you'd rather spend your time gaming than playing with AI models on your PC, the 9070s' AI compute units won't necessarily be wasted.\n\nJust like Nvidia, AMD is leaning hard on AI upscaling techniques to juice performance figures this generation. This technology works by initially rendering games at lower resolutions, say 1080p, and then taking advantage of the AI accelerators to upscale those frames to 4K. Nvidia calls this tech Deep Learning Super Sampling (DLSS), while AMD calls its equivalent FidelityFX Super Resolution (FSR).\n\nWith FSR4 enabled, AMD claims the 9070 XT is capable of more than doubling frame rates versus 4K, but again, that's because, with FSR enabled, the graphics cores aren't actually rendering those frames at 4K.\n\nAlongside FSR, the cards will also support frame generation, which, rather than using its AI accelerators to upscale lower resolution frames, boosts frame rates by generating intermediate frames.\n\nRemember when MSRP meant something?\n\nAs for pricing, AMD's RX 9070 will hit shelves beginning March 6 with a manufacturer suggested retail price (MSRP) of $549, while the more powerful XT variant has an MSRP of $599. This puts the RX 9070 directly in contention with Nv's similarly priced RTX 5070, while the 9070 XT undercuts the RTX 5070 TI by $150 while claiming to deliver similar performance.\n\nHowever, that assumes you can actually get one at that price.\n\nAs we've seen with Nvidia's 50-series cards, MSRP doesn't count for much if board partners and retailers don't take the suggestion. Looking on Newegg and BestBuy, we see that multiple 5070 TIs \u2013 a part with an MSRP of $749 \u2013 listed at closer to $900 or even $1,000.\n\nMeanwhile, Nvidia's $2,000 flagship 5090 can be found listed for anywhere from $400 to $1,000 over sticker. But even if you're willing to pay the markup for Nvidia's 50-series cards, your next challenge will be finding one in stock.\n\nThus, the success of AMD's RX 9070 and 9070 XT will likely end up having less to do with how well they stack up against Nvidia in gaming than whether you can actually buy the cards at anything resembling their launch pricing. \u00ae",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia Stock Rises. It\u2019s Cheaper Than Walmart.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-tech-selloff-analysts-2beb1ebf",
            "snippet": "Nvidia stock rose early Friday in the wake of fresh Trump tariff threats on Mexico, Canada and China on Thursday\u2014but it's not all bad news for the world's...",
            "score": 0.8992597460746765,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Nvidia's stock got crunched after earnings. Here's what Wall Street thinks.",
            "link": "https://www.marketwatch.com/livecoverage/nvidia-earnings-stock-market-results-guidance-ai-blackwell",
            "snippet": "Nvidia reported its fourth-quarter results after Wednesday's closing bell. Investors and analysts have focused on early sales of the company's new Blackwell...",
            "score": 0.5692355632781982,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Is $3 Trillion Nvidia A Value Stock?",
            "link": "https://www.forbes.com/sites/johnbuckingham/2025/02/28/is-3-trillion-nvidia-nvda-a-value-stock/",
            "snippet": "Market heavyweight Nvidia (NVDA) delivered blockbuster results in Q4 of its fiscal year ended January 2025, which saw revenue soar to a record $39.3...",
            "score": 0.9392676949501038,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Thinking About Buying The Nvidia Stock Dip? 4 Reasons To Stay Away Now.",
            "link": "https://www.barchart.com/story/news/31177900/thinking-about-buying-the-nvidia-stock-dip-4-reasons-to-stay-away-now",
            "snippet": "Nvidia is down 10% for the year and is now in a bear market territory. The stock's near-term outlook is not looking too great, even as it remains a good...",
            "score": 0.8611904978752136,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-27": {
        "0": {
            "title": "DLSS 4 With Multi Frame Generation Available Today In NARAKA: BLADEPOINT",
            "link": "https://www.nvidia.com/en-us/geforce/news/naraka-bladepoint-dlss-4-multi-frame-generation/",
            "snippet": "DLSS also accelerates performance in Monster Hunter Wilds, and the DLSS 4 Plugin for Unreal Engine 5 is available now.",
            "score": 0.9262385368347168,
            "sentiment": null,
            "probability": null,
            "content": "More than 700 games and applications feature RTX technologies, and each week new games integrating NVIDIA DLSS, NVIDIA Reflex, and advanced ray-traced effects are released or announced, delivering the definitive PC experience for GeForce RTX players.\n\nFollowing the release of DLSS 4 with Multi Frame Generation for Delta Force: Black Hawk Down, Indiana Jones and the Great Circle\u2122, and Marvel Rivals, this week sees the addition of the performance multiplying tech to NARAKA: BLADEPOINT.\n\nAdditionally, Monster Hunter Wilds launches today with DLSS Frame Generation, DLSS Super Resolution, and ray-traced reflections. And the DLSS 4 Plugin for Unreal Engine 5 is available now.\n\nFor all the details, read on.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia earnings, outlook top Wall Street forecasts as CEO Jensen Huang touts 'light speed' AI advances",
            "link": "https://finance.yahoo.com/news/nvidia-earnings-outlook-top-wall-street-forecasts-as-ceo-jensen-huang-touts-light-speed-ai-advances-090009668.html",
            "snippet": "Nvidia reported its Q4 earnings after the bell on Wednesday.",
            "score": 0.5900010466575623,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) reported its fourth quarter earnings after the bell on Wednesday, beating analysts' expectations on the top and bottom lines and issuing solid Q1 guidance.\n\nNvidia's stock rose 2.5% before the bell on Thursday, coming back from small loss earlier in premarket trading as investors digested the report.\n\nNvidia's earnings come as the company girds itself for potential 25% tariffs on chips imported into the US and the threat of increased export controls on its shipments to China. The AI giant is also contending with the fallout from claims that Chinese startup DeepSeek developed its AI models using less powerful Nvidia chips than its US rivals, putting into question whether Big Tech companies are over-investing in AI.\n\nFor the quarter, Nvidia reported earnings per share (EPS) of $0.89 on revenue of $39.3 billion. Wall Street was expecting EPS of $0.84 on revenue of $38.2 billion. The company said it expects Q1 revenue of $43 billion plus or minus 2%, better than the $42.3 billion expected.\n\nData center revenue clocked in at $35.6 billion versus expectations of $34 billion in the quarter.\n\n\"We\u2019ve successfully ramped up the massive-scale production of Blackwell AI supercomputers, achieving billions of dollars in sales in its first quarter,\" CEO Jensen Huang said in a statement. \"AI is advancing at light speed as agentic AI and physical AI set the stage for the next wave of AI to revolutionize the largest industries.\u201d\n\nAccording to Nvidia CFO Colette Kress, cloud service providers made up 50% of Nvidia's data center revenue in the quarter. The company reported similar results in Q3.\n\nThe company's Blackwell line of chips contributed billions in sales for the quarter, Kress said.\n\nRead more: How does Nvidia make money?\n\n\"We delivered $11.0 billion of Blackwell architecture revenue in the fourth quarter of fiscal 2025, the fastest product ramp in our company\u2019s history.\"\n\nNvidia's gaming revenue, however, fell 11% year over year in Q4 due to supply constraints around its latest gaming chips.\n\nNvidia is the reigning champion of AI chips, and it\u2019s not losing that crown anytime soon. Its chips are the envy of Silicon Valley and beyond, and its competitors are still far from overtaking its performance advantage.\n\nBig Tech companies Amazon (AMZN), Google (GOOG, GOOGL), Meta (META), and Microsoft (MSFT) are spending billions of dollars building out their AI data centers, and a chunk of that is going straight to Nvidia.\n\nBut shares of those same companies are also struggling in the early months of 2025.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "How Nvidia Adapted Its Chips to Stay Ahead of an AI Industry Shift",
            "link": "https://www.wsj.com/tech/ai/how-nvidia-adapted-its-chips-to-stay-ahead-of-an-ai-industry-shift-044edaad",
            "snippet": "The chip giant has stayed relevant even as DeepSeek and other 'reasoning' AI models rise.",
            "score": 0.8430755138397217,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia CEO Huang says AI has to do '100 times more' computation now than when ChatGPT was released",
            "link": "https://www.nbcnews.com/business/business-news/nvidia-ceo-huang-says-ai-100-computation-now-chatgpt-was-released-rcna194081",
            "snippet": "Nvidia CEO Jensen Huang said next-generation AI will need 100 times more compute than older models as a result of new reasoning approaches that think \u201cabout...",
            "score": 0.8731549978256226,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang said next-generation AI will need 100 times more compute than older models as a result of new reasoning approaches that think \u201cabout how best to answer\u201d questions step by step.\n\n\u201cThe amount of computation necessary to do that reasoning process is 100 times more than what we used to do,\u201d Huang told CNBC\u2019s Jon Fortt in an interview on Wednesday following the chipmaker\u2019s fiscal fourth-quarter earnings report.\n\nHe cited models including DeepSeek\u2019s R1, OpenAI\u2019s GPT-4 and xAI\u2019s Grok 3 as models that use a reasoning process.\n\nNvidia reported results that topped analysts\u2019 estimates across the board, with revenue jumping 78% from a year earlier to $39.33 billion. Data center revenue, which includes Nvidia\u2019s market-leading graphics processing units, or GPUs, for artificial intelligence workloads, soared 93% to $35.6 billion, now accounting for more than 90% of total revenue.\n\nThe company\u2019s stock still hasn\u2019t recovered after losing 17% of its value on Jan. 27, its worst drop since 2020. That plunge came due to concerns sparked by Chinese AI lab DeepSeek that companies could potentially get greater performance in AI on far lower infrastructure costs.\n\nHuang pushed back on that idea in the interview on Wednesday, saying DeepSeek popularized reasoning models that will need more chips.\n\n\u201cDeepSeek was fantastic,\u201d Huang said. \u201cIt was fantastic because it open sourced a reasoning model that\u2019s absolutely world class.\u201d\n\nNvidia has been restricted from doing business in China due to export controls that were increased at the end of the Biden administration.\n\nHuang said that the company\u2019s percentage of revenue in China has fallen by about half due to the export restrictions, adding that there are other competitive pressures in the country, including from Huawei.\n\nDevelopers will likely search for ways around export controls through software, whether it be for a supercomputer, a personal computer, a phone or a game console, Huang said.\n\n\u201cUltimately, software finds a way,\u201d he said. \u201cYou ultimately make that software work on whatever system that you\u2019re targeting, and you create great software.\u201d\n\nHuang said that Nvidia\u2019s GB200, which is sold in the United States, can generate AI content 60 times faster than the versions of the company\u2019s chips that it sells to China under export controls.\n\nNvidia counts on billions of dollars of infrastructure spend annually from the largest tech companies in the world for an outsized amount of its revenue. The company has been the biggest beneficiary of the AI boom, with revenue more than doubling in five straight quarters through mid-2024 before growth decelerated slightly.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "S&P 500 ends down as Nvidia tumbles following report",
            "link": "https://www.reuters.com/markets/us/nasdaq-futures-lead-gains-nvidia-results-ease-ai-demand-fears-2025-02-27/",
            "snippet": "The S&P 500 and Nasdaq ended sharply lower on Thursday, weighed down by a slump in chipmaker Nvidia after its quarterly report failed to rekindle Wall...",
            "score": 0.9308909177780151,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Nvidia eyes everyday AI to sustain rapid revenue growth",
            "link": "https://www.ciodive.com/news/nvidia-blackwell-gpu-generative-ai-revenue-growth/741188/",
            "snippet": "The chipmaker saw quarterly revenues nearly double year over year to almost $40 billion as it ramped up production of the Blackwell GPU family.",
            "score": 0.9493369460105896,
            "sentiment": null,
            "probability": null,
            "content": "Listen to the article 4 min This audio is auto-generated. Please let us know if you have feedback\n\nDive Brief:\n\nNvidia is banking on everyday AI usage to maintain high levels of revenue growth as the technology is integrated into everyday processes, Jensen Huang said Wednesday, during the company\u2019s Q4 2025 earnings call.\n\n\"AI has gone mainstream and it's being integrated into every application,\" Huang said. \"AI is used in delivery services everywhere, shopping services everywhere. If you were to buy a quart of milk that's delivered to you, AI was involved.\"\n\nThe GPU chipmaker saw revenues nearly double year over year, growing 78% to $39.3 billion during the three-month period ending Jan. 26. Fiscal year revenues increased 114% to $130.5 billion, as Nvidia ramped up production of the Blackwell GPU family introduced last year.\n\nDive Insight:\n\nNvidia rode a post-ChatGPT wave of generative AI enthusiasm through two years of sustained and rapid growth. Consumption of the GPU giant\u2019s hardware has steadily snowballed since February 2023, when the company reported flat growth on $27 billion in fiscal year revenues.\n\nThe Nvidia name became synonymous with the frenzy and hype surrounding large language model technologies, as its revenues more than quadrupled in 24 months.\n\nA big chunk of the gains came from the largest cloud service providers.\n\nHyperscalers accounted for roughly half of $282 billion spent on data center hardware and software in 2024, a year that saw investments in infrastructure spike by 34% year over year, according to Synergy Research Group. Spending on AI hardware alone was an estimated $120 billion last year, IDC market research found.\n\nNvidia attributed roughly half of $35.6 billion in Q4 data center segment revenue to the large cloud service providers, noting sales had nearly doubled year on year.\n\n\"Large CSPs were some of the first to stand up Blackwell,\" Nvidia EVP and CFO Colette Kress said Wednesday, pointing to deployments in AWS, Azure, Google Cloud and Oracle Cloud Infrastructure to meet \"surging customer demand for AI.\"\n\nEnterprise customers accounted for the other half of data center hardware sales, as model fine-tuning, agentic workflows and GPU-accelerated data processing gained momentum, Kress said.\n\nAWS, Microsoft and Google are each planning to continue high levels of infrastructure spend to build out AI compute capacity this year, a trend that bodes well for the GPU business. But Nvidia is banking on enterprise consumption to drive revenue growth, too, as the company prepares to roll out a new chip configuration \u2014 Blackwell Ultra \u2014 during the second half of the year.\n\nHuang said he expects enterprise and industrial consumption of AI resources to overtake model training in the long term.\n\n\u201cWe have two additional scaling dimensions,\u201d said Huang. \u201cPost-training scaling, where reinforcement learning, fine-tuning and model distillation require orders of magnitude more compute than pretraining alone, [and] inference time scaling and reasoning, where a single query can demand 100x more compute.\u201d",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia Stock: Why Today\u2019s Big Drop Is a Chance to Buy",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-drop-buy-986f5222",
            "snippet": "Nvidia's earnings offered a clear takeaway: The company's growth prospects are as strong as ever. Investors shouldn't miss out on the chance to buy the...",
            "score": 0.5653326511383057,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Earnings Spark Chip, AI Stock Selloff",
            "link": "https://www.investopedia.com/nvidia-earnings-spark-chip-ai-stock-selloff-11687937",
            "snippet": "Stocks slid Thursday, weighed down by chipmaker Nvidia, which led semiconductor and other AI stocks lower.",
            "score": 0.9347658157348633,
            "sentiment": null,
            "probability": null,
            "content": "Stocks slid Thursday, weighed down by chip giant Nvidia (NVDA) in the wake of Wednesday's earnings report, leading semiconductor and other AI stocks lower.\n\nNvidia shares finished Thursday off more than 8%, putting the stock down more than 10% this year. Fellow chipmakers Broadcom (AVGO) and Micron (MU) both fell more than 6%, while the PHLX Semiconductor Index (SOX) dropped 6% itsef.\n\nNvidia\u2019s slump weighed on the major stock indexes broadly. The tech-heavy Nasdaq Composite ended down 2.8%, the S&P 500 was off 1.6%, with seven of its 11 sectors in the red.\n\nNvidia surpassed Street expectations with its fourth-quarter earnings report on Wednesday evening. The company\u2019s data center revenue, a proxy for AI demand, nearly doubled from the same quarter last year. Yet the stock wavered, seesawing between gains and losses in extended trading last night and this morning.\n\nMarket participants on Wednesday afternoon were prepared for a big post-earnings stock move, with options pricing predicting shares would rise or fall 8% by the end of this week.\n\nThursday\u2019s selloff implied that Nvidia\u2019s results were not good enough for investors, as concerns about excessive AI spending and economic conditions have taken the wind out of the AI trade\u2019s sails in recent weeks. Investors have sold off richly-priced AI stocks in the last week, a trend that continued at a rapid clip during Thursday\u2019s selloff.\n\nShares of Super Micro Computer (SMCI), which soared yesterday after the AI server maker narrowly met a deadline to avoid having its stock removed from the Nasdaq, fell 16%. Nuclear power provider Vistra (VST), which rose more than 200% last year, dropped 12% as the broader AI slump overshadowed its own better-than-expected earnings report. Palantir (PLTR), another of Wall Street's favorite AI plays, slid 5%.\n\nThis article has been updated since it was first published to reflect fresh share-price information and context.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Wall Street falls sharply as Nvidia tumbles 8.5% and AI mania falters",
            "link": "https://www.latimes.com/business/story/2025-02-27/wall-street-falls-sharply-as-nvidia-tumbles-8-5-and-ai-mania-falters",
            "snippet": "U.S. stock indexes fell sharply as Wall Street's frenzy around artificial-intelligence technology faltered some more.",
            "score": 0.9697904586791992,
            "sentiment": null,
            "probability": null,
            "content": "The New York Stock Exchange is seen in New York on Wednesday.\n\nU.S. stock indexes fell sharply Thursday as Wall Street\u2019s frenzy around artificial-intelligence technology faltered some more.\n\nThe Standard & Poor\u2019s 500 sank 1.6% for its fifth drop in six days after setting an all-time high last week. Concerns about the U.S. economy\u2019s future have been behind much of the drop, including worries about how tariffs pushed by President Trump could worsen inflation, and Wall Street\u2019s main measure of health has lost all but 1.4% of its rally since election day.\n\nThe Dow Jones industrial average dropped 193 points, or 0.4%, and the Nasdaq composite tumbled 2.8%.\n\nAdvertisement\n\nWeighing most heavily on the market was superstar stock Nvidia, one of Wall Street\u2019s most influential companies that\u2019s been leading the market for years. After initially rising at the open of trading following a better-than-expected profit report, Nvidia quickly slid to a loss of 8.5%.\n\nBetter-than-expected earnings reports have become routine for Nvidia, whose chips are powering the surge into artificial intelligence technology, but this was the company\u2019s first since DeepSeek shook the entire AI industry.\n\nAfter the Chinese upstart said it developed a large language model that can compete with the world\u2019s best without using the most expensive chips, Wall Street had to question all the spending it assumed would go into Nvidia\u2019s chips and the ecosystem that\u2019s built around the AI boom, such as electricity to power large data centers.\n\nAdvertisement\n\nNvidia\u2019s performance for the latest quarter, along with its forecasts for upcoming results, were \u201cgood enough to keep the debate moving in a positive direction,\u201d according to analysts at UBS led by Timothy Arcuri.\n\nBut it apparently wasn\u2019t enough to send Nvidia\u2019s stock higher, particularly given criticism that its price had leaped too high, too quickly. After more than tripling two years ago, Nvidia\u2019s stock more than doubled last year as its sales exploded.\n\nThe market also soured on Salesforce, which fell 4% despite topping analysts\u2019 profit expectations for the latest quarter. Several analysts called the performance solid, and the company continued to tout its AI offerings, but it gave a forecast for upcoming revenue that fell short of expectations.\n\nAdvertisement\n\nOne AI-related company bucking the trend was Snowflake. The AI data cloud company rose 4.5% after delivering stronger profit and revenue for the latest quarter than analysts expected.\n\nIt joined a range of stocks on the more staid end of Wall Street, ones that didn\u2019t grab as many headlines as AI stocks in recent years. Despite the sharp loss for the overall S&P 500, close to 2 out of every 5 stocks in the index climbed.\n\nA 1.7% rise for Berkshire Hathaway, the company run by famed investor Warren Buffett, was one of the strongest upward forces on the index. The owner of Geico, BNSF railroad and other businesses has built a hoard of unused cash recently. That could indicate that Buffett, who\u2019s famous for buying stocks when prices are low, may not see much worth purchasing in a market that critics say looks too expensive.\n\nSome investors have been waiting for other stocks to pick up the market\u2019s leadership baton from Nvidia and the handful of big stocks that dominated for years. Nvidia alone accounted for a little more than 22% of the entire total return for the S&P 500 index last year.\n\nAll told, the S&P 500 fell 94.49 points to 5,861.57 Thursday. The Dow Jones industrial average dropped 193.62 to 43,239.50, and the Nasdaq composite tumbled 530.84 to 18,544.42.\n\nIn the bond market, Treasury yields swung after Trump\u2019s latest announcement on tariffs on his Truth Social website. He said \u201cthe proposed TARIFFS scheduled to go into effect on MARCH FOURTH will, indeed, go into effect, as scheduled\u201d for imports from Canada and Mexico. He also said he would add an additional 10% tariff on Chinese products on that date.\n\nAdvertisement\n\nSuch moves could push up prices for U.S. households when inflation has already proved to be stubborn. Wall Street has been hoping the threats are merely leverage that Trump will use to negotiate with other countries before ultimately inflicting less pain on the economy than feared.\n\nBut even if that proves to be the case, all the talk on tariffs has already gotten U.S. households to feel more nervous about the economy. That\u2019s dangerous because their strong spending has been a main reason the U.S. economy has avoided a recession.\n\nSuch uncertainty also pressures the Federal Reserve, which has few if any tools to help an economy where growth is slowing and inflation is rising at the same time.\n\nJeff Schmid, president of the Federal Reserve Bank of Kansas City, said in a speech Thursday that he has \u201cbecome more cautious\u201d in his hopes that inflation will continue to ease. He also said that discussions with people in his district suggest \u201celevated uncertainty might weigh on growth\u201d for the economy.\n\nFor now, at least, the U.S. economy appears to be in solid shape. The government on Thursday left alone its estimate for the U.S. economy\u2019s performance during the last three months of 2024, though it raised its estimate for a measure of inflation during the quarter.\n\nA separate report said more U.S. workers applied for unemployment benefits last week. Although the number is at a three-month high, it\u2019s still nowhere close to where it\u2019s been in past recessions.\n\nAdvertisement\n\nIn the bond market, the yield on the 10-year Treasury edged up to 4.27% from 4.26%.\n\nIn stock markets abroad, indexes were mixed across Europe and Asia. Germany\u2019s DAX lost 1.1%, while Japan\u2019s Nikkei 225 added 0.3%.\n\nChoe writes for the Associated Press. AP business writers Matt Ott and Elaine Kurtenbach contributed to this report.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "March Into Gaming With GeForce NOW\u2019s 14 Must-Play Titles for Spring",
            "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-march-2025/",
            "snippet": "14 games joining GeForce NOW in March, including 'Assassin's Creed Shadows', 'Split Fiction', 'inZoi' and more.",
            "score": 0.8497602343559265,
            "sentiment": null,
            "probability": null,
            "content": "GeForce NOW is blooming further with an array of 14 new titles in March.\n\nA garden of gaming delights will have members marching straight into action and adventure this spring, with Ubisoft\u2019s Assassin\u2019s Creed Shadows, Tripwire Interactive\u2019s Killing Floor 3 and Hazelight Studio\u2019s Split Fiction coming to the cloud next week at launch.\n\nStart off with six games coming to the cloud this week, including Halo: The Master Chief Collection. And don\u2019t miss out on the latest update for miHoYo\u2019s hit game Honkai: Star Rail.\n\nGeForce NOW one-month and six-month Performance and Ultimate memberships are available for new members to purchase again in the US, Canada and Europe, as well as the free tier. Stay tuned for updates as more membership options become available.\n\nSplit Adventure\n\nHazelight Studios, creators of the acclaimed It Takes Two, are returning with a new and imaginative co-op game: Split Fiction. The narrative-driven adventure follows Mio, a writer of science fiction, and Zoe, a fantasy author, who find themselves trapped in a simulation stealing their stories. Adventure across wildly shifting worlds, ranging from the dazzling, neon-drenched landscapes of futuristic cyberpunk cities to the enchanted, dragon-populated depths of ancient forests.\n\nEach level introduces fresh abilities, mini games and chaotic scenarios. Tame dragons, snowboard through explosions, master laser swords, solve gravity-defying puzzles, fight robotic parking attendants, outdance monkeys and flee supernovas while building trust between strangers. Play online or in couch co-op mode, using innovative split-screen mechanics that demand genuine teamwork and optimal communication between players to succeed. The Friend\u2019s Pass feature lets one player host the full game while their partner joins for free.\n\nAdventure through the genre-bending worlds of Split Fiction across devices with a GeForce NOW membership. With cloud saves and instant access, members never have to miss a moment to dive in whenever their gaming buddy\u2019s available.\n\nThe Legend Lives On\n\nHalo: The Master Chief Collection comprises six legendary games for members to play \u2014 Halo: Reach, Halo: Combat Evolved Anniversary, Halo 2: Anniversary, Halo 3, Halo 3: ODST and Halo 4. Dive into more than 60 adrenaline-fueled campaign missions, from the explosive beginnings of the Halo saga to the high-stakes battles that shaped its epic legacy.\n\nWith breathtaking visuals, fast gameplay and stunning multiplayer arenas, this collection delivers heart-pounding action and unforgettable thrills. Whether a seasoned Spartan or new to the fight, Halo: The Master Chief Collection will keep players on the edges of their seats.\n\nGear up for an exceptional gaming experience with these upcoming titles and more. Ultimate members can stream in stunning 4K resolution at over 120 frames per second, powered by NVIDIA DLSS and Reflex technologies \u2014 delivering seamless gameplay on virtually any device.\n\nNew to Town\n\nThe newest Honkai: Star Rail update, \u201cLight Slips the Gate, Shadow Greets the Throne,\u201d is now available for members to stream in the cloud. Version 3.1 introduces a new chapter in the Flame-Chase Journey, playable characters and more. Explore the \u201cGrove of Epiphany,\u201d a new map ravaged by the black tide, where players must rescue survivors and reclaim Cerces\u2019 Coreflame.\n\nTwo new playable characters are Tribbie, a five-star Quantum character on the Path of Harmony, and Mydei, a five-star Imaginary character on the Path of Destruction. The update also includes the return of limited five-star characters Yunli and Huohuo, a new season of the Divergent Universe game mode with a \u201cDay and Night System,\u201d as well as the Awooo Firm event where players manage a chimera squad in Okhema.\n\nLook for the following games available to stream in the cloud this week:\n\nBalatro (New release on Xbox, available on PC Game Pass)\n\n(New release on Xbox, available on PC Game Pass) The Dark Crystal: Age of Resistance Tactics (Xbox, available on the Microsoft Store)\n\n(Xbox, available on the Microsoft Store) The Evil Within 2 (Epic Games Store, Steam, and Xbox available on PC Game Pass)\n\n(Epic Games Store, Steam, and Xbox available on PC Game Pass) Halo: The Master Chief Collection (Steam and Xbox, available on PC Game Pass)\n\n(Steam and Xbox, available on PC Game Pass) Murky Divers (Steam)\n\n(Steam) Somerville (Xbox, available on the Microsoft Store)\n\nHere\u2019s what to expect for March:\n\nDragonkin: The Banished (New release on Steam Mar. 6)\n\n(New release on Steam Mar. 6) Split Fiction (New release on EA App and Steam, Mar. 6)\n\n(New release on EA App and Steam, Mar. 6) Split Fiction: Friend\u2019s Pass (New release on EA App and Steam, Mar. 6 App)\n\n(New release on EA App and Steam, Mar. 6 App) Assassin\u2019s Creed Shadows (New release on Steam and Ubisoft Connect, Mar. 20)\n\n(New release on Steam and Ubisoft Connect, Mar. 20) Wreckfest 2 (New release on Steam, Mar. 20)\n\n(New release on Steam, Mar. 20) Killing Floor 3 (New Release on Steam, Mar. 25)\n\n(New Release on Steam, Mar. 25) Atomfall (New release on Steam and Xbox available on PC Game Pass, Mar. 27)\n\n(New release on Steam and Xbox available on PC Game Pass, Mar. 27) The First Berserker: Khazan (New Release on Steam, Mar. 27\n\n(New Release on Steam, Mar. 27 inZOI (New release on Steam, Mar. 28)\n\n(New release on Steam, Mar. 28) City Transport Simulator: Tram (Steam)\n\n(Steam) Dave the Diver (Steam)\n\n(Steam) The Legend of Heroes: Trails through Daybreak II (Steam)\n\n(Steam) Motor Town: Behind The Wheel (Steam)\n\n(Steam) Potion Craft: Alchemist Simulator (Steam)\n\nFull of Games in February\n\nIn addition to the 17 games announced last month, six more joined the GeForce NOW library, including Halo: The Master Chief Collection being added this week:\n\nUNDER NIGHT IN-BIRTH II Sys:Celes didn\u2019t make it to the cloud this month. Stay tuned to GFN Thursday for updates.\n\nWhat are you planning to play this weekend? Let us know on X or in the comments below.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-26": {
        "0": {
            "title": "Nvidia posted another strong quarterly report. What to know, by the numbers",
            "link": "https://apnews.com/article/nvidia-chips-revenue-deepseek-apple-1063d50f560f48c04fb97539d566c46e",
            "snippet": "Wall Street once again fixated on an earnings report from Nvidia, the main player in the artificial intelligence mania. The chipmaker reported earnings...",
            "score": 0.9216167330741882,
            "sentiment": null,
            "probability": null,
            "content": "Wall Street once again fixated on an earnings report from Nvidia, the main player in the artificial intelligence mania. The chipmaker reported earnings after the bell Wednesday that topped analyst forecasts, and shares added 2% in after-hours trading, perhaps indicating that Wall Street quietly hoped for even better results.\n\nThis is Nvidia\u2019s first earnings report since a Chinese upstart, DeepSeek, upended the artificial-intelligence industry by saying it has developed a large language model that can compete with big U.S. rivals without having to use the most expensive chips. Nvidia\u2019s market value saw a one-day drop of nearly $600 billion after the news came out.\n\nHere\u2019s a look at Nvidia, by the numbers.\n\n$3.215 trillion\n\nNvidia\u2019s total market value as of the close Wednesday, before the release of its earnings report. That\u2019s second in the S&P 500 behind Apple ($3.611 trillion). Microsoft is third with a market value just below $3 trillion ($2.972 trillion). Two years ago, Nvidia\u2019s market value was below $600 billion.\n\n-17%\n\nThe one-day drop in Nvidia shares on Jan. 27 after DeepSeek\u2019s revelation called into question all the spending Wall Street had assumed would go into not only Nvidia\u2019s chips but also the ecosystem that\u2019s built around the AI boom. The stock rose more than 170% in 2024.\n\n$39.33 billion\n\nNvidia\u2019s revenue for the fourth quarter, easily topping Wall Street\u2019s estimate and up 78% from last year\u2019s fourth quarter.\n\n$130.5 billion\n\nNvidia\u2019s revenue for the fiscal year that ended in January 2025. That was more than double its revenue for fiscal 2024 and more than four times its receipts the year before that.\n\n22%\n\nThat\u2019s approximately how much of the S&P 500\u2019s gain for last year could be attributed to Nvidia alone, according to S&P Dow Jones Indices. By comparison, Amazon accounted for about 6% of the gain. Nvidia replaced Intel in the Dow Jones Industrial Average in November.\n\n649 billion\n\nThat\u2019s how many dozen eggs one could buy with Nvidia\u2019s market value of $3.215 trillion, using the average price of $4.95 per dozen for January from the Federal Reserve. Egg prices have spiked due to an outbreak of bird flu, and supplies of eggs have run short in some areas.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "NVIDIA Announces Financial Results for Fourth Quarter and Fiscal 2025",
            "link": "https://www.globenewswire.com/news-release/2025/02/26/3033384/0/en/NVIDIA-Announces-Financial-Results-for-Fourth-Quarter-and-Fiscal-2025.html",
            "snippet": "Record quarterly revenue of $39.3 billion, up 12% from Q3 and up 78% from a year agoRecord quarterly Data Center revenue of $35.6 billion, up 16% from Q3...",
            "score": 0.6865065693855286,
            "sentiment": null,
            "probability": null,
            "content": "Record quarterly revenue of $39.3 billion, up 12% from Q3 and up 78% from a year ago\n\nRecord quarterly Data Center revenue of $35.6 billion, up 16% from Q3 and up 93% from a year ago\n\nRecord full-year revenue of $130.5 billion, up 114%\n\n\n\nSANTA CLARA, Calif., Feb. 26, 2025 (GLOBE NEWSWIRE) -- NVIDIA (NASDAQ: NVDA) today reported revenue for the fourth quarter ended January 26, 2025, of $39.3 billion, up 12% from the previous quarter and up 78% from a year ago.\n\nFor the quarter, GAAP earnings per diluted share was $0.89, up 14% from the previous quarter and up 82% from a year ago. Non-GAAP earnings per diluted share was $0.89, up 10% from the previous quarter and up 71% from a year ago.\n\nFor fiscal 2025, revenue was $130.5 billion, up 114% from a year ago. GAAP earnings per diluted share was $2.94, up 147% from a year ago. Non-GAAP earnings per diluted share was $2.99, up 130% from a year ago.\n\n\u201cDemand for Blackwell is amazing as reasoning AI adds another scaling law \u2014 increasing compute for training makes models smarter and increasing compute for long thinking makes the answer smarter,\u201d said Jensen Huang, founder and CEO of NVIDIA.\n\n\u201cWe\u2019ve successfully ramped up the massive-scale production of Blackwell AI supercomputers, achieving billions of dollars in sales in its first quarter. AI is advancing at light speed as agentic AI and physical AI set the stage for the next wave of AI to revolutionize the largest industries.\u201d\n\nNVIDIA will pay its next quarterly cash dividend of $0.01 per share on April 2, 2025, to all shareholders of record on March 12, 2025.\n\nQ4 Fiscal 2025 Summary\n\nGAAP ($ in millions, except earnings\n\nper share) Q4 FY25 Q3 FY25 Q4 FY24 Q/Q Y/Y Revenue $39,331 $35,082 $22,103 Up 12% Up 78% Gross margin 73.0% 74.6% 76.0% Down 1.6 pts Down 3.0 pts Operating expenses $4,689 $4,287 $3,176 Up 9% Up 48% Operating income $24,034 $21,869 $13,615 Up 10% Up 77% Net income $22,091 $19,309 $12,285 Up 14% Up 80% Diluted earnings per share* $0.89 $0.78 $0.49 Up 14% Up 82%\n\n\n\n\n\nNon-GAAP ($ in millions, except earnings\n\nper share) Q4 FY25 Q3 FY25 Q4 FY24 Q/Q Y/Y Revenue $39,331 $35,082 $22,103 Up 12% Up 78% Gross margin 73.5% 75.0% 76.7% Down 1.5 pts Down 3.2 pts Operating expenses $3,378 $3,046 $2,210 Up 11% Up 53% Operating income $25,516 $23,276 $14,749 Up 10% Up 73% Net income $22,066 $20,010 $12,839 Up 10% Up 72% Diluted earnings per share* $0.89 $0.81 $0.52 Up 10% Up 71%\n\n\n\nFiscal 2025 Summary\n\nGAAP ($ in millions, except earnings\n\nper share) FY25 FY24 Y/Y Revenue $130,497 $60,922 Up 114% Gross margin 75.0% 72.7% Up 2.3 pts Operating expenses $16,405 $11,329 Up 45% Operating income $81,453 $32,972 Up 147% Net income $72,880 $29,760 Up 145% Diluted earnings per share* $2.94 $1.19 Up 147%\n\n\n\n\n\nNon-GAAP ($ in millions, except earnings\n\nper share) FY25 FY24 Y/Y Revenue $130,497 $60,922 Up 114% Gross margin 75.5% 73.8% Up 1.7 pts Operating expenses $11,716 $7,825 Up 50% Operating income $86,789 $37,134 Up 134% Net income $74,265 $32,312 Up 130% Diluted earnings per share* $2.99 $1.30 Up 130%\n\n*All per share amounts presented herein have been retroactively adjusted to reflect the ten-for-one stock split, which was effective June 7, 2024.\n\nOutlook\n\nNVIDIA\u2019s outlook for the first quarter of fiscal 2026 is as follows:\n\nRevenue is expected to be $43.0 billion, plus or minus 2%.\n\nGAAP and non-GAAP gross margins are expected to be 70.6% and 71.0%, respectively, plus or minus 50 basis points.\n\nGAAP and non-GAAP operating expenses are expected to be approximately $5.2 billion and $3.6 billion, respectively.\n\nGAAP and non-GAAP other income and expense are expected to be an income of approximately $400 million, excluding gains and losses from non-marketable and publicly-held equity securities.\n\nGAAP and non-GAAP tax rates are expected to be 17.0%, plus or minus 1%, excluding any discrete items.\n\n\n\nHighlights\n\nNVIDIA achieved progress since its previous earnings announcement in these areas:\n\nData Center\n\nGaming and AI PC\n\nFourth-quarter Gaming revenue was $2.5 billion, down 22% from the previous quarter and down 11% from a year ago. Full-year revenue rose 9% to $11.4 billion.\n\nAnnounced new GeForce RTX\u2122 50 Series graphics cards and laptops powered by the NVIDIA Blackwell architecture, delivering breakthroughs in AI-driven rendering to gamers, creators and developers.\n\nLaunched GeForce RTX 5090 and 5080 graphics cards, delivering up to a 2x performance improvement over the prior generation.\n\nIntroduced NVIDIA DLSS 4 with Multi Frame Generation and image quality enhancements, with 75 games and apps supporting it at launch, and unveiled NVIDIA Reflex 2 technology, which can reduce PC latency by up to 75%.\n\nUnveiled NVIDIA NIM microservices, AI Blueprints and the Llama Nemotron family of open models for RTX AI PCs to help developers and enthusiasts build AI agents and creative workflows.\n\n\n\nProfessional Visualization\n\nFourth-quarter revenue was $511 million, up 5% from the previous quarter and up 10% from a year ago. Full-year revenue rose 21% to $1.9 billion.\n\nUnveiled NVIDIA Project DIGITS, a personal AI supercomputer that provides AI researchers, data scientists and students worldwide with access to the power of the NVIDIA Grace\u2122 Blackwell platform.\n\nAnnounced generative AI models and blueprints that expand NVIDIA Omniverse\u2122 integration further into physical AI applications, including robotics, autonomous vehicles and vision AI.\n\nIntroduced NVIDIA Media2, an AI-powered initiative transforming content creation, streaming and live media experiences, built on NIM and AI Blueprints.\n\nAutomotive and Robotics\n\nFourth-quarter Automotive revenue was $570 million, up 27% from the previous quarter and up 103% from a year ago. Full-year revenue rose 55% to $1.7 billion.\n\nAnnounced that Toyota, the world\u2019s largest automaker, will build its next-generation vehicles on NVIDIA DRIVE AGX Orin\u2122 running the safety-certified NVIDIA DriveOS operating system.\n\nPartnered with Hyundai Motor Group to create safer, smarter vehicles, supercharge manufacturing and deploy cutting-edge robotics with NVIDIA AI and NVIDIA Omniverse.\n\nAnnounced that the NVIDIA DriveOS safe autonomous driving operating system received ASIL-D functional safety certification and launched the NVIDIA DRIVE\u2122 AI Systems Inspection Lab.\n\nLaunched NVIDIA Cosmos\u2122, a platform comprising state-of-the-art generative world foundation models, to accelerate physical AI development, with adoption by leading robotics and automotive companies 1X, Agile Robots, Waabi, Uber and others.\n\nUnveiled the NVIDIA Jetson Orin Nano\u2122 Super, which delivers up to a 1.7x gain in generative AI performance.\n\nCFO Commentary\n\nCommentary on the quarter by Colette Kress, NVIDIA\u2019s executive vice president and chief financial officer, is available at https://investor.nvidia.com.\n\nConference Call and Webcast Information\n\nNVIDIA will conduct a conference call with analysts and investors to discuss its fourth quarter and fiscal 2025 financial results and current financial prospects today at 2 p.m. Pacific time (5 p.m. Eastern time). A live webcast (listen-only mode) of the conference call will be accessible at NVIDIA\u2019s investor relations website, https://investor.nvidia.com. The webcast will be recorded and available for replay until NVIDIA\u2019s conference call to discuss its financial results for its first quarter of fiscal 2026.\n\nNon-GAAP Measures\n\nTo supplement NVIDIA\u2019s condensed consolidated financial statements presented in accordance with GAAP, the company uses non-GAAP measures of certain components of financial performance. These non-GAAP measures include non-GAAP gross profit, non-GAAP gross margin, non-GAAP operating expenses, non-GAAP operating income, non-GAAP other income (expense), net, non-GAAP net income, non-GAAP net income, or earnings, per diluted share, and free cash flow. For NVIDIA\u2019s investors to be better able to compare its current results with those of previous periods, the company has shown a reconciliation of GAAP to non-GAAP financial measures. These reconciliations adjust the related GAAP financial measures to exclude stock-based compensation expense, acquisition-related and other costs, other, gains from non-marketable and publicly-held equity securities, net, interest expense related to amortization of debt discount, and the associated tax impact of these items where applicable. Free cash flow is calculated as GAAP net cash provided by operating activities less both purchases related to property and equipment and intangible assets and principal payments on property and equipment and intangible assets. NVIDIA believes the presentation of its non-GAAP financial measures enhances the user\u2019s overall understanding of the company\u2019s historical financial performance. The presentation of the company\u2019s non-GAAP financial measures is not meant to be considered in isolation or as a substitute for the company\u2019s financial results prepared in accordance with GAAP, and the company\u2019s non-GAAP measures may be different from non-GAAP measures used by other companies.\n\n\n\n\n\nNVIDIA CORPORATION CONDENSED CONSOLIDATED STATEMENTS OF INCOME (In millions, except per share data) (Unaudited) Three Months Ended Twelve Months Ended January 26, January 28, January 26, January 28, 2025 2024 2025 2024 Revenue $ 39,331 $ 22,103 $ 130,497 $ 60,922 Cost of revenue 10,608 5,312 32,639 16,621 Gross profit 28,723 16,791 97,858 44,301 Operating expenses Research and development 3,714 2,465 12,914 8,675 Sales, general and administrative 975 711 3,491 2,654 Total operating expenses 4,689 3,176 16,405 11,329 Operating income 24,034 13,615 81,453 32,972 Interest income 511 294 1,786 866 Interest expense (61 ) (63 ) (247 ) (257 ) Other, net 733 260 1,034 237 Other income (expense), net 1,183 491 2,573 846 Income before income tax 25,217 14,106 84,026 33,818 Income tax expense 3,126 1,821 11,146 4,058 Net income $ 22,091 $ 12,285 $ 72,880 $ 29,760 Net income per share: Basic $ 0.90 $ 0.51 $ 2.97 $ 1.21 Diluted $ 0.89 $ 0.49 $ 2.94 $ 1.19 Weighted average shares used in per share computation: Basic 24,489 24,660 24,555 24,690 Diluted 24,706 24,900 24,804 24,940\n\n\n\n\n\nNVIDIA CORPORATION CONDENSED CONSOLIDATED BALANCE SHEETS (In millions) (Unaudited) January 26, January 28, 2025 2024 ASSETS Current assets: Cash, cash equivalents and marketable securities $ 43,210 $ 25,984 Accounts receivable, net 23,065 9,999 Inventories 10,080 5,282 Prepaid expenses and other current assets 3,771 3,080 Total current assets 80,126 44,345 Property and equipment, net 6,283 3,914 Operating lease assets 1,793 1,346 Goodwill 5,188 4,430 Intangible assets, net 807 1,112 Deferred income tax assets 10,979 6,081 Other assets 6,425 4,500 Total assets $ 111,601 $ 65,728 LIABILITIES AND SHAREHOLDERS\u2019 EQUITY Current liabilities: Accounts payable $ 6,310 $ 2,699 Accrued and other current liabilities 11,737 6,682 Short-term debt - 1,250 Total current liabilities 18,047 10,631 Long-term debt 8,463 8,459 Long-term operating lease liabilities 1,519 1,119 Other long-term liabilities 4,245 2,541 Total liabilities 32,274 22,750 Shareholders\u2019 equity 79,327 42,978 Total liabilities and shareholders\u2019 equity $ 111,601 $ 65,728\n\n\n\n\n\nNVIDIA CORPORATION CONDENSED CONSOLIDATED STATEMENTS OF CASH FLOWS (In millions) (Unaudited) Three Months Ended Twelve Months Ended January 26, January 28, January 26, January 28, 2025 2024 2025 2024 Cash flows from operating activities: Net income $ 22,091 $ 12,285 $ 72,880 $ 29,760 Adjustments to reconcile net income to net cash provided by operating activities: Stock-based compensation expense 1,321 993 4,737 3,549 Depreciation and amortization 543 387 1,864 1,508 Deferred income taxes (598 ) (78 ) (4,477 ) (2,489 ) Gains on non-marketable equity securities and publicly-held equity securities, net (727 ) (260 ) (1,030 ) (238 ) Other (138 ) (109 ) (502 ) (278 ) Changes in operating assets and liabilities, net of acquisitions: Accounts receivable (5,370 ) (1,690 ) (13,063 ) (6,172 ) Inventories (2,424 ) (503 ) (4,781 ) (98 ) Prepaid expenses and other assets 331 (1,184 ) (395 ) (1,522 ) Accounts payable 867 281 3,357 1,531 Accrued and other current liabilities 360 1,072 4,278 2,025 Other long-term liabilities 372 305 1,221 514 Net cash provided by operating activities 16,628 11,499 64,089 28,090 Cash flows from investing activities: Proceeds from maturities of marketable securities 1,710 1,731 11,195 9,732 Proceeds from sales of marketable securities 177 50 495 50 Proceeds from sales of non-marketable equity securities - - 171 1 Purchases of marketable securities (7,010 ) (7,524 ) (26,575 ) (18,211 ) Purchase related to property and equipment and intangible assets (1,077 ) (253 ) (3,236 ) (1,069 ) Purchases of non-marketable equity securities (478 ) (113 ) (1,486 ) (862 ) Acquisitions, net of cash acquired (542 ) - (1,007 ) (83 ) Other 22 - 22 (124 ) Net cash used in investing activities (7,198 ) (6,109 ) (20,421 ) (10,566 ) Cash flows from financing activities: Proceeds related to employee stock plans - - 490 403 Payments related to repurchases of common stock (7,810 ) (2,660 ) (33,706 ) (9,533 ) Payments related to tax on restricted stock units (1,861 ) (841 ) (6,930 ) (2,783 ) Repayment of debt - - (1,250 ) (1,250 ) Dividends paid (245 ) (99 ) (834 ) (395 ) Principal payments on property and equipment and intangible assets (32 ) (29 ) (129 ) (74 ) Other - - - (1 ) Net cash used in financing activities (9,948 ) (3,629 ) (42,359 ) (13,633 ) Change in cash, cash equivalents, and restricted cash (518 ) 1,761 1,309 3,891 Cash, cash equivalents, and restricted cash at beginning of period 9,107 5,519 7,280 3,389 Cash, cash equivalents, and restricted cash at end of period $ 8,589 $ 7,280 $ 8,589 $ 7,280 Supplemental disclosures of cash flow information: Cash paid for income taxes, net $ 4,129 $ 1,874 $ 15,118 $ 6,549 Cash paid for interest $ 22 $ 26 $ 246 $ 252\n\n\n\n\n\nNVIDIA CORPORATION RECONCILIATION OF GAAP TO NON-GAAP FINANCIAL MEASURES (In millions, except per share data) (Unaudited) Three Months Ended Twelve Months Ended January 26, October 27, January 28, January 26, January 28, 2025 2024 2024 2025 2024 GAAP cost of revenue $ 10,608 $ 8,926 $ 5,312 $ 32,639 $ 16,621 GAAP gross profit $ 28,723 $ 26,156 $ 16,791 $ 97,858 $ 44,301 GAAP gross margin 73.0 % 74.6 % 76.0 % 75.0 % 72.7 % Acquisition-related and other costs (A) 118 116 119 472 477 Stock-based compensation expense (B) 53 50 45 178 141 Other (C) - - 4 (3 ) 40 Non-GAAP cost of revenue $ 10,437 $ 8,759 $ 5,144 $ 31,992 $ 15,963 Non-GAAP gross profit $ 28,894 $ 26,322 $ 16,959 $ 98,505 $ 44,959 Non-GAAP gross margin 73.5 % 75.0 % 76.7 % 75.5 % 73.8 % GAAP operating expenses $ 4,689 $ 4,287 $ 3,176 $ 16,405 $ 11,329 Stock-based compensation expense (B) (1,268 ) (1,202 ) (948 ) (4,559 ) (3,408 ) Acquisition-related and other costs (A) (43 ) (39 ) (18 ) (130 ) (106 ) Other (C) - - - - 10 Non-GAAP operating expenses $ 3,378 $ 3,046 $ 2,210 $ 11,716 $ 7,825 GAAP operating income $ 24,034 $ 21,869 $ 13,615 $ 81,453 $ 32,972 Total impact of non-GAAP adjustments to operating income 1,482 1,407 1,134 5,336 4,162 Non-GAAP operating income $ 25,516 $ 23,276 $ 14,749 $ 86,789 $ 37,134 GAAP other income (expense), net $ 1,183 $ 447 $ 491 $ 2,573 $ 846 Gains from non-marketable equity securities and publicly-held equity securities, net (727 ) (37 ) (260 ) (1,030 ) (238 ) Interest expense related to amortization of debt discount 1 1 1 4 4 Non-GAAP other income (expense), net $ 457 $ 411 $ 232 $ 1,547 $ 612 GAAP net income $ 22,091 $ 19,309 $ 12,285 $ 72,880 $ 29,760 Total pre-tax impact of non-GAAP adjustments 756 1,371 875 4,310 3,928 Income tax impact of non-GAAP adjustments (D) (781 ) (670 ) (321 ) (2,925 ) (1,376 ) Non-GAAP net income $ 22,066 $ 20,010 $ 12,839 $ 74,265 $ 32,312 Diluted net income per share (E) GAAP $ 0.89 $ 0.78 $ 0.49 $ 2.94 $ 1.19 Non-GAAP $ 0.89 $ 0.81 $ 0.52 $ 2.99 $ 1.30 Weighted average shares used in diluted net income per share computation (E) 24,706 24,774 24,900 24,804 24,936 GAAP net cash provided by operating activities $ 16,628 $ 17,629 $ 11,499 $ 64,089 $ 28,090 Purchases related to property and equipment and intangible assets (1,077 ) (813 ) (253 ) (3,236 ) (1,069 ) Principal payments on property and equipment and intangible assets (32 ) (29 ) (29 ) (129 ) (74 ) Free cash flow $ 15,519 $ 16,787 $ 11,217 $ 60,724 $ 26,947 (A) Acquisition-related and other costs are comprised of amortization of intangible assets, transaction costs, and certain compensation charges and are included in the following line items: Three Months Ended Twelve Months Ended January 26, October 27, January 28, January 26, January 28, 2025 2024 2024 2025 2024 Cost of revenue $ 118 $ 116 $ 119 $ 472 $ 477 Research and development $ 27 $ 23 $ 12 $ 79 $ 49 Sales, general and administrative $ 16 $ 16 $ 6 $ 51 $ 57 (B) Stock-based compensation consists of the following: Three Months Ended Twelve Months Ended January 26, October 27, January 28, January 26, January 28, 2025 2024 2024 2025 2024 Cost of revenue $ 53 $ 50 $ 45 $ 178 $ 141 Research and development $ 955 $ 910 $ 706 $ 3,423 $ 2,532 Sales, general and administrative $ 313 $ 292 $ 242 $ 1,136 $ 876 (C) Other consists of IP-related costs and assets held for sale related adjustments\n\n(D) Income tax impact of non-GAAP adjustments, including the recognition of excess tax benefits or deficiencies related to stock-based compensation under GAAP accounting standard (ASU 2016-09). (E) Reflects a ten-for-one stock split on June 7, 2024\n\n\n\n\n\nNVIDIA CORPORATION RECONCILIATION OF GAAP TO NON-GAAP OUTLOOK Q1 FY2026 Outlook ($ in millions) GAAP gross margin 70.6 % Impact of stock-based compensation expense, acquisition-related costs, and other costs 0.4 % Non-GAAP gross margin 71.0 % GAAP operating expenses $ 5,150 Stock-based compensation expense, acquisition-related costs, and other costs (1,550 ) Non-GAAP operating expenses $ 3,600\n\nAbout NVIDIA\n\nNVIDIA (NASDAQ: NVDA) is the world leader in accelerated computing.\n\n\n\nFor further information, contact: Stewart Stecker Mylene Mangalindan Investor Relations Corporate Communications NVIDIA Corporation NVIDIA Corporation sstecker@nvidia.com mmangalindan@nvidia.com\n\n\n\nCertain statements in this press release including, but not limited to, statements as to: AI advancing at light speed as agentic AI and physical AI set the stage for the next wave of AI to revolutionize the largest industries; expectations with respect to growth, performance and benefits of NVIDIA\u2019s products, services and technologies, including Blackwell, and related trends and drivers; expectations with respect to supply and demand for NVIDIA\u2019s products, services and technologies, including Blackwell, and related matters including inventory, production and distribution; expectations with respect to NVIDIA\u2019s third party arrangements, including with its collaborators and partners; expectations with respect to technology developments and related trends and drivers; future NVIDIA cash dividends or other returns to stockholders; NVIDIA\u2019s financial and business outlook for the first quarter of fiscal 2026 and beyond; projected market growth and trends; expectations with respect to AI and related industries; and other statements that are not historical facts are risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic and political conditions; NVIDIA\u2019s reliance on third parties to manufacture, assemble, package and test NVIDIA\u2019s products; the impact of technological development and competition; development of new products and technologies or enhancements to NVIDIA\u2019s existing product and technologies; market acceptance of NVIDIA\u2019s products or NVIDIA\u2019s partners\u2019 products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of NVIDIA\u2019s products or technologies when integrated into systems; and changes in applicable laws and regulations, as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company\u2019s website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances.\n\n\u00a9 2025 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, GeForce RTX, NVIDIA Cosmos, NVIDIA Spectrum-X, NVIDIA DGX, NVIDIA DRIVE, NVIDIA DRIVE AGX Orin, NVIDIA Grace, NVIDIA Jetson Orin Nano, NVIDIA NIM and NVIDIA Omniverse are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and/or other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice.\n\nA photo accompanying this announcement is available at https://www.globenewswire.com/NewsRoom/AttachmentNg/aabe86db-ce89-4434-b83c-495082979801",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia sales grow 78% on AI demand, company gives strong guidance",
            "link": "https://www.cnbc.com/2025/02/26/nvidia-nvda-earnings-report-q4-2025.html",
            "snippet": "Nvidia reported fourth-quarter earnings that beat Wall Street expectations. The company also provided strong guidance for the current quarter.",
            "score": 0.9518258571624756,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang presents the Nvidia Blackwell platform at an event ahead of the Computex forum, in Taipei, Taiwan, on June 2, 2024.\n\nNvidia reported fiscal fourth-quarter earnings after the bell on Wednesday that beat Wall Street expectations. The company also provided strong guidance for the current quarter.\n\nThe company's report and guidance signals that the chipmaker is confident it will be able to continue its historic run of growth driven by artificial intelligence well into 2025. Shares were flat in extended trading.\n\nHere's how the company did, compared with estimates from analysts polled by LSEG:\n\nRevenue : $39.33 billion vs. $38.05 billion estimated\n\n: $39.33 billion vs. $38.05 billion estimated Earnings per share: 89 cents adjusted vs. 84 cents estimated\n\nNvidia said it expected about $43 billion in first-quarter revenue, plus or minus 2%, versus $41.78 billion expected per LSEG estimates. The first-quarter forecast implies year-to-year growth of about 65% from a year earlier, a slowdown from 262% annual growth in the same period a year prior.\n\nChief Financial Officer Colette Kress said the company expects \"a significant ramp\" of sales of Blackwell, its next-generation AI chip, in the first quarter.\n\nNet income during the quarter rose to $22.09 billion, or 89 cents per diluted share, versus $12.29 billion, or 49 cents per share, in the year-ago period.\n\nNvidia reported a 73% gross margin in the quarter, which was down three points on an annual basis. The company said the decline in gross margin was due to newer data center products that were more complicated and expensive.\n\nRevenue continues to surge at Nvidia as the company rides the AI boom with its data center graphics processing units, or GPUs, which comprise the vast majority of the market for AI accelerators. Nvidia's revenue in the quarter rose 78% from $35.1 billion, and full fiscal-year revenue for Nvidia rose 114% to $130.5 billion.\n\nHowever, Nvidia's growth is slowing as the company becomes larger. During the fourth-quarter of fiscal 2024, Nvidia sales more than tripled.\n\nMuch of the focus this calendar year is on how quickly the company can ship its next-generation AI processors, called Blackwell.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia Revenue Spikes as Agentic AI Aims to Transform Businesses",
            "link": "https://www.pymnts.com/earnings/2025/nvidia-ceo-q4-revenue-up-80-yoy-as-agentic-ai-aims-to-transform-businesses/",
            "snippet": "Nvidia CEO Jensen Huang said Wednesday (Feb. 26) that sales of the company's most advanced chip architecture hit a record in the fourth quarter,...",
            "score": 0.945234477519989,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang said Wednesday (Feb. 26) that sales of the company\u2019s most advanced chip architecture hit a record in the fourth quarter, and it is a harbinger of even more demand ahead because the artificial intelligence (AI) era has just begun.\n\n\u201cAI is advancing at light speed,\u201d Huang said in a conference call with Wall Street analysts to discuss earnings. \u201cWe\u2019re just at the start of the age of AI.\u201d\n\nThe architecture for the Blackwell GPU, which was delayed for two months due to technical issues, booked $11 billion in revenue just in one quarter, with large cloud companies leading the sales. Data center revenues more than doubled, driven by demand for Blackwell and its H200 chip series.\n\nHeading into Nvidia\u2019s earnings, investors were concerned that the low-cost, high-performing AI models developed by Chinese AI startup DeepSeek would lead to weaker Nvidia chip demand. DeepSeek\u2019s V3 model only cost $5.6 million to train using 2,000 slower H800 Nvidia chips. In contrast, OpenAI\u2019s GPT-4 cost $100 million to train on 25,000 H100 Nvidia GPUs.\n\nBut Huang said that on the contrary, DeepSeek open-sourced its high-performing reasoning model, which put it in the hands of many startups and other users. This expands the market for AI workloads, and by extension, GPUs.\n\nAdded Brian Colello, Morningstar analyst, in a recent research note: Despite DeepSeek\u2019s offering a less expensive solution, \u201cAI GPU demand still exceeds supply, so while slimmer models may enable greater development for the same number of chips, we still think tech firms will continue to buy all the GPUs they can as part of this AI \u2018gold rush.\u2019\u201d\n\nFrom Consumer AI to Business AI\n\nHuang said the world is just two years into the current wave of AI advancements, starting with generative AI, which powered consumer use of AI, and now moving into AI agents, which will power business use of AI. Following these two trends, he sees the next stage is physical AI like robots.\n\n\u201cAI has gone mainstream\u201d and one day it will be embedded in all industries, Huang believes.\n\nMeanwhile, AI is already ushering in a new wave in software development. In the past, software ran on CPUs, but in the age of AI, software would run on machine learning, Huang said. \u201cThis is the future of software,\u201d he said.\n\nFurther, data centers will become \u201cAI factories,\u201d since they will be processing AI workloads that will power most industries in the world, Huang believes.\n\n\u201cWe have a fairly good line of sight of the amount of capital investment that data centers are building out towards,\u201d Huang said. \u201cWe know that going forward, the vast majority of software is going to be based on machine learning, and so accelerated computing and generative AI and reasoning AI are going to be the type of architecture you\u2019d want in your data center.\u201d\n\nAsked about headwinds from U.S. export controls of its most advanced chips to China, the company\u2019s second-largest market, Nvidia officials said sales of its chips to China are about half from what they were before export restrictions. But as a percentage of revenue, it has remained the same.\n\nIn 2022, the Biden administration banned sales of certain chips to China due to national security concerns. Under the Trump administration, however, the concern is about possible tariffs on chips that would affect its business.\n\nNvidia said the impact is unknown and will depend on the timing, countries affected and the size of the tariffs.\n\nNvidia Earnings Beat Consensus\n\nThe chipmaker, which just ended its fiscal year 2025, reported net income of $22.1 billion, or 89 cents per share, for the fourth quarter. Revenue came in at $39.3 billion. The quarter beat Wall Street analysts\u2019 consensus earnings of 85 cents per share and $38.16 billion in revenue.\n\nYear over year, quarterly net income was up 80% and revenue rose by 78%.\n\nFor the year, Nvidia reported net income of $72.9 billion, up 145% from a year ago, on revenue of $130.5 billion, up 114% from the prior year. Earnings per share came to $2.94 for the year, which is a penny shy of consensus. But Nvidia beat on the topline; analysts expected $129.28 billion.\n\nFor the first quarter of fiscal 2026, Nvidia expects to record revenue of $43 billion, plus or minus 2%. Analysts are expecting revenue of $42.05 billion.\n\nShares of Nvidia were about flat up in after-hours trading. The company\u2019s market cap was $3.22 trillion as of market close on Wednesday (Feb. 26), second only to Apple as the most valuable in the world.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia doubled profits in 2024. And its outlook is rosy despite AI jitters",
            "link": "https://www.cnn.com/2025/02/26/tech/nvidia-earnings-ai-growth/index.html",
            "snippet": "Nvidia exceeded Wall Street's expectations as it closed out 2024, achieving stunning sales and profit growth \u2014 and much of Silicon Valley is likely...",
            "score": 0.9522861838340759,
            "sentiment": null,
            "probability": null,
            "content": "New York CNN \u2014\n\nNvidia exceeded Wall Street\u2019s expectations as it closed out 2024, achieving stunning sales and profit growth \u2014 and much of Silicon Valley is likely breathing a sigh of relief at what the results say about the artificial intelligence industry.\n\nThe chipmaking giant\u2019s shares dipped just over 1% in after-hours trading Wednesday, immediately after the company released its earnings report for the quarter ended in January and its full 2025 fiscal year. However, shares quickly turned positive, gaining 2.7% within half an hour of the report\u2019s release.\n\nNvidia raked in $39.3 billion in sales in the January quarter, slightly above Wall Street\u2019s expectations and 78% higher than the same period in the prior year. Profits from the quarter grew 72% year-over-year to $22 billion.\n\nFor the full year, the company\u2019s profits more than doubled to $74.3 billion, capping off a banner year in which Nvidia cemented itself as perhaps the most important player in the growing AI industry.\n\nAnd Nvidia expects its growth to continue. The company said it expects sales to grow 65% year-over-year to $43 billion in the current quarter, exceeding analysts\u2019 expectations.\n\nThe results underscore that despite increased competition, Nvidia remains unmatched at producing the chips many companies use to power AI systems. Expectations were heightened heading into Wednesday\u2019s report given Nvidia\u2019s role as a barometer of the broader tech sector, which has faced tough investor questions about AI spending. Shares of fellow tech giants Google, Meta, Microsoft, Amazon and Meta were all trading higher after-hours following Nvidia\u2019s report.\n\nIn recent weeks, Silicon Valley has been processing what the launch of DeepSeek \u2014 a highly capable but more efficient AI model from a year-old Chinese startup \u2014 means for the American AI industry.\n\nSome industry experts suggested that the launch of the less power-hungry rival could add to existing fears that US tech giants are overspending on AI infrastructure without sufficient return in that investment. Any pullback in that spending could hit Nvidia\u2019s bottom line, given the company\u2019s central role in providing many of the chips to run AI systems.\n\nNvidia\u2019s share price has fallen 5% since the start of this year as investors digest the DeepSeek news, although shares remain up 65% from a year ago.\n\nAnd where Nvidia\u2019s shares go, much of the rest of the tech market tends to follow. The tech-heavy Nasdaq Composite index is down 1% since the start of this year, dragged down by weak tech stock performance.\n\n\u201cNvidia is the bellwether and market-darling stock that is of vital importance to the broader markets,\u201d Chris Brigati, chief investment officer at investment firm SWBC, said in emailed commentary ahead of Nvidia\u2019s report.\n\nStill, analysts have been quick to point out that major AI players have announced plans to continue funneling tens of billions of dollars into infrastructure, despite the questions DeepSeek has raised. And some experts say a push toward more efficient, inexpensive AI models would actually be a boon to Nvidia and other AI players by accelerating the technology\u2019s adoption.\n\nWedbush analyst Dan Ives said he expects to see $325 billion in capital expenditures this year from just the \u201cMagnificent Seven\u201d tech companies \u2014 Microsoft, Amazon, Meta, Apple, Alphabet, Nvidia and Tesla \u2014 with much of that spending aimed at supporting AI growth.\n\n\u201cWe have seen NOT ONE AI enterprise deployment slow down or change due to the DeepSeek situation,\u201d Ives said in emailed commentary Tuesday. \u201cNo customer wants to \u2018lose their place in line\u2019 as it is described to us for Nvidia\u2019s next gen chips.\u201d\n\nDespite jitters over DeepSeek and some early challenges in rolling out Nvidia\u2019s new Blackwell chips, \u201cNvidia\u2019s results reaffirm that it continues to lead the AI landscape, sidelining skeptics,\u201d Emarketer technology analyst Jacob Bourne said following Wednesday\u2019s results. He added that investment from tech giants \u201cdemonstrate sustained demand for Nvidia\u2019s hardware.\u201d\n\nAsked about his expectations for future AI growth on a call with analysts Wednesday, Nvidia CEO Jensen Huang laid out a vision for a future in which AI has permeated numerous areas of life. He pointed to the auto industry as an example, saying that employees would use AI agents to work more productively, while the vehicles themselves would also be infused with AI.\n\n\u201cSomeday, there will be a billion cars on the road and every single one of those cars will be robotic cars,\u201d he said. \u201cAnd they\u2019ll all be collecting data, and we\u2019ll be improving them using an AI factory.\u201d\n\nThis story has been updated with additional developments and context.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Explore How RTX AI PCs and Workstations Supercharge AI Development at NVIDIA GTC 2025",
            "link": "https://blogs.nvidia.com/blog/rtx-ai-garage-gtc-2025-sessions/",
            "snippet": "At the global AI conference, experts from across the ecosystem will share insights on deploying AI locally.",
            "score": 0.9097992181777954,
            "sentiment": null,
            "probability": null,
            "content": "At the global AI conference, experts from across the ecosystem will share insights on deploying AI locally.\n\nGenerative AI is redefining computing, unlocking new ways to build, train and optimize AI models on PCs and workstations. From content creation and large and small language models to software development, AI-powered PCs and workstations are transforming workflows and enhancing productivity.\n\nAt GTC 2025, running March 17\u201321 in the San Jose Convention Center, experts from across the AI ecosystem will share insights on deploying AI locally, optimizing models and harnessing cutting-edge hardware and software to enhance AI workloads \u2014 highlighting key advancements in RTX AI PCs and workstations.\n\nDevelop and Deploy on RTX\n\nRTX GPUs are built with specialized AI hardware called Tensor Cores that provide the compute performance needed to run the latest and most demanding AI models. These high-performance GPUs can help build digital humans, chatbots, AI-generated podcasts and more.\n\nWith more than 100 million GeForce RTX and NVIDIA RTX\u2122 GPUs users, developers have a large audience to target when new AI apps and features are deployed. In the session \u201cBuild Digital Humans, Chatbots, and AI-Generated Podcasts for RTX PCs and Workstations,\u201d Annamalai Chockalingam, senior product manager at NVIDIA, will showcase the end-to-end suite of tools developers can use to streamline development and deploy incredibly fast AI-enabled applications.\n\nModel Behavior\n\nLarge language models (LLMs) can be used for an abundance of use cases \u2014 and scale to tackle complex tasks like writing code or translating Japanese into Greek. But since they\u2019re typically trained with a wide spectrum of knowledge for broad applications, they may not be the right fit for specific tasks, like nonplayer character dialog generation in a video game. In contrast, small language models balance need with reduced size, maintaining accuracy while running locally on more devices.\n\nIn the session \u201cWatch Your Language: Create Small Language Models That Run On-Device,\u201d Oluwatobi Olabiyi, senior engineering manager at NVIDIA, will present tools and techniques that developers and enthusiasts can use to generate, curate and distill a dataset \u2014 then train a small language model that can perform tasks designed for it.\n\nMaximizing AI Performance on Windows Workstations\n\nOptimizing AI inference and model execution on Windows-based workstations requires strategic software and hardware tuning due to diverse hardware configurations and software environments. The session \u201cOptimizing AI Workloads on Windows Workstations: Strategies and Best Practices,\u201d will explore best practices for AI optimization, including model quantization, inference pipeline enhancements and hardware-aware tuning.\n\nA team of NVIDIA software engineers will also cover hardware-aware optimizations for ONNX Runtime, NVIDIA TensorRT and llama.cpp, helping developers maximize AI efficiency across GPUs, CPUs and NPUs.\n\nAdvancing Local AI Development\n\nBuilding, testing and deploying AI models on local infrastructure ensures security and performance even without a connection to cloud-based services. Accelerated with NVIDIA RTX GPUs, both Dell Pro Max AI and Z by HP solutions provide powerful tools for on-prem AI development, helping professionals maintain control over data and IP while optimizing performance.\n\nLearn more by attending the following sessions:\n\nDevelopers and enthusiasts can get started with AI development on RTX AI PCs and workstations using NVIDIA NIM microservices. Rolling out today, the initial public beta release includes the Llama 3.1 LLM, NVIDIA Riva Parakeet for automatic speech recognition (ASR), and YOLOX for computer vision.\n\nNIM microservices are optimized, prepackaged models for generative AI. They span modalities important for PC development, and are easy to download and connect to via industry-standard application programming interfaces.\n\nAttend GTC 2025\n\nFrom the keynote by NVIDIA founder and CEO Jensen Huang to over 1,000 inspiring sessions, 300+ exhibits, technical hands-on training and tons of unique networking events \u2014 GTC is set to put a spotlight on AI and all its benefits.\n\nFollow NVIDIA AI PC on Facebook, Instagram, TikTok and X \u2014 and stay informed by subscribing to the RTX AI PC newsletter.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia Is Far From Running Out of Road",
            "link": "https://www.wsj.com/tech/ai/nvidia-is-far-from-running-out-of-road-068bb01c",
            "snippet": "New Blackwell chip is off to a roaring start, while hit to profit margins should be short-lived.",
            "score": 0.47091707587242126,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "After waiting with bated breath, Wall Street absorbs Nvidia's latest earnings",
            "link": "https://www.nbcnews.com/business/business-news/nvidia-earnings-relief-ai-rcna193903",
            "snippet": "The GPU chipmaker has become one of the most closely watched companies in the world as a barometer for AI investment \u2014 and the broader stock market. Nvidia...",
            "score": 0.8974436521530151,
            "sentiment": null,
            "probability": null,
            "content": "Rarely has a single company's earnings report been so closely watched by Wall Street as a make-or-break moment for the broader market.\n\nNvidia, the chipmaker whose central role in the global artificial intelligence arms race has turned it into one of the world's most valuable companies, reported its much-anticipated quarterly results Wednesday. It posted $39.3 billion in revenue, up a hefty 78% for the quarter that ended in January and ahead of analysts' expectations.\n\nBut for many investors and analysts, the wildly profitable company's financials reveal more than just the state of Nvidia's own operations. And many are likely to take notice of the chipmaker's forward-looking statements Wednesday, which signaled slowing profit-margin growth.\n\nThe chip giant's importance is due to two major factors: its sizable valuation (meaning that its stock price can weigh heavily on the market as a whole, including the blue-chip Dow Jones Industrial Average) and its role in the AI sector (which now factors heavily into broader U.S. economic growth).\n\nAhead of the earnings, Dan Ives, a managing director and senior equity research analyst at Wedbush Securities, said it would be a \"massive day\" for global markets looking to \"gauge the demand trajectory of the AI Revolution.\" Among X's more finance- and tech-focused crowd, the earnings had caused enough anticipation to spark some tongue-in-cheek jokes.\n\nAs if to underscore just how on edge traders were, Nvidia's stock price swung higher and then lower and then higher again in the minutes right after the report came out. As of about 5 p.m. ET, the shares were up about 2% from where they closed Wednesday afternoon.\n\nBrand-name tech firms looking to build out their AI capabilities have purchased Nvidia's graphics processing unit (GPU) chips by the heap \u2014 a buying spree that has turbocharged the company's share price. Between the start of 2023 and the end of 2024, Nvidia's stock climbed some 880%, making it at the time the most valuable publicly traded U.S. firm, with a market capitalization over $3 trillion. Thanks to that surge, Nvidia is now the second-most-important component of the S&P 500 index, behind only Apple.\n\nNvidia's earnings are heavily scrutinized as a barometer for the economy at large, the growth of which during the past two years has been powered in large part by investments in AI and data center capacity. Yet so far this year, Nvidia shares were down 5% heading into its earnings report amid growing doubts about whether the breakneck pace of global AI investment could be sustained.\n\nThis week, a report emerged that Microsoft, one of Nvidia's largest customers, was pulling back on its data-center spending. Microsoft has since denied that \u2014 and many of Nvidia's other key customers, including Google parent Alphabet, Facebook parent Meta and Amazon, all plan to make significant investments.\n\nNvidia, and the AI landscape in general, has also been shaken by the seemingly sudden rise of DeepSeek, a China-backed AI platform that reportedly requires far fewer computing resources \u2014 and mostly taps chips made by Nvidia rival Intel.\n\nMore broadly, the U.S. economy is showing signs of a slowdown amid questions about the strength of consumers and the prospect of higher inflation tied to President Donald Trump's plans to impose tariffs.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia Stock Wobbles On Q4 Details. Blackwell Sales Look Strong, For Now.",
            "link": "https://www.investors.com/news/technology/nvidia-stock-nvda-fiscal-q4-2025-earnings/",
            "snippet": "AI chipmaker Nvidia beat Q4 targets. The technology giant's stock rallied but then the shares slipped in extended trading.",
            "score": 0.7647766470909119,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) late Wednesday beat Wall Street's targets for its fiscal fourth quarter and guided above views for the current period. Nvidia stock rebounded after sliding in late trades. But the shares faded again, last down a fraction after the earnings call ended.\n\nThe Santa Clara, Calif.-based company earned an adjusted 89 cents a share on sales of $39.33 billion in the quarter ended Jan. 26. Analysts polled by FactSet had expected earnings of 85 cents a share on sales of $38.1 billion. In the year-earlier period, Nvidia earned an adjusted 52 cents a share on sales of $22.1 billion.\n\nNvidia's fiscal Q4 was the first to include sales of the company's next-generation AI processor, called Blackwell.\n\nFor the current quarter, Nvidia forecast revenue of $43 billion, vs. expectations for $42.07 billion. In the same quarter last year, Nvidia posted sales of $26.04 billion.\n\nIn after-hours trading on the stock market today, Nvidia stock alternated between modest gains and losses. During recent trades, it was down 1% to 129.94. During the regular session Wednesday, Nvidia stock rose 3.7% to close at 131.28.\n\nNvidia stock had fallen in the previous three trading sessions as investors considered multiple perceived risks.\n\nNvidia Stock In Consolidation Pattern\n\nIBD MarketSurge charts show that Nvidia stock is in a consolidation pattern with a buy point of 153.13. However, it is in the lower half of its base.\n\nAlso, in a negative sign, Nvidia stock was trading below its 50-day moving average line, a key support level. However, it had found support at its 200-day moving average.\n\nNvidia stock has moved sideways for several months amid a host of concerns. They include worries that Nvidia's hyperscale cloud customers might slow spending on data centers or rethink their costly capex plans. Nvidia also faces competition from its own customers developing custom AI chips with Broadcom (AVGO) and Marvell Technology (MRVL). Plus, Nvidia faces the prospect of increased U.S. export controls on the chips it can sell to China. Also, Nvidia's Blackwell chips are said to have encountered production issues that could slow their ramp.\n\nNvidia stock is on the IBD Tech Leaders list.\n\n6:24 p.m. ET\n\nNvidia Stock Slides Again As Call Ends\n\nAI is moving beyond perception and generative AI to reasoning and that's going to require many times more computer processors, CEO Jensen Huang said.\n\nThe AI revolution is just getting started as artificial intelligence shifts to agentic AI and physical AI, he said.\n\nThe earnings call has ended. Nvidia shares faded again in late trades. The stock was last down a fraction.\n\n5:45 p.m. ET\n\nBlackwell Ultra, Rubin Up Next\n\nOn the conference call, Chief Executive Jensen Huang said Nvidia remains on an annual rhythm for new product releases.\n\nThe follow-up to Blackwell, known as Blackwell Ultra, is on track to launch later this year, he said. Huang said he will discuss the upcoming Blackwell Ultra, its successor called Rubin, and \"one click after Rubin\" in a keynote at the company's GTC conference in March.\n\n5:40 p.m. ET\n\nBlackwell Weighing On Margins\n\nChief Financial Officer Colette Kress said Nvidia's adjusted gross margin is expected to be 71% in the current quarter. And it will be in the \"low 70s\" as Nvidia ramps Blackwell production.\n\n\"We expect a significant ramp of Blackwell in the first quarter,\" she said.\n\nNvidia's gross margins should return to the mid-70s later in the current fiscal year, she said.\n\nNvidia stock still up 1.8%.\n\n5: 21 p.m. ET\n\nBlackwell Exceeds Targets\n\n\"In Q4, Blackwell sales exceeded our expectations,\" CFO Colette Kress said on a conference call with analysts.\n\nCustomers \"are racing to scale their infrastructure\" for AI computing applications, she said. Demand for high-performance processors is increasing along with new AI applications with enhanced capabilities.\n\nNvidia up nearly 2%.\n\n5:06 p.m. ET\n\nNvidia's Gross Margin Declines\n\nNvidia garnered $11 billion of Blackwell architecture revenue in the fourth quarter, Chief Financial Officer Colette Kress said in prepared remarks. Blackwell was \"the fastest product ramp in our company's history,\" she said.\n\n\"Blackwell sales were led by large cloud service providers which represented approximately 50% of our Data Center revenue,\" she said.\n\nNvidia's adjusted gross profit margin declined to 73.5% in fiscal Q4, vs. 75% in Q3 and 76.7% in the year-earlier period.\n\nThe gross margin decline was \"primarily due to a transition to more complex and higher cost systems within Data Center.\"\n\nBut after sliding in extended trading, Nvidia rose nearly 3% as the tech giant's earnings call began.\n\n4:50 p.m. ET\n\nBlackwell Demand Called 'Amazing'\n\nOn a year-over-year basis, Nvidia's adjusted Q4 earnings rose 71% while sales increased 78%.\n\nNvidia's data center revenue surged 93% from a year ago to $35.6 billion. Sales in the segment were up 16% from the fiscal third quarter.\n\n\"Demand for Blackwell is amazing as reasoning AI adds another scaling law \u2014 increasing compute for training makes models smarter and increasing compute for long thinking makes the answer smarter,\" Chief Executive Jensen Huang said in a news release.\n\n\"We've successfully ramped up the massive-scale production of Blackwell AI supercomputers, achieving billions of dollars in sales in its first quarter,\" he said. \"AI is advancing at light speed as agentic AI and physical AI set the stage for the next wave of AI to revolutionize the largest industries.\"\n\nNvidia stock recovers, but still down a fraction.\n\n4:35 p.m. ET\n\nNvidia Stock Slips\n\nAI giant posted results that beat estimates. Stand by for more live coverage.\n\n3:35 p.m. ET\n\nNvidia Stock Hangs On To Gains\n\nNvidia stock were ahead nearly 3% with less than a half hour left in the trading session. The broader market was in the red, with the Dow pulling back a fraction.\n\n2:32 p.m. ET\n\nNvidia Stock Pulls Back From Highs\n\nIn intraday trading on Wednesday, Nvidia shares climbed as high as 133.73, up 5.6% on the day. It almost touched its 50-day line but then reversed.\n\nIn recent trades, Nvidia stock was up 2.7% to 130.02.\n\n11:50 a.m. ET\n\nWhat To Do After Nvidia Earnings?\n\nIn a client note Wednesday morning, BofA Securities analyst Vivek Arya provided three potential scenarios for investors to consider before Nvidia's earnings report. They included a base case, bull case and bear case.\n\nThe baseline view is for a fourth-quarter beat with an in-line outlook for the first quarter on a slower Blackwell ramp and looming China trade restrictions. The bull scenario would be if Q4 sales and Q1 revenue guidance both beat views by $1 billion to $2 billion-plus and for Q1 gross margin to stay above 71% to 72%. Both would support Nvidia stock gains, Arya said.\n\nHowever, under the bear case, Nvidia stock is likely to fall to the lower end of its recent trading range, Arya said. The bear case calls for in-line Q4 results and in-line to lower Q1 guidance, 70% or lower gross margin with no reiteration of a second-half 2025 pick-up in business.\n\nBut in most scenarios, Nvidia stock is likely to rise in the lead up to the company's GTC conference, aka \"AI Woodstock,\" Arya said. GTC is scheduled for March 17-21 in San Jose, Calif.\n\nArya rates Nvidia stock as buy with a price target of 190.\n\n9:45 a.m. ET\n\nNvidia Stock Leads Market Higher\n\nNvidia rose more than 3% in early trades as the broader market advanced. More stock market news.\n\n9:15 a.m. ET\n\nNvidia Stock: Will It 'Do Enough To Reverse Weakness'?\n\nMizuho Securities trading desk analyst Jordan Klein summed up the fears of investors in a client note early Wednesday.\n\nHe said Nvidia's report and earnings call are probably going to \"set the tone for the next move\" in tech stocks and equities broadly.\n\n\"I just worry NVDA alone is not going to say and do enough to break the momentum downtrend and \u2026 reverse the weakness and uncertainty surrounding many of the real and quality AI and tech names over the near term. I think that comes later in (the) year,\" he said.\n\nNvidia stock up more than 2% premarket.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nAdobe Brings Photoshop To Mobile Devices As Freemium App\n\nBroadcom Advances AI Data Center Interconnect Technology\n\nSee Stocks On The List Of Leaders Near A Buy Point\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nGet Stock Ideas From IBD Experts Each Morning Before The Open\n\nFutures: Nvidia Falls Despite Strong Earnings, Guidance",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "Here\u2019s What To Expect From Nvidia\u2019s \u2018Massive\u2019 Earnings Report",
            "link": "https://www.forbes.com/sites/dereksaul/2025/02/26/heres-what-to-expect-from-nvidias-massive-earnings-report/",
            "snippet": "Long a Wall Street favorite, Nvidia stock has struggled recently, gaining 4% over the last six months, worse than the S&P 500's 7% return.",
            "score": 0.913036048412323,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-25": {
        "0": {
            "title": "Cisco, Nvidia To Create \u2018Cross-Portfolio\u2019 Networking Offerings For AI Data Centers",
            "link": "https://www.crn.com/news/networking/2025/cisco-nvidia-to-create-cross-portfolio-networking-solutions-for-ai-data-centers",
            "snippet": "Cisco and Nvidia announced a landmark deal on Tuesday in which the two companies have agreed to develop joint offerings that will simplify the build-out of...",
            "score": 0.717798113822937,
            "sentiment": null,
            "probability": null,
            "content": "Cisco, Nvidia To Create \u2018Cross-Portfolio\u2019 Networking Offerings For AI Data Centers\n\nIn a landmark deal unveiled Tuesday, Cisco and Nvidia say they will create a \u2018cross-portfolio unified architecture to simplify building AI-ready data center networks,\u2019 which includes a plan by Nvidia to integrate Cisco\u2019s Silicon One chip into its Spectrum-X Ethernet platform.\n\nCisco Systems and Nvidia said they hope to speed up enterprise AI adoption with a landmark deal they unveiled Tuesday that involves the joint development of Ethernet-based networking offerings for AI data centers.\n\nThe networking and AI computing giants said in a press release that the expanded partnership\u2019s focus is the creation of a \u201ccross-portfolio unified architecture to simplify building AI-ready data center networks,\u201d which is intended to make it easier for customers and partners to \u201cintegrate and standardize on both Cisco and Nvidia technology.\u201d\n\n[Related: Nvidia Hires Top Cisco Inventor Amid Big Networking Sales Push]\n\nJim Kavanaugh, co-founder and CEO of solution provider powerhouse World Wide Technology, hailed the expanded Nvidia-Cisco partnership, which he said \u201chas already resulted in cutting-edge AI solutions that drive innovation and business transformation.\u201d\n\n\u201cBy leveraging the strengths of Cisco\u2019s deep expertise in the data center and Nvidia\u2019s advanced AI technologies, we are poised to deliver unparalleled value to our customers and help them navigate the complexities of AI adoption with confidence,\u201d said Kavanaugh, whose company is No. 7 on CRN\u2019s 2024 Solution Provider 500 list, in a statement.\n\nThe new collaboration includes a plan by Nvidia to incorporate Cisco\u2019s Silicon One application-specific integrated circuit (ASIC) into its Spectrum-X Ethernet networking platform, which it said last August was on track to become a \u201cmultibillion-dollar product line within a year.\u201d\n\nThe two companies said this move makes Cisco \u201cthe only partner silicon supported in Spectrum-X Ethernet solutions,\u201d adding that it will enable interoperability between both companies\u2019 networking architectures\u201d to provide \u201csimplified, full-stack solutions.\u201d This will allow customers and partners to use \u201ctheir existing management tools and processes, spanning front- and back-end networks,\u201d according to Cisco and Nvidia.\n\nThis integration between Nvidia\u2019s and Cisco\u2019s networking architectures will also allow their customers to \u201cbenefit from current and future technology advancements in the Nvidia Spectrum-X platform such as adaptive routing, telemetry, congestion control and low latency, as well as Cisco\u2019s broader networking, security and digital resilience portfolio, including the Splunk data platform,\u201d the companies said.\n\nAs part of the partnership, Cisco plans to develop data center switches using Nvidia\u2019s Spectrum-X Ethernet platform, which the companies said will provide customers and partners \u201cwith more choices and flexibility.\n\nThe networking giant also plans to work with Nvidia to \u201ccreate and validate Nvidia Cloud Partner and Enterprise Reference Architectures based on Nvidia Spectrum-X with Cisco Silicon One, Hyperfabric, Nexus, UCS Compute, Optics and other Cisco technologies.\u201d\n\nCisco plans to make its Silicon One-based switches\u2014including Cisco Nexus, Cisco Nexus Hyperfabric and Cisco USC products\u2014compatible with Nvidia\u2019s Spectrum-X platform and reference architecture by the middle of the year. The companies said the timeline for Cisco switches that incorporate Spectrum-X will be announced at a later date.\n\n\u201cTogether, Cisco and Nvidia are partnering to remove barriers for customers and ensure they can optimize their infrastructure investments to unlock the power of AI,\u201d said Cisco Chair and CEO Chuck Robbins in a statement.\n\n\u201cNvidia Spectrum-X is Ethernet enhanced and supercharged for AI,\u201d Nvidia founder and CEO Jensen Huang added in his own statement. \u201cTogether with Cisco\u2019s enterprise platforms and global reach, we can help companies worldwide build state-of-the-art Nvidia infrastructure as they race to transform with AI.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "AI innovation starts here\u2014join Microsoft at NVIDIA GTC",
            "link": "https://azure.microsoft.com/en-us/blog/ai-innovation-starts-here-join-microsoft-at-nvidia-gtc/",
            "snippet": "Join Microsoft at NVIDIA GTC AI Conference to discover how Azure's end-to-end AI platform can accelerate your AI innovation. Learn more.",
            "score": 0.915412425994873,
            "sentiment": null,
            "probability": null,
            "content": "The NVIDIA GTC AI Conference is the place to be to discover the latest AI technology and solutions.\n\nAI is within reach of any company, large or small. But many companies are unclear where and how to practically apply AI within their business; have concerns about security, privacy and compliance; and perceive AI as complex and costly to integrate into existing services, products and processes. Rapidly evolving AI technologies can also make it difficult to keep up to date and skilling up technical AI expertise can take time.\n\nAzure at NVIDIA GTC Learn how to accelerate your AI innovation. Join Microsoft at the NVIDIA GTC AI Conference. Register today >\n\nCreate the future now with Azure AI\n\nMicrosoft can help companies of any size and AI skill level build and scale exceptional AI solutions quickly, securely and responsibly. The Azure AI platform provides a comprehensive suite of models, services, tools, and the latest AI technologies that can power your AI transformation.\n\nAzure AI Build and scale generative AI systems.\n\nWith the broadest choice in models and pre-trained AI services to best fit your business, Azure AI can help you innovate fast. Explore, compare, swap, deploy and evaluate using large language models (LLMs), large multimodal models (LMMs), small language models (SLMs), domain specific models, industry models in one model catalog\u2014including Azure OpenAI Service.\n\nAll developers can quickly become AI developers. Build without boundaries with Azure AI Foundry, a trusted, integrated platform for developers and IT administrators to design, customize, and manage AI applications and agents. Access collaborative, comprehensive generative AI operations tools to accelerate the development lifecycle and differentiate your apps. Assess and protect apps with configurable evaluations, safety filters, and security controls.\n\nPower your AI innovation with a highly scalable and performant AI infrastructure that is optimized from silicon to systems\u2014across every layer of the stack from the datacenter to the network to deliver peak performance. Azure AI infrastructure leverages the most advanced technologies and the full-stack NVIDIA accelerated computing platform for generative AI and industrial to power your AI innovations. And with built-in data privacy, security, safety, and copyright protection you bring AI to market confidently.\n\nAzure AI INfrastucture Deliver high-power performance.\n\nMicrosoft for Startups helps startups leverage AI by providing free access to industry-leading AI services, up to $150,000 in Azure credits, and expert guidance through unlimited 1:1 meetings. The program also offers training resources, community networking opportunities, and tools for responsible AI use, enabling startups to build and deploy AI solutions efficiently and cost-effectively.\n\nExperience the innovation at the NVIDIA GTC AI Conference\n\nWhether you\u2019re a developer, researcher, or business leader, the NVIDIA GTC AI Conference is the place to be to discover the latest AI technology and solutions. Come see Azure AI and NVIDIA AI in action.\n\nJoin Microsoft March 17-21, 2025 in San Jose, California (or virtually) to explore the latest AI technology and solutions and discover how Azure\u2019s end-to-end AI platform can accelerate your AI innovation. From live Microsoft sessions to hands-on experiences and networking events, you can connect with and learn from Microsoft AI experts, partners, customers, and your peers. Microsoft is a proud Elite sponsor of NVIDIA GTC 2025.\n\nAttend Microsoft talks and panels\n\nLearn how to innovate fast with the broadest choice in models and pre-trained AI services, comprehensive generative AI operations tools and the most scalable and performance infrastructure. Attend Microsoft talks and panels in the GTC main program. Click on session titles to add to your GTC schedule.\n\nFeatured sessions\n\nS74603: Integrate NVIDIA NIM with Azure Container Apps Serverless GPUs Speaker: Devanshi Joshi, Senior Product Marketing Manager, Azure Digital Applications\n\n\n\nS74600: Unlock AI Potential with Azure Infrastructure: Innovations, Best Practices Speaker: Kanchan Mehrotra, Group Technical Manager, Azure HPC & AI\n\n\n\nTalks and panels\n\nVisit Microsoft booth #514\n\nExperience the latest in Azure AI. Join live discussion sessions at Microsoft\u2019s in-booth theater, connect with Microsoft and partner AI experts, and try out the latest AI technology and hardware. Stop by Microsoft booth #514 daily to:\n\nAttend in-booth information sessions .\n\n. Get hands-on with the latest AI solutions in our demo pods .\n\n. Check our latest AI technology in our hardware bar .\n\n. Take a break in our lounge area and enjoy free coffee served by Artly , the barista bot.\n\n, the barista bot. Enter for a chance to win cool stuff!\n\nGet social\n\nConnect with Microsoft executives and AI experts and your peers at our social events at GTC. Enjoy drinks, food, and networking. Microsoft and NVIDIA will host an invitation-only executive dinner on Tuesday, March 18, a night of networking food and fun on Wednesday, March 19, and a brunch for women in technology on Thursday, March 20. Contact your Microsoft representative to request an invitation.\n\nLearn more about Azure AI and NVIDIA AI",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "GeForce RTX 5090, 5080 & 5070 Ti Laptop Pre-Orders Available Now | GeForce News",
            "link": "https://www.nvidia.com/en-us/geforce/news/geforce-rtx-50-series-laptop-pre-orders/",
            "snippet": "Multiply mobile performance using DLSS 4 with Multi Frame Generation, work and play for longer thanks to Max-Q, and complete AI and creative tasks in record...",
            "score": 0.9466410875320435,
            "sentiment": null,
            "probability": null,
            "content": "Starting now, you can pre-order GeForce RTX 5090, GeForce RTX 5080, and GeForce RTX 5070 Ti laptops ahead of their release in the coming months. Models will be available from top OEM partners including ASUS, GIGABYTE, HP, Lenovo, MECHREVO, MSI and Razer are available for pre-order now, with more coming soon. Availability will vary by country for each OEM partner and model.\n\nPowered by NVIDIA Blackwell, GeForce RTX 50 Series laptops bring game-changing capabilities to gamers and creators. Packed with incredible AI and neural rendering capabilities, GeForce RTX 50 Series Laptop GPUs enable new experiences and deliver unprecedented levels of detail in the latest, greatest games.\n\nMultiply performance using DLSS 4 with Multi Frame Generation, generate AI content at incredible speeds, and unleash your creativity with NVIDIA Studio apps and enhancements. All in the thinnest and longest lasting RTX laptops, optimized by new Max-Q technologies.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Exclusive: Nvidia's H20 chip orders jump as Chinese firms adopt DeepSeek's AI models, sources say",
            "link": "https://www.reuters.com/technology/artificial-intelligence/nvidias-h20-chip-orders-jump-chinese-firms-adopt-deepseeks-ai-models-sources-say-2025-02-25/",
            "snippet": "SINGAPORE/BEIJING, Feb 25 (Reuters) - Chinese companies are ramping up orders for Nvidia's (NVDA.O) , opens new tab H20 artificial intelligence chip due to...",
            "score": 0.788211464881897,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia is fighting both Trump and China",
            "link": "https://www.economist.com/business/2025/02/25/nvidia-is-fighting-both-trump-and-china",
            "snippet": "One honourable exception to the roster of tech billionaires standing behind Donald Trump at the president's inauguration on January 20th was Jensen Huang,...",
            "score": 0.8427814245223999,
            "sentiment": null,
            "probability": null,
            "content": "O ne honourable exception to the roster of tech billionaires standing behind Donald Trump at the president\u2019s inauguration on January 20th was Jensen Huang, the chief executive of Nvidia. He took a quieter approach, instead meeting Mr Trump at the White House just over ten days later. Earlier that week his company, the dominant supplier of artificial-intelligence ( AI ) chips, had lost $600bn in market value during a sell-off precipitated by the release of the latest AI models from DeepSeek, a Chinese firm. Mr Trump\u2019s plans to respond to the upstart model-maker may have been as much on Mr Huang\u2019s mind as DeepSeek itself.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Hell Freezes Over: Cisco And Nvidia Cross-Pollenate AI Networking",
            "link": "https://www.nextplatform.com/2025/02/25/hell-freezes-over-cisco-and-nvidia-cross-pollenate-ai-networking/",
            "snippet": "UPDATED Networking giant Cisco Systems and AI platform provider Nvidia have hammered out a deal to mix and match each other's technologies to create a.",
            "score": 0.49725520610809326,
            "sentiment": null,
            "probability": null,
            "content": "UPDATED Networking giant Cisco Systems and AI platform provider Nvidia have hammered out a deal to mix and match each other\u2019s technologies to create a broader set of AI networking options for their respective and \u2013 importantly, prospective \u2013 customers.\n\nNvidia has been a competitor of Cisco\u2019s in the datacenter since acquiring Mellanox Technologies in March 2019 for $6.9 billion, but they have generally kept to their patches. Cisco is the enterprise and service provider giant that dabbled a bit in AI and HPC as it relates mostly to financial services firms, while Nvidia had expertise in HPC and among certain hyperscalers and cloud builders trying to build low latency networks, often with InfiniBand but increasingly with Ethernet that has been gussied up with RDMA memory technologies and other techniques borrowed from InfiniBand.\n\nBut the minute Cisco entered the merchant silicon market with its Silicon One ASICs, which sport switch and router functions that have a common underlying architecture and which in recent months have had packet spraying and other congestion control techniques added to them to herd the massive \u201celephant flows\u201d on the backend networks used in AI training clusters, a collision between Nvidia and Cisco was imminent. But instead of bumper cars, what the market will now have is a covalent bond, a sharing of technologies and more choices.\n\nWhere there were once three options \u2013 the Cumulus Linux or SONiC network operating systems on Spectrum 4 switch ASICs paired with BlueField 3 DPUs from Nvidia or the NX-OS network operating system on Silicon One switch ASICs from Cisco \u2013 now there are many options, perhaps more than a half of dozen depending on how you count it and what the two companies do under their new AI networking partnership.\n\nUnder that partnership announced today, a bunch of things are happening. First, Cisco is porting its NX-OS networking operating system to run on Nvidia\u2019s Spectrum 4 ASICs, which are at the heart of the Spectrum-X networking platform that includes BlueField 3 DPUs for all kinds of congestion control and adaptive routing that is offloaded from servers and switches to these auxiliary processors.\n\nThese congestion control and adaptive routing features are part of the InfiniBand switch ASICs and MLNX-OS network operating system that Mellanox created for InfiniBand decades ago. We strongly suspect that in the wake of Nvidia buying Cumulus Networks in May 2020, all kinds of stuff from MLNX-OS (and possibly the Onyx NOS that goes end of life this year) was moved over to the Cumulus Linux NOS and has also been ported to the BlueField 3 DPUs to make up the Spectrum-X software stack. (It is hard to prove, but logical.) Suffice it to say, Nvidia has lots and lots of NOS expertise in its own right.\n\nOf course, so does Cisco with its venerable Internetwork Operating System (IOS), which predates even commercial-grade Unixes and is based on its own kernel. The more modern NX-OS for the Nexus line of switches that date from the late 2000s is based on a Linux kernel, as is the Extensible Operating System (EOS) from Arista Networks.\n\nUnder the new partnership between Cisco and Nvidia, companies will be able to buy Cisco Nexus switches based on the Spectrum 4 switch ASICs and run the familiar NX-OS on them and still get the benefits that come with the Spectrum-X stack because really a lot of the smart stuff for shaping traffic and controlling security for Spectrum-X happens at the DPU level.\n\nIn the past, before Silicon One was launched way back in 2019, Cisco has gone out to merchant silicon makers and built Nexus switches around their ASICs because they have capabilities that filled in niches with specific customers. We detailed such diverse switches way back in 2018 when Cisco was using ASICs from Broadcom, Innovium, and Barefoot Networks as well as its own homegrown chips. To our knowledge, Cisco never make a switch using Mellanox or Nvidia ASICs, and this will be the first time. Gilad Shainer, senior vice-president of marketing for Mellanox networking at Nvidia, tells The Next Platform that the deal between Nvidia and Cisco covers Spectrum 4 and multiple generations of ASICs out beyond that in the Spectrum family.\n\nNvidia will not be reselling these Cisco Nexus switches based on NX-OS, but will promote them as being part of the Spectrum-X family since interconnects using them will include DPUs for the reasons outlined above. Nvidia will continue to make and sell its own Spectrum-4 switches and will make switches for the foreseeable future. This is not about Nvidia getting out of the switch manufacturing business.\n\nIt is about creating a Cisco-flavored Spectrum-X offering, which both Nvidia and Cisco both believe is necessary to make a more familiar and palatable AI cluster offering to customers used to having Cisco as their networking vendor. It doesn\u2019t hurt that Cisco has over 90,000 customers for its Unified Computing System (UCS) server platforms, which sport integrated Nexus networking and a Nexus Dashboard for managing it. If you want to sell Cisco enterprise and service provider customers AI clusters \u2013 as Nvidia does \u2013 you can\u2019t expect them to give up on Nexus. They won\u2019t do it, because the only thing stickier in the datacenter than a NOS is a database.\n\nCisco will also be creating Nexus switches that are based on its own Silicon One ASICs that can also in theory run SONiC or proprietary NOSes, but which will be considered a peer in the Spectrum-X lineup even through it is not running a port of Nvidia\u2019s Cumulus Linux. (We had originally thought that Cumulus Linux was being ported to the Silicon One ASICs, which made sense to us, but that is not what is happening.)\n\nThe new Nexus switch that will be paired with Nvidia BlueField 3 DPUs will be based on the Silicon One G200 ASIC. We did a deep drilldown into the G200 back in June 2023 when it was launched, and this 51.2 Tb/sec ASIC is the first chip equipped with the packet spraying techniques that are necessary for managing AI elephant flows. The Spectrum-4 ASIC from Nvidia and the Tomahawk 5 ASIC from Broadcom also deliver 51.2 Tb/sec of aggregate throughput.\n\nAs far as we know, Nvidia has no intention of selling any Silicon One switches itself, just like it will not resell Cisco Nexus switches using its Spectrum 4 ASIC.\n\nCisco is the first such networking partner to have this Spectrum-X partnership with Nvidia, but we can envision that Arista Networks (really just a proxy for Broadcom ASICs at this point) might also want a similar deal. Shainer does not think this is a particularly useful idea. But he didn\u2019t put the kibosh on the idea, either.\n\n\u201cI\u2019m very frank, and I\u2019m telling you that at this point that we\u2019re focusing on what we announced with Cisco.\u201d Shainer explains. \u201cThere is a good amount of work that we are going to do between us. There are a lot of things we want to do together. We want to go to enterprises with AI. We want to do a lot of activities together. What\u2019s going to happen in far future, who knows?\u201d\n\nIt will be interesting to see if either Nvidia or Cisco create a commercial distribution of SONiC for these switches. Both companies have software development kits to support proprietary NOSes, and further they have support for the Switch Abstraction Interface (SAI) layer that rides on top of those SDKs and shims underneath SONiC, which is itself based on a Linux kernel even though it was created by Microsoft, which would seemingly be allergic to Linux but is not. (You cannot be a cloud and not support Linux, and enthusiastically at that.) Microsoft donated SONiC to the open source community in March 2016.\n\nWe wonder how much pressure certain enterprises and service providers were giving Nvidia and Cisco to work together to give them more options on how to deploy Cisco NOSes and Spectrum-X add-ons to either Nvidia or Cisco switch ASICs. Shainer downplayed this as the motivation for the partnership, saying that this is just a practical approach to getting Cisco customers fired up about AI clusters and reducing the friction of adoption into Cisco shops for Nvidia iron. We suppose it all comes to the same, whether it is a push or a pull. The important thing is that Cisco and Nvidia are working together.\n\nThe first Spectrum-4 switches running NX-OS and the Spectrum-X add-ons as well as the first Silicon One switches bearing the Spectrum X deal of approval and paired with Nvidia DPUs and Nvidia NOS SDKs are expected later this year \u2013 very likely during the summer.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia Investor Worries Mount Ahead Of AI Chipmaker's Q4 Report",
            "link": "https://www.investors.com/news/technology/nvidia-stock-retreats-before-fiscal-q4-report/",
            "snippet": "Nvidia stock is facing a wall of worry ahead of the AI chipmaker's fiscal fourth-quarter report, due out late Wednesday.",
            "score": 0.9267565011978149,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock is facing a wall of worry ahead of the AI chipmaker's fiscal fourth-quarter report, due out after the market close on Wednesday.\n\nWall Street expects Nvidia to earn an adjusted 85 cents a share, up 63% year over year, on sales of $38.1 billion, up 72%, in the quarter ended Jan. 26. That would represent a noticeable deceleration from the heady growth numbers it posted throughout 2024.\n\nFor the current quarter, its fiscal Q1, analysts are modeling Nvidia earnings of 91 cents a share, up 49%, on sales of $42.07 billion, up 62%.\n\nTruist Securities analyst William Stein sees \"three big concerns\" heading into the Q4 report.\n\nFirst, will there be an \"air pocket\" in sales as customers shift from Hopper systems to Blackwell systems? Second, will production challenges facing Blackwell processors limit their availability? And third, will hyperscale cloud computing services rethink their capex plans after China's DeepSeek showed that AI systems can be made at a lower cost?\n\nIn a client note Monday, Stein said those concerns have created \"lots of noise,\" but other data points suggest continued strong sales and demand for Nvidia chips. He rates Nvidia stock as buy with a price target of 204.\n\nNvidia Stock Retreats Before Q4 Report\n\nOn the stock market today, Nvidia stock fell 2.8% to close at 126.63.\n\nOn Monday, Nvidia stock dropped 3.1% to close at 130.28. Nvidia and AI chip peers Broadcom (AVGO) and Marvell Technology (MRVL) fell on a disputed report that Microsoft (MSFT) was pulling back on its data center expansion plans.\n\nWedbush Securities analyst Daniel Ives expects a positive report from Nvidia.\n\n\"The market is heavily skewed negative right now around tech sentiment with any whisper of worries/concern from DeepSeek to Microsoft capex causing a brutal ripple impact across the tech ecosystem,\" Ives said in a client note Tuesday. \"We expect another robust performance and clear beat-and-raise special from Nvidia that should calm the nerves of investors.\"\n\nDe-Risk Trade Is Underway\n\nJordan Klein, a trading desk analyst with Mizuho Securities, said a good report from Nvidia won't matter much to investors.\n\n\"Nvidia earnings will not matter all that much, nor change the current unwind, de-risk trade underway,\" Klein said in a client note Tuesday. \"Nvidia management cannot really say anything all that new or conclusive that would push investors headfirst back into AI semis and hardware-related longs that to me just want to go lower on a unwind of all the retail and momentum froth.\"\n\nMorgan Stanley analyst Joseph Moore reiterated his overweight, or buy, rating on Nvidia stock with a price target of 152 on Monday.\n\nIn a client note, Moore said the quarter and outlook for Nvidia have improved over the last 60 days. However, the possibility of further U.S. export controls on semiconductors to China continues to weigh on the company.\n\nMoore expects an in-line guide from Nvidia for the current quarter because of export uncertainty.\n\n\"We remain convinced (that) once we get past export controls there will be positive momentum into\" second-half 2025, he said.\n\nNvidia stock is on the IBD Tech Leaders list.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nBroadcom Advances AI Data Center Interconnect Technology\n\nNvidia Stock Pauses Despite Debut Of Blackwell. What The King Of AI Chips Needs Now.\n\nSee Stocks On The List Of Leaders Near A Buy Point\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Nvidia earnings preview: Watching for DeepSeek AI fallout",
            "link": "https://www.axios.com/2025/02/25/nvidia-earnings-deepseek-ai-chip",
            "snippet": "Nvidia investors will be watching for any clues on how the DeepSeek surprise is reshaping the AI chip giant's trajectory when it reports earnings Wednesday...",
            "score": 0.9165518879890442,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia stock drops ahead of earnings as investors weigh potential Trump export rules, Blackwell delays",
            "link": "https://finance.yahoo.com/news/nvidia-stock-drops-ahead-of-earnings-as-investors-weigh-potential-trump-export-rules-blackwell-delays-171110699.html",
            "snippet": "Nvidia stock has dropped over 9% in the days leading up to its highly anticipated earnings report set for Wednesday, with tariff and chip delay reports...",
            "score": 0.9493709802627563,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock continued its recent slide on Tuesday as investors weighed potential delays in the ramp-up of its AI Blackwell chips and a report of possible new export rules from the Trump administration.\n\nThe stock dropped 2.8% Tuesday after Bloomberg News reported late Monday night that the Trump administration is looking to further tighten US export rules on the chip sector in an effort to restrict China\u2019s advancement in the AI space.\n\nTrump is looking to sanction specific Chinese companies and further restrict international companies from maintaining semiconductor gear in the country, Bloomberg reported.\n\nThe news comes more than a month after Chinese firm DeepSeek introduced new, cost-efficient AI models that rocked US markets.\n\nOver the last five trading days, Nvidia stock has lost over 9%, a decline that comes ahead of the chipmaker's fourth quarter earnings report scheduled for Wednesday after the bell.\n\nNvidia stock dropped 4% Friday and an additional 3% Monday as macroeconomic uncertainty surrounding Trump\u2019s trade policies stoked fears of inflation and drove down major stock indexes.\n\nAnother challenge for Nvidia investors ahead of the report came in a note to investors on Monday from Evercore ISI analyst Mark Lipacis, who suggested Nvidia\u2019s ramp-up of its latest Blackwell AI chips could be delayed.\n\n\u201cOur checks suggested Blackwell ramp [was] pushed to mid 2025 from 1H25 [the first half of 2025],\" he wrote.\n\nLipacis added, \u201cWe believe some hyperscalers will likely push some purchases from NVDA, however many noted demand for NVDA GPUs [graphics processing units, or AI chips] still outstrips supply, and absent B100 [Blackwell] availability, NVDA\u2019s current solution, H100 would be purchased instead.\u201d\n\nLipacis maintained his Buy rating on the stock.\n\nNvidia declined Yahoo Finance's request for comment.\n\nNvidia\u2019s Blackwell chips have faced overheating issues and glitches.\n\nThe Information reported in December that some of Nvidia's top customers \u2014 Microsoft (MSFT), Amazon (AMZN), Google (GOOG), and Meta (META) \u2014 had cut orders of its Blackwell products due to those issues. Those four customers alone purchased an estimated $44 billion worth of Nvidia GPUs in the 2024 calendar year, according to a DA Davidson analysis.\n\nStill, Wall Street analysts maintained their bullish outlooks on the stock ahead of its fourth quarter earnings call Wednesday, despite concerns related to export controls, DeepSeek, and delays of its Blackwell chips.\n\nNvidia logo is seen in this illustration taken, January 27, 2025. REUTERS/Dado Ruvic/Illustration/File Photo \u00b7 Reuters / Reuters\n\nDA Davidson analyst Gil Luria said commentary on Blackwell delays should be taken \u201cwith a grain of salt.\u201d",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Why Nvidia\u2019s earnings are important to the entire U.S. stock market",
            "link": "https://apnews.com/article/nvidia-jensen-huang-artificial-intelligence-ai-9566689191825de4d83900603e0355c4",
            "snippet": "LOS ANGELES (AP) \u2014 Sales of Nvidia's artificial intelligence chip Blackwell will be top of mind when the company releases its latest financial results...",
            "score": 0.8424932360649109,
            "sentiment": null,
            "probability": null,
            "content": "LOS ANGELES (AP) \u2014 Sales of Nvidia\u2019s artificial intelligence chip Blackwell will be top of mind when the company releases its latest financial results Wednesday, with analysts eyeing future demand amid a Chinese upstart\u2019s claim that it can train competitive AI models using far fewer resources.\n\nWall Street expects Nvidia to report fourth-quarter adjusted earnings of 85 cents per share on revenue of $38.08 billion, according to FactSet. The company\u2019s net income is expected to reach $19.58 billion.\n\nWhat happens with Nvidia matters for the entire U.S. stock market. The chip company has grown into the second-largest company on Wall Street, which means the stock\u2019s movement carries more weight on the S&P 500 and other indexes than every company except Apple. The tech giant, based in Santa Clara, California, is now worth over $3 trillion.\n\nNvidia and other companies benefiting from the AI boom have been a major reason the S&P 500 has climbed to record after record recently, with the latest coming last week. Their explosion of profits has helped to propel the market despite worries about stubbornly high inflation and possible pain coming for the U.S. economy from tariffs and other policies of President Donald Trump.\n\nNvidia alone accounted for more than a fifth of all of the S&P 500 index\u2019s total return last year. None of the other 499 companies in the index came close. If Nvidia can\u2019t keep up its momentum, particularly when critics say its stock price has climbed too much and too quickly, Americans holding S&P 500 index funds in their 401(k) and other investing accounts could be set for pain.\n\nThe fourth-quarter earnings will be the company\u2019s first report since Chinese company DeepSeek boasted it had developed a large language model that could compete with ChatGPT and other U.S. rivals, but was more cost-effective in its use of Nvidia chips to train the system on troves of data.\n\nThe frenzy over DeepSeek caused $595 billion in Nvidia\u2019s wealth to briefly vanish. But the company in a statement commended DeepSeek\u2019s work as \u201can excellent AI advancement\u201d that leveraged \u201cwidely-available models and compute that is fully export control compliant.\u201d\n\nNvidia had carved out an early lead in the AI applications race, partly because of founder and CEO Jensen Huang\u2019s successful bet on the chip technology used to fuel the industry. The company is no stranger to big bets. Nvidia\u2019s invention of the graphics processor unit, or GPU, in 1999 helped spark the growth of the PC gaming market and redefined computer graphics.\n\nNvidia will release its quarterly earnings after the market closes Wednesday.\n\n\u2014\u2014\n\nAssociated Press writer Stan Choe contributed to this report.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-24": {
        "0": {
            "title": "Nvidia GeForce RTX 5070 Ti review: bad start, decent GPU",
            "link": "https://www.rockpapershotgun.com/nvidia-geforce-rtx-5070-ti-review",
            "snippet": "I'm a bit late to the Nvidia GeForce RTX 5070 Ti, which means that this review \u2013 while already a dismal failure in the glowing robot eyes of the Internet...",
            "score": 0.891086757183075,
            "sentiment": null,
            "probability": null,
            "content": "Zotac GeForce RTX 5070 Ti AMP Extreme Infinity specs: CUDA Cores: 8960\n\n8960 Base Clock Speed: 2.3GHz\n\n2.3GHz Boost Clock Speed: 2.51GHz\n\n2.51GHz VRAM: 16GB GDDR7\n\n16GB GDDR7 Power: 300W\n\n300W Recommended System Power: 750W\n\n750W Price: \u00a3910 / $930\n\nI\u2019m a bit late to the Nvidia GeForce RTX 5070 Ti, which means that this review \u2013 while already a dismal failure in the glowing robot eyes of the Internet nowness machine \u2013 can at least factor in the context of the GPU\u2019s first few days on sale. Said context can be summarised thusly: Ah. Umm. Errrrrrrr.\n\nNo, early life has not been kind to the RTX 5070 Ti. Inadequate stocks and the lack of an Nvidia-made Founders Edition, unlike those of the RTX 5090 and RTX 5080, have cut the chances of finding one at the intended \u00a3729 / $749 RRP to pretty much nada. Not to mention how some models are shipping with missing render output units, potentially hurting performance. It\u2019s a saddening start, made all the more unfortunate by the fact that the RTX 5070 Ti really isn\u2019t a bad graphics card in itself.\n\nIf it were me, sure, I\u2019d probably hold out for a base price model \u2013 the one I\u2019ve been testing, Zotac\u2019s GeForce RTX 5070 Ti AMP Extreme Infinity, ups itself to \u00a3910 / $930, with styling that\u2019s slightly too 'Decepticon\u2019s lunchbox' for my tastes. But within all that bulk is a GPU that can both eat max-quality 1440p alive and withstand the rigours of 4K, especially with the help of the DLSS 4 Multi Frame Generation (MFG) that the RTX 4070 Ti and RTX 4070 Ti Super miss out on.\n\nNvidia GeForce RTX 5070 Ti review: 4K benchmarks\n\nTo show what I mean, we return to the RPS test rig, which I\u2019ve just updated with an AMD Ryzen 7 9800X3D (taking the place of the Intel Core i9-13900K). For the second time in 2025, this kills the comparison value of all previous GPU benchmark results \u2013 you\u2019re welcome! \u2013 but the 9800X3D should prove the better option in time, especially for the 1080p and 1440p tests where it most comfortably beats the i9-13900K. Anyhow, look, graphs:\n\nImage credit: Rock Paper Shotgun\n\nIn addition to thumping the RTX 4070 Ti in relatively faster games like Shadow of the Tomb Raider and Metro Exodus, the RTX 5070 Ti also keeps its head much better in tricky ones like Cyberpunk 2077 F1 24. It\u2019s actually knocking on the RTX 5080\u2019s door in the latter, and is more or less neck and neck in Assassin\u2019s Creed Mirage.\n\nI didn\u2019t have a handy RTX 4070 Ti Super to test with the new rig, though extrapolating from how it compared to the RTX 4070 Ti in previous tests, I\u2019d be surprised if you wouldn\u2019t still get double-digit framerate upgrades from all of these games outside Horizon Forbidden West. The RTX 5070 Ti also sticks with the last-gen Super version\u2019s VRAM upgrades, and goes one step further by switching its 16GB of 256-bit memory to GDDR7. Definitely enough for 4K, in other words.\n\nImage credit: Rock Paper Shotgun\n\nThe RTX 5070 Ti is also more flexible with its frame generation. The usual disclaimers apply: Nvidia\u2019s AI generated frames increase input lag rather than cutting it, so you should always ensure a stable base of traditionally rendered frames before trying frame gen. But that\u2019s more attainable with the new model than it was on the RTX 4070 Ti \u2013 with upscaling, the RTX 5070 Ti can just about manage playable framerates with Cyberpunk 2077 and Alan Wake 2\u2019s harshest settings, path tracing included.\n\nImage credit: Rock Paper Shotgun\n\nObviously, 4x MFG puts visual smoothness beyond anything the RTX 4070 Ti can hope for. What\u2019s arguably more important, though, is that the older GPU falls short of 30fps, meaning it\u2019s not suitable for running this calibre of quality settings with out without frame gen. The RTX 5070 Ti is.\n\nNvidia GeForce RTX 5070 Ti review: 1440p benchmarks\n\nThe RTX 5070 Ti also makes sense as a luxury 1440p driver. Although neither of its predecessors were lacking here, the new GPU still makes some sizeable gains, with Horizon Forbidden West and Total War: Warhammer III benefitting especially. There also looks to be plenty of headroom for more pixel-heavy ultrawide resolutions, like 3440x1440.\n\nImage credit: Rock Paper Shotgun\n\nWithout the added strain of 4K, the RTX 5070 Ti can also really start excelling with those difficult path tracing and ray tracing effects. Take Alan Wake 2: the RTX 4070 Ti is plenty playable here, averaging 45fps with the aid of Quality-level DLSS upscaling, but the RTX 5070 Ti\u2019s 55fps is visibly slicker and slightly more responsive in the hand. Then, once again, MFG steps in to win the numbers game decisively.\n\nImage credit: Rock Paper Shotgun\n\nThe only real cause for pause here \u2013 not that you can simply hop on Ebuyer right now and rush your way to an RTX 5070 Ti purchase \u2013 is that it\u2019ll be worth waiting to see how the incoming RTX 5070 compares, possibly as well as the AMD Radeon 9070 XT. The RTX 5070 Ti can evidently coast its way to smooth 1440p, though its close proximity to the RTX 5080 also makes me wonder if it might be more of a 'budget' (relatively speaking) 4K contender, with cheaper alternatives making for better value on Quad HD monitors. We\u2019ll see.\n\nNvidia GeForce RTX 5070 Ti review: 1080p benchmarks\n\nUnlike with the RTX 5080, I suspect there might be at least some maddened souls who\u2019d consider an RTX 5070 Ti for humble 1080p. Thus, here\u2019s a graph to show how silly that would be:\n\nImage credit: Rock Paper Shotgun\n\nIt\u2019s not that the RTX 5070 Ti stops being faster than what came before \u2013 granted, with the aid of perhaps the most bottleneck-avoidant CPU on the planet \u2013 but when you get so far into the triple digits, numerical advantages cease to have the impact that they do below 120fps or so. Especially so, if you don\u2019t have the kind of high-end monitor whose refresh rate is enough to keep pace with the frame output.\n\nThere are certainly much more efficient graphics cards for 1080p, with the RTX 5070 Ti being rated for 300W of max power drinkage and requiring at least a 700W PSU. 750W, in the case of this Zotac model, though at least it\u2019s not an energy hog as a matter of course: I recorded it peaking at 253W, well shy of its official maximum, while running an intensive Cyberpunk 2077 session. I\u2019ll also concede that one upside to its goliath dimensions is cooling performance, with the GPU temp only reaching 54\u00b0c under load. That\u2019s positively icy by modern graphics card standards; the high-tech, dual-axial fan design of the RTX 5080 Founders Edition cooler could only keep its lid at 65\u00b0c.\n\nImage credit: Rock Paper Shotgun\n\nFor \u00a3729, you could chalk up the RTX 5070 Ti as another serviceable if rarely exciting Nvidia GPU. As we\u2019ve seen, however, the problem is that you have about as much chance of getting this card for \u00a3729 as you do of buying Alexandra Palace for a tenner. And when actual prices are running so close to the entry level of the RTX 5080 \u2013 which, for all its own stock shortages, has a Founders Edition that is at least sold in theory for under a grand \u2013 then it\u2019s fair to start asking questions. Questions like \"Why should I not just get the 5080 instead?\" and \"How, Nvidia, did you forget what happened in 2020?\"\n\nWhile the RTX 5080 goes harder on PSUs, and doesn\u2019t provide much meaningful extra oomph at 1440p, it is more generally futureproofed, with a marked 4K advantage that can be enjoyed today. At saner prices, the RTX 5070 Ti would still be more of a crowdpleaser, but there are clearly some kinks that need working out before it\u2019s seriously worth your cash.\n\nThis review is based on a retail unit provided by the manufacturer.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "NVIDIA Earth-2 Features First Gen AI to Power Weather Super-Resolution for Continental US",
            "link": "https://blogs.nvidia.com/blog/earth-2-ai-high-resolution-forecasts/",
            "snippet": "Up to 10000 times more energy efficient than traditional high-resolution weather prediction, Earth-2's CorrDiff model could improve disaster preparedness...",
            "score": 0.7320770025253296,
            "sentiment": null,
            "probability": null,
            "content": "Your browser doesn't support HTML5 video. Here is a link to the video instead.\n\nUp to 10,000 times more energy efficient than traditional high-resolution weather prediction, Earth-2\u2019s CorrDiff model could improve disaster preparedness and save lives by enabling AI-powered kilometer-scale forecasts.\n\nTo better prepare communities for extreme weather, forecasters first need to see exactly where it\u2019ll land.\n\nThat\u2019s why weather agencies and climate scientists around the world are harnessing NVIDIA CorrDiff, a generative AI weather model that enables kilometer-scale forecasts of wind, temperature, and precipitation type and amount. It\u2019s part of the NVIDIA Earth-2 platform for simulating weather and climate conditions.\n\nThe paper behind CorrDiff was featured today in Communications Earth and Environment, part of the Nature portfolio of scientific journals. Available as an easy-to-deploy NVIDIA NIM microservice, the model is already being used by weather technology companies, researchers and government agencies to enhance their forecasts.\n\nWith the rising frequency of extreme weather events, fast, high-resolution predictions of weather phenomena could help mitigate risks to people, communities and economies by supporting risk assessment, evacuation planning, disaster management and the development of climate-resilient infrastructure.\n\nWeather agencies and startups across the globe are adopting CorrDiff and other Earth-2 tools to improve the resolution and precision of forecasts for extreme weather phenomena, renewable energy management and agricultural planning.\n\nHigh-Fidelity Forecasts on the Horizon\n\nCorrDiff uses generative AI to sharpen the precision of coarse-resolution weather models \u2014 resolving atmospheric data from 25-kilometer scale down to 2 kilometers using diffusion modeling, the same kind of AI model architecture that powers today\u2019s text-to-image generation services.\n\nIn addition to boosting image resolution, CorrDiff can also predict related variables that weren\u2019t present in the input data \u2014 such as radar reflectivity, which is used as an indicator of rain location and intensity.\n\nCorrDiff was trained on the Weather Research and Forecasting model\u2019s numerical simulations to generate weather patterns at 12x higher resolution.\n\nThe initial CorrDiff model, announced at NVIDIA GTC 2024 and described in the Communications Earth and Environment paper, was optimized on Taiwan weather data in collaboration with its Central Weather Administration.\n\nNVIDIA researchers and engineers next worked to efficiently scale the model to cover a larger section of the globe. The version released as an NVIDIA NIM microservice at Supercomputing 2024 covers the continental United States \u2014 trained on U.S. weather data, with sample datasets of real-world natural disasters including hurricanes, floods, winter storms, tornados and cold waves.\n\nThe optimized CorrDiff NIM microservice for U.S. data is 500x faster and 10,000x more energy-efficient than traditional high-resolution numerical weather prediction using CPUs.\n\nThe research team behind CorrDiff continues to advance the model\u2019s capabilities, and has released additional generative AI diffusion models showing how the model could be enhanced to more robustly resolve small-scale details in different environments \u2014 and better capture rare or extreme weather events.\n\nCorrDiff could also help with downwash prediction \u2014 when strong winds funnel down to street level, damaging buildings and affecting pedestrians \u2014 in urban areas.\n\nWeather Agencies Put CorrDiff on the Map\n\nMeteorological agencies and companies around the globe are tapping CorrDiff to accelerate predictions with applications in regional forecasting, renewable energy and disaster management.\n\nTaiwan\u2019s National Science and Technology Center for Disaster Reduction, for instance, has deployed the CorrDiff to support disaster alerts in the region, enabling an estimated gigawatt-hour of energy savings due to the energy efficiency of CorrDiff running on the NVIDIA AI platform. CorrDiff predictions are embedded in the center\u2019s disaster monitoring site, helping Taiwan forecasters better prepare for typhoons.\n\nDiscover Earth-2 at NVIDIA GTC\n\nLearn more about AI applications using Earth-2 at NVIDIA GTC, the global AI conference taking place March 17-21 in San Jose, California. Relevant sessions include:\n\nLearn how AI is transforming climate science in the TEDx talk below by Mike Pritchard, director of climate simulation research at NVIDIA:",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia takes EU antitrust regulators to court for probing AI startup Run:ai bid",
            "link": "https://www.reuters.com/technology/nvidia-takes-eu-antitrust-regulators-court-probing-ai-startup-runai-bid-2025-02-24/",
            "snippet": "U.S. chipmaker Nvidia has sued EU antitrust regulators for accepting an Italian request last year to scrutinise its acquisition of AI startup Run:ai,...",
            "score": 0.5913147330284119,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia investors, AI boosters await key earnings report",
            "link": "https://www.marketplace.org/2025/02/24/nvidia-investors-ai-boosters-await-key-earnings-report/",
            "snippet": "The vaunted chipmaker is a bellwether for tech stocks and the generative AI industry. But uncertainty has crept into views of the company.",
            "score": 0.8948653340339661,
            "sentiment": null,
            "probability": null,
            "content": "We get earnings reports from publicly traded companies every quarter. They\u2019re essentially a set of metrics that illustrate a company\u2019s financial performance, which investors typically compare to Wall Street\u2019s estimates.\n\nMost of them come and go without much fanfare. But this week, we\u2019ll see a report from a company that everybody seems to be watching. It\u2019s already making headlines, and the numbers don\u2019t even come out until Wednesday.\n\nThe company is Nvidia \u2014 a tech giant, chipmaker and bellwether for the artificial intelligence industry. But, still, it\u2019s just an earnings report.\n\nThere are two main reasons to watch Nvidia, said Erik Gordon, a professor at the University of Michigan\u2019s business school. \u201cThe first is its sheer size and its influence on the stock market.\u201d\n\nGordon said last year, Nvidia\u2019s stock contributed 22% of the S&P 500\u2019s total gain. The other reason? \u201cNvidia\u2019s sort of the poster stock of AI. It\u2019s the way most of us can invest in AI.\u201d\n\nNvidia\u2019s success is based on its graphics processing units, or GPUs, making up around 90% of its market.\n\n\u201cDuring the dot-com era, there was something like 2,888 startups that went public. So far, there have been zero startups that have gone public for generative AI,\u201d said Peter Cohan, professor of management practice at Babson College and author of a book on generative AI.\n\nThat\u2019s not normal, said Cohan.\n\nNvidia became such a gorilla in part because it got its start long before AI took off. Cohan said its GPUs were originally used for gaming.\n\n\u201cThen used for crypto mining, and now it has proliferated to be used by data centers that are training and operating large language models like ChatGPT,\u201d said Cohan.\n\nNvidia\u2019s report this week is drawing even more attention than usual because more analysts have raised questions about its continued dominance. One problem, said Matt Bryson of Wedbush Securities: DeepSeek, a Chinese AI company, made a splash with its AI model and chatbot in January.\n\n\u201cChina\u2019s development suggested that you can do more with fewer hardware resources, so in this case, fewer Nvidia GPUs,\u201d said Bryson.\n\nNvidia\u2019s stock plunged 17% after DeepSeek\u2019s announcement. The second point of vulnerability is Nvidia\u2019s new so-called Superchip for the next generation of AI.\n\n\u201cThere have been some fits and starts in terms of production,\u201d said Bryson.\n\nBut Bryson isn\u2019t concerned. He said the new chip is ramping up after a slow start, and if DeepSeek makes AI more affordable to more companies, it could actually increase demand for AI hardware.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia to report Q4 earnings Wednesday as tariff, export control threats loom",
            "link": "https://finance.yahoo.com/news/nvidia-to-report-q4-earnings-wednesday-as-tariff-export-control-threats-loom-090009145.html",
            "snippet": "Nvidia (NVDA) will report its highly anticipated fourth quarter earnings after the bell on Wednesday. Analysts and investors will be focused on how much...",
            "score": 0.9470021724700928,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) reported its fourth quarter earnings after the bell on Wednesday, beating analysts' expectations on the top and bottom lines and issuing solid Q1 guidance.\n\nNvidia's stock rose 2.5% before the bell on Thursday, coming back from small loss earlier in premarket trading as investors digested the report.\n\nNvidia's earnings come as the company girds itself for potential 25% tariffs on chips imported into the US and the threat of increased export controls on its shipments to China. The AI giant is also contending with the fallout from claims that Chinese startup DeepSeek developed its AI models using less powerful Nvidia chips than its US rivals, putting into question whether Big Tech companies are over-investing in AI.\n\nFor the quarter, Nvidia reported earnings per share (EPS) of $0.89 on revenue of $39.3 billion. Wall Street was expecting EPS of $0.84 on revenue of $38.2 billion. The company said it expects Q1 revenue of $43 billion plus or minus 2%, better than the $42.3 billion expected.\n\nData center revenue clocked in at $35.6 billion versus expectations of $34 billion in the quarter.\n\n\"We\u2019ve successfully ramped up the massive-scale production of Blackwell AI supercomputers, achieving billions of dollars in sales in its first quarter,\" CEO Jensen Huang said in a statement. \"AI is advancing at light speed as agentic AI and physical AI set the stage for the next wave of AI to revolutionize the largest industries.\u201d\n\nAccording to Nvidia CFO Colette Kress, cloud service providers made up 50% of Nvidia's data center revenue in the quarter. The company reported similar results in Q3.\n\nThe company's Blackwell line of chips contributed billions in sales for the quarter, Kress said.\n\nRead more: How does Nvidia make money?\n\n\"We delivered $11.0 billion of Blackwell architecture revenue in the fourth quarter of fiscal 2025, the fastest product ramp in our company\u2019s history.\"\n\nNvidia's gaming revenue, however, fell 11% year over year in Q4 due to supply constraints around its latest gaming chips.\n\nNvidia is the reigning champion of AI chips, and it\u2019s not losing that crown anytime soon. Its chips are the envy of Silicon Valley and beyond, and its competitors are still far from overtaking its performance advantage.\n\nBig Tech companies Amazon (AMZN), Google (GOOG, GOOGL), Meta (META), and Microsoft (MSFT) are spending billions of dollars building out their AI data centers, and a chunk of that is going straight to Nvidia.\n\nBut shares of those same companies are also struggling in the early months of 2025.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Nvidia Stock To $70?",
            "link": "https://www.forbes.com/sites/greatspeculations/2025/02/24/nvidias-50-downside/",
            "snippet": "Nvidia, the $3 Trillion AI posterchild, is scheduled to report earnings on Wednesday, February 26. The company will likely continue to report robust demand...",
            "score": 0.8235993385314941,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "What Analysts Think of Nvidia Stock Ahead of Earnings",
            "link": "https://www.investopedia.com/what-analysts-think-of-nvidia-stock-ahead-of-earnings-q4-fy2025-update-11683525",
            "snippet": "Nvidia is set to report fourth-quarter results after the market closes Wednesday, with analysts widely bullish on the AI chipmaker's stock.",
            "score": 0.9336270093917847,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia is set to report fourth-quarter results after the market closes Wednesday.\n\nThe chipmaker is expected to post another quarterly sales record on strong demand for its advanced chips.\n\nAnalysts are widely bullish on the AI chipmaker's stock, with all but one of the analysts tracked by Visible Alpha issuing a \"buy\" or equivalent rating.\n\nNvidia (NVDA) is set to report fourth-quarter results after the market closes Wednesday, with analysts widely bullish on the AI chipmaker's stock.\n\nAll but one of the 18 analysts covering the stock tracked by Visible Alpha have issued \u201cbuy\u201d or equivalent ratings, with one analyst giving the stock a \u201chold\u201d rating. Their consensus price target of about $175 would imply 32% upside from Wednesday's intraday price.\n\nWedbush and Oppenheimer analysts, who reiterated $175 price targets in the days ahead of the report, suggested booming demand for the company's advanced chips as Big Tech firms ramp up spending on AI infrastructure could lead to another strong quarter.\n\nNvidia is projected to post record quarterly revenue of $38.34 billion, up 73% year-over-year, according to estimates compiled by Visible Alpha. Net income is expected to climb to $21.1 billion, from $12.84 billion a year earlier.\n\nUBS analysts, who maintained a $185 price target, noted \"investor expectations having crept up a bit recently,\" and added supply chain improvements could mean higher sales of Nvidia\u2019s Blackwell line. UBS nearly doubled its estimate for Blackwell\u2019s contribution to fourth-quarter revenue to $9 billion, up from $5 billion previously.\n\nOppenheimer also indicated the rapid rise of Chinese AI startup DeepSeek could ultimately prove \"positive\" for the chipmaker, as competition pushes Nvidia's American clients to step up their efforts in the AI race instead of pulling back.\n\nShares of Nvidia were up close to 5% at $132.40 in intraday trading Wednesday. They've gained more than two-thirds of their value over the past 12 months.\n\n\n\nUPDATE\u2014Feb. 26, 2025: This article has been updated since it was first published to reflect more recent analyst estimates and share price values.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia admits some early RTX 5080 cards are missing ROPs, too",
            "link": "https://www.theverge.com/news/618748/nvidia-admits-the-rtx-5080-is-affecte",
            "snippet": "Nvidia has confirmed to The Verge that RTX 5080 production was affected by the same issue as earlier 50-series cards, but RTX 5070 will not.",
            "score": 0.9185587167739868,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and founding member of The Verge who covers gadgets, games, and toys. He spent 15 years editing the likes of CNET, Gizmodo, and Engadget.\n\nWhen Nvidia originally confirmed that some of its new RTX 50-series graphics cards had a \u201crare\u201d manufacturing issue that left them missing some promised render units and a slight amount of performance as a result, it only named three affected cards: the RTX 5090, RTX 5090D, and RTX 5070 Ti. But now, Nvidia has confirmed to us that RTX 5080 production was affected by the same issue as well.\n\n\u201cUpon further investigation, we\u2019ve identified that an early production build of GeForce RTX 5080 GPUs were also affected by the same issue. Affected consumers can contact the board manufacturer for a replacement,\u201d Nvidia GeForce global PR director Ben Berraondo tells The Verge.\n\nIn response to The Verge\u2019s questions, Berraondo adds that \u201cno other Nvidia GPUs have been affected\u201d \u2014 we specifically asked about the upcoming RTX 5070, and he says it\u2019s not affected either. Nor should any cards be affected that were produced more recently: \u201cThe production anomaly has been corrected,\u201d he says. In case you\u2019re wondering, he also told us that Nvidia was not aware of these issues before it launched these GPUs.\n\nHere\u2019s the company\u2019s full amended statement:\n\nWe have identified a rare issue affecting less than 0.5% (half a percent) of GeForce RTX 5090 / 5090D, RTX 5080, and 5070 Ti GPUs which have one fewer ROP than specified. The average graphical performance impact is 4%, with no impact on AI and Compute workloads. Affected consumers can contact the board manufacturer for a replacement. The production anomaly has been corrected.\n\nOne specific Redditor was the one to discover that their RTX 5080 also demonstrated the issue; he\u2019s since said he\u2019s worked out a deal to hand that card to GamersNexus, which is investigating the RTX 50-series issues, for more study.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "There Are Too Many Damn Problems With Nvidia\u2019s $2,000 RTX 5090",
            "link": "https://gizmodo.com/rtx-5090-problems-2000567696",
            "snippet": "There's an issue with some of the Nvidia GeForce RTX 50-series graphics cards, and it's down to the very components inside the GPU. Nvidia said it has...",
            "score": 0.8148174285888672,
            "sentiment": null,
            "probability": null,
            "content": "There\u2019s an issue with some of the Nvidia GeForce RTX 50-series graphics cards, and it\u2019s down to the very components inside the GPU. Nvidia said it has identified a \u201crare issue\u201d with a small number of RTX 5090 and RTX 5070 Ti graphics cards that makes them run at a lower performance than they should. The good news is that Nvidia promises manufacturers that it will replace the impacted cards. The bad news is you\u2019ll still struggle to buy one, even while some users complained their cards failed to work on startup or\u2014in one case\u2014literally caught on fire.\n\nRTX 5090 owners have been complaining online for over a week that some cards shipped with fewer ROPs, AKA render/raster output units than they should have. The RTX 5090 review units supposedly have 176 ROPs enabled, according to Nvidia\u2019s white paper on the RTX 50-series, but some AIB variants may be carrying Blackwell architecture with fewer functioning ROPs. On Saturday, Nvidia confirmed board owners\u2019 worst fears with The Verge, saying the issue impacted .5% of the RTX 5090 and RTX 5070 Ti cards sold so far. The company said this led to around \u201c4%\u201d worse graphics performance. The issue with the Blackwell infrastructure also impacted the China-centric RTX 5090D. Nvidia said it has fixed the manufacturing issue.\n\nOwners need to figure out for themselves whether their ultra-expensive graphics card is underperforming. TechPowerUp, which helped identify the issue, cited the Zotac RTX 5090 Solid cards were underperforming using the site\u2019s GPU-Z tool, which can spell out the number of ROPs in your card. TechPowerUp identified eight ROP units missing on the 5090, resulting in 4.5% raster capabilities with the Zotac card. The raster output units, which assist in the final pieces of the RTX rendering pipeline, were either missing or defective. ROPs are more important for tasks like anti-aliasing and texture rendering, so users may not experience as big a drop in some games as others.\n\nThe RTX 5070 Ti is supposed to have 96 ROPs, as our review unit supports. Some forum users (via Tom\u2019sHardware) claim some cards have shipped with 88 ROPs, leading to a supposed 10% drop in performance in some games. To remedy the situation, users will need to contact these separate companies and work with them on replacements. Gizmodo reached out to Zotac to identify how users can seek GPU replacements, and we\u2019ll update this story when we hear back.\n\nThe issue may be present on Founder Edition cards and the full range of AIBs from Asus, MSI, and Gigabyte, to name a few. If Nvidia could correct the issue with software updates, it would. This indicates the issue is a flaw with at least a few of the Blackwell GB202 chips themselves, which indicates the problem stemmed from Nvidia as the original manufacturer.\n\nThe RTX 50-series launch has been met with more than the usual number of hangups. The so-called \u201cCablegate\u201d 12VHPWR cable melting SNAFU of the Nvidia GeForce RTX 4090 launch seems blas\u00e9 compared to this wild west of GPU controversies. Still, there is a growing sense among the Nvidia user community that the latest cards are stretching what a regular consumer-end PC can handle, even without the 12VHPWR connector.\n\nOn Reddit (via VideoCardz), one user claimed that they were using an Asus ROG Astral RTX 5090 and experienced a catastrophic failure where his board \u201ccaught on fire.\u201d The Astral is a massive four-slot, four-fan variant of the top-end GPU that costs more than $3,000. The user inquisition claims they had played some games on their PC and were browsing the web when the PC shut down. On a restart, smoke started coming out of the case, and they discovered burn marks on the GPU and motherboard.\n\nWithout jumping to conclusions, there\u2019s not much to go on about why that GPU would burst into flames, though judging by the images, the issue did not start on the power connector. YouTuber Buildzoid blamed the GPU\u2019s power stage, not the capacitor as some users originally speculated. The issue is likely on the part of Asus, rather than Nvidia.\n\nNvidia clearly needed to rethink its stock and quality assurance plans before launching the RTX 50-series. Reviewers expect some growing pains during the review process for a new product, especially one as technical as a GPU. While testing the RTX 5090 and RTX 5070 Ti, we experienced multiple issues where the PC would load to a black screen after booting from shutoff or after an extended sleep period. Nvidia said it repaired those issues with its latest driver update, 572.47, though users continue expressing discontent on the official Nvidia forums. Nvidia reps told users they are still investigating the issue. The fix may have to come in a driver or VBIOS update.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia unveils generative AI model for local weather forecasts",
            "link": "https://www.axios.com/2025/02/24/nvidia-ai-weather-forecast-corediff",
            "snippet": "The new model results in forecasts at smaller scales, which can make them more accurate.",
            "score": 0.9075520634651184,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-23": {
        "0": {
            "title": "One ridiculous chart on Nvidia ahead of earnings",
            "link": "https://finance.yahoo.com/news/one-ridiculous-chart-on-nvidia-ahead-of-earnings-133020976.html",
            "snippet": "Is Nvidia just too cheap?",
            "score": 0.8812044262886047,
            "sentiment": null,
            "probability": null,
            "content": "This is The Takeaway from today's Morning Brief, which you can sign up to receive in your inbox every morning along with:\n\nThe chart of the day\n\nWhat we're watching\n\nWhat we're reading\n\nEconomic data releases and earnings\n\nI have asked around my circles and can't seem to get a straight answer on why Nvidia's (NVDA) stock looks so cheaply valued on a price-to-earnings multiple basis.\n\nSo, I am coming to you for answers \u2014 drop them to me on X @BrianSozzi. What am I missing with Nvidia? Let's discuss!\n\nI caution this is not me suggesting a Nvidia buying spree heading into earnings on Feb. 26. Yahoo Finance isn't a stock trading platform or investment bank. We are in the context game as it pertains to investing.\n\nThis is just a general callout that the market may have a flaw in how it's valuing Nvidia.\n\nNvidia is among the most cheaply valued AI stocks at the moment, if you can wrap your head around that one!\n\nOn a forward price-to-earnings multiple basis, Yahoo Finance data shows Nvidia trading at 31 times. Broadcom (AVGO) and Marvell Technology (MRVL) are valued at 35 times and 41 times, respectively. Arm Holdings (ARM) clocks in at 76 times.\n\nListen: What Bill Gates thinks of Nvidia's CEO\n\nZoom out further, and Nvidia's stock is trading at a discount to several other \"Magnificent Seven\" members.\n\nTesla's (TSLA) stock is trading at 121 times forward earnings. Amazon (AMZN) trades at 36 times.\n\nThere are two reasons for this odd valuation level on Nvidia, the former analyst in me posits.\n\nOne, the Street is underbaking its forward estimates on Nvidia's earnings power.\n\nYahoo Finance data shows Nvidia's first quarter earnings per share (EPS) trend has drifted modestly lower over the past 30 days. The Street has also not pushed up its 2025 EPS estimates on Nvidia for more than 60 days.\n\nI find this bizarre.\n\nDespite China-based DeepSeek rocking the super-bullish AI thesis earlier this year, Wall Street still sees Nvidia profiting from the global buildout of AI infrastructure. Aggressive 2025 capital expenditures assumptions by hyperscalers such as Amazon (AMZN) and Meta (META) shared during this earnings season underscore the point.\n\n\"Over the coming decades, the investment [in artificial intelligence] is happening,\" Russell Investments chief investment officer Kate El-Hillow told me on Yahoo Finance's Opening Bid podcast (video above).\n\nThen the other possible explanation is with EPS estimates not rising, Nvidia's stock price is in wait-and-see mode. While the stock has rallied hard off the February DeepSeek lows, it has still underperformed the S&P 500 (^GSPC) this year. Shares are down from early November 2024 highs.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Wall St Week Ahead Nvidia to offer AI trades reality check",
            "link": "https://www.reuters.com/markets/us/wall-st-week-ahead-nvidia-offer-ai-trades-reality-check-2025-02-21/",
            "snippet": "Nvidia's profit report next week could steer the U.S. stock market's course, as investors seek confirmation that the AI-driven investment trend,...",
            "score": 0.8044491410255432,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "All eyes on Nvidia\u2019s earnings: A high-stakes moment for AI stocks",
            "link": "https://www.home.saxo/content/articles/equities/all-eyes-on-nvidias-earnings-a-high-stakes-moment-for-ai-stocks-24022025",
            "snippet": "Nvidia is no longer just a semiconductor company \u2013 it's the backbone of artificial intelligence. Its GPUs power the AI models behind OpenAI's ChatGPT,...",
            "score": 0.7683687210083008,
            "sentiment": null,
            "probability": null,
            "content": "Key points:\n\nNvidia\u2019s earnings will set the tone for AI stocks, with sky-high expectations leaving little room for error.\n\nNvidia\u2019s earnings will set the tone for AI stocks, with sky-high expectations leaving little room for error. Strong AI demand continues, but supply chain constraints and competition could challenge Nvidia\u2019s dominance.\n\nMarket volatility is expected, with options pricing in a 7-8% move\u2014investors should brace for sharp swings.\n\nThe AI revolution has a king, and its name is Nvidia. But will its upcoming earnings prove it deserves the throne, or are cracks beginning to form? As the dominant force in AI infrastructure, Nvidia\u2019s report on 26 February is set to be a defining moment\u2014not just for the company, but for the entire AI sector.\n\n\n\nWith sky-high expectations baked into its stock price, Nvidia must deliver flawless execution to keep the rally alive. If it stumbles, it could send shockwaves through AI stocks and the broader market.\n\nThe key numbers to watch\n\nInvestors are expecting another blockbuster quarter from Nvidia.\n\nRevenue is forecasted to hit USD 38 billion, marking a 72% year-over-year increase.\n\nis forecasted to hit USD 38 billion, marking a 72% year-over-year increase. Earnings per share (EPS) is projected to rise to USD 0.84, a 62% jump.\n\n\"Nvidia has a strong track record, having beaten Wall Street estimates in 16 of the last 18 quarters \u2013 but with expectations already sky-high, even a solid beat may not be enough to keep the stock moving higher.\"\n\nInvestors will also focus on gross margins, a key measure of profitability, particularly as the company rolls out its next-generation Blackwell chips. While product launches can put pressure on margins, Nvidia has maintained a strong profitability buffer, reporting a 75% gross margin last quarter, with guidance set around 73% for the upcoming quarters. Investors will be looking for reassurance that these levels remain intact despite rising production costs.\n\n\n\nData centre growth remains Nvidia\u2019s primary revenue driver, fuelled by major hyperscalers increasing their AI infrastructure investments. The company has a strong track record, having beaten Wall Street estimates in 16 of the last 18 quarters \u2013 but with expectations already sky-high, even a solid beat may not be enough to keep the stock moving higher.\n\n\n\nWhy Nvidia\u2019s earnings matter\n\nNvidia is no longer just a semiconductor company \u2013 it\u2019s the backbone of artificial intelligence. Its GPUs power the AI models behind OpenAI\u2019s ChatGPT, Meta\u2019s AI initiatives, and Tesla\u2019s autonomous driving technology. With a valuation exceeding USD 3.3 trillion, Nvidia\u2019s stock performance has a significant impact on the broader market, particularly tech-heavy indices like the Nasdaq. Given its dominance, any signs of weakness in Nvidia\u2019s report could have outsized effects on investor sentiment towards AI stocks as a whole.\n\nChallenges to Nvidia\u2019s AI leadership\n\nNvidia\u2019s highly anticipated Blackwell architecture is expected to deliver a massive leap in AI performance, but execution risks remain. Supply chain constraints and production delays could impact shipment volumes, affecting near-term revenue growth. Investors will be keen to hear whether Nvidia can meet demand or if customers may need to look elsewhere.\n\n\n\nAt the same time, competition is mounting. In January, Nvidia\u2019s stock dropped 17% in one day after Chinese AI firm DeepSeek claimed it could train models with significantly fewer GPUs. While Nvidia quickly rebounded, the episode raised concerns about efficiency gains reducing the company\u2019s long-term growth trajectory. CEO Jensen Huang has dismissed these worries, emphasising that AI workloads are only becoming more computationally intensive. However, investors will be watching closely for any updates on potential efficiency threats.\n\nAI spending isn\u2019t slowing down\u2014but can Nvidia capture it?\n\nAmazon, Google, and Microsoft are investing billions into AI infrastructure, ensuring continued demand for Nvidia\u2019s chips. Meanwhile, Europe is stepping up its AI investments. France has committed substantial private sector funding towards AI infrastructure, while the European Union is working to expand AI supercomputing capabilities to strengthen its competitiveness in the global AI race. These initiatives reflect a strategic push to reduce dependence on external technology providers.\n\n\n\nAt the same time, Elon Musk has made major investments in Nvidia hardware, reinforcing the company\u2019s critical role in powering next-generation AI applications. However, with Blackwell supply constraints and rising competition, Nvidia must execute flawlessly to capture this demand.\n\n\u201cThe options market is pricing in a 7-8% move post-earnings, indicating a wide range of possible outcomes.\u201d\n\nHow investors should prepare for volatility\n\nNvidia\u2019s earnings reports are consistently among the most market-moving events in tech, and this one is no different. The options market is pricing in a 7-8% move post-earnings, indicating a wide range of possible outcomes. Given Nvidia\u2019s dominance in AI, any deviation from expectations \u2013 whether positive or negative \u2013 could have an outsized impact on the stock price.\n\n\n\nHow could the stock react? Here are three possible scenarios:\n\nIf Nvidia beats expectations and raises guidance: Expect a strong rally, but profit-taking could limit gains.\n\nIf Nvidia meets expectations but doesn\u2019t raise guidance: The stock could remain flat or even dip, as much of the optimism is already priced in.\n\nIf Nvidia misses expectations or warns on supply chain issues: A sharp decline is likely, but long-term investors may see a buying opportunity.\n\nGiven the mix of long-term investors, short-term traders, and hedge funds holding Nvidia, even a strong report could lead to volatility as some take profits. Investors should be prepared for sharp moves in either direction.\n\n\n\nWhat to watch in the earnings call\n\nInvestors should pay close attention to management\u2019s commentary on:\n\nBlackwell chip supply : Can Nvidia meet demand, or will production constraints limit growth?\n\n: Can Nvidia meet demand, or will production constraints limit growth? Margins and profitability : Will Nvidia maintain its historically high gross margins amid rising production costs?\n\n: Will Nvidia maintain its historically high gross margins amid rising production costs? Tariffs and trade risks : Could regulatory challenges impact global sales, particularly in China?\n\n: Could regulatory challenges impact global sales, particularly in China? Hyperscaler AI spending : Are Amazon, Microsoft, and Google continuing to ramp up their AI investments?\n\n: Are Amazon, Microsoft, and Google continuing to ramp up their AI investments? Guidance for next quarter: Does Nvidia expect growth to remain strong, or are signs of slowing demand emerging?\n\nA defining moment for AI stocks\n\nThis earnings report isn\u2019t just about Nvidia - it\u2019s about whether the AI revolution can maintain its breakneck pace. If Nvidia delivers, AI stocks could surge higher. If it stumbles, the sector may see a much-needed reality check on how fast AI can grow.\n\nOne thing is clear: Nvidia is still the driving force behind AI\u2019s growth, and this report will either confirm its dominance or expose vulnerabilities. Either way, investors should buckle up - it\u2019s going to be a wild ride.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Should You Buy Nvidia Stock Hand Over Fist Before Feb. 26?",
            "link": "https://www.fool.com/investing/2025/02/23/should-you-buy-nvidia-stock-hand-over-fist-before/",
            "snippet": "The DeepSeek dip is nearly over for Nvidia (NVDA -0.14%). I'm referring, of course, to the thrashing Nvidia's share price took last month when many...",
            "score": 0.9311837553977966,
            "sentiment": null,
            "probability": null,
            "content": "The DeepSeek dip is nearly over for Nvidia (NVDA 5.27%). I'm referring, of course, to the thrashing Nvidia's share price took last month when many investors panicked about the threat presented by Chinese artificial intelligence (AI) company DeepSeek. Although Nvidia's stock sank as much as 21% below its previous high, most of that loss has evaporated.\n\nBut while it's too late to buy Nvidia on the DeepSeek-induced dip, another potential catalyst beckons. The GPU maker is scheduled to announce its fiscal 2025 fourth-quarter and full-year results later this week. Should you buy Nvidia stock hand over fist before Feb. 26?\n\nThe underlying premise of the question\n\nPeople concerned about whether to buy Nvidia stock in the next three trading days might be focusing on whether there's a good possibility that Nvidia's share price will jump after its Q4 update after the market closes on Feb. 26.\n\nHistory does signal that a post-earnings jump could be coming. Especially following OpenAI's launch of ChatGPT in November 2022, Nvidia has a great track record of exceeding Wall Street's earnings expectations. And its share price has often performed well afterward.\n\nThe \"E's\" on the above chart show when Nvidia reported its quarterly earnings. After six of those nine updates, the stock rose afterward. However, astute readers will notice that Nvidia's shares didn't rise immediately after its two most recent quarterly updates.\n\nWill Nvidia stock jump after its Q4 update?\n\nThe short answer to this question is that there's no way to know for sure. However, we can make an educated guess.\n\nFirst, it's important to understand what it will take for Nvidia to beat Wall Street's expectations. The average Q4 revenue estimate of analysts surveyed by LSEG is $38.13 billion. The average earnings per share (EPS) estimate is $0.85. To hit these numbers, Nvidia must deliver year-over-year revenue growth of roughly 72.5% and EPS growth of 63.5%.\n\nNvidia could top analysts' estimates even with slowing growth. The company reported year-over-year revenue growth of 94% in the third quarter of fiscal 2025 and EPS growth of 103%. However, management's Q4 guidance projects revenue of $37.5 billion, plus or minus 2%. Nvidia will have to be near the upper end of the range to perform better than Wall Street expects.\n\nFor the stock to jump enough to justify buying it hand over fist before the Q4 update, though, Nvidia can't merely scrape by with a revenue and earnings beat. It will either need to handily exceed estimates and/or provide an especially encouraging outlook for fiscal 2026. Can the company do this? I think the chances are pretty good for three main reasons.\n\nFirst, Nvidia CFO Colette Kress said in the company's Q3 earnings call, \"Blackwell demand is staggering.\" She added that Nvidia was on track to top its previous revenue estimate for the new GPU chips even though it couldn't keep up with demand.\n\nSecond, several of Nvidia's biggest customers revealed in recent weeks that they're continuing to invest heavily in AI infrastructure. Amazon, Microsoft, Google parent Alphabet, and Meta Platforms were all singing from the same page in their latest quarterly updates. That bodes well for Nvidia.\n\nThird, those huge customers aren't flocking to Advanced Micro Devices, Nvidia's primary rival. AMD reported strong Q4 revenue growth earlier this month but lower than expected.\n\nThe longer-term question\n\nI won't be surprised at all if Nvidia beats Wall Street's Q4 estimates and provides a strong outlook, with its shares popping when the market opens on Feb. 27. But investing is not about the short term or trying to jump in and out of stocks ahead of a certain move. The long-term question is: Can Nvidia's momentum continue much longer?\n\nSome believe the answer to that question is \"no.\" They point to Nvidia's valuation (shares trade at 32.6 times forward earnings). They predict the demand for AI chips will wane, perhaps in part due to more efficient models such as DeepSeek's that require fewer GPUs.\n\nI'm more optimistic, albeit cautiously so. My hunch is that advances in AI will spur greater demand for Nvidia's chips rather than lead to lower demand. I also fully expect Nvidia will continue to out-innovate the competition.\n\nSure, Nvidia's momentum will eventually slow. It's inevitable. However, buying the stock before Feb. 26 is probably a smart move, in my view. Even if you don't invest in Nvidia by then, the stock could still have plenty of room to run afterward.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Watch These Nvidia Stock Price Levels With Earnings Report Set for Release Wednesday",
            "link": "https://www.investopedia.com/watch-these-nvidia-stock-price-levels-with-earnings-report-set-for-release-wednesday-11684900",
            "snippet": "Nvidia shares will be on watchlists as the AI favorite gears up to release its highly anticipated earnings report after Wednesday's closing bell.",
            "score": 0.9402245283126831,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia shares will be on watchlists as the AI favorite gears up to release its highly anticipated earnings report after Wednesday\u2019s closing bell.\n\nA bearish engulfing pattern formed on the chart Friday, signaling a potential move lower ahead of the chipmaker\u2019s results.\n\nInvestors should watch crucial support levels on Nvidia's chart around $130, $113, and $102, while also monitoring key resistance levels near $153 and $174.\n\nNvidia (NVDA) shares will be on watchlists as the AI favorite gears up to release its highly anticipated earnings report after Wednesday\u2019s closing bell.\n\nThe chipmaking giant, which has a track record of blowing past Wall Street expectations amid insatiable demand for its lineup of AI silicon, is projected to report a 73% jump in fourth-quarter revenue from a year earlier and post net income of $21.08 billion, up from $12.84 billion.\n\nNvidia shares trade flat on the year, but have gained 12% this month as of Friday's close, with analysts remaining widely bullish on the chipmaker's stock as big tech hyperscalers continue to ramp up spending on AI infrastructure. The stock fell 4.1% to $134.43 on Friday amid a broader sell-off on Wall Street.\n\nBelow, we take a closer look at Nvidia\u2019s chart and use technical analysis to identify crucial price levels worth watching out.\n\nShares Trade in Descending Channel\n\nNvidia shares have consolidated in a descending channel since mid-December, with trading volumes declining over that period.\n\nMore recently, a bearish engulfing pattern formed on the chart Friday, signaling a potential move lower ahead of the chipmaker\u2019s results. Interestingly, this same candlestick pattern has appeared on two other occasions since the stock set its record high in early January, both of which preceded further selling.\n\nMeanwhile, the relative strength index (RSI) has mimicked the price, putting in lower highs since the start of the descending channel, indicating waning buying momentum.\n\nLet\u2019s identify several crucial support and resistance levels on Nvidia\u2019s chart that investors may be watching.\n\nCrucial Support Levels to Watch\n\nThe first lower level to eye sits around $130. The shares could find support in this area near a trendline that links the prominent August peak with troughs that formed on the chart in December and January.\n\nSelling below this location may see the shares decline to the $113 level. Investors may look for buying opportunities in this region near this month\u2019s swing low, which closely aligns with a range of comparable trading levels on the chart stretching back to May last year. This area also lies just above a projected bars pattern target that takes the stock\u2019s move lower in late January and repositions it from the descending channel\u2019s top trendline.\n\nA more significant post-earnings drop in Nvidia shares could bring the $102 level into play, a location on the chart where the shares may encounter support near the opening price of the late May breakaway gap and a series of prices situated around the August and September lows.\n\nKey Resistance Levels to Monitor\n\nA breakout above the descending channel\u2019s upper trendline could initially see the shares make another attempt at the $153 level. This area on the chart would likely provide overhead resistance near the stock\u2019s all-time high (ATH).\n\nFinally, investors can forecast a bullish target above the ATH by using the measured move technique, also known as the measuring principle.\n\nTo apply the analysis, we calculate the wide of the descending channel in points and add that amount to the pattern\u2019s top trendline. For example, we add $32 to $142, which forecasts a target of $174, a location nearly 30% above Friday\u2019s closing price where investors may decide to lock in profits.\n\nThe comments, opinions, and analyses expressed on Investopedia are for informational purposes only. Read our warranty and liability disclaimer for more info.\n\nAs of the date this article was written, the author does not own any of the above securities.\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia confirms 'rare issue' with some RTX 5090 and RTX 5070 Ti GPUs \u2013 here\u2019s how to check if you\u2019re affected and to get a replacement",
            "link": "https://www.techradar.com/computing/gpu/nvidia-confirms-rare-issue-with-some-rtx-5090-and-rtx-5070-ti-gpus-heres-how-to-check-if-youre-affected-and-to-get-a-replacement",
            "snippet": "Roughly 0.5% of these new GPUs are affected by a hardware-level fault causing lower than expected frame rates.",
            "score": 0.7401851415634155,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has confirmed that approximately 1 in 200 of its RTX 5090 and RTX 5070 Ti GPUs have an issue with their graphics chips\n\nThe problem is a loss of ROPs, a key element of the inner workings of the GPU\n\nNvidia says those with an affected graphics card should contact the maker to arrange a replacement\n\nFollowing reports of some RTX 5090 GPUs failing to perform as well as they should in gaming, Nvidia has confirmed that there is an issue with the chips in the Blackwell flagship, as well as the newly arrived RTX 5070 Ti GPU.\n\nThis is a hardware-level problem, meaning it's a fault deep in the chip which can't be fixed, and it's slowing down these graphics cards by an appreciable (albeit variable) amount.\n\nIn a statement addressing the matter, Nvidia told The Verge: \"We have identified a rare issue affecting less than 0.5% (half a percent) of GeForce RTX 5090 / 5090D and 5070 Ti GPUs which have one fewer ROP than specified.\n\n\"The average graphical performance impact is 4%, with no impact on AI and Compute workloads. Affected consumers can contact the board manufacturer for a replacement. The production anomaly has been corrected.\"\n\nYour first question may well be: what's a ROP, then? ROP stands for Raster Operations Pipeline, and this is hardware that\u2019s a key part of the process of rendering the graphics for your PC games. (It\u2019s a lot more complicated than that, in reality, but that\u2019s all you really need to know).\n\nWith fewer of those pipelines available to deal with the relevant graphics processing tasks during gameplay, unsurprisingly, performance is a bit slower.\n\nAlso, if you're wondering about the mentioned RTX 5090D, that's the variant of the Blackwell flagship sold in China, which was involved in the initial reports of this issue \u2013 notably the RTX 5070 Ti wasn't, though.\n\nGet daily insight, inspiration and deals in your inbox Sign up for breaking news, reviews, opinion, top tech deals, and more. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nThis whole episode unfolded yesterday, having first emerged courtesy of TechPowerUp\u2019s review of a Zotac RTX 5090 Solid graphics card (via VideoCardz).\n\nIn its review, the tech site found that this third-party model was somehow underperforming versus an Nvidia RTX 5090 Founders Edition (the performance baseline used by TechPowerUp in gauging the relative power of the flagship GPU variants).\n\nIndeed, the Zotac RTX 5090 was around 5% slower than Nvidia\u2019s own model, while running at the same clock speeds, which obviously didn\u2019t make much sense. Not until TechPowerUp investigated and found this wasn\u2019t an issue pertaining to faulty cooling (or other likely-seeming root causes), but in fact that the Zotac GPU was missing ROPs.\n\nThe RTX 5090 graphics card was showing 168 ROPs enabled (in the GPU-Z utility) rather than the expected count (and official spec) of 176 ROPs.\n\nAll vendors are potentially affected by this gremlin in the works, of course, as this is an issue with the chips produced by Nvidia, and sent to third-party partners to be used in the manufacturing of their graphics cards. That was swiftly shown yesterday as reports started to come in, as folks started checking their boards for this issue.\n\nWhile in its statement, Nvidia mentions the lack of one ROP, it is referring to one block of them, so as observed, the ROP count is reduced by eight (the number in a block) with graphics cards that have this issue.\n\n(Image credit: ShutterStock)\n\nHow to check if your Blackwell GPU might be affected\n\nTo check your RTX 5090 or 5070 Ti, you can fire up a tool that peers deep into the innards of your hardware, monitoring and reporting back on multiple elements of the spec. Obviously what you\u2019re looking for is the ROPs count, and that can be provided by GPU-Z as already mentioned, or an alternative utility like HWiNFO (and probably other software out there, no doubt).\n\nIn GPU-Z, you\u2019ll find the ROPs number listed in the Graphics Card tab, on the seventh line down, over on the left-hand side (we\u2019ve got an explainer here, if you want further details on GPU-Z). For the RTX 5090, the number should be 176, whereas 168 is what the impaired flagship models are showing. With the RTX 5070 Ti, the correct spec is 96 ROPs, so in theory, it will be reduced to 88 ROPs (but I haven't seen confirmation of that yet, so perhaps it could have less of an impact).\n\nIf you do have an RTX 5090 or 5070 Ti with this problem, how much will it affect you in practical terms? Well, that varies as I already mentioned, although as stated the average impact is a performance loss of something in the order of 5% (or thereabouts \u2013 Nvidia is saying 4%).\n\nHowever, you may not notice any difference at all in some cases, as one game may use the mentioned pipelines (ROPs) more heavily, while another may hardly touch them at all. So some games could be slowed down by more than 5%, and others may have a negligible loss in frame rates (such a low impact you\u2019d never be able to tell).\n\nHowever, before you go thinking that maybe this isn't such a big deal after all, rest assured, it is. A fault like this should not have cleared quality assurance and made it into production hardware in the first place. And when you recall how much buyers have forked out for the RTX 5090 in particular \u2013 the MSRP is a true wallet-worrier, and many folks have overpaid beyond that \u2013 well, you can start to see how this is a big letdown.\n\nIf you have an RTX 5090 or RTX 5070 Ti, check your graphics card in GPU-Z as outlined above. If your model is showing a loss of ROPs, as advised by Nvidia, contact your board manufacturer and begin the process of having the graphics card replaced.\n\nThat could be a troublesome matter, though, for those who may have sold their old GPU when they upgraded (if they need to send the faulty Blackwell graphics card back, before receiving a new one \u2013 and are left with a gaming PC without an engine, essentially). The other worry is that it's not like fresh stock is going to be easy to come by, either, right now.\n\nFuture GPUs shouldn't suffer from this issue because as Nvidia observes, the 'production anomaly' here has been fixed, as you would hope.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "NVIDIA is Creating a Problem It Can Profit From",
            "link": "https://hackernoon.com/nvidia-is-creating-a-problem-it-can-profit-from",
            "snippet": "NVIDIA's RTX 50 series pricing is an absolute joke. The company is not even pretending to care about gamers anymore.",
            "score": 0.9433782696723938,
            "sentiment": null,
            "probability": null,
            "content": "Alright, let\u2019s talk about the absolute insanity that is NVIDIA\u2019s RTX 50 series pricing, because at this point, they\u2019re not even pretending to care about gamers anymore. It\u2019s all about AI, and we\u2019re just here getting price-gouged while they focus on enterprise customers.\n\nThe Prices Are a Complete Joke\n\nLet\u2019s just take a moment to appreciate how absurdly overpriced these GPUs are:\n\n\n\n\n\nRTX 5090 \u2013 $4,039 AUD\n\nRTX 5080 \u2013 $2,019 AUD\n\nRTX 5070 Ti \u2013 $1,509 AUD\n\nRTX 5070 \u2013 $1,109 AUD\n\n\n\nFOUR THOUSAND DOLLARS for a single GPU? That\u2019s not a gaming card. That\u2019s a high-end workstation meant for AI and data centers, yet NVIDIA is still pretending it\u2019s a consumer product. Even the so-called \u201cmid-range\u201d cards, which used to be affordable back in the day, now cost as much as an entire high-end gaming PC from a few years ago.\n\nNVIDIA Doesn\u2019t Care About Gamers Anymore\n\nOnce upon a time, NVIDIA was all about gaming\u2014now? They\u2019re fully invested in AI, machine learning, and data centers. Their investor calls barely mention gaming anymore because it\u2019s just a side hustle at this point.\n\n\n\n\n\nIf you think these insane price hikes are about next-level gaming performance, think again. This is NVIDIA shifting its business model, focusing on AI while still milking gamers for everything they can. They know businesses will pay insane amounts for AI compute, so they\u2019re pricing their GPUs accordingly.\n\n\n\n\n\nAnd the worst part? Gamers aren\u2019t even the target audience anymore. NVIDIA doesn\u2019t care if you can afford it or not\u2014because their focus is now AI companies, research institutions, and cloud providers.\n\nGreat Frames? You Need DLSS 4.0\u2014Because Native Performance Isn't Enough\n\nHere\u2019s the kicker: even if you drop thousands on one of these GPUs, you\u2019re still not getting great frame rates natively. NVIDIA has stopped focusing on raw power and instead forces you to rely on their AI-powered DLSS 4.0 upscaling just to get high frame rates.\n\n\n\n\n\nLet that sink in. You\u2019re spending thousands of dollars, and you still have to use AI tricks to get good performance. Native rasterization performance? Not as impressive as you\u2019d expect. Instead, NVIDIA pushes their fake frames technology (Frame Generation) to inflate FPS numbers.\n\n\n\n\n\nSo, if you thought you could just buy a 5090 and brute-force your way to native 4K 120 FPS\u2014think again. You\u2019re forced to use DLSS, which means:\n\n\n\n\n\nYou're not getting \"true\" frames. A huge chunk of your FPS is generated by AI, meaning it\u2019s not actual rendered frames.\n\n\n\nDLSS can introduce artifacts. Sure, it looks fine most of the time, but in fast-paced games, ghosting and motion blur can still be noticeable.\n\n\n\nNot every game supports it. If a game doesn\u2019t have DLSS, you\u2019re stuck with the card\u2019s raw performance, which might not be as impressive as you\u2019d expect for the price.\n\n\n\nSo why are we paying thousands for a GPU that doesn\u2019t deliver native high FPS without software trickery?\n\nNVIDIA is Creating the Problem They Profit From\n\nAnd let\u2019s not forget artificial scarcity. NVIDIA could easily make more GPUs, but instead, they limit supply so that prices stay inflated. Then, they release AI-focused GPUs, knowing full well that those will sell out first, driving gamers to pay more than they should just to get their hands on something decent.\n\nFinal Thoughts: Gamers Are Being Milked\n\nPC gaming was supposed to be a place where you could build a powerful system for a fair price. Now, thanks to NVIDIA\u2019s greed, it\u2019s a luxury hobby. They\u2019re prioritizing AI, enterprise, and cloud computing, leaving gamers behind while still charging them insane prices.\n\n\n\n\n\nIf you\u2019re thinking about upgrading, consider AMD or Intel, because at this point, NVIDIA doesn\u2019t deserve your money anymore.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Largest U.S. Pension Sold Nvidia, Super Micro, AT&T Stock. It Bought Rivian.",
            "link": "https://www.barrons.com/articles/nvidia-stock-super-micro-att-rivian-aaac806e",
            "snippet": "Calpers slashed stakes in Nvidia, Super Micro, and AT&T, and increased an investment in Rivian in the fourth quarter.",
            "score": 0.94316166639328,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "How Nvidia's stock responds will be huge for the S&P 500 this week",
            "link": "https://www.thestreet.com/investing/stocks/investors-hope-nvidia-will-give-markets-much-needed-cheer",
            "snippet": "A year ago, chipmaker Nvidia (NVDA) was really riding high. Sales of its chips were jumping as it transformed itself from a maker of computer chips used for...",
            "score": 0.8887411952018738,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Huawei\u2019s Ascend 910C: A Bold Challenge to NVIDIA in the AI Chip Market",
            "link": "https://www.unite.ai/huaweis-ascend-910c-a-bold-challenge-to-nvidia-in-the-ai-chip-market/",
            "snippet": "910C: A Bold Challenge to NVIDIA in the AI Chip Market. AI Tools, Generative AI Business Plans, Cartoonize Code, Emails, Headshots, Images, Media Kits, Music,...",
            "score": 0.8919228315353394,
            "sentiment": null,
            "probability": null,
            "content": "The Artificial Intelligence (AI) chip market has been growing rapidly, driven by increased demand for processors that can handle complex AI tasks. The need for specialized AI accelerators has increased as AI applications like machine learning, deep learning, and neural networks evolve.\n\nNVIDIA (NVDA +6.43%) has been the dominant player in this domain for years, with its powerful Graphics Processing Units (GPUs) becoming the standard for AI computing worldwide. However, Huawei has emerged as a powerful competitor with its Ascend series, leading itself to challenge NVIDIA's market dominance, especially in China. The Ascend 910C, the latest in the line, promises competitive performance, energy efficiency, and strategic integration within Huawei\u2019s ecosystem, potentially reshaping the dynamics of the AI chip market.\n\nBackground on Huawei's Ascend Series\n\nHuawei\u2019s entry into the AI chip market is part of a broader strategy to establish a self-reliant ecosystem for AI solutions. The Ascend series began with the Ascend 310, designed for edge computing, and the Ascend 910, aimed at high-performance data centers. Launched in 2019, the Ascend 910 was recognized as the world's most powerful AI processor, delivering 256 teraflops (TFLOPS) of FP16 performance.\n\nBuilt on Huawei\u2019s proprietary Da Vinci architecture, the Ascend 910 offers scalable and flexible computing capabilities suitable for various AI workloads. The chip's emphasis on balancing power with energy efficiency laid the groundwork for future developments, leading to the improved Ascend 910B and the latest Ascend 910C.\n\nThe Ascend series is also part of Huawei's effort to reduce dependence on foreign technology, especially in light of U.S. trade restrictions. By developing its own AI chips, Huawei is working toward a self-sufficient AI ecosystem, offering solutions that range from cloud computing to on-premise AI clusters. This strategy has gained traction with many Chinese companies, particularly as local firms have been encouraged to limit reliance on foreign technology, such as NVIDIA's H20. This has created an opportunity for Huawei to position its Ascend chips as a viable alternative in the AI space.\n\nThe Ascend 910C: Features and Specifications\n\nThe Ascend 910C is engineered to offer high computational power, energy efficiency, and versatility, positioning it as a strong competitor to NVIDIA's A100 and H100 GPUs. It delivers up to 320 TFLOPS of FP16 performance and 64 TFLOPS of INT8 performance, making it suitable for a wide range of AI tasks, including training and inference.\n\nThe Ascend 910C delivers high computational power, consuming around 310 watts. The chip is designed for flexibility and scalability, enabling it to handle various AI workloads such as Natural Language Processing (NLP), computer vision, and predictive analytics. Additionally, the Ascend 910C supports high bandwidth memory (HBM2e), essential for managing large datasets and efficiently training complex AI models. The chip's software compatibility, including support for Huawei's MindSpore AI framework and other platforms like TensorFlow and PyTorch, makes it easier for developers to integrate into existing ecosystems without significant reconfiguration.\n\nHuawei vs. NVIDIA: The Battle for AI Supremacy\n\nNVIDIA has long been the leader in AI computing, with its GPUs serving as the standard for machine learning and deep learning tasks. Its A100 and H100 GPUs, built on the Ampere and Hopper architectures, respectively, are currently the benchmarks for AI processing. The A100 can deliver up to 312 TFLOPS of FP16 performance, while the H100 offers even more robust capabilities. NVIDIA's CUDA platform has significantly advanced, creating a software ecosystem that simplifies AI model development, training, and deployment.\n\nDespite NVIDIA's dominance, Huawei's Ascend 910C aims to offer a competitive alternative, particularly within the Chinese market. The Ascend 910C performs similarly to the A100, with slightly better power efficiency. Huawei's aggressive pricing strategy makes the Ascend 910C a more affordable solution, offering cost savings for enterprises that wish to scale their AI infrastructure.\n\nHowever, the software ecosystem remains a critical area of competition. NVIDIA\u2019s CUDA is widely adopted and has a mature ecosystem, while Huawei\u2019s MindSpore framework is still growing. Huawei\u2019s efforts to promote MindSpore, particularly within its ecosystem, are essential to convince developers to transition from NVIDIA's tools. Despite this challenge, Huawei has been progressing by collaborating with Chinese companies to create a cohesive software environment supporting the Ascend chips.\n\nReports indicate that Huawei has started distributing prototypes of the Ascend 910C to major Chinese companies, including ByteDance, Baidu, and China Mobile. This early engagement suggests strong market interest, especially among companies looking to reduce dependency on foreign technology. As of last year, Huawei\u2019s Ascend solutions were used to train nearly half of China\u2019s top 70 large language models, demonstrating the processor's impact and widespread adoption.\n\nThe timing of the Ascend 910C launch is significant. With U.S. export restrictions limiting access to advanced chips like NVIDIA's H100 in China, domestic companies are looking for alternatives, and Huawei is stepping in to fill this gap. Huawei's Ascend 910B has already gained traction for AI model training across various sectors, and the geopolitical environment is driving further adoption of the newer 910C.\n\nWhile NVIDIA is projected to ship over 1 million H20 GPUs to China, generating around $12 billion in revenue, Huawei\u2019s Ascend 910C is expected to generate $2 billion in sales this year. Moreover, companies adopting Huawei\u2019s AI chips may become more integrated into Huawei\u2019s broader ecosystem, deepening reliance on its hardware and software solutions. However, this strategy may also raise concerns among businesses about becoming overly dependent on one vendor.\n\nStrategic Partnerships and Alliances\n\nHuawei has made strategic partnerships to drive the adoption of the Ascend 910C. Collaborations with major tech players like Baidu, ByteDance, and Tencent have facilitated the integration of Ascend chips into cloud services and data centers, ensuring that Huawei\u2019s chips are part of scalable AI solutions. Telecom operators, including China Mobile, have incorporated Huawei\u2019s AI chips into their networks, supporting edge computing applications and real-time AI processing.\n\nThese alliances ensure that Huawei's chips are standalone products and integral parts of broader AI solutions, making them more attractive to enterprises. Additionally, this strategic approach allows Huawei to promote its MindSpore framework, building an ecosystem that could rival NVIDIA\u2019s CUDA platform over time.\n\nGeopolitical factors have significantly influenced Huawei's strategy. With U.S. restrictions limiting its access to advanced semiconductor components, Huawei has increased its investments in R&D and collaborations with domestic chip manufacturers. This focus on building a self-sufficient supply chain is critical for Huawei's long-term strategy, ensuring resilience against external disruptions and helping the company to innovate without relying on foreign technologies.\n\nTechnical Edge and Future Outlook\n\nThe Ascend 910C has gained prominence with its strong performance, energy efficiency, and integration into Huawei\u2019s ecosystem. It competes closely with NVIDIA\u2019s A100 in several key performance areas. For tasks that require FP16 computations, like deep learning model training, the chip\u2019s architecture is optimized for high efficiency, resulting in lower operational costs for large-scale use.\n\nHowever, challenging NVIDIA\u2019s dominance is no easy task. NVIDIA has built a loyal user base over the years because its CUDA ecosystem offers extensive development support. For Huawei to gain more market share, it must match NVIDIA's performance and offer ease of use and reliable developer support.\n\nThe AI chip industry will likely keep evolving, with technologies like quantum computing and edge AI reshaping the domain. Huawei has ambitious plans for its Ascend series, with future models promising even better integration, performance, and support for advanced AI applications. By continuing to invest in research and forming strategic partnerships, Huawei aims to strengthen its foundations in the AI chip market.\n\nThe Bottom Line\n\nIn conclusion, Huawei\u2019s Ascend 910C is a significant challenge to NVIDIA\u2019s dominance in the AI chip market, particularly in China. The 910C\u2019s competitive performance, energy efficiency, and integration within Huawei\u2019s ecosystem make it a strong contender for enterprises looking to scale their AI infrastructure.\n\nHowever, Huawei faces significant hurdles, especially competing with NVIDIA's well-established CUDA platform. The success of the Ascend 910C will rely heavily on Huawei's ability to develop a robust software ecosystem and strengthen its strategic partnerships to solidify its position in the evolving AI chip industry.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-02-22": {
        "0": {
            "title": "Nvidia Stock Investors Got Shocking Updates From Microsoft and Google",
            "link": "https://www.fool.com/investing/2025/02/22/nvidia-stock-investors-got-shocking-updates-from-m/",
            "snippet": "Google Cloud is the first cloud provider to have both the GB200 and B200 Nvidia systems running.",
            "score": 0.4656347930431366,
            "sentiment": null,
            "probability": null,
            "content": "Google Cloud is the first cloud provider to have both the GB200 and B200 Nvidia systems running.\n\nIn today's video, I discuss Nvidia (NVDA 5.27%) and recent updates impacting the semiconductor giant. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the after-market prices of Feb. 19, 2025. The video was published on Feb. 19, 2025.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "Futures: All Eyes On Nvidia After These Stocks Lead Sell-Off",
            "link": "https://www.investors.com/market-trend/stock-market-today/dow-jones-futures-nvidia-palantir-axon-tesla-sell-off/",
            "snippet": "Dow Jones futures: The stock market tumbled for the week, led by highly valued growth plays such as Palantir and Tesla. Nvidia earnings loom.",
            "score": 0.5817141532897949,
            "sentiment": null,
            "probability": null,
            "content": "Dow Jones futures rose Monday morning, along with S&P 500 futures and Nasdaq futures, with Nvidia (NVDA) in focus this coming week. The stock market rally suffered a big setback last week. The S&P 500 hit record highs on Wednesday, but the major indexes ultimately suffered solid to sharp losses, with the Dow Jones, Nasdaq and Russell 2000 breaking key\u2026",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia explains the missing ROPs \u2014 defective silicon in 0.5% of RTX 5090 and 5070 Ti GPUs",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-explains-the-missing-rops-defective-silicon-in-0-5-percent-of-rtx-5090-and-5070-ti-gpus",
            "snippet": "Nvidia says that a few RTX 5090 / 5090D and RTX 5070 Ti GPUs shipped with defective ROPs, telling affected users to contact the manufacturer for a...",
            "score": 0.9423973560333252,
            "sentiment": null,
            "probability": null,
            "content": "Several Nvidia GeForce RTX 50-series GPUs have been missing ROPs, suggesting that some GB202 and GB203 chips used in those cards are defective. This has led to many graphics cards from different manufacturers, including Zotac, MSI, Gigabyte, and even Nvidia\u2019s Founders Edition GPUs, having eight fewer ROPs than specified. The company has stated Tom\u2019s Hardware, saying this only affected a few of its GPUs.\n\n\u201cWe have identified a rare issue affecting less than 0.5% (half a percent) of GeForce RTX 5090 / 5090D and 5070 Ti GPUs, which have one fewer ROP than specified. The average graphical performance impact is 4%, with no impact on AI and Compute workloads,\u201d an Nvidia representative told Tom's Hardware. \u201cAffected consumers can contact the board manufacturer for a replacement. The production anomaly has been corrected.\u201d\n\nSo, the problem is indeed caused by an issue in production and quality control, which allowed a few defective GB202 and GB203 chips to pass inspection and be installed on GPUs. It\u2019s a good thing that the issue was caught early, though, as Nvidia said that even its RTX 5070 Ti GPUs, which were released a couple of days ago, were also affected. Consumers who bought any affected cards would not get the expected performance.\n\nTeam Green said those affected by the missing ROPs should contact their board manufacturer to RMA their less performant GPUs. Hopefully, they won\u2019t have to wait long to get a proper replacement despite the ongoing RTX 50-series shortage. A few retailers expect supply to stabilize in four months.\n\nThis has been one of the many issues plaguing the RTX 50 series since its highly anticipated launch. A major concern returning from the RTX 4090 is the melting power cable problem, which the company has previously claimed was already fixed. Several RTX 50-series GPUs are also experiencing crashes and instability, resulting in BSODs and black screens. Nvidia said it\u2019s already investigating the issue but hasn\u2019t yet released a timeline for a fix.\n\nMany gamers and enthusiasts are predictably disappointed with this launch. Many say that the performance improvements do not match the price increases. With all these issues cropping up less than a month after their release, Nvidia has its hands full, trying to fix them.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "How NVIDIA Stock Might React To Upcoming Earnings?",
            "link": "https://www.trefis.com/stock/nvda/articles/562723/how-nvidia-stock-might-react-to-upcoming-earnings/2025-02-22",
            "snippet": "NVIDIA (NASDAQ:NVDA) is set to report its earnings on Feb 26th, 2026 after market close. The semiconductor giant has $3.3 Tril in current market...",
            "score": 0.8431035280227661,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA (NASDAQ:NVDA) is set to report its earnings on Feb 26th, 2026 after market close. The semiconductor giant has $3.3 Tril in current market capitalization. Revenue over the last twelve months was $113 Bil, and the company was operationally profitable with $71 Bil in operating profits and net income of $63 Bil. While the post-earnings stock reaction will depend on how the results and outlook stack up against investor expectations, a detailed look at historical results can help if you are an event-driven trader. Here is how \u2013 either understand the historical odds and position yourself prior to the earnings announcement, or look at the correlation between immediate return and medium-term return post earnings and take a trade one-day after the announcement.\n\nNVIDIA\u2019s Historical Odds Of Positive Post-Earnings Return\n\nSome observations on one-day (1D) post earnings returns:\n\nIn the past 5 years, 20 earnings data points recorded, with 11 positive and 9 negative one-day (1D) returns observed. In summary, positive 1D returns seen about 55% of the times\n\nand one-day (1D) returns observed. In summary, positive 1D returns seen about 55% of the times In fact, this percentage has increased to 58% if we consider last 3 year data\n\nMedian of the 11 positive returns = 4.9%, and median of the 9 negative returns = -5.9%\n\nAdditional data for observed returns 5-days (5D), and 30-days (30D) post earnings are summarized along with the statistics, in the table below.\n\nCorrelation Between 1D, 5D, and 30D Historical Returns\n\nA relatively less risky strategy (though not useful if correlation is low) is to understand the correlation between short-term and medium-term returns post earnings, find a pair that has highest correlation and execute the appropriate trade. For example, if 1D and 5D show the highest correlation, a trader can position themselves \u201clong\u201d for the next 5 days if 1D post-earnings return is positive. Here is some correlation data based on 5-year and 3-year (more recent) history. Note that the correlation 1D_5D refers to correlation between 1D post-earnings returns and subsequent 5D returns.\n\nIs There Any Correlation With Peer Earnings?\n\nSometimes, peer performance can have influence on post-earnings stock-reaction. In fact, the pricing-in might begin before the earnings are announced. Here is some historical data on past post-earnings performance of NVIDIA stock compared with stock performance of peers that reported earnings just prior to NVIDIA. For fair comparison, peer stock returns also represent post-earnings one day (1D) return.\n\nLearn more about Trefis RV strategy that has outperformed its all-cap stocks benchmark (combination of all 3, the S&P 500, S&P mid-cap, and Russell 2000), to produce strong returns for investors. Separately, if you want upside with a smoother ride than an individual stock like NVIDIA, consider the High Quality portfolio, which has outperformed the S&P, and clocked >91% returns since inception.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Retail Prices For NVIDIA GeForce RTX 5070 Ti Go Crazy: Custom Models Now Listed For $2,000+",
            "link": "https://wccftech.com/retail-prices-nvidia-geforce-rtx-5070-ti-go-crazy-custom-models-listed-2000-usd/",
            "snippet": "The NVIDIA GeForce RTX 5070 Ti is being listed for over twice the MSRP. Many custom editions are listed at over $2000.",
            "score": 0.9115829467773438,
            "sentiment": null,
            "probability": null,
            "content": "Just a day after the GPU's launch, the GeForce RTX 5070 Ti is retailing for over 2X the official MSRP set by NVIDIA.\n\nWhile NVIDIA RTX 5090 isn't available at $2,000, the GeForce RTX 5070 Ti definitely is\n\nIf you thought the $1,000 price tag for the GeForce RTX 5070 Ti was high, wait until you find some cards going for the MSRP of RTX 5090. This is one of the most insane prices we have seen for the RTX 50 cards, which shows how some sellers or 'scalpers' are taking advantage of this situation.\n\nAmazon, which is one of the largest retailers in the world, has revealed some crazy RTX 5070 Ti GPU listings. Contrary to what you would expect, the custom AIB editions for the card aren't crossing the $1,000 mark this time but a whopping $2,000. Take a look at the Gigabyte RTX 5070 Ti Eagle OC ICE SFF listing. It's listed at a whopping $2,199 while the ASUS TUF Gaming RTX 5070 Ti OC Edition is priced at an unbelievable price tag of $2,379.\n\n2 of 9\n\nThankfully, some sellers are 'generous' enough to not charge you that much. If you don't want to pay over $2,000 for the RTX 5070 Ti, then they are offering GPUs for as low as \u00a31,387.17 or $1753.37 with tax included. The lowest you can pay is $1,699 for the Gigabyte RTX 5070 Ti WindForce OC SFF edition and this is the same story for every other edition you can think of.\n\nImage Credit: @RedGamingTech\n\nThe GPU went out of stock instantly when launched, as if each retailer had no more than a couple of GPUs. This was seen worldwide as NVIDIA didn't supply enough GPUs to its board partners, making the RTX 5070 Ti case worse than the RTX 5080. The RTX 5070 Ti won't be available easily in the next several weeks, and we have seen retailers stopping pre-orders already.\n\nIn some regions, the situation is so bad that the card won't make it to the market even in the next month at some retailers. This marks the second half of 2025, despite the card launching in the first quarter. The RTX 5070 Ti isn't even that impressive to start with, and on top of that, the sky-high prices of the GPU make it totally nonsense.\n\nIf you are waiting for the GeForce RTX 5070 and RTX 5060/Ti GPUs, then you should probably prepare yourself for another paper launch. The GPUs are facing bugs and will need several more weeks to launch, as per a new report. At this point, the whole RTX 50 series launch has become one of the worst in GPU launch history.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Tech stocks like Nvidia have room to run ahead of earnings, Bank of America says",
            "link": "https://www.cnbc.com/2025/02/22/buy-nvidia-ahead-of-earnings-bank-of-america-says.html",
            "snippet": "Tech stocks like Nvidia have room to run ahead of earnings, Bank of America says. Published Sat, Feb 22 20257:52 AM EST.",
            "score": 0.7585694789886475,
            "sentiment": null,
            "probability": null,
            "content": "There's a slate of tech stocks that are well positioned ahead of earnings \u2013 and worth snapping up, according to Bank of America. The firm said that investors should take advantage of any pullback in shares of companies like Nvidia . Other buy-rated stocks include Workday , Dell and Marvell Technology. Nvidia Buy the recent dip in shares of the chip giant, the firm said. Nvidia is down more than 4% in the past month. \"The next important test for AI bulls comes on Feb-26 when NVDA reports FQ4 results,\" analyst Vivek Arya said. The analyst said that despite the stock's choppiness, he still sees a slew of positive catalysts ahead. These include \"NVDA's leading new product pipeline and TAM expansion into robotics and quantum technologies at [its] upcoming GTC [global processing unit tech] conference,\" according to Arya. The analyst said that the quarterly report should have enough earnings per share \"substance even if less sizzle.\" Marvell Technology Arya also said positive catalysts are building for Marvell. The firm is expecting solid fiscal fourth-quarter earnings results when the semiconductor company reports in early March . \"We note overall improving AI visibility into FY26/27E as the cloud capex outlook continues to increase, and MRVL's custom silicon pipeline/execution remains solid amid a fast-growing TAM [total addressable market],\" Arya wrote. The company also has a much-anticipated investor day coming up in early June, which should be a key tailwind for the stock, the firm said. Arya thinks the company could raise its near-term artificial intelligence revenues at the event. Meanwhile, shares are down 6% in 2025. \"Buy on AI share gains,\" the analyst said. Dell Analyst Wamsi Mohan is sticking with shares of Dell. \"Dell will report F4Q on Feb 27th and we believe the discussion will be focused on AI server backlog/Blackwell delays,\" he said. Blackwell is Nvidia's graphic processing unit, which Dell utilizes. Mohan acknowledged that Dell's AI server segment could be \"challenged,\" but the firm ultimately sees the issue as \"transitory.\" \"As Dell begins to deliver on the demand for AI servers and customers shift more enterprise/sovereign, revs/margins should shift higher over time,\" he wrote. The analyst did lower his price target on the stock to $150 per share from $155, but he said Dell remains well positioned for the long haul. Shares are up nearly 45% over the last 12 months. Workday \"WDAY's topline growth rate has likely bottomed at 14%, and any improvement would serve as a catalyst for the stock. We believe there are some leading indicators for a better enterprise applications spending environment, which could drive the growth higher as we move through FY26. \u2026 WDAY has a differentiated SaaS platform that leads in Human Capital Management (HCM) and is emerging as a leader in Financials.\" Marvell Technology \"Buy on AI share gains. \u2026 We note overall improving AI visibility into FY26/27E as the cloud capex outlook continues to increase, and MRVL's custom silicon pipeline/execution remains solid amid a fast-growing TAM. \u2026 We also flag the upcoming Jun-10 Investor Day as a catalyst, where MRVL could raise n-t [near term] AI target to $8bn.\" Dell \"Dell will report F4Q on Feb 27th and we believe the discussion will be focused on AI server backlog/Blackwell delays. \u2026 While the near-term set-up could be challenged on AI server revs/margins, we believe this to be transitory. As Dell begins to deliver on the demand for AI servers and customers shift more enterprise/sovereign, revs/margins should shift higher over time.\" Nvidia \"EPS could have enough substance even if less sizzle. The next important test for AI bulls comes on Feb-26 when NVDA reports FQ4 results. \u2026 The stock could be volatile post results, but we expect positive momentum to resume as investors look forward to NVDA's leading new product pipeline and TAM expansion into robotics and quantum technologies at upcoming GTC conference.\"",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "NVIDIA Bought This Chinese Stock. Should You?",
            "link": "https://www.nanalyze.com/videos/nvidia-bought-this-chinese-stock-should-you/",
            "snippet": "NVIDIA just released their latest 13-F report showing a new holding: WeRide. WRD stock was consequently up over 80% on the day, capturing the attention of...",
            "score": 0.9282153844833374,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "AMD Navi 48 RDNA4 GPU has 53.9 billion transistors, more than NVIDIA GB203",
            "link": "https://videocardz.com/pixel/amd-navi-48-rdna4-gpu-has-53-9-billion-transistors-more-than-nvidia-gb203",
            "snippet": "AMD is sharing the first information on the next-gen GPU from the Radeon RX 9070 series. The first cards will feature the Navi 48 GPU, which is the larger...",
            "score": 0.7755725979804993,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "2 Reasons to Buy Nvidia Stock in the Wake of DeepSeek",
            "link": "https://www.msn.com/en-us/money/companies/2-reasons-to-buy-nvidia-stock-in-the-wake-of-deepseek/ar-AA1zoDC0",
            "snippet": "After soaring more than 800% over the last two years, shares of Nvidia (NASDAQ: NVDA) have stumbled out of the gate in 2025.",
            "score": 0.8962504863739014,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "After $600 billion Nvidia wipeout and losing nearly 20% of his personal net worth during DeepSeek rout, Je",
            "link": "https://m.economictimes.com/news/international/us/after-600-billion-nvidia-wipeout-and-losing-nearly-20-of-his-personal-net-worth-during-deepseek-rout-jensen-huang-hits-back-says-investors-got-it-wrong-on-selloff/articleshow/118483019.cms",
            "snippet": "Nvidia CEO Jensen Huang addressed concerns after a significant market drop due to the launch of DeepSeek's R1 AI model, which triggered fears of reduced...",
            "score": 0.8670898675918579,
            "sentiment": null,
            "probability": null,
            "content": "Misunderstanding behind the market reaction\n\nJensen Huang defends Nvidia's role in AI's future\n\nFAQs\n\n\n\n\n\n(You can now subscribe to our\n\n(You can now subscribe to our Economic Times WhatsApp channel\n\nNvidia CEO Jensen Huang addressed investors' concerns after his company faced a market wipeout that erased nearly $600 billion from its market value, as per a report. The tech giant\u2019s stock plunged due to the release of Chinese AI startup DeepSeek\u2019s R1 reasoning model, which led to investor fears that the demand for Nvidia's high-performance chips could soon fade.According to Fortune, Huang explained that the market\u2019s reaction to DeepSeek\u2019s R1 model was based on a misunderstanding. Huang acknowledged that DeepSeek\u2019s new model was impressive, but he also claimed that future AI models will continue to depend heavily on Nvidia\u2019s computing power, especially when it comes to post-training processes that refine AI systems, as per the report.After the release of DeepSeek\u2019s R1 model, which allegedly used cheaper and lower-capability chips, it sent shockwaves through the market, as per reports. As it sparked fears that big tech companies would reduce their need for Nvidia\u2019s advanced chips.According to Fortune, this misperception led to Nvidia\u2019s stock taking a major hit, and Huang personally lost nearly 20% of his net worth. However, Nvidia has since rebounded, recovering much of its losses.Huang attributed the panic to what he claimed was a flawed understanding among the investor community. He said \u201cI think the market responded to R1 as an- Oh my gosh, AI is finished,\u201d as quoted by Fortune. He pointed out that the release caused some investors to believe that AI companies no longer needed high-powered computing. In reality, he explained, the demand for such computing power is only increasing as the AI sector continues to evolve, as per the report.According to Huang, there\u2019s been a misconception in the market about the relationship between AI pretraining and inference. He claimed \u201cI don\u2019t know whose fault it is, but obviously, that paradigm is wrong,\u201d as quoted in the report. Huang made it clear that AI scaling, particularly when it comes to improving AI models' reasoning abilities, still requires immense computing resources, and Nvidia remains central to that process, as per the report.According to Fortune, despite the concerns sparked by DeepSeek\u2019s R1 model, Huang took the opportunity to praise the Chinese startup's efforts, calling the excitement around the open-sourcing of R1 \u201cincredible.\u201dHe also noted that DeepSeek\u2019s advancements have garnered praise from some of the biggest names in tech, including Google\u2019s Sundar Pichai, Apple\u2019s Tim Cook, and Microsoft\u2019s Satya Nadella, even as the launch led to a massive $1 trillion loss in US tech stocks.The stock dropped after DeepSeek\u2019s R1 AI model made investors worry that Nvidia's chips wouldn't be needed as much in the future. This panic led to a massive selloff.Huang called DeepSeek\u2019s R1 model \u201cimpressive,\u201d but he believes AI will still rely heavily on Nvidia's chips for refining models after their initial training.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-21": {
        "0": {
            "title": "Nvidia Wavers Amid $113 Billion Data Center Target As Earnings Loom; Is Nvidia A Buy Or Sell Now?",
            "link": "https://www.investors.com/research/nvda-stock-is-nvidia-a-buy-february-2025/",
            "snippet": "Is Nvidia Stock A Buy? Looking at chart signals and technical measures can help investors assess whether Nvidia stock is a buy now. Shares remained above their...",
            "score": 0.5473860502243042,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) rose 3% early Friday after Foxconn, formally known as Hon Hai Precision Industry, projected higher artificial intelligence sales in its fourth-quarter results, potentially benefitting Nvidia stock. Foxconn, an Apple (AAPL) partner, expects first-quarter AI server revenue to double from both a year ago and from the prior quarter, signaling a strong demand for AI chips. But is Nvidia\u2026",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "What Analysts Think of Nvidia Stock Ahead of Earnings",
            "link": "https://www.investopedia.com/what-analysts-think-of-nvidia-stock-ahead-of-earnings-q4-fy2025-11683525",
            "snippet": "Nvidia is set to report fourth-quarter results after the market closes Wednesday, with analysts widely bullish on the AI chipmaker's stock.",
            "score": 0.9336270093917847,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia is set to report fourth-quarter results after the market closes Wednesday.\n\nThe chipmaker is expected to post another quarterly sales record on strong demand for its advanced chips.\n\nAnalysts are widely bullish on the AI chipmaker's stock, with all but one of the analysts tracked by Visible Alpha issuing a \"buy\" or equivalent rating.\n\nNvidia (NVDA) is set to report fourth-quarter results after the market closes Wednesday, with analysts widely bullish on the AI chipmaker's stock.\n\nAll but one of the 18 analysts covering the stock tracked by Visible Alpha have issued \u201cbuy\u201d or equivalent ratings, with one analyst giving the stock a \u201chold\u201d rating. Their consensus price target of about $175 would imply 32% upside from Wednesday's intraday price.\n\nWedbush and Oppenheimer analysts, who reiterated $175 price targets in the days ahead of the report, suggested booming demand for the company's advanced chips as Big Tech firms ramp up spending on AI infrastructure could lead to another strong quarter.\n\nNvidia is projected to post record quarterly revenue of $38.34 billion, up 73% year-over-year, according to estimates compiled by Visible Alpha. Net income is expected to climb to $21.1 billion, from $12.84 billion a year earlier.\n\nUBS analysts, who maintained a $185 price target, noted \"investor expectations having crept up a bit recently,\" and added supply chain improvements could mean higher sales of Nvidia\u2019s Blackwell line. UBS nearly doubled its estimate for Blackwell\u2019s contribution to fourth-quarter revenue to $9 billion, up from $5 billion previously.\n\nOppenheimer also indicated the rapid rise of Chinese AI startup DeepSeek could ultimately prove \"positive\" for the chipmaker, as competition pushes Nvidia's American clients to step up their efforts in the AI race instead of pulling back.\n\nShares of Nvidia were up close to 5% at $132.40 in intraday trading Wednesday. They've gained more than two-thirds of their value over the past 12 months.\n\n\n\nUPDATE\u2014Feb. 26, 2025: This article has been updated since it was first published to reflect more recent analyst estimates and share price values.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Jensen Huang says investors got it wrong over DeepSeek stock sell off that wiped $600bn from Nvidia",
            "link": "https://fortune.com/2025/02/21/jensen-huang-deepseek-stock-sell-nvidia-value/",
            "snippet": "Nvidia CEO Jensen Huang has addressed a market selloff that wiped $600 billion off his company\u2014and caused his personal wealth to plummet.",
            "score": 0.9511622190475464,
            "sentiment": null,
            "probability": null,
            "content": "\u00a9 2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information\n\nFORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "If You'd Invested $1,000 in Nvidia Stock 5 Years Ago, Here's How Much You'd Have Today",
            "link": "https://www.fool.com/investing/2025/02/21/if-youd-invested-1000-in-nvidia-stock-5-years-ago/",
            "snippet": "Coming up just short of the 4% gain that the S&P 500 has enjoyed since the start of the year, Nvidia (NVDA -1.24%) has risen only about 3.4% in 2025.",
            "score": 0.935318112373352,
            "sentiment": null,
            "probability": null,
            "content": "Coming up just short of the 4% gain that the S&P 500 has enjoyed since the start of the year, Nvidia (NVDA 5.27%) has risen only about 3.4% in 2025. It may feel unfamiliar to see the artificial intelligence (AI) superstar in this position considering it soared 171% in 2024 while the S&P 500 ripped more than 23% higher.\n\nBut how about expanding your perspective to a few years ago? Had you picked up Nvidia stock five years ago, would you be looking at a profit from your investment or would you still be waiting for it to recover?\n\nThis semiconductor specialist has skyrocketed through the stratosphere\n\nWhile AI stocks have only become prominent on most investors' radars in the past couple of years, Nvidia had been working to emerge as the industry leader even back in 2020, when it acquired Swiftstack, a data storage company whose technology had already been used in Nvidia's graphics processing units (GPUs). A flurry of transactions continued over the ensuing years as Nvidia pursued an aggressive growth-through-acquisition strategy.\n\nA more robust intellectual property portfolio isn't the only hallmark of Nvidia's history over the past five years. The company has also achieved improvements on a variety of financial metrics as customers continue to source their GPUs and other semiconductors from Nvidia. From growing its top line to generating stronger earnings and free cash flow, Nvidia's financial successes have provided investors with plenty to cheer.\n\nOver the past five years, the market has consistently rewarded Nvidia for its achievements, launching it about 1,820% higher. Those who purchased $1,000 in stock five years ago have seen their original investments grow to $19,270 as I write this.\n\nIs it too late to scoop up Nvidia stock?\n\nDespite Nvidia stock's stellar performance, those interested in a position still have a great opportunity with the stock trading at a discount to its historical valuation. Shares are trading at 54.8 times trailing earnings, which admittedly seems pricey, but it's less expensive than their five-year average P/E of 73.4.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Deutsche Bank Urges Caution with Nvidia (NVDA) Stock Ahead of Earnings",
            "link": "https://www.tipranks.com/news/deutsche-bank-urges-caution-with-nvidia-nvda-stock-ahead-of-earnings",
            "snippet": "Deutsche Bank ($DB) is urging investors to proceed with caution regarding Nvidia ($NVDA) stock ahead of the chipmaker's next earnings print on Feb. 26.",
            "score": 0.7221632599830627,
            "sentiment": null,
            "probability": null,
            "content": "Deutsche Bank (DB) is urging investors to proceed with caution regarding Nvidia (NVDA) stock ahead of the chipmaker\u2019s next earnings print on Feb. 26.\n\nLeading into the highly-anticipated quarterly results, Deutsche Bank is one of the few Wall Street firms that is not recommending NVDA stock. In a new report, the bank reiterated its Hold rating on Nvidia\u2019s stock and maintained a $140 price target on the shares, which is close to where they\u2019re currently trading.\n\nRoss Seymore, a top five-star rated analyst who has a 72% success rate, said he expects Nvidia to post strong financial results for the final quarter of 2024. But he also believes the company\u2019s guidance for the current quarter will only match Wall Street\u2019s consensus estimates, leaving the share price with little room for further gains.\n\nSluggish Start to the Year\n\nIn his report, Seymore wrote that, \u201cWe see limited upside in the (April quarter) guidance given the ongoing complexities of the Blackwell ramp, with the buyside expecting flattish quarter-over-quarter guidance.\u201d He also sees issues with Nvidia\u2019s \u201cexpansive capital budget\u201d used to retain the company\u2019s edge in artificial intelligence (AI) microchips and processors.\n\nSeymore expects that investors will question whether the high level of demand for Nvidia processors can be sustained moving forward, which is also likely to stall NVDA stock\u2019s march higher. Nvidia\u2019s shares have gotten off to a sluggish start this year, having gained only 3.52% following a steep selloff in January prompted by the emergence of China\u2019s DeepSeek AI app.\n\nIs NVDA Stock a Buy?\n\nNvidia\u2019s stock currently has a consensus Strong Buy rating among 32 Wall Street analysts. That rating is based on 30 Buy and two Hold recommendations assigned in the last three months. The average NVDA price target of $179.77 implies 29.52% upside from current levels.\n\nRead more analyst ratings on NVDA stock\n\nQuestions or Comments about the article? Write to editor@tipranks.com",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia: A Golden Buying Opportunity Before Earnings (NASDAQ:NVDA)",
            "link": "https://seekingalpha.com/article/4760715-nvidia-golden-buying-opportunity-before-earnings",
            "snippet": "Discover why Nvidia Corporation's undervalued stock could soar, with analysts predicting strong earnings and YOY growth. Click for my NVDA earnings preview.",
            "score": 0.6902827024459839,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "UBS Expects Strong Nvidia Earnings, Forecasts Blackwell Ramp-Up in 2025",
            "link": "https://finance.yahoo.com/news/ubs-expects-strong-nvidia-earnings-175052521.html",
            "snippet": "UBS anticipates strong Nvidia earnings, with revenue meeting or exceeding estimates.",
            "score": 0.9507614374160767,
            "sentiment": null,
            "probability": null,
            "content": "UBS analysts expect Nvidia (NVDA, Financials) to report strong earnings, with revenue and data center performance meeting or exceeding investor expectations.\n\nBased on data center sector contribution of $38.5 billion to $39 billion, the company anticipates fiscal first-quarter revenue guidance to be between $42.5 billion and $43 billion.\n\nA major modification in Nvidia's revenue recognition methodology will show income from Blackwell compute boards at shipping instead of depending only on OEM/ODM partner whole rack sales. Given computing board sales rising ahead of complete GB200 rack shipments, analysts think Nvidia is controlling a short-term supply chain imbalance.\n\nCustomers value availability over waiting for GB200 racks, so high demand is pushing a change toward Blackwell SKUs like as the B200 on the air-cooled HGX platform. With estimates of Blackwell-related income in the fourth quarter at over $9 billion, UBS projects that it might more than quadruple to over $20 billion in the first quarter, corresponding to roughly 700,000 units delivered.\n\nAhead of a larger GB200 rack launch, which UBS expects to spike in March 2025, Nvidia is likely to keep supplying computing boards. End hyperscale customers help to partly fund certain OEM/ODM inventory finance for Blackwell.\n\nForecasting $234 billion in 2025 sales and an earnings per share projection of $5.33, UBS has a Buy recommendation on Nvidia, with possible rises to $6.25 or higher in 2026. For the shares, UBS has a target price of $185.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Nvidia earnings preview: Everything this top portfolio manager expects",
            "link": "https://www.thestreet.com/video/nvidia-earnings-preview",
            "snippet": "Here's what to watch when Nvidia reports earnings after the closing bell on February 26.",
            "score": 0.8993315696716309,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia CEO Jensen Huang Says the DeepSeek Reaction Was Wrong. Here\u2019s Why.",
            "link": "https://www.barrons.com/articles/deepseek-nvidia-ceo-jensen-huang-4ab1b0fa",
            "snippet": "DeepSeek's innovations will lead to greater demand for artificial-intelligence hardware, not less, says Nvidia CEO Jensen Huang.",
            "score": 0.5174829959869385,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NVIDIA\u2019s GeForce RTX 5090 \u201cGB202\u201d Chip Turns Out To Be Defective For \u201cSpecific\u201d Units In The Market; All Variants Potentially Affected By The Issue",
            "link": "https://wccftech.com/nvidia-geforce-rtx-5090-gb202-chip-reportedly-defective-for-units-in-the-market/",
            "snippet": "NVIDIA's GeForce RTX 5090 GPUs are now said to be experiencing another fiasco, as the onboard \"Blackwell\" GB202 turns out to be defective.",
            "score": 0.9646267294883728,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's GeForce RTX 5090 GPUs are now said to be experiencing another fiasco, as the onboard \"Blackwell\" GB202 turns out to be defective for many units out there.\n\nNVIDIA's Flagship RTX Blackwell GPU Sees Missing ROPs In GPU-Z & Similar Tests; Issues Rumored To Lie With The GB202 Chip\n\nWell, when would gamers find peace in the GPU markets? Initially, we had inventory issues, with NVIDIA's flagship RTX Blackwell model being available in \"marginal\" quantities, and now, it is reported that Team Green's silicon has turned out defective for some of the models out there, creating a massive performance degradation. We recently reported about a ZOTAC GeForce RTX 5090 experiencing this issue, and now, according to @MEGAsizeGPU, the core issue actually lies in Blackwell's GB202 chip, which we'll discuss next.\n\nThe root cause is the chip. A small batch of GB202 is defective, and the bios can not do anything with this issue. \u2014 MEGAsizeGPU (@Zed__Wang) February 21, 2025\n\nFor those unaware, NVIDIA's GeForce RTX 5090 is showing up over the internet with fewer ROPs in GPU-Z, which basically means that consumers experiencing the problem have received a model that has degraded performance compared to normal models. Initially, it was claimed that there's an issue with GPU-Z for some of the variants out there; however, now, it is stated that the problem lies with the defective GB202 chip, which isn't good news at all.\n\nTo back the claim of this issue not being associated with the GPU-Z, an affected RTX 5090 variant was tested at HWINFO, and the reduction of ROPs was noticed here as well, suggesting a much significant problem behind it. Given that we are in the early stages of the problem, a limited number of SKUs are affected. However, it isn't limited to just a single AIB model since it is rumored that all variants, including the FE model, could potentially be affected.\n\n@BuildLabEx\n\nSo, what now? Well, we have reached out to NVIDIA for an update, and considering that the issue lies with the GB202 chip, it is likely that Team Green will push out replacements under RMA, and we are hoping that the problem doesn't spread further, since it would prove to be troublesome for NVIDIA, given that their \"mid-range\" GPUs are already said to see a production delay due to performance issues.\n\nWe request that our viewers check their respective models to see if there is any issue with the ROP count; if it is less than 176, let us know. For now, we are unaware of whether the problem is with other models, like the GeForce RTX 5080, since the fiasco is still developing.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-20": {
        "0": {
            "title": "It\u2019s a Sign: AI Platform for Teaching American Sign Language Aims to Bridge Communication Gaps",
            "link": "https://blogs.nvidia.com/blog/ai-sign-language/",
            "snippet": "The Signs platform is creating a validated dataset for sign language learners and developers of ASL-based AI applications.",
            "score": 0.6676471829414368,
            "sentiment": null,
            "probability": null,
            "content": "American Sign Language is the third most prevalent language in the United States \u2014 but there are vastly fewer AI tools developed with ASL data than data representing the country\u2019s most common languages, English and Spanish.\n\nNVIDIA, the American Society for Deaf Children and creative agency Hello Monday are helping close this gap with Signs, an interactive web platform built to support ASL learning and the development of accessible AI applications.\n\nSign language learners can access the platform\u2019s validated library of ASL signs to expand their vocabulary with the help of a 3D avatar that demonstrates signs \u2014 and use an AI tool that analyzes webcam footage to receive real-time feedback on their signing. Signers of any skill level can contribute by signing specific words to help build a video dataset for ASL.\n\nThe dataset \u2014 which NVIDIA aims to grow to 400,000 video clips representing 1,000 signed words \u2014 is being validated by fluent ASL users and interpreters to ensure the accuracy of each sign, resulting in a high-quality visual dictionary and teaching tool.\n\n\u201cMost deaf children are born to hearing parents. Giving family members accessible tools like Signs to start learning ASL early enables them to open an effective communication channel with children as young as six to eight months old,\u201d said Cheri Dowling, executive director of the American Society for Deaf Children. \u201cAnd knowing that professional ASL teachers have validated all the vocabulary on the platform, users can be confident in what they\u2019re learning.\u201d\n\nNVIDIA teams plan to use this dataset to further develop AI applications that break down communication barriers between the deaf and hearing communities. The data is slated to be available to the public as a resource for building accessible technologies including AI agents, digital human applications and video conferencing tools. It could also be used to enhance Signs and enable ASL platforms across the ecosystem with real-time, AI-powered support and feedback.\n\nSupporting ASL Education, Exploring Language Nuance\n\nDuring the data collection phase, Signs already provides a powerful platform for ASL language acquisition, offering opportunities for individuals to learn and practice an initial set of 100 signs so they can more effectively communicate with friends or family members who use ASL.\n\n\u201cThe Signs learning platform could help families with deaf children quickly search for a specific word and see how to make the corresponding sign. It\u2019s a tool that can help support their everyday use of ASL outside of a more formal class,\u201d Dowling said. \u201cI see both kids and parents exploring it \u2014 and I think they could play with it together.\u201d\n\nWhile Signs currently focuses on hand movements and finger positions for each sign, ASL also incorporates facial expressions and head movements to convey meaning. The team behind Signs is exploring how these non-manual signals can be tracked and integrated in future versions of the platform.\n\nThey\u2019re also investigating how other nuances, like regional variations and slang terms, can be represented in Signs to enrich its ASL database \u2014 and working with researchers at the Rochester Institute of Technology\u2019s Center for Accessibility and Inclusion Research to evaluate and further improve the user experience of the Signs platform for deaf and hard-of-hearing users.\n\n\u201cImproving ASL accessibility is an ongoing effort,\u201d said Anders Jessen, founding partner of Hello Monday/DEPT, which built the Signs web platform and previously worked with the American Society for Deaf Children on Fingerspelling.xyz, an application that taught users the ASL alphabet. \u201cSigns can serve the need for advanced AI tools that help transcend communication barriers between the deaf and hearing communities.\u201d\n\nThe dataset behind Signs is planned for release later this year.\n\nStart learning or contributing with Signs at signs-ai.com, and learn more about NVIDIA\u2019s trustworthy AI initiatives. Attendees of NVIDIA GTC, a global AI conference taking place March 17-21 in San Jose, will be able to participate in Signs live at the event.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia GeForce RTX 5070 Ti review: An RTX 4080 for $749, at least in theory",
            "link": "https://arstechnica.com/gadgets/2025/02/nvidia-geforce-rtx-5070-ti-review-an-rtx-4080-for-749-at-least-in-theory/",
            "snippet": "Nvidia's RTX 50-series makes its first foray below the $1,000 mark starting this week, with the $749 RTX 5070 Ti\u2014at least in theory.",
            "score": 0.9032675623893738,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's RTX 50-series makes its first foray below the $1,000 mark starting this week, with the $749 RTX 5070 Ti\u2014at least in theory.\n\nThe third-fastest card in the Blackwell GPU lineup, the 5070 Ti is still far from \"reasonably priced\" by historical standards (the 3070 Ti was $599 at launch). But it's also $50 cheaper and a fair bit faster than the outgoing 4070 Ti Super and the older 4070 Ti. These are steps in the right direction, if small ones.\n\nWe'll talk more about its performance shortly, but at a high level, the 5070 Ti's performance falls in the same general range as the 4080 Super and the original RTX 4080, a card that launched for $1,199 just over two years ago. And it's probably your floor for consistently playable native 4K gaming for those of you out there who don't want to rely on DLSS or 4K upscaling to hit that resolution (it's also probably all the GPU that most people will need for high-FPS 1440p, if that's more your speed).\n\nBut it's a card I'm ambivalent about! It's close to 90 percent as fast as a 5080 for 75 percent of the price, at least if you go by Nvidia's minimum list prices, which for the 5090 and 5080 have been mostly fictional so far. If you can find it at that price\u2014and that's a big \"if,\" since every $749 model is already out of stock across the board at Newegg\u2014and you're desperate to upgrade or are building a brand-new 4K gaming PC, you could do worse. But I wouldn't spend more than $749 on it, and it might be worth waiting to see what AMD's first 90-series Radeon cards look like in a couple weeks before you jump in.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia CEO Jensen Huang directly addresses the DeepSeek stock sell-off, saying investors got it wrong",
            "link": "https://www.businessinsider.com/nvidia-ceo-jensen-huang-addresses-deepseek-stock-sell-off-2025-2",
            "snippet": "DeepSeek is an exciting development and not a threat to compute demand, Nvidia CEO Jensen Huang said in an interview that aired Thursday.",
            "score": 0.6871175169944763,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang said investors misinterpreted DeepSeek's AI advancements.\n\nDeepSeek's large language models were built with weaker chips, rattling markets in January.\n\nIn a pre-taped interview released Thursday, Huang emphasized the importance of AI post-training.\n\nInvestors took away the wrong message from DeepSeek's advancements in AI, Nvidia CEO Jensen Huang said at a virtual event aired Thursday.\n\nDeepSeek, a Chinese AI firm owned by the hedge fund High-Flyer, released a competitive, open-source reasoning model named R1 in January. The firm said the large language model underpinning R1 was built with weaker chips and a fraction of the funding of the predominant, Western-made AI models.\n\nInvestors reacted to this news by selling off Nvidia stock, resulting in a $600 billion loss in market capitalization. Huang himself temporarily lost nearly 20% of his net worth in the rout. The stock has since recovered much of its lost value.\n\nHuang said in Thursday's pre-recorded interview, which was produced by Nvidia's partner DDN and part of an event debuting DDN's new software platform, Infinia, that the dramatic market response stemmed from investors' misinterpretation.\n\nInvestors have raised questions as to whether trillions in spending on AI infrastructure by Big Tech firms is needed, if less computing power is required to train models. Jensen said the industry still needed computing power for post-training methods, which allow AI models to draw conclusions or make predictions after training.\n\nAs post-training methods grow and diversify, the need for the computing power Nvidia chips provide will also grow, he continued.\n\n\"From an investor perspective, there was a mental model that the world was pre-training and then inference. And inference was, you ask an AI a question, and it instantly gives you an answer,\" he said at Thursday's event, adding, \"I don't know whose fault it is, but obviously that paradigm is wrong.\"\n\nPre-training is still important, Huang said, but post-training is the \"most important part of intelligence\" and \"where you learn to solve problems.\"\n\nDeepSeek's innovations energize the AI world, he said.\n\n\"It is so incredibly exciting. The energy around the world as a result of R1 becoming open-sourced \u2014 incredible,\" Huang said.\n\nNvidia spokespeople have addressed the market reaction with written statements to a similar effect, though Huang had yet to make public comments on the topic until Thursday's event.\n\nHuang has been defending against the growing concern that model scaling is in trouble for months. Even before DeepSeek burst into the public consciousness in January, reports that model improvements at OpenAI were slowing down roused suspicions that the AI boom might not deliver on its promise \u2014 and Nvidia, therefore, wouldn't continue to cash in at the same rate.\n\nIn November, Huang stressed that scaling was alive and well and that it had simply shifted from training to inference. Huang also said Thursday that post-training methods were \"really quite intense\" and that models would keep improving with new reasoning methods.\n\nHuang's DeepSeek comments may serve as a preview for Nvidia's first earnings call of 2025, scheduled for February 26. DeepSeek has become a popular topic of discussion on earnings calls for companies across the tech spectrum, from Airbnb to Palantir.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia RTX 5070 Ti Restock: Where to Buy the Newest GPU Before It Sells Out",
            "link": "https://www.techtimes.com/articles/309456/20250221/nvidia-rtx-5070-ti-restock-where-buy-newest-gpu-before-it-sells-out.htm",
            "snippet": "Nvidia's RTX 5070 Ti is here at $749, but with demand skyrocketing, buyers must act fast before it sells out\u2014here's where to grab one before it's gone.",
            "score": 0.9398638010025024,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia is giving \"Priority Access\" to those buyers of RTX 5080 and 5090 FE GPUs. The case for the other GPUS like the RTX 5070 Ti is rather different: you need to quickly look for restocks.\n\nPriced at $749 in the US and \u00a3729 in the UK, the GPU is available starting Thursday, Feb. 20. If you're planning to upgrade your gaming setup, you'll need to act fast\u2014expect high demand and limited stock, just like the two previously mentioned GPUs.\n\nNvidia RTX 5070 Ti Restock: Where to Buy\n\nDemand is expected to be astronomically high, so finding a brand-new RTX 5070 Ti at launch will prove difficult. The following are the best places to look for one, according to TechRadar's suggestion.\n\nFor US Buyers\n\nNvidia: Buy third-party GPUs straight from Nvidia's site.\n\nAmazon: Early stock could sell out quickly, but is a good choice for later re-stocking.\n\nBest Buy: Several third-party RTX 5070 Ti cards.\n\nNewegg: Variety of Asus, Gigabyte, and MSI cards.\n\nB&H: Very limited stock alerts.\n\nMicro Center: Will be available in-store on Feb. 20.\n\nAdorama: Should list RTX 5070 Ti models as soon as they open.\n\nFor UK Buyers\n\nNvidia: Direct sale of third-party models.\n\nAmazon UK: Should have stock in different configurations.\n\nEbuyer: Already listing RTX 5070 Ti models, but pre-orders are not yet available.\n\nScan: Carrying Zotac, Gigabyte, and others on launch day.\n\nOverclockers UK: Should sell RTX 5070 Ti GPUs from February 20 at 2 PM.\n\nBox.co.uk: Another trustworthy source for third-party stock.\n\nEE: To our surprise, this mobile network provider will be selling RTX 5070 Ti GPUs on a first-come, first-served basis.\n\nRead more: Nvidia Shuts Down Rumors of Cutting Product Supply to China Amid Antitrust Probe\n\nPrebuilt Gaming PCs and Laptops with RTX 5070 Ti\n\nIf finding a standalone RTX 5070 Ti is tough, look to prebuilt gaming PCs and laptops with the GPU pre-installed. These might provide a better opportunity to acquire the card without scalper prices.\n\nSpeaking of scalpers, don't fall victim to eBay sellers who sell fake RTX 5090. Recently, we learned that they were preying on gullible gamers.\n\nKeep an eye out for preassembled units from manufacturers such as:\n\nAlienware (Dell)\n\nMSI\n\nAsus ROG\n\nCyberPowerPC\n\nOrigin PC\n\nRetailers such as Newegg, Best Buy, and Amazon will most likely have RTX 5070 Ti gaming laptops and desktops soon.\n\nWhy the RTX 5070 Ti Is Worth the Upgrade\n\nIf you've been holding off on an upgrade because of the RTX 5080 and 5090's steep price tags, the RTX 5070 Ti delivers high-end performance at a more reasonable price. Here's what makes it a better choice than previous GPU models.\n\nNext-Gen Performance: Built on Nvidia's Blackwell architecture, offering improved efficiency and speed.\n\n5th-Gen Tensor Cores: Optimized for AI-powered gaming enhancements like DLSS 3.5.\n\nMulti-Frame Generation: Smoother frame rates with better rendering techniques.\n\nRay Tracing Enhancements: Improved real-time lighting, shadows, and reflections.\n\nAffordability: Priced at $749, it's a mid-range beast compared to other RTX 5000 series models that have price tags over $1,000.\n\nAct Quickly Before It Sells Out\n\nThe RTX 5070 Ti is set to be one of the most in-demand GPUs of the year, and stock will vanish quickly. If you're serious about getting one, be prepared to refresh retailer pages, set stock alerts, and move fast when listings go live.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia Analyst Sees Higher Blackwell Supplies; Google Preview Bullish For Stock; Is Nvidia A Buy Now?",
            "link": "https://www.investors.com/research/nvda-stock-is-nvidia-a-buy-february-2025/",
            "snippet": "Nvidia Stock: Morgan Stanley Top 2025 Pick. Shares gained nearly 4% on Dec. 20 after Moore named Nvidia a \"top 2025 pick.\" But he trimmed his price target on...",
            "score": 0.6701194047927856,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) rose 3% early Friday after Foxconn, formally known as Hon Hai Precision Industry, projected higher artificial intelligence sales in its fourth-quarter results, potentially benefitting Nvidia stock. Foxconn, an Apple (AAPL) partner, expects first-quarter AI server revenue to double from both a year ago and from the prior quarter, signaling a strong demand for AI chips. But is Nvidia\u2026",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Opinion: Nvidia options traders are vastly overestimating how earnings will move the stock. Just look at these charts.",
            "link": "https://www.marketwatch.com/story/nvidia-options-traders-are-vastly-overestimating-how-earnings-will-move-the-stock-691a6db1",
            "snippet": "Nvidia options aren't a good trade, this expert says. Here's what he likes.",
            "score": 0.9020291566848755,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "What Nvidia bulls are whispering about before its big earnings report",
            "link": "https://finance.yahoo.com/news/what-nvidia-bulls-are-whispering-about-before-its-big-earnings-report-120347483.html",
            "snippet": "The Street is gearing up for another big read on the AI trade with Nvidia earnings.",
            "score": 0.9247862100601196,
            "sentiment": null,
            "probability": null,
            "content": "The Street is sticking with Nvidia (NVDA) into its market-moving earnings report next week.\n\nDespite China-based DeepSeek rocking the super bullish AI thesis earlier this year, Wall Street still sees Nvidia profitting from the global buildout of AI infrastructure. Aggressive 2025 capital expenditures assumptions by hyperscalers such as Amazon (AMZN) and Meta (META) shared during this earnings season underscore the point.\n\n\"Over the coming decades the investment [in artificial intelligence] is happening,\" Russell Investments chief investment officer Kate El-Hillow tells me.\n\nHowever, that's not to say there aren't signs of caution going into Nvidia's earnings report.\n\nYahoo Finance data shows Nvidia's first quarter earnings per share (EPS) trend has drifted modestly lower over the past 30 days. The Street has also not pushed up its 2025 EPS estimates on Nvidia for more than 60 days.\n\nNvidia is also among the most cheaply valued AI stocks at the moment.\n\nOn a forward price-to-earnings (PE) multiple basis, Yahoo Finance data shows Nvidia trading at 32 times. Broadcom (AVGO) and Marvell Technology (MRVL) are valued at 35 times and 41 times, respectively. Arm Holdings (ARM) clocks in at 76 times.\n\n\"I would say I wouldn't want to be Jensen [Nvidia CEO Jensen Huang] necessarily because, wow, other people are working on the same things,\" Microsoft (MSFT) co-founder Bill Gates told me on Yahoo Finance's Opening Bid podcast (listen in below).\n\nThis embedded content is not available in your region.\n\nHere are the vibes on the Street a few days ahead of Nvidia's earnings on Feb. 26.\n\nHSBC analyst Ryan Mellor\n\nRating: Reiterated Buy\n\nPrice Target: $175, down from $185\n\n\"Despite recent concerns over the long-term demand impact from DeepSeek's lower cost approach, most of the hyperscalers have actually revised up 2025 capex by 4%-26% during the earnings season, supporting our thesis that overall AI GPU demand outlook remains strong.\"\n\n\"Nevertheless, we are lowering our FY26 estimate for data center revenue by 11% to $209 billion on the back of lower NVL rack [Nvidia data center product] assumptions, but our FY26 data center revenue forecast still remains 14% above consensus of $184 billion. However, we believe there remains greater pressure for Nvidia to deliver stronger second half fiscal year momentum on the back of its B300/GB300 roadmap to see if there is potential room for further upside.\"\n\nKeybanc Capital Markets analyst John Vinh\n\nRating: Reiterated Overweight\n\nPrice Target: $190, up from $180\n\n\"While we do believe that manufacturing constraints are limiting shipments of GB200 NVL server racks, we believe this will be more than offset by the following: 1) given the lower initial manufacturing yields of GB200 NVL, we believe customers have been able to push out orders of GB200 and backfill with HGX-based B200 servers with x86 head nodes; 2) DeepSeek, as well as limited supply of Huawei's Ascend AI ASIC [custom chip], has created a surge in demand for H20 GPUs from China CSPs; 3) we believe Nvidia's customers, especially communications service providers, are financing its inventory at EMS providers, so effectively sell-in shipments from Nvidi to EMS are recognized as revenues. As such, we are raising estimates and our price target and believe Nvidia's strong results should alleviate any concerns that DeepSeek could derail near-term AI capex intensity.\"",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia boss saves California College of the Arts with mega donation",
            "link": "https://sfstandard.com/2025/02/20/nvidia-ceo-saves-arts-college-cca/",
            "snippet": "California College of the Arts has received a package of donations worth $45 million, half of which came from Nvidia boss Jensen Huang.",
            "score": 0.6203466653823853,
            "sentiment": null,
            "probability": null,
            "content": "Huang\u2019s contribution, his first for CCA, is the largest donation in the school\u2019s history by $5 million.\n\nBattered by budget shortfalls and declining enrollment, the San Francisco college announced Friday that it had raised $22.5 million in donations from the board, former trustees, alumni, and Bay Area arts, culture, and tech donors. The Jen-Hsun & Lori Huang Foundation, a philanthropic nonprofit created in 2007, matched that number.\n\nCalifornia College of the Arts has received a package of donations worth $45 million, half of which came from the world\u2019s 11th-richest man, Nvidia co-founder and CEO Jensen Huang.\n\nThe funds arrived at a crucial moment for the school. Since the pandemic, CCA has faced declining enrollment and financial setbacks, even as a $123 million campus expansion, funded entirely by donors, opened in the fall.\n\nCCA President David Howse announced in August that the school was facing a $20 million budget gap. In September, it laid off 23 staffers, 10% of its workforce. There were 1,848 students in 2019, versus 1,295 this school year. Howse said applications for next year are slightly down.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia (NVDA) Q4 Earnings: Can it Beat High Expectations?",
            "link": "https://www.tastylive.com/news-insights/nvidia-nvda-q4-earnings-beat-high-expectations",
            "snippet": "NVIDIA reports earnings as stock bounces back from DeepSeek selloff. Market eyes $38B revenue target amid fierce AI chip competition.",
            "score": 0.9440158605575562,
            "sentiment": null,
            "probability": null,
            "content": "Its stock is bouncing back from the DeepSeek selloff. Can it hit the $38 billion revenue target?\n\nNvidia will report quarterly earnings on Feb. 26 after the market closes, and it's expected to announce earnings-per-share of $0.85 on $38.13 billion in revenue.\n\nWhile the stock took a nosedive after the DeepSeek news, it has sharply recovered and still leads the industry in hardware.\n\nThe expected stock price move is just over 8% for earnings week.\n\n\n\nNVIDIA Earnings Preview\n\nIn just a few days, we'll end this earnings season with a bang with NVIDIA (NVDA) reporting quarterly earnings next Wednesday, after the stock market closes. After the massive drop in NVDA's stock price following the DeepSeek announcement, it's full steam ahead because we've seen the exact opposite market movement since then. Since its descent to a low of $113.01 in early February, the stock is hovering at around $140 per share.\n\n\n\n\n\n\n\n\n\nNVIDIA is expected to report earnings-per-share (EPS) of $0.85 on $38.13 billion in revenue. Both figures are significantly higher than last quarter's estimates, with an increase of $0.10 in EPS and $5 billion in revenue. NVDA has boasted a perfect three-for-three over the last few announcements\u2014each time it has exceeded EPS and revenue estimates.\n\nJensen Huang, founder and CEO of NVIDIA, had very strong positive words in the last earnings call: \u201cThe age of AI is in full steam, propelling a global shift to NVIDIA computing ... Demand for Hopper and anticipation for Blackwell \u2014 in full production \u2014 are incredible as foundation model makers scale pre-training, post-training and inference ... AI is transforming every industry, company and country. Enterprises are adopting agentic AI to revolutionize workflows. Industrial robotics investments are surging with breakthroughs in physical AI. And countries have awakened to the importance of developing their national AI and infrastructure ...\"\n\nAfter the knee-jerk reaction following the DeepSeek news, both the downside and upside, the market is watching NVDA earnings on the Feb. 26. Looking at the implied volatility of the options market in NVDA helps us put context around expected stock price moves.\n\n\n\n\n\n\n\n\n\nFor earnings week, NVDA stock has a +/- $11.90 expected range based on current implied volatility. Implied volatility is derived from options prices and can help us understand how much movement is priced in to each expiration cycle. The earnings expected move is about 8% of the notional value of the stock price, which puts NVDA earnings on the higher end of the typical 5%-10% range that most companies have in terms of implied volatility for earnings.\n\nLooking to April 2025, we can see a +/- $20.50 stock price expected move for NVDA. This means the earnings call makes up for over 50% of the expected move priced into the next 56 days. That's a pretty big chunk, which confirms the market is ready for some large potential price swings after earnings.\n\n\n\n\n\nBullish on NVDA stock for earnings\n\nIf you're bullish on NVDA for earnings, you want to see another strong earnings report, with the AI chip giant exceeding EPS and revenue estimates yet again. More involvement in the mega-cap stocks in AI means more demand for chipmakers, and that plays right into the hands of a company like NVIDIA. It will be interesting to see how the landscape looks for NVIDIA in 2025 with earnings commentary. But for the recent rally to sustain, you'd have to imagine some big numbers are required on Wednesday.\n\n\n\n\n\nBearish on NVDA stock for earnings\n\nIf you're bearish on NVDA for earnings, you want to see some sputtering from the recent bullish momentum the stock has had over the past few weeks. A high earnings implied volatility isn't always a good thing either because it means there is a lot of uncertainty around what will happen after the announcement. That\u2019s something to keep an eye on ahead of earnings, but if NVDA posts its first EPS or revenue miss in many quarters, we could see the stock fall afterward.\n\nTune in to Options Trading Concepts Live at 11 a.m. CST on Wednesday ahead of the announcement for some options strategies in NVDA for earnings.\n\n\n\nMike Butler, tastylive director of market intelligence, has been in the markets and trading for a decade. He appears on Options Trading Concepts Live, airing Monday-Friday. @tradermikeyb\n\nFor live daily programming, market news and commentary, visit tastylive or the YouTube channels tastylive (for options traders), and tastyliveTrending for stocks, futures, forex & macro.\n\nTrade with a better broker, open a tastytrade account today. tastylive, Inc. and tastytrade, Inc. are separate but affiliated companies.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "Nvidia slashes stake in emerging rival as AI arms race heats up",
            "link": "https://www.thestreet.com/investing/nvidia-slashes-stake-in-emerging-rival-as-ai-arms-rate-heats-up",
            "snippet": "Nvidia has trimmed its nascent investment portfolio by around 30%, including a big change in a key holding of a former takeover target.",
            "score": 0.9514427185058594,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-19": {
        "0": {
            "title": "Massive Foundation Model for Biomolecular Sciences Now Available via NVIDIA BioNeMo",
            "link": "https://blogs.nvidia.com/blog/evo-2-biomolecular-ai/",
            "snippet": "Evo 2, a powerful biomolecular AI model, provides insights into DNA, RNA and proteins across diverse species.",
            "score": 0.9095980525016785,
            "sentiment": null,
            "probability": null,
            "content": "Evo 2, a powerful new AI model built using NVIDIA DGX Cloud on Amazon Web Services (AWS), provides insights into DNA, RNA and proteins across diverse species.\n\nScientists everywhere can now access Evo 2, a powerful new foundation model that understands the genetic code for all domains of life. Unveiled today as the largest publicly available AI model for genomic data, it was built on the NVIDIA DGX Cloud platform in a collaboration led by nonprofit biomedical research organization Arc Institute and Stanford University.\n\nEvo 2 is available to global developers on the NVIDIA BioNeMo platform, including as an NVIDIA NIM microservice for easy, secure AI deployment.\n\nTrained on an enormous dataset of nearly 9 trillion nucleotides \u2014 the building blocks of DNA and RNA \u2014 Evo 2 can be applied to biomolecular research applications including predicting the form and function of proteins based on their genetic sequence, identifying novel molecules for healthcare and industrial applications, and evaluating how gene mutations affect their function.\n\n\u201cEvo 2 represents a major milestone for generative genomics,\u201d said Patrick Hsu, Arc Institute cofounder and core investigator, and an assistant professor of bioengineering at the University of California, Berkeley. \u201cBy advancing our understanding of these fundamental building blocks of life, we can pursue solutions in healthcare and environmental science that are unimaginable today.\u201d\n\nThe NVIDIA NIM microservice for Evo 2 enables users to generate a variety of biological sequences, with settings to adjust model parameters. Developers interested in fine-tuning Evo 2 on their proprietary datasets can download the model through the open-source NVIDIA BioNeMo Framework, a collection of accelerated computing tools for biomolecular research.\n\n\u201cDesigning new biology has traditionally been a laborious, unpredictable and artisanal process,\u201d said Brian Hie, assistant professor of chemical engineering at Stanford University, the Dieter Schwarz Foundation Stanford Data Science Faculty Fellow and an Arc Institute innovation investigator. \u201cWith Evo 2, we make biological design of complex systems more accessible to researchers, enabling the creation of new and beneficial advances in a fraction of the time it would previously have taken.\u201d\n\nEnabling Complex Scientific Research\n\nEstablished in 2021 with $650 million from its founding donors, Arc Institute empowers researchers to tackle long-term scientific challenges by providing scientists with multiyear funding \u2014 letting scientists focus on innovative research instead of grant writing.\n\nIts core investigators receive state-of-the-art lab space and funding for eight-year, renewable terms that can be held concurrently with faculty appointments with one of the institute\u2019s university partners, which include Stanford University, the University of California, Berkeley, and the University of California, San Francisco.\n\nBy combining this unique research environment with accelerated computing expertise and resources from NVIDIA, Arc Institute\u2019s researchers can pursue more complex projects, analyze larger datasets and more quickly achieve results. Its scientists are focused on disease areas including cancer, immune dysfunction and neurodegeneration.\n\nNVIDIA accelerated the Evo 2 project by giving scientists access to 2,000 NVIDIA H100 GPUs via NVIDIA DGX Cloud on AWS. DGX Cloud provides short-term access to large compute clusters, giving researchers the flexibility to innovate. The fully managed AI platform includes NVIDIA BioNeMo, which features optimized software in the form of NVIDIA NIM microservices and NVIDIA BioNeMo Blueprints.\n\nNVIDIA researchers and engineers also collaborated closely on AI scaling and optimization.\n\nApplications Across Biomolecular Sciences\n\nEvo 2 can provide insights into DNA, RNA and proteins. Trained on a wide array of species across domains of life \u2014 including plants, animals and bacteria \u2014 the model can be applied to scientific fields such as healthcare, agricultural biotechnology and materials science.\n\nEvo 2 uses a novel model architecture that can process lengthy sequences of genetic information, up to 1 million tokens. This widened view into the genome could unlock scientists\u2019 understanding of the connection between distant parts of an organism\u2019s genetic code and the mechanics of cell function, gene expression and disease.\n\n\u201cA single human gene contains thousands of nucleotides \u2014 so for an AI model to analyze how such complex biological systems work, it needs to process the largest possible portion of a genetic sequence at once,\u201d said Hsu.\n\nIn healthcare and drug discovery, Evo 2 could help researchers understand which gene variants are tied to a specific disease \u2014 and design novel molecules that precisely target those areas to treat the disease. For example, researchers from Stanford and the Arc Institute found that in tests with BRCA1, a gene associated with breast cancer, Evo 2 could predict with 90% accuracy whether previously unrecognized mutations would affect gene function.\n\nIn agriculture, the model could help tackle global food shortages by providing insights into plant biology and helping scientists develop varieties of crops that are more climate-resilient or more nutrient-dense. And in other scientific fields, Evo 2 could be applied to design biofuels or engineer proteins that break down oil or plastic.\n\n\u201cDeploying a model like Evo 2 is like sending a powerful new telescope out to the farthest reaches of the universe,\u201d said Dave Burke, Arc\u2019s chief technology officer. \u201cWe know there\u2019s immense opportunity for exploration, but we don\u2019t yet know what we\u2019re going to discover.\u201d\n\nRead more about Evo 2 on the NVIDIA Technical Blog and in Arc\u2019s technical report.\n\nSee notice regarding software product information.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Stock Investors Got Amazing Robotics News",
            "link": "https://www.fool.com/investing/2025/02/19/nvidia-stock-investors-got-amazing-robotics-news/",
            "snippet": "In today's video, I discuss Nvidia (NVDA 0.63%) and recent updates affecting the semiconductor giant. To learn more, check out the short video,...",
            "score": 0.6116430759429932,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia could benefit from the growth of robotics.\n\nIn today's video, I discuss Nvidia (NVDA 5.27%) and recent updates affecting the semiconductor giant. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the market prices of Feb. 14, 2025. The video was published on Feb. 17, 2025.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Why one Nvidia bull thinks things look 'spectacular' heading into the AI chipmaker's earnings",
            "link": "https://finance.yahoo.com/news/why-one-nvidia-bull-thinks-things-look-spectacular-heading-into-the-ai-chipmakers-earnings-111429732.html",
            "snippet": "The Nvidia (NVDA) bulls continue to defend their favorite stock in the world heading into the chipmaker's earnings report next week.",
            "score": 0.8844566345214844,
            "sentiment": null,
            "probability": null,
            "content": "The Nvidia (NVDA) bulls continue to defend their favorite stock in the world heading into the chipmaker's earnings report next week.\n\nDespite DeepSeek's challenge to thinking about AI costs, some analysts are staying upbeat on Nvidia. However, that comes with a few notes of caution \u2014 in particular, about a first quarter outlook that may not satisfy elevated market expectations.\n\n\"Near-term dynamics are fluid (January quarter to July quarter) \u2026 while long-term (October quarter and onward) our work suggests looks frankly spectacular,\" Loop Capital analyst Ananda Baruah wrote in a client note on Wednesday.\n\nBaruah reiterated a $175 price target on Nvidia, which assumes 25% upside from current levels.\n\nAdded Baruah, \"Big picture we believe that two to three year Street numbers remain low as our work with both customers and the Nvidia build ecosystem points [to] ... Nvidia GPU reaching 10 million to 12 million as hyperscalers look to increase their percentage on non-CPU compute to 50% plus in coming years (from ~10% currently). Remember \u2026 for Nvidia the story is accelerated compute + Gen AI, which means it is facing two $1.0 trillion compute market opportunities ahead of it in coming years, each of which is just at the very start.\"\n\nNvidia stock was up less than 1% in premarket trading at $140 each.\n\nWhile Nvidia shares have rallied back about 23% from the early February lows, sentiment on the company's fundamentals has become more mixed.\n\nPodcast: How Bill Gates views Nvidia\n\nEvercore analyst Mark Lipacis wrote in a recent note that there are three reasons for the more cautious tone: 1) DeepSeek lowering AI demand in aggregate, 2) DeepSeek shifting AI compute cycles away from Nvidia GPUs and to ASICs [custom chips], and 3) Blackwell chip delays.\n\nChina-based DeepSeek surprised markets in late January after unveiling RI, its AI model that gives a ChatGPT-esque performance at a cheaper price tag. RI cost a reported $5.6 million to build a base model, compared with the hundreds of millions of dollars incurred at US-based companies such as OpenAI and Anthropic.\n\nFears mounted instantly that US companies are overspending on AI infrastructure, which includes Nvidia chips.\n\n\"Conventional wisdom all of last year was that training amazing models was going to be possible for only a handful of companies,\" Snowflake (SNOW) CEO Sridhar Ramaswamy told me on Yahoo Finance's Opening Bid podcast. \"What DeepSeek has done over the past few weeks is shatter that belief by saying they can train a model for $6 million.\"",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia Revives 'Verified Priority Access' for 5080, 5090 Sales",
            "link": "https://www.pcmag.com/news/nvidia-might-sell-you-an-rtx-5080-or-5090-directly-just-fill-out-this-form",
            "snippet": "The company is resuming its 'Verified Priority Access' program to sell the hard-to-find GeForce RTX 5080 and 5090 graphics cards to Nvidia users.",
            "score": 0.6986819505691528,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia is launching \u2018priority access\u2019 to help fans buy RTX 5080 and 5090 FE GPUs",
            "link": "https://www.theverge.com/news/616138/nvidia-verified-priority-access-to-help-fans-buy-rtx-5080-and-5090",
            "snippet": "Nvidia is introducing \u201cVerified Priority Access\u201d for RTX 5090 and RTX 5080 Founders Edition cards, which are almost impossible to buy otherwise.",
            "score": 0.528291642665863,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and founding member of The Verge who covers gadgets, games, and toys. He spent 15 years editing the likes of CNET, Gizmodo, and Engadget.\n\nNvidia has yet to explain why it launched its GeForce RTX 5090 and 5080 GPUs with barely any inventory, some major launch driver issues, and the occasional melting power connector, but it has apparently reconsidered its stance when it comes to scalpers. The company\u2019s just announced a way for Nvidia fans to sign up for \u201cVerified Priority Access\u201d to buy the elusive two-slot SFF-friendly RTX 5090 and 5080 Founders Edition graphics cards.\n\nLike a similar Verified Priority Access program for the RTX 4090, the new program is invite-only, but this time you\u2019ll apply for access by filling out this form rather than being pre-selected. The site will check that you\u2019ve already had an Nvidia account (accounts created after January 30th need not apply) and ask you whether you\u2019d prefer a 5090 or a 5080. Then, it\u2019ll apparently use an algorithm to figure out if you\u2019re a real gamer (analyzing your Nvidia app / GeForce Experience use) before offering a card. Limit one per person.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Going Into Earnings, Is Nvidia Stock a Buy, a Sell, or Fairly Valued?",
            "link": "https://www.morningstar.com/stocks/going-into-earnings-is-nvidia-stock-buy-sell-or-fairly-valued-5",
            "snippet": "Nvidia NVDA is set to release its fourth-quarter earnings report on Feb. 26. Here's Morningstar's take on what to look for in Nvidia's earnings and stock.",
            "score": 0.8702357411384583,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Is Nvidia Stock A Buy, Sell, Or Hold Before February 26?",
            "link": "https://www.barchart.com/story/news/31013128/is-nvidia-stock-a-buy-sell-or-hold-before-february-26",
            "snippet": "While Nvidia's stock has soared on AI dominance, can it sustain momentum as Q4 earnings release next week, or is it time for investors to trim positions?",
            "score": 0.9102823734283447,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia brings back scalper-beating Verified Priority Access program for RTX 50 Founders Edition GPUs",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-brings-back-scalper-beating-verified-priority-access-for-geforce-rtx-50-founders-edition-boards",
            "snippet": "Select Nvidia customers in the U.S. can get GeForce RTX 5080 FE and GeForce RTX 5090 FE graphics cards directly from Nvidia with Verified Priority Access...",
            "score": 0.6181502938270569,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia this week launched its Verified Priority Access for the GeForce RTX 5080 Founders Edition and GeForce RTX 5090 Founders Edition add-in-boards allowing a limited number of verified U.S. customers to purchase some of the best graphics cards directly from the Nvidia Marketplace without any hassle.\n\nTo qualify, users must have an Nvidia Account created before January 30, 2025 and submit their interest through a form. Selected users will receive email notifications sometimes next week and will be able to purchase Nvidia's GeForce RTX 5080 Founders Edition and GeForce RTX 5090 Founders Edition boards at MSRPs \u2014 $999 and 1999, respectively \u2014 without having to pay a premium to retailers or scalpers.\n\nBy launching its Verified Priority Access program, Nvidia ensures that some \u2014 we do not know how many though \u2014 of its loyal customers will be able to lay their hands on its latest products without having to pay extra or camp near traditional retailers. Considering that these people are loyal GeForce users and registered at Nvidia's forums, the green company can fully expect them to spread the word how good the GeForce RTX 5080 and GeForce RTX 5090 graphics cards are in the forums, which will work as advertising and will to some degree reassure the gamers crowd that the latest AIBs from Nvidia are available relatively widely.\n\nAs an added bonus, Nvidia will be able to sell its own Founders Edition graphics cards directly to gamers, thus earning some additional cash and not sharing the profit with its add-in-board partners as the VPA program does not cover products even from the close allies of Nvidia in the U.S., such as Asus, Gigabyte, MSI, or PNY.\n\nThis Verified Priority Access program is currently limited to U.S. users and applies only to the lates GeForce RTX 5080 and RTX 5080 Founders Edition models. It is unclear whether Nvidia also plans to launch its VPA program for its GeForce RTX 5070 Ti FE when and if it becomes available.\n\nThis is not the first time when Nvidia launches its Verified Priority Access program with its range-topping graphics cards. In 2022 the company already offered such a program for its GeForce RTX 4090. Later on the company expanded it with GeForce RTX 4080 and with products by its AIB partners.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia\u2019s Big Plan to Beat Back RTX 5090 Scalpers Is the Same as Everybody Else\u2019s",
            "link": "https://gizmodo.com/nvidias-big-plan-to-beat-back-rtx-5090-scalpers-is-the-same-as-everybody-else-2000566114",
            "snippet": "Nvidia's 'priority access' raffle may help you get a Nvidia GeForce RTX 5090 or 5080 FE at the base price.",
            "score": 0.8848065137863159,
            "sentiment": null,
            "probability": null,
            "content": "The markup on Nvidia\u2019s $2,000 GeForce RTX 5090 and $1,000 RTX 5080 GPUs has reached new heights. Cards fly off store shelves as soon as they\u2019re posted online, and then they end up on eBay and other reselling platforms at a 50% or 150% markup. Nvidia\u2019s new plan to help you get an FE card for the base price involves entering your name in a raffle and hoping your one of the lucky few who gets drawn out of a hat.\n\nNvidia\u2019s process is very simple. As noted on its official forums Wednesday, anybody with a Nvidia account before Jan. 30 can access this form. You can select either one or both GeForce RTX 5090 or RTX 5080, then hit the button to enroll. Nvidia did not state how many Founders Edition GPUs it will have in stock, though the lucky winners should get notified via email if they\u2019re selected next week. Note, you\u2019ll still need to cough up the $1,000 or $2,000 to own Nvidia\u2019s top-of-the-line Blackwell cards, but this may be the only way some PC gamers may hope to grab one at the base price.\n\nNvidia\u2019s program is similar to what other OEMs have landed on, specifically Zotac, with its Discord-based raffle to let users buy a card at MSRP rather than at a markup. Other AIM cards from Asus and MSI have increased in price by upwards of 18% since the RTX 5090 launch in January. The r/nvidia subreddit has continued to record 5090 prices jumping by $300 to $400. The MSRP RTX 5080 16G Ventus 3X from MSI went from $1,000 to $1,139. Meanwhile, MSI\u2019s RTX 5090 32G Vanguard SOC Launch Edition jumped by $310 to $2,689. Meanwhile, resellers pawn these same GPUs on eBay for over $4,000. Some OC models are running for close to or over $6,000.\n\nThe direct-to-consumer model shouldn\u2019t be the first way companies should think to beat scalpers. Because manufacturers can\u2019t control the wide breadth of online retailers, the best way to avoid price gouging is to have enough stock that scalpers can\u2019t purchase all of them. Nvidia was warning about low stock values even before launch, and physical retailers found they only had a paltry number of GPUs to sell to customers. When Nvidia launched the RTX 5090 on Jan. 30, online stock sold out in less than 30 minutes.\n\nFeb. 20 marks the launch of the Nvidia GeForce RTX 5070 Ti, a card we thought was solid at $750 for 4K gaming. Then again, you\u2019d be lucky to find it for $750. We\u2019re already seeing certain markets planning to sell non-OC cards for over $900 and some OC variants for over $1,000. At that price, you may as well wait to buy the RTX 5080 (as if it\u2019s even possible to purchase that card at MSRP). We\u2019ll update this story if the stock situation for that card is as dire as the card\u2019s more powerful siblings. Nvidia already pushed the launch of the RTX 5070 back to March 5.\n\nFor its part, Nvidia shared multiple links with Gizmodo for stock-clocked 5070 Ti cards at MSRP before the launch date. However, checking all those links now lists the cards as out of stock at the time of publication. This situation is not tenable, and even if Nvidia launches another priority access scheme for the 5070 family, it won\u2019t be enough to meet the overall demand. We don\u2019t suggest anybody buy a card for $600 to $3,000 above MSRP, no matter how many frames you think you\u2019ll get in Cyberpunk 2077.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Multiple Vulnerabilities Discovered in NVIDIA CUDA Toolkit",
            "link": "https://unit42.paloaltonetworks.com/nvidia-cuda-toolkit-vulnerabilities/",
            "snippet": "Unit 42 researchers detail nine vulnerabilities discovered in NVIDIA's CUDA-based toolkit. The affected utilities help analyze cubin (binary) files.",
            "score": 0.5558441281318665,
            "sentiment": null,
            "probability": null,
            "content": "Executive Summary\n\nThis article reviews nine vulnerabilities we recently discovered in two utilities called cuobjdump and nvdisasm, both from NVIDIA's Compute Unified Device Architecture (CUDA) Toolkit. We have coordinated with NVIDIA, and the company has released an update in February 2025 to address these issues.\n\nThe vulnerabilities are tracked as the following Common Vulnerabilities and Exposures (CVEs):\n\nIntroduced in 2006, CUDA is a parallel computing platform and programming model. As part of NVIDIA's CUDA Toolkit, developers use the cuobjdump and nvdisasm tools to analyze CUDA binary files used in programs to run on NVIDIA graphics processing unit (GPU) hardware.\n\nWhile these two tools don't directly execute CUDA code, they are essential for developers to inspect and optimize CUDA-based programs for NVIDIA GPUs. Successfully exploiting the associated vulnerabilities might lead to limited denial of service or limited information disclosure. Potential attackers could impact organizations through vulnerable versions of cuobjdump and nvdisasm in targeted developer environments.\n\nPalo Alto Networks customers are better protected from the potential impact of these vulnerabilities through our Next-Generation Firewall (NGFW) with Cloud-Delivered Security Services that include Advanced Threat Prevention.\n\nWe also recommend using the most recent CUDA Toolkit release to avoid vulnerable versions of cuobjdump and nvdisasm.\n\nThe Unit 42 Incident Response Team can also be engaged to help with a compromise or to provide a proactive assessment to lower your risk.\n\nRelated Unit 42 Topics Vulnerabilities\n\nNVIDIA CUDA Toolkit\n\nLaunched in 2006, CUDA is a parallel computing platform and programming model developed by NVIDIA. Developers use this platform to create software that harnesses the computing power of NVIDIA GPUs for various computing tasks that require significant parallel processing power. These tasks include artificial intelligence (AI), scientific research and multimedia processing.\n\nDevelopers use the CUDA Toolkit for a development environment to create these GPU-accelerated applications. The CUDA Toolkit can be used in Windows or Linux environments. In either operating system, the developed code is stored in CUDA binary files.\n\nCUDA Binary (Cubin) Files\n\nA CUDA binary is a type of executable file that stores CUDA code, including instructions designed for NVIDIA GPUs. CUDA binaries use a .cubin file extension in their file names, so we commonly refer to these as \"cubin\" files.\n\nCubin follows a standardized ELF format [PDF] found in Linux and Unix. Cubin files include sections for the actual executable code, alongside additional information like symbols, relocation data and debugging details for CUDA code to run on NVIDIA GPUs.\n\nA cubin file typically consists of code for both the host (CPU) and device (GPU) portions of a program. Cubin files are produced by compiling the source code written in CUDA C/C++.\n\nCubin files are easily identifiable through common utilities like the file command. Figure 1 shows the results from running a file command on a cubin file named normal.cubin in a terminal from a Linux environment. The results indicate it is a 64-bit ELF using the NVIDIA CUDA architecture.\n\nWe can further confirm that the cubin file used in Figure 1 follows the ELF format by using tools like 010 Editor. Figure 2 shows the contents of normal.cubin in 010 Editor running an ELF binary template (ELF.bt) to parse and interpret the structure of the binary. The results in the lower half of the image further confirm normal.cubin follows the ELF format.\n\nCuobjdump and Nvdisasm\n\nWe discovered vulnerabilities in two tools from the CUDA Toolkit used to inspect and analyze cubin files. These tools are command-line utilities named cuobjdump and nvdisasm. Before examining the associated vulnerabilities, we should understand how these two tools work.\n\nCuobjdump\n\nDevelopers use the CUDA Toolkit command-line utility cuobjdump to inspect and analyze cubin files. Output from cuobjdump presents cubin data in a human-readable format. This tool has several command-line options that developers can use to return information on different aspects of a cubin file.\n\nFor example, the --dump-elf option returns an information dump on a cubin file's ELF Object sections, which can give a general overview of a cubin file. Figure 3 displays the output of cuobjdump on a cubin file using the --dump-elf option.\n\nNvdisasm\n\nThe nvdisasm command-line tool is a disassembler for cubin files. Like cuobjdump, this tool takes content from a cubin file and converts it to a human-readable format. However, unlike cuobjdump, developers use nvdisasm to gain insight into the low-level operations of their code after it\u2019s been compiled but before it runs on the GPU.\n\nThis tool has several command-line options that focus on the functionality of a cubin file's CUDA code. These options can provide different aspects and levels of detail on the disassembled code.\n\nTo see the resulting disassembly without any attempt to beautify it, we can use the --print-raw option. Figure 4 shows the output of nvdisasm on a cubin file using the --print-raw option.\n\nThe cuobjdump tool works on both standalone cubin files (compiled CUDA binaries) and host binaries (executable files containing embedded CUDA code). In contrast, nvdisasm is more specialized, focusing solely on cubin files. However, nvdisasm offers more detailed and comprehensive output, making it a powerful tool for in-depth analysis. NVIDIA provides a comparison table that efficiently displays the differences between these two tools.\n\nA basic understanding of these two tools allows us to better understand the associated vulnerabilities we discovered.\n\nReview of the Vulnerabilities\n\nDuring a security evaluation of the NVIDIA CUDA Toolkit, we conducted an extensive fuzz test on cuobjdump and nvdisasm. We ran a file fuzzer on both applications for a month. The results revealed six vulnerabilities in cuobjdump and three vulnerabilities in nvdisasm.\n\nWe were able to successfully identify and trigger these vulnerabilities during our testing. To mitigate the risk of these vulnerabilities being weaponized, we will not publicly share specific details.\n\nUltimately, older versions of cuobjdump and nvdisasm could potentially be exploited by using these tools to analyze a maliciously manipulated cubin file.\n\nThe vulnerabilities we discovered in cuobjdump and nvdisasm are classified as two types:\n\nInteger overflow: Code in a vulnerable application processes an integer value that is too large to store in the intended location\n\nOut-of-bounds read: Code in a vulnerable application reads data past the end or before the beginning of an intended buffer\n\nSuccessfully exploiting these vulnerabilities could lead to:\n\nLimited denial of service\n\nLimited information disclosure\n\nThese vulnerabilities have been assigned Common Vulnerability Scoring System (CVSS) numbers ranging from 2.8 to 3.3 representing a Low level of impact.\n\nTable 1 shows the vulnerabilities we discovered in cuobjdump.\n\nCVE Designator Vulnerability Description CVSS Score CVE-2024-53870 Integer overflow vulnerability in cuobjdump . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when a user runs cuobjdump on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 3.3 CVE-2024-53872 Out-of-bounds read vulnerability in cuobjdump . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when a user runs cuobjdump on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 3.3 CVE-2024-53873 Integer overflow vulnerability in cuobjdump . By manipulating a cubin file, an attacker can potentially trigger a heap buffer overflow when a user runs cuobjdump on the file. A successful exploit of this vulnerability may lead to limited denial of service, code execution and limited information disclosure. 3.3 CVE-2024-53874 Out-of-bounds read vulnerability in cuobjdump . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when a user runs cuobjdump on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 3.3 CVE-2024-53875 Out-of-bounds read vulnerability in cuobjdump . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when a user runs cuobjdump on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 3.3 CVE-2024-53878 Out-of-bounds read vulnerability in cuobjdump . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when a user runs cuobjdump on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 2.8\n\nTable 1. Breakdown of vulnerabilities in cuobjdump.\n\nTable 2 shows the vulnerabilities we discovered in nvdisasm.\n\nCVE Designator Vulnerability Description CVSS Score CVE-2024-53871 Out-of-bounds read vulnerability in nvdisasm . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when a user runs nvdisasm on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 3.3 CVE-2024-53876 Out-of-bounds read vulnerability in nvdisasm . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when the user runs nvdisasm on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 3.3 CVE-2024-53877 Out-of-bounds read vulnerability in nvdisasm . By manipulating a cubin file, an attacker can potentially trigger an out-of-bounds read when the user runs nvdisasm on the file. A successful exploit of this vulnerability may lead to limited denial of service and limited information disclosure. 3.3\n\nTable 2. Breakdown of vulnerabilities in nvdisasm.\n\nConclusion\n\nNVIDIA's CUDA Toolkit is a fundamental component of the broader CUDA ecosystem, which supports the development, deployment and execution of CUDA programs.\n\nWhile cuobjdump and nvdisasm are not directly involved in executing CUDA code, they are essential for developers looking to inspect and optimize their GPU programs.\n\nVulnerabilities in tools like cuobjdump and nvdisasm have wider implications, because they are part of the CUDA Toolkit. Attackers could possibly target organizations if these vulnerabilities exist in their development environments. CUDA is widely used in security-sensitive applications in generative AI, machine learning and scientific computing. We recommend that developers use the most up-to-date version of this and any other development platform.\n\nNVIDIA released a security update to address these vulnerabilities in February 2025, so concerned parties can update to the latest version and avoid these vulnerabilities.\n\nPalo Alto Networks Protection and Mitigation\n\nPalo Alto Networks customers are better protected by our products like Next-Generation Firewall (NGFW) with Cloud-Delivered Security Services that include Advanced Threat Prevention.\n\nNGFW with an Advanced Threat Prevention subscription can identify and block the command injection traffic, when following best practices, via the following Threat Prevention signatures: 95847, 95848, 95849, 95850, 95852, 95853, 95854, 95855, 95856\n\nIf you think you may have been compromised or have an urgent matter, get in touch with the Unit 42 Incident Response team or call:\n\nNorth America: Toll Free: +1 (866) 486-4842 (866.4.UNIT42)\n\nUK: +44.20.3743.3660\n\nEurope and Middle East: +31.20.299.3130\n\nAsia: +65.6983.8730\n\nJapan: +81.50.1790.0200\n\nAustralia: +61.2.4062.7950\n\nIndia: 00080005045107\n\nDisclosure Timeline\n\nReport date: October 2024\n\nConfirmed date: Nov. 15, 2024\n\nCVEs assigned date: Jan. 7, 2025\n\nRelease date: Feb. 18, 2025\n\nAdditional Resources",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-18": {
        "0": {
            "title": "Nvidia Soars As Stargate Sparks More AI Spending Plans, Earnings Loom; Is Nvidia A Buy Now?",
            "link": "https://www.investors.com/research/nvda-stock-is-nvidia-a-buy-february-2025/",
            "snippet": "Is Nvidia Stock A Buy? Nvidia is well positioned to weather higher-for-longer interest rates in 2025 than some rivals, according to Kathleen Brooks, research...",
            "score": 0.7555555701255798,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) rose 3% early Friday after Foxconn, formally known as Hon Hai Precision Industry, projected higher artificial intelligence sales in its fourth-quarter results, potentially benefitting Nvidia stock. Foxconn, an Apple (AAPL) partner, expects first-quarter AI server revenue to double from both a year ago and from the prior quarter, signaling a strong demand for AI chips. But is Nvidia\u2026",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Stock Is Rising. It\u2019s Nearly Recovered From DeepSeek Rout.",
            "link": "https://www.barrons.com/articles/nvidia-stock-today-deepseek-ai-600c33e8",
            "snippet": "Nvidia stock rose early Tuesday as it moved within touching distance of recouping all its losses from the DeepSeek market rout.",
            "score": 0.9519587159156799,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Paul Singer's Elliott Management bets against Nvidia after calling it a 'bubble' and saying AI is 'overhyped'",
            "link": "https://www.businessinsider.com/nvidia-elliott-management-stock-short-bubble-ai-paul-singer-2025-2",
            "snippet": "Elliott Management had \"at least $600 million in downside exposure\" to Nvidia stock at the end of December, one analyst said.",
            "score": 0.926905632019043,
            "sentiment": null,
            "probability": null,
            "content": "Elliott Management bet against Nvidia using put options last quarter, filings show.\n\nPaul Singer's firm had \"at least $600 million in downside exposure\" to the chip maker, one analyst said.\n\nElliott told clients last year that Nvidia was in a \"bubble\" and AI was \"overhyped.\"\n\nElliott Management bet big against Nvidia after telling clients the chipmaker's stock was in a \"bubble\" and artificial intelligence was \"overhyped.\"\n\nBillionaire investor Paul Singer's firm bought put options last quarter on 1.45 million Nvidia shares with a notional value of about $195 million as of December 31, a Securities and Exchange Commission filing revealed on Friday.\n\nThe activist-investing specialist also owned puts worth a notional $1.1 billion on the Invesco QQQ ETF, which tracks the Nasdaq-100 index. Moreover, it held puts worth a notional $4.2 billion on the SPDR S&P 500 ETF Trust, which follows the S&P 500 index. Nvidia is the second-largest constituent of both indexes, after Apple.\n\nBetween the three short positions, Elliott had \"at least $600 million in downside exposure to Nvidia directly or indirectly\" at the end of December, Gerry Fowler, head of European equity strategy at UBS, told Business Insider.\n\nElliott, which manages roughly $70 billion of assets, appears to have \"specifically shorted Nvidia via put options\" and bet against the largest US companies more broadly to hedge its risk on long positions such as Southwest Airlines and Pinterest, Fowler said.\n\nHowever, Fowler emphasized the strike prices and maturities of the puts aren't disclosed, so the \"cost of this protection could be quite low or high \u2014 we just don't know.\"\n\nIn a client letter obtained by The Financial Times last year, Elliott's bosses said that Nvidia and other Big Tech stocks were in \"bubble land\" and questioned whether the massive demand for Nvidia's graphics chips would last.\n\nThey also predicted that some AI applications were destined to always cost too much, function poorly, consume too much energy, or betray users' trust.\n\nElliott has only disclosed a Nvidia position once before, according to SEC filings dating back to 2001. It owned 5,000 shares worth $4.5 million at the end of March 2024 but sold them within three months.\n\nNvidia stock is almost flat this year after soaring just over 100% in the past 12 months, leaving it worth $3.45 trillion \u2014 second only to Apple in value.\n\nElliott's US stock portfolio was worth about $9 billion at December's close, excluding options and convertible debt securities. Its top positions included $2 billion stakes in Triple Flag Precious Metals and Southwest Airlines, and a $1.8 billion position in Suncor Energy.\n\nIt's worth highlighting that Form 13Fs only provide a snapshot of a firm's US stock holdings on a single day, roughly six weeks before their release. They also exclude shares sold short, stakes in private companies, foreign-listed assets, and non-stock assets such as gold and real estate, meaning they don't always paint a full picture of an investor's strategy.\n\nElliott declined to comment.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Prediction: Nvidia Stock Is Going to Drop After Feb. 26",
            "link": "https://www.fool.com/investing/2025/02/18/prediction-nvidia-stock-is-going-to-drop-after-feb/",
            "snippet": "Expectations are high as Nvidia prepares to report earnings on Feb. 26. Heavy planned spending on AI infrastructure by tech giants should benefit Nvidia for...",
            "score": 0.972030520439148,
            "sentiment": null,
            "probability": null,
            "content": "Tech giants are dumping mountains of cash into artificial intelligence (AI) data centers. Microsoft plans to spend $80 billion this year to expand its AI capacity. Amazon is boosting its total capital spending, spread across its retail and cloud businesses, to $100 billion to accelerate its AI efforts. And Meta Platforms will pour $65 billion into its data centers to fuel its AI ambitions.\n\nOn the surface, this all sounds great for Nvidia (NVDA 5.27%) stock. The company dominates the market for powerful AI accelerators, and second place AMD has already put forth a disappointing forecast for its own AI chips sales this year. Nvidia is going to scoop up the lion's share of the spending that goes toward AI accelerators.\n\nTrees don't grow to the sky\n\nForecasts for AI accelerator sales paint a rosy picture for Nvidia. AMD, for example, once predicted that AI accelerators would generate $500 billion in industrywide revenue in 2028.\n\nHowever, investors need to ask an important follow-up question: How can that spending possibly be justified?\n\nIf companies are going to spend half-a-trillion dollars a year on AI accelerators, not to mention many billions more on other data center gear, those investments will need to pay off in the form of new sources of revenue or cost savings.\n\nIs that realistic? What will generate that new revenue?\n\nNvidia's valuation is built on optimism -- namely, optimism that its revenue and profit can continue to grow at blistering rates for years. The stock currently trades for more than 40 times expected earnings for fiscal 2025.\n\nThis is a company that's already worth more than $3 trillion and generated $20 billion in adjusted net income last quarter. The market for AI accelerators must continue to grow rapidly for Nvidia's stock price to make any sense.\n\nIt's not that investors are discounting the possibility that demand for AI accelerators flatlines -- it's that investors are discounting the possibility that demand collapses. What will happen to Nvidia stock if, after Microsoft, Amazon, and Meta hurl hundreds of billions of dollars into AI data centers, those companies fail to generate a reasonable return on investment?\n\nThe AI investment boom feels like a massive case of FOMO (fear of missing out). Tech giants are terrified of being left behind, so they're throwing caution to the wind.\n\nThere's genuinely an enormous amount of demand for AI computing capacity right now, but how much of that is experimentation? In other words, how much of that is companies trying out AI to see if it makes financial sense? When some of those experiments don't pan out, what will happen to demand?\n\nThen there's DeepSeek, the Chinese company that trained an AI model that can compete with the best AI models U.S. companies have to offer for a fraction of the price. If training top-tier AI models no longer requires megaclusters of AI accelerators, what will happen to demand?\n\nThis might be peak Nvidia\n\nI'm calling it: Peak Nvidia is almost here, and it's close to the point where the stories and predictions keeping the AI boom afloat start to fall apart. It's not that AI isn't an impressive and useful technology. Like the internet, it's revolutionary.\n\nBut also like the internet 25 years ago, it's creating expectations that appear untethered from reality. The internet changed the world but also ruined plenty of investors along the way.\n\nI could be very wrong about this. It's certainly possible that the hundreds of billions of dollars being spent on AI infrastructure will make financial sense in the end and demand for AI accelerators will continue to grow far into the future. Maybe Nvidia will put out a stellar forecast that drives the stock to new highs. Anything is possible.\n\nStill, I don't think I'd want to be an Nvidia shareholder on Feb. 26 when the company reports its latest results. Expectations are sky high, and any hint of trouble could send the stock plunging.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia Soars As Stargate Sparks More AI Spending Plans, Earnings Loom; Is Nvidia A Buy Now?",
            "link": "https://www.yahoo.com/news/m/dc591aa5-8656-399f-a2b0-90fb4c8c329e/nvidia-soars-as-stargate.html",
            "snippet": "Nvidia climbed Tuesday to start a holiday-shortened week. Further, Nvidia stock has cleared a key resistance level headed into earnings on Feb. 26.",
            "score": 0.7555555701255798,
            "sentiment": null,
            "probability": null,
            "content": "36,718 people played the daily Crossword recently. Can you solve it faster than others?\n\n36,718 people played the daily Crossword recently. Can you solve it faster than others?",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia slashes stake in emerging rival as AI arms rate heats up",
            "link": "https://www.thestreet.com/investing/nvidia-slashes-stake-in-emerging-rival-as-ai-arms-rate-heats-up",
            "snippet": "Nvidia has trimmed its nascent investment portfolio by around 30%, including a big change in a key holding of a former takeover target.",
            "score": 0.9596076607704163,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Analysts See Buying Opportunity in NVIDIA Before Earnings",
            "link": "https://www.marketbeat.com/originals/analysts-see-buying-opportunity-in-nvidia-before-earnings/",
            "snippet": "NVIDIA analysts say this stock is a buy ahead of earnings and forecast a solid double-digit upside that could be achieved this year.",
            "score": 0.514420747756958,
            "sentiment": null,
            "probability": null,
            "content": "Analysts say NVIDIA NASDAQ: NVDA is a buy ahead of earnings because the results will be robust, high-double-digit growth is likely, and the guidance will also be sound. The long-term outlook is also robust because the semiconductor company has been investing in its full stack of AI products and services. The company is leaning into an increasingly specialized group of products that will underpin AI growth and advancement across industries and verticals.\n\nNVIDIA MarketRank\u2122 Stock Analysis Overall MarketRank\u2122 98th Percentile Analyst Rating Moderate Buy Upside/Downside 41.0% Upside Short Interest Level Healthy Dividend Strength Weak Environmental Score -1.26 News Sentiment 0.92 Insider Trading Selling Shares Proj. Earnings Growth 43.68% See Full Analysis\n\nThe analyst activity leading into the release is mixed, including a couple of price target reductions, but overall, it is bullish and leading the market higher. The sentiment is firm at a Moderate Buy with a bullish bias due to the high number of Buy or higher ratings, about 85%, and the price target trend.\n\nGet NVIDIA alerts: Sign Up\n\nThe consensus price target reported by MarketBeat trended steadily higher in 2024 and set new highs in 2025, suggesting a 20% upside relative to mid-February price action and another 30% at the high end. The high-end target of $220 was first set by Rosenblatt Securities in early January and then matched by Tigress Financial late in the month. Tigress Financials' target is the most recent target on record, compounded by an upgrade to Strong Buy from Buy. Assuming NVIDIA provides another good report, these trends should continue.\n\nAnalysts from Evercore ISI added the stock to their Tactical Outperform list, citing the company\u2019s market-leading technology. In their view, NVIDIA is still five to ten years ahead of the competition and the #1 choice for hyperscalers such as Google NASDAQ: GOOGL, Amazon NASDAQ: AMZN, Microsoft NASDAQ: MSFT, Oracle NYSE: ORCL, and Meta Platforms NASDAQ: META. Evercore ISI rates NVIDIA at Outperform with a $190 price target, 13% above the consensus.\n\nNVIDIA Growth Slows in 2025: So What? It's Growing and Has Lots of Cash\n\nNVIDIA\u2019s growth is slowing in 2025 and is expected to continue slowing as the year progresses, but it doesn\u2019t matter. Trading over 40x earnings in 2025, it is valued at roughly 15x earnings compared to 2034, and the forecasts are likely low. The company has outperformed the consensus revenue estimates reported by MarketBeat 100% of the time since before 2020 and the earnings estimates since 2023. Revenue growth will be sustained at a high-double-digit pace in 2025 and 2026, then slow to a more sustainable low-teen CAGR in the future.\n\nCritical takeaways for 2025 are that FQ4 results, due in February, will be up 8% or more sequentially, 72% compared to the prior year, and more than 800% compared to 2020. Growth is expected to continue for at least another decade. Earnings are also central because the company has amassed significant earnings leverage and cash. At the end of FQ3, the balance sheet highlights a nearly 50% increase in cash and double-digit increases in receivables, inventory, and current and total assets. Total assets are up almost 50% and only partially offset by increased liability. Equity is up 50%, and leverage is low. The $38.5 billion is more than total liability, leaving the company in a net cash position.\n\nThe net cash position and balance are critical because of the potential for capital returns. The company already pays dividends and buys shares but in token amounts. The balance sheet can sustain more aggressive capital returns because of the cash flow and growth outlook and may be announced in 2025. Until then, the dividend is worth less than 0.10% in yield but growing at a high-double-digit CAGR, and repurchases reduced the share count by 0.4% in the first three months of the fiscal year.\n\nNVIDIA Rebounds, New Highs Are in Sight\n\nNVIDIA\u2019s share price hit a four-month low in late January following the Deepseek news, but the rebound is already underway. The stock is up 20% since hitting the low, and the market is set up to move higher. Overbought conditions in the MACD and stochastic have been relieved, showing early signs of a trend-following signal. The critical resistance is at the current all-time high, which will likely be tested before or concurrent with the earnings report. If the market moves to new highs, it will likely continue advancing by $35 to $50 to align with the high-end range of analysts' targets.\n\nBefore you consider NVIDIA, you'll want to hear this.\n\nMarketBeat keeps track of Wall Street's top-rated and best performing research analysts and the stocks they recommend to their clients on a daily basis. MarketBeat has identified the five stocks that top analysts are quietly whispering to their clients to buy now before the broader market catches on... and NVIDIA wasn't on the list.\n\nWhile NVIDIA currently has a Moderate Buy rating among analysts, top-rated analysts believe these five stocks are better buys.\n\nView The Five Stocks Here",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Nvidia RTX 5070 Ti Is 17% Faster Than 4070 Ti Super In Leaked 3DMark Results",
            "link": "https://www.extremetech.com/gaming/nvidia-rtx-5070-ti-is-17-faster-than-4070-ti-super-in-leaked-3dmark-results",
            "snippet": "The Nvidia RTX 5070 Ti is the next new-generation graphics card to hit store shelves, and though we only expect it to be there for a moment before going out...",
            "score": 0.7174397110939026,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Nvidia to deliver 'modest' earnings beat, but outlook 'could face headwinds': BofA",
            "link": "https://www.investing.com/news/stock-market-news/nvidia-to-deliver-modest-earnings-beat-but-outlook-could-face-headwinds-bofa-3874240",
            "snippet": "Investing.com -- Bank of America expects Nvidia (NASDAQ:NVDA) to modestly exceed earnings expectations when it reports fiscal fourth-quarter results on...",
            "score": 0.9095778465270996,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Super Micro surges on progress hitching its wagon to Nvidia\u2019s rocket ship",
            "link": "https://sherwood.news/markets/super-micro-surges-on-progress-hitching-its-wagon-to-nvidias-rocket-ship/",
            "snippet": "The mass proliferation of Nvidia's Blackwell chip and Super Micro's server solutions go hand in hand....",
            "score": 0.8830344080924988,
            "sentiment": null,
            "probability": null,
            "content": "The ramp higher in shares of Super Micro Computer continues as traders continue to hope that a rapidly approaching hurdle will be cleared, allowing for rapid growth in revenues as the AI data center boom displays staying power. The company is up double digits as of 11:10 a.m. ET to lead all S&P 500 constituents.\n\nBy all accounts, demand for Nvidia\u2019s relatively new Blackwell GPU continues to exceed supply. Super Micro is aiming to hitch its wagon to this chip star by creating server infrastructure to utilize these Blackwell chips in a data center environment. In early February, Super Micro said that its server infrastructure to support these advanced semiconductors had reached full production availability.\n\nAll the while, Super Micro\u2019s management has yet to file the necessary reports with the Nasdaq to avoid delisting, with a due date of February 25. Its business update pointed to a relatively sluggish outlook through July, but with guidance for a boom in revenue growth thereafter. For its fiscal 2026 (July 2025 through June 2026), management is targeting revenues of $40 billion, up from about $24.25 billion for the 12 months prior.\n\nIf delays in rolling out its infrastructure for Blackwell, rather than the accounting issues swirling around the company, have been the proximate cause for its recently underwhelming sales figures and lackluster near-term forecasts, then the company\u2019s sales outlook may soon be at an all-systems-go inflection point (pardon the pun). After all, through all of Super Micro\u2019s struggles, Nvidia CEO Jensen Huang kept referring to the server company as one of the chip designer\u2019s \u201cgreat partners.\u201d\n\n\u201cWe believe delays in Blackwell availability drove much of its $3-$5 billion cut in its 2025 sales view, and it should recover much of that in 2026,\u201d Bloomberg Intelligence analyst Woo Jin Ho wrote. \u201cPrior to the company's filing challenges, consensus was $34 billion in 2026 sales. Assuming its pipeline of deals stayed intact, baking in the deferred 2025 work implies $37-$39 billion for 2026.\u201d\n\nThe consensus forecast for 2026 revenues among analysts polled by Bloomberg currently stands at about $33 billion, though a couple of these estimates are fairly stale.\n\nThe stock is on track for its second-best month on record.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-02-17": {
        "0": {
            "title": "Is Intel about to challenge Nvidia and AMD with its next-gen Celestial GPU?",
            "link": "https://www.dlcompare.com/gaming-news/is-intel-about-to-challenge-nvidia-and-amd-with-its-next-gen-celestial-gpu-50915",
            "snippet": "Intel may soon launch its new Celestial GPU to take on the two giants of the chip industry.",
            "score": 0.866815447807312,
            "sentiment": null,
            "probability": null,
            "content": "Although Intel has just introduced its mid-range Battlemage GPU lineup\u2014featuring the Arc B580 and B570, which have received positive reviews\u2014the company does not seem to be stopping there. Recent rumors suggest that the \"blue team\" is quietly developing the next-generation Celestial GPU, which promises to be a formidable rival to Nvidia and AMD.\n\nLeaked sources indicate that the Arc Celestial will be built on the brand-new Xe3P architecture, rather than the previous Xe3. Notably, Intel may manufacture Celestial chips in-house using its internal INTC process instead of relying on TSMC, as it has in the past. This shift would allow Intel to have greater control over chip quality and supply while reducing dependence on external partners.\n\nThe Xe3P architecture is expected to deliver superior performance and improved power efficiency compared to previous generations. Additionally, new technologies such as AI-powered enhancements, advanced data processing, and frame generation could be integrated, unlocking significant potential for Intel\u2019s next-gen graphics cards.\n\nCurrently, Nvidia continues to dominate the GPU market, while AMD is working to close the gap. Although Intel's mid-range Battlemage GPUs have provided consumers with more choices, they have yet to pose a serious threat to the two industry giants.\n\nCould Celestial be Intel\u2019s secret weapon to dethrone Nvidia? While it's too early to say for sure, these rumors have already sparked excitement in the tech community. A more intense GPU market competition will undoubtedly benefit consumers with better products and more competitive pricing.\n\nAnd as always, if you are looking to elevate your next gaming experience, check out our comparator for the best offer on the Intel Core Ultra 7 200 CPU today.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Amazon, Microsoft, Alphabet, and Meta Just Gave Nvidia Great News. But Does That Make The Stock A Buy Before Feb. 26?",
            "link": "https://www.fool.com/investing/2025/02/17/amazon-microsoft-alphabet-and-meta-just-gave-nvidi/",
            "snippet": "Nvidia's \"Magnificent Seven\" peers are doubling down on their AI infrastructure spending this year.",
            "score": 0.9101428985595703,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's \"Magnificent Seven\" peers are doubling down on their AI infrastructure spending this year.\n\nThe stock market appears to be in the middle of quite the storm as of late. In the final days of January, a Chinese start-up called DeepSeek sent shock waves around the world after it released an artificial intelligence (AI) model similar to ChatGPT -- and claims to have trained its AI with legacy architectures that aren't widely leveraged by U.S. developers.\n\nWhile technology stocks in general have been more volatile since DeepSeek's arrival, none has taken a hit as hard as Nvidia (NVDA 5.27%). Investors are panicking that Nvidia's newer chipware may not be as much of a necessity, given DeepSeek's claims.\n\nAlthough the uncertainty surrounding Nvidia's future prospects is disorienting, some of the company's biggest partners have been dropping clues as to how DeepSeek is influencing their own AI roadmaps. I'll analyze what these trends are and assess how they might impact Nvidia.\n\nExamining big tech's AI infrastructure plans\n\nNvidia makes its fortune primarily through its compute and networking business, which sells advanced chipsets called graphics processing units (GPUs). GPUs are integral for generative AI applications, and are primarily housed in large data centers.\n\nWhile DeepSeek claims to have built its AI using Nvidia's old H800 GPUs, it's been difficult to verify how accurate that is. Nevertheless, all of Nvidia's \"Magnificent Seven\" peers have reported earnings for the full calendar year 2024, and there is a common thread stitching their broader AI fabric together.\n\nMicrosoft kicked things off by announcing the company will be spending $80 billion this year on data centers and other AI infrastructure projects. Meta Platforms followed up with its own aggressive spending plans -- hinting that the company could see capital expenditures (capex) up to $65 billion this year. Lastly, Alphabet and Amazon are projected to spend an estimated $180 billion combined on AI capex this year.\n\nOn the surface, big tech's rising capex plans this year should be seen as a positive thing for Nvidia. However, when Nvidia reports earnings on Feb. 26, there is one thing in particular I think investors need to be laser-focused on.\n\nWhat should investors be looking for when Nvidia reports earnings?\n\nInvestors should listen carefully to management's commentary as it relates to the spending from big tech I outlined. While Nvidia isn't going to capture all of this spend, investors should still be able to get a glimpse into how much capex the Magnificent Seven will be allocating toward Nvidia.\n\nIn my eyes, the best way to figure this out is to look at Nvidia's financial guidance. If the company's growth rates are accelerating, I'd say this is a good proxy that big tech will be spending heavily on Nvidia's newest chip architectures this year. But on the flip side, if Nvidia's guidance calls for a material deceleration in growth, it could be that its biggest customers are still buying from Nvidia, but doing so in a more protracted way.\n\nIs Nvidia stock a buy before Feb. 26?\n\nIt's rare that I encourage timing an investment. As long-term investors, buying stock on a specific day isn't the most important factor. Rather, it's more important to reassess your conviction in your holdings, and so long as you remain optimistic, using a strategy of dollar-cost averaging over the course of many years is generally a recipe for success.\n\nThis is a rare occasion where I think buying the dip in Nvidia prior to earnings could be a good formula.\n\nConsidering history suggests that Nvidia stock will indeed rise following its fourth-quarter earnings, combined with big tech's capex plans just for this year, and the share price recovery from the DeepSeek sell-off pictured, I'm cautiously optimistic that Nvidia stock is a lucrative opportunity right now.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia or Alphabet: Billionaire Ken Griffin Bets Big on One Top AI Stock",
            "link": "https://www.tipranks.com/news/nvidia-or-alphabet-billionaire-ken-griffin-bets-big-on-one-top-ai-stock",
            "snippet": "AI mania has taken the world by storm, with the game-changing tech seemingly poised to permeate almost every aspect of our day-to-day lives over the next...",
            "score": 0.8800894021987915,
            "sentiment": null,
            "probability": null,
            "content": "AI mania has taken the world by storm, with the game-changing tech seemingly poised to permeate almost every aspect of our day-to-day lives over the next few years. But is it really as inevitable as it seems?\n\nWhile one fear tied to the rise of AI revolves around concerns it will make many jobs obsolete, Billionaire Ken Griffin is doubtful AI will soon replace human jobs, citing drawbacks in machine learning models when applied to certain situations.\n\n\u201cMachine learning models do not do well in a world where regimes shift,\u201d noted the Citadel founder and CEO. Illustrating his point, he added, \u201cSelf-driving cars don\u2019t work very well in the North due to snow. When the terrain changes, they have no idea what to do. Machine learning models do much better when there\u2019s consistency.\u201d\n\nGriffin is no luddite, however, his hedge fund and electronic market-making firm have been leaders in automation, and his skeptical stance does not mean he has been entirely shying away from the AI opportunity.\n\nLately, the billionaire has been fine-tuning his AI stock portfolio, shuffling his stakes in industry giants Nvidia (NASDAQ:NVDA) and Alphabet (NASDAQ:GOOGL). He\u2019s made a big bet on one while trimming his holdings in the other.\n\nSo, let\u2019s take a closer look at these two tech colossuses and by delving into the TipRanks database, we can also find out the broader Street view on these names.\n\nNvidia\n\nTalk of AI and its impact on the stock market and the conversation will naturally turn to Nvidia. While the chip giant was far from a minnow when AI first started making its impact following the release of ChatGPT, Nvidia\u2019s positioning in the space has turned it into one of the world\u2019s most valuable companies.\n\nThe reason for that is pretty straightforward; the Jensen Huang-led firm simply makes the best AI chips out there and no other company has so far managed to seriously impact its dominant position. The proof is in the chip pudding: Nvidia commands more than 80% of the AI chip market.\n\nBut are cracks starting to appear in the tremendous growth story? One recent headwind has revolved around the delay of its latest GPU architecture, Blackwell, but the company might be facing a threat that seemingly came out of nowhere last month. Chinese startup DeepSeek released its latest AI model with claims it can outperform its US peers. Not only that, it also said it was developed at a fraction of the price, meaning that maybe companies didn\u2019t need to spend enormous sums of money developing their AI models.\n\nWhat followed was a slice of history with Nvidia losing the biggest chunk of market cap over a single session, shedding $590 billion of its value in one day.\n\nMaybe Griffin sensed a day of reckoning for the high-flying stock was about to play out. During 4Q24, he sold 57% (3,993,858 shares) of his NVDA holdings. It should be noted that he still holds 3,128,677 shares, currently worth more than $434 million.\n\nAccording to D.A. Davidson analyst Gil Luria, it might be time to approach the Nvidia story with caution. Looking at the situation following DeepSeek\u2019s noisy entry, he thinks Nvidia might have to face a new reality moving forward.\n\n\u201cWe believe we are approaching peak demand, possibly as early as this year as NVDA\u2019s large customers evaluate [the] recent developments,\u201d the 5-star analyst said. \u201cThe notion that radically lower prices will led to much higher demand is almost a truism, but in previous instances took time to play out. We believe that lower inference prices will allow enterprises to roll out some new tools faster than they were otherwise planning to, but do not believe there are enough compelling applications for that to increase short-term demand by the 20-30x factor required to make up for the lower need for compute. We see the availability of ready-for-primetime scalable applications as the real gating factor and that may take 2-3 years to materialize. More importantly, as it relates to NVDA, if these new applications get deployed on device, the increased demand may not increase the TAM.\u201d\n\nTo this end, Luria rates NVDA shares as Neutral, while his $135 price target suggests the stock will remain rangebound for the time being. (To watch Luria\u2019s track record, click here)\n\nThat said, most on the Street are still getting behind the AI chip leader; based on 37 Buys vs. 3 Holds, the stock claims a Strong Buy consensus rating. Going by the $179.03 average price target, a year from now, shares will be changing hands for a ~29% premium. (See Nvidia stock forecast)\n\nAlphabet\n\nWhile Nvidia is a company that so far has benefited immensely from AI\u2019s rise, the tech has been seen as something of an issue for Alphabet \u2013 parent company of Google, YouTube, Android, Waymo, DeepMind, and other subsidiaries.\n\nDespite actually being an AI pioneer, having been using the tech long before the current AI boom, the rise of GenAI is perceived as a real threat to Google\u2019s core Search business. It\u2019s a narrative that makes sense in a way. AI-powered chatbots like ChatGPT, Claude, and Perplexity are changing how people seek information. Instead of googling something and clicking on links, users can now get information without leaving the platform. This reduces Google\u2019s main bread-and butter \u2013 ad revenue, which heavily relies on search traffic. There\u2019s also direct competition from other AI-powered search engines, with Microsoft, for example, integrating AI into Bing.\n\nAlphabet, for its part, has no intention of getting left behind, and has been aggressively expanding its AI capabilities to counter the threat. It has integrated Gemini AI (formerly Bard) into Search and other products and is competing with Microsoft and AWS by offering AI-powered tools for businesses and cloud computing.\n\nThat said, the shares fell following the recent Q4 readout with Cloud growth being a particularly sore point. Google Cloud\u2019s revenue fell short of expectations, coming at $11.96 billion vs. the Street\u2019s forecast of $12.19 billion. Additionally, the company forecasted $75 billion in capital expenditures for 2025, far above the consensus estimate of $58.8 billion.\n\nGriffin, however, must think all these worries are overblown. During Q4 he significantly increased his GOOGL holdings by 135% with the purchase of 786,737 shares. His total stake now stands at 1,372,475 shares, worth more than $254 million at the current share price.\n\nMirroring Griffin\u2019s move, RBC\u2019s Brad Erickson, an analyst ranked in the top 2% of Street stock experts, strikes a confident tone regarding the search giant\u2019s prospects. Following the Q4 results, he wrote, \u201cThe company\u2019s AI rollout is accelerating with Search demonstrating improvements as opposed to disruption (consistent with our checks coming in), capacity constraints which likely explain the Cloud miss should ease, Google I/O sets up for a bevy of new AI product announcements and management just lowered the bar on margins given Capex & headcount commentary. No doubt the impact of AI for GOOGL will continue to be hotly debated but intellectual consistency across the space would suggest GOOGL\u2019s playing offense on AI & we\u2019d agree.\u201d\n\n\u201cWe\u2019d expect a number of substantive announcements at I/O in early May, however, given the clear breakneck pace of model rollouts in fueling the various products (7 products or platforms now with over 2B users which leverage Gemini), we see the narrative around GOOGL\u2019s overall AI positioning as improving over the medium term versus getting worse,\u201d the 5-star analyst further said.\n\nConveying his confidence, Erickson rates GOOGL shares as Outperform (i.e., Buy) while his $235 price target makes room for 12-month returns of 27%. (To watch Erickson\u2019s track record, click here)\n\n26 other analysts join Erickson in the bull camp and with the addition of 10 Holds, the consensus view is that this stock is a Moderate Buy. The average target stands at $215.85, suggesting the shares will climb 16.5% higher in the months ahead. (See GOOGL stock forecast)\n\nTo find good ideas for AI stocks trading at attractive valuations, visit TipRanks\u2019 Best Stocks to Buy, a tool that unites all of TipRanks\u2019 equity insights.\n\nDisclaimer: The opinions expressed in this article are solely those of the featured analysts. The content is intended to be used for informational purposes only. It is very important to do your own analysis before making any investment.\n\nQuestions or Comments about the article? Write to editor@tipranks.com",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Small investors bought the dip in Nvidia by a record amount Monday",
            "link": "https://www.aol.com/small-investors-bought-dip-nvidia-184323913.html",
            "snippet": "Retail investors rushed into Nvidia on Monday, signaling Main Street support for the chipmaker despite the emergence of an artificial intelligence model...",
            "score": 0.7489387392997742,
            "sentiment": null,
            "probability": null,
            "content": "Everyday traders bought more than $562 million worth of Nvidia shares on balance Monday.\n\nRetail investors rushed into Nvidia on Monday, signaling Main Street support for the chipmaker despite the emergence of an artificial intelligence model from China that battered its shares and caused a historic, $600 billion loss in market value.\n\nEveryday traders bought more than $562 million worth of Nvidia shares on balance Monday, according to data from Vanda Research that subtracts total outflows from inflows. That marked a record for daily net inflows into Nvidia as mom-and-pop investors bucked their institutional counterparts, who dumped the stock en masse.\n\nThe buy-in from individuals came as Nvidia suffered its biggest one-day loss, tumbling around 17%, since the onset of the Covid pandemic in March 2020.\n\nMonday\u2019s plunge came in the wake of news that an AI model from Chinese startup DeepSeek scored high performance marks more cheaply and in far less time than Western counterparts.\n\nThe development raised doubts about the U.S. strategy of spending huge sums on AI and the data centers they require, just as President Donald Trump last week announced a multi-billion dollar AI project called Stargate. The sudden rise of DeepSeek also rang alarm bells that America may not lead in AI technology, offering chilling reminders of what some described as a \u201cSputnik moment\u201d at the dawn of the Space Race.\n\nNvidia told CNBC on Monday that DeepSeek\u2019s model was an \u201cexcellent AI advancement.\u201d DeepSeek\u2019s offering reportedly outperformed the best models of OpenAI and other U.S. competitors, further stoking concerns about the status of the U.S. in AI.\n\nFor their part, however, individual investors were unfazed. Data from Vanda shows the chipmaker was the most-purchased security by average investors on net in 2024 \u2014 surpassing even the SPDR S&P 500 ETF Trust (SPY), which tracks the S&P 500.\n\nOne of the buyers on Monday was Nirav Patel. The technology manager said he spent hours testing DeepSeek\u2019s model and concluded that, while development costs have come down, more chips will be needed to handle the increase in demand that should accompany growing affordability.\n\n\u201cIn my opinion, you will see much higher adoption of reasoning AI models,\u201d Patel said. \u201cWith adoption, you need more compute, and so you\u2019ll need more Nvidia chips basically.\u201d\n\nThe show of support from small-scale traders is the latest example of retail investors diverging from monolithic Wall Street, as happened during the meme stock craze that captivated U.S. markets during the pandemic. The difference now being that individuals can\u2019t swing the price of Nvidia, with a market value Tuesday near $3 trillion, the way they could small-cap stocks such as video game retailer GameStop or movie theater chain AMC four years ago.\n\nDespite the contrast in size, there were similar overtones on Monday, however. Nvidia was the most-mentioned stock on the popular WallStreetBets Reddit forum over the past 24 hours, with mentions surging more than 175% as its shares plunged, according to Quiver Quantitative data as of Tuesday morning.\n\nOne Reddit user posted a photo of their Nvidia position on the WallStreetBets forum with the title \u201cin Huang we trust,\u201d a reference to Nvidia CEO Jensen Huang. Another said Monday\u2019s moves were a \u201cclassic overreaction\u201d and \u201cmissed the bigger picture.\u201d\n\nMore from CNBC:",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia GeForce RTX 5070 Ti graphics cards launch next week \u2014 will you be able to get one?",
            "link": "https://www.msn.com/en-us/lifestyle/shopping/nvidia-geforce-rtx-5070-ti-graphics-cards-launch-next-week-will-you-be-able-to-get-one/ar-AA1z0pYo",
            "snippet": "Nvidia officially announced that the affordable RTX 5070 Ti graphics cards are coming in a week.",
            "score": 0.8556773662567139,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Startup Positron Takes On Nvidia With FPGAs",
            "link": "https://www.eetimes.com/startup-positron-takes-on-nvidia-with-fpgas/",
            "snippet": "AI systems startup Positron is working on its own AI inference chip, selling FPGA-based appliances in the meantime.",
            "score": 0.9190098643302917,
            "sentiment": null,
            "probability": null,
            "content": "//php echo do_shortcode('[responsivevoice_button voice=\"US English Male\" buttontext=\"Listen to Post\"]') ?>\n\nData center AI systems startup Positron, just 18 months old, has been shipping its FPGA-based LLM inference systems to customers since last summer, and recently delivered the first systems in a multi-million-dollar order to its Tier 2 CSP customer, Positron CTO and co-founder Thomas Sohmers told EE Times.\n\n\u201cIt\u2019s been a really great start to the year and we expect that customer and a number of others to be scaling significantly in the first half of this year,\u201d Sohmers said.\n\nA further 20 potential customers are currently evaluating Positron\u2019s FPGA-based AI appliance, Atlas, either directly or remotely, Sohmers said. Positron customers include enterprises running on-prem or co-located infrastructure, and Tier 2 CSPs.\n\n\u201cMost of the conversations we\u2019ve been having, especially about larger scale [deployments], have been with companies that either are themselves CSPs or companies providing scaled web services,\u201d Sohmers added.\n\nPartner Content View All\n\nBy GLOBAL UNICHIP CORP. (GUC) 03.13.2025 By GigaDevice 03.11.2025 By Menta 03.06.2025\n\nPositron\u2019s team ships the company\u2019s first AI inference appliances earlier in 2024 (Source: Positron)\n\nSeed funding\n\nPositron was founded in April 2023 by Sohmers and chief scientist Edward Kmett. Both had previously been working at AI inference startup Groq. The company appointed a new CEO, Mitesh Agrawal, who joined from AI CSP Lambda earlier this month, and raised $23.5 million in funding.\n\nPositron co-founder and CTO Thomas Sohmers (left) with new CEO Mitesh Agrawal (Source: Positron)\n\n\u201cWhen we founded Positron, it was focused on the fact that only two things matter\u2014having a completely seamless experience going from Nvidia-based systems\u2026and the failure point we saw for so many AI chip startups is they just took way too long and way too much to get to market,\u201d Sohmers said, noting that while the company is working on its own AI inference accelerator ASIC, its first- and second-generation Atlas systems are FPGA-based.\n\nFPGAs cannot offer the FLOPS of GPUs or ASIC solutions, but they have other benefits, he said.\n\n\u201cWe don\u2019t want to spend the massive amount of time and money on building an ASIC until we are absolutely sure we have product-market fit,\u201d Sohmers said. \u201cWhile other AI chip companies have their own unique problems, they all have problems with product-market fit, especially with first-generation devices. Going with FPGAs enabled us to do very rapid iteration, and start that iteration with customers [on board].\u201d\n\nCustomer dollars coming in are the best indicator of product-market fit, he added.\n\nSohmers\u2019 pre-Groq experience at AI CSP Lambda highlighted the need for rapid iteration cycles and for constant dialog with customers. Not all AI chip companies appreciate this, he said.\n\n\u201cThe most important thing is making sure you\u2019re building the right thing, and constantly updating your assumptions about that,\u201d Sohmers said.\n\nWhile Positron does have a customer purchasing its PCIe cards by the thousands, Atlas\u2019 appliance form factor better suits most cloud companies who are used to buying Nvidia boxes, plugging them in and renting them out.\n\n\u201cWe believe this appliance model\u2014just tokens in, tokens out, a black box\u2014is the easiest way for customers to purchase our hardware and get all the benefits of owning it from a capex and opex perspective,\u201d Sohmers said. \u201cBut it also makes it easy for those customers to replace or augment their existing Nvidia-based systems.\u201d\n\nMemory bandwidth utilization\n\nPositron\u2019s Atlas LLM inference appliance offers 70% faster performance (tokens per second) versus the same inference workload on Nvidia Hopper-based systems, at 3.5\u00d7 the performance per Watt and 3.5\u00d7 the performance per dollar, based on Positron\u2019s current software release. It is based on Altera\u2019s Agilex-7M FPGAs, which currently only Positron is authorized to ship, given that the parts are not yet at general availability, Sohmers said. These FPGAs come with 32GB HBM.\n\nThe current gen of Atlas is a 4U system using four FPGAs on PCIe cards. It is designed to be a turnkey appliance, ingesting binaries from HuggingFace or customers\u2019 proprietary models, in a zero-step process (no need to re-compile).\n\n\u201cA one-step process was a step too many,\u201d Sohmers said. \u201cThere is nothing new or unique that needs to be done to run models on Positron.\u201d\n\nA next-generation platform will use Positron\u2019s custom module form factor (analogous to Nvidia SXM) to shrink a four-FPGA system into 2U, with significantly expanded DDR memory. That system is due later this year and Sohmers said it is projected to offer 5\u00d7 the performance of Nvidia Blackwell. The big jump in performance from Positron\u2019s first gen is down to forthcoming software optimizations and FPGA optimizations, which can unlock more performance (including shifting more operations from host CPU onto the FPGA), though the first-gen version of Atlas will also get these updates as they become available.\n\nSo, how does Positron get better performance from hardware with fewer FLOPs and minimal memory? Sohmers explained that while CNNs are compute-bound, transformers are memory-bound, both in terms of memory bandwidth and memory capacity. GPU-based inference solutions have been shown to use less than 30% of their theoretical peak memory bandwidth for transformer inference. The Altera Agilex-7M is the only FPGA with both HBM and DDR5 memory, and while its compute FLOPS may be limited, memory bandwidth is what matters, Sohmers said.\n\n\u201cYou may be paying for a very expensive memory and very high theoretical memory bandwidth [with GPUs], but fundamentally due to GPU architectures, you\u2019re not able to achieve anywhere close to that memory bandwidth,\u201d he said. \u201cOur design implemented on the FPGA is actually achieving and sustaining 93% of its theoretical memory bandwidth, across all use cases.\u201d\n\nThe remaining 7% of performance cannot be obtained due to the inability to control HBM\u2019s refresh cycles, he added.\n\nHow the company gets this memory bandwidth utilization is Positron\u2019s key IP; Sohmers said the company works at lower levels than Altera\u2019s Quartus tools allow to maximize the density of its matmul array and the memory interconnect that feeds it. Positron was achieving 65-70% of theoretical peak memory bandwidth with its initial prototypes based on previous-generation HBM-equipped Stratix devices. But upgrading to Agilex meant the team could take advantage of Altera\u2019s new hardened Fabric NoC, which is designed to support fast transfer between the FPGA\u2019s memories, rather than relying on channels that are also used for the rest of the chip\u2019s programmable logic resources. The new NoC has dedicated pathways from the HBM to SRAM blocks anywhere in the programmable logic array.\n\n\u201cSince this is a new feature, we worked very closely with the Altera team to make sure we could actually utilize it to its maximum potential,\u201d Sohmers said. \u201cThere was a lot of new thinking required within our linear algebra systolic array design to make sure we could keep up at the reprogrammable clock rate, to make sure there was that one-to-one balance between FLOPS and memory.\u201d\n\nThe Agilex-7M has four channels of DDR5, as well as 32GB HBM 2e that Positron uses as separate memories (not a tiered cache system) combined with some \u201cfancy tricks\u201d in SRAM. HBM is used where high performance is required\u2014in this case, storing model weights. DDR is used to store user context, KV cache and different models to be swapped in (like different LoRA fine tunings, which can be applied to different users within a batch, Sohmers said). Up to 512GB of additional DDR5 can be attached.\n\nHardware roadmap\n\nGiven the FPGA\u2019s reprogrammability, could Positron specialize its appliances further for certain applications within inference to gain extra performance?\n\nSohmers said the company did not want to risk ending up in a niche (beyond LLM inference), but that as the company expands its team beyond its 15 current employees following its new funding, \u201cthere are definitely areas we could optimize and potentially have that available to customers in the same physical appliance.\u201d\n\nPositron is also working on an ASIC version of its FPGA-based design. This ASIC will use LPDDR only (no HBM).\n\n\u201cWith LPDDR 5X and 6, we\u2019re able to get massively higher capacity than HBM at a quarter of the cost per Gigabyte,\u201d Sohmers said. \u201cThe packaging will be a regular organic substrate, and that drastically reduces the cost of the product.\u201d\n\nWhile LPDDR is not as fast as HBM, using Positron\u2019s IP to get close to the theoretical peak memory bandwidth more than makes up for it, he said. Positron can also directly control the memory refreshes on DDR, which enables the company to get even closer to theoretical peak performance than it could with HBM, without the power or cost overheads HBM has.\n\nSohmers also hinted that the new ASIC will be set up for networking; Agilex FPGAs each have three 400 Gbps networking transceivers, unlike GPUs which require additional NICs. Positron can connect 256 FPGAs point-to-point without additional switches, he said.\n\nU.S. fabrication of chips and systems for AI infrastructure may hold some advantages (given U.S. vice president JD Vance\u2019s comments at the AI Summit in Paris last week that \u201cthe Trump administration will ensure that the most powerful AI systems are built in the U.S. with American designed and manufactured chips\u201d).\n\nAgilex-7M FPGAs are fabbed at IFS in Chandler, Arizona. Positron is aiming for U.S. fabrication of its ASIC on TSMC N5 in Tuscon, Arizona, while LPDDR vendors are expanding U.S. fabrication, Sohmers said. PCIe cards will be assembled in Camarillo, Calif., with final assembly and test currently in Spokane, Washington, but planned to move to Reno, Nevada, in the near future.\n\nPositron is aiming to sample its ASIC in Q1 2026.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia stock falls after notching record high ahead of CES AI superchip debut [Video]",
            "link": "https://www.aol.com/finance/nvidia-stock-rallies-ceo-jensen-133746340.html",
            "snippet": "Nvidia stock (NVDA) fell Tuesday, reversing direction after a rally fueled by CEO Jensen Huang's keynote at the tech industry's annual CES trade show in Las...",
            "score": 0.9508345723152161,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock (NVDA) tumbled more than 6% Tuesday, a day after shares closed at a record high in anticipation of CEO Jensen Huang\u2019s keynote at the tech industry\u2019s annual CES trade show in Las Vegas.\n\nHuang's presentation on Monday night gave a flurry of updates on upcoming Nvidia products, previewing what\u2019s next in the burgeoning artificial intelligence market and for other emerging technologies. Shares jumped as much as 2.5% early Tuesday before reversing direction. The chip maker was the Dow's worst performer of the session.\n\nNvidia stock's decline comes amid a broader slump in stocks Tuesday after November job openings data came in mixed and separate economic data stoked inflation concerns.\n\nNvidia shares are still up roughly 190% from last year. Its updates at CES fueled more bullish takes on the stock.\n\nAnalysts at Stifel, Wedbush, and Truist Securities on Tuesday reiterated their Buy ratings on the stock. On average, Wall Street analysts tracked by Yahoo Finance see Nvidia shares rising to $172.80 over the next 12 months.\n\n\u201c[T]he company continues to position itself more favorably \u2014 not just in the datacenter but increasingly at all areas of the edge \u2014 from client compute to autonomous vehicles to robotics \u2014 supporting revenue growth and our Buy rating on the stock,\u201d Truist Securities analyst William Stein, who holds a Buy rating on the stock, wrote in a note to investors Tuesday morning.\n\nOne of Nvidia's most notable updates at the trade show: a new, pint-sized artificial intelligence superchip called GB10 used in its (also new) client supercomputer.\n\nThe supercomputer \u2014 sized to fit on the average-sized desk \u2014 is part of Nvidia\u2019s Project DIGITS announced Monday, advertised to developers, researchers, and students, and the device will be available in May for $3,000.\n\nJensen Huang speaking at NVIDIA Keynote at Michelob Ultra Arena in Las Vegas, NV, on Jan. 6, 2025. Credit: DeeCee Carter/MediaPunch /IPX (DeeCee Carter/MediaPunch/MediaPunch/IPx)\n\nNvidia also unveiled major updates about its robotics strategy. The chip giant debuted its Cosmos platform, which offers AI models for developing humanoid robots as well as autonomous vehicles.\n\nNvidia shares closed at a record high of $149.43 Monday ahead of Huang\u2019s keynote \u2014 eclipsing its prior record close of $148.88 reached on Nov. 7.\n\nWedbush analyst and Nvidia bull Dan Ives said he sees robotics and autonomous technology representing a $1 trillion market for the company. Huang put that number higher in his keynote, saying that autonomous driving technologies alone \u201cwill likely be the first multitrillion-dollar robotics industry.\"\n\nAdditionally, Nvidia showcased new Blackwell-generation gaming GPUs (graphics processing units) and applications for developers to launch their own custom AI agents. Yahoo Finance\u2019s Dan Howley reported that Nvidia could debut a successor to its Blackwell generation AI chips during its GTC conference in March.\n\nStockStory aims to help individual investors beat the market.\n\nLaura Bratton is a reporter for Yahoo Finance. Follow her on X @LauraBratton5.\n\nClick here for the latest stock market news and in-depth analysis, including events that move stocks\n\nRead the latest financial and business news from Yahoo Finance",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Micron (MU) Ramps Up Production of HBM3E Memory for Nvidia",
            "link": "https://www.tipranks.com/news/micron-mu-ramps-up-production-of-hbm3e-memory-for-nvidia",
            "snippet": "Micron Technology ($MU) aims to bolster its position in the high-bandwidth memory (HBM) market. The company plans to ramp up production of its 12-stack...",
            "score": 0.8409222364425659,
            "sentiment": null,
            "probability": null,
            "content": "Micron Technology (MU) aims to bolster its position in the high-bandwidth memory (HBM) market. The company plans to ramp up production of its 12-stack HBM3E memory and supply it to its key customer, Nvidia (NVDA). It must be noted that the 12-stack HBM3E memory boasts 20% lower power consumption and 50% higher capacity compared to the 8-stack HBM3E memory standard, making it ideal for power-efficient and high-demanding AI workloads.\n\nMU completed the development of 12-stack HBM in September 2024 and has actively showcased samples to key customers, including Nvidia. Importantly, Micron plans to prioritize 12-stack HBM production in the second half of 2025.\n\nLooking ahead, Micron is also developing HBM4, with plans to make it available in 2026. It is worth highlighting that HBM4 is expected to deliver a performance boost of over 50% compared to HBM3E.\n\nCompetition in HBM Market Heats Up\n\nWhile Micron is leading the HBM market, Samsung Electronics (SSNLF) faces challenges. Samsung is still struggling to secure Nvidia\u2019s validation for its 8-layer HBM3E. Further, the company\u2019s 12-layer version is under development with limited production expected until the second quarter of 2025. This delay could impact Samsung\u2019s market share in the high-performance computing (HPC) market, where HBM is a critical component.\n\nTo counter MU\u2019s progress, Samsung seeks to improve its HBM3E memory by upgrading its manufacturing processes and materials. The company also plans to introduce HBM4 using advanced 1c DRAM technology by 2025.\n\nAt the same time, SK Hynix, another major player in the semiconductor market, is speeding up the development of HBM4 to meet the rising demand for advanced memory solutions.\n\nIs MU a Good Stock to Buy Right Now?\n\nTurning to Wall Street, MU stock has a Strong Buy consensus rating based on 21 Buys and two Holds assigned in the last three months. At $137.15, the average Micron stock price target implies a 37.81% upside potential. Shares of the company have gained 18.25% year-to-date.\n\nSee more MU analyst ratings\n\nDisclosure\n\nQuestions or Comments about the article? Write to editor@tipranks.com",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "300 Billion Reasons to Buy Nvidia Stock Like There's No Tomorrow",
            "link": "https://www.fool.com/investing/2025/02/17/300-billion-reasons-to-buy-nvidia-stock/",
            "snippet": "Despite DeepSeek rattling markets, large tech companies continue to spend heavily on artificial intelligence (AI) infrastructure.",
            "score": 0.9389064311981201,
            "sentiment": null,
            "probability": null,
            "content": "After a huge run higher, Nvidia (NVDA 5.27%) stock hasn't even been keeping up with the overall market in recent months. There are several reasons for that, but the big question for investors is whether it's now time to take advantage of Nvidia's stagnant share price.\n\nThe stock soared about 85% over the last year, yet it is lower than it was four months ago, even as the S&P 500 has a total return of about 4% in that time. But now it looks like there are over 300 billion more reasons to buy the stock. That's because several big tech companies plan to spend as much as $320 billion on data centers and artificial intelligence (AI) infrastructure over the next year.\n\nHopper to Blackwell to Rubin AI solutions\n\nNvidia's recent success is relatively easy to explain. Its advanced AI graphics processing unit (GPU) chips are in high demand. Using management's guidance for its soon-to-be-reported fiscal fourth quarter, revenue for the fiscal year ended in late January should show year-over-year growth of about 110%. That's especially impressive considering quarterly revenue is approaching $40 billion.\n\nNvidia also shared its plans with investors for continued innovation that should keep driving demand. Sales of its H100 and H200 GPU chips have been boosting revenue growth, and Nvidia now has its Blackwell AI architecture in production.\n\nCEO Jensen Huang has called demand for Blackwell \"insane.\" Investors will hear an update on its Blackwell sales when Nvidia reports earnings on Feb. 26. The company might also discuss the next-generation Rubin AI platform that is due in 2026.\n\nMassive total addressable market\n\nOne recent headwind for Nvidia stock was the surprising announcement last month from privately owned Chinese start-up DeepSeek. That company reportedly created a high-performing large language model (LLM) for just $6 million. While many question the authenticity of that total capital cost, the DeepSeek product raised uncertainty about how much large-cap tech companies would continue to spend on Nvidia AI products.\n\nBut those companies aren't throttling back on spending. Meta Platforms, Amazon, Alphabet, and Microsoft each announced spending plans for data centers and AI infrastructure in 2025. As a group, the investments could total as much as $320 billion over the course of just one year.\n\nAmazon expects to lead the way with $100 billion in capital expenditure. CEO Andy Jassy stated, \"The vast majority of that capex spend is on AI for AWS (Amazon Web Services).\" Alphabet plans about $75 billion and Meta $65 billion. Microsoft will continue on its plan for $80 billion in AI investments through June of this year.\n\nThat company said it is already seeing returns on its investment. Satya Nadella, chairman and CEO of Microsoft, noted, \"Already, our AI business has surpassed an annual revenue run rate of $13 billion, up 175% year over year.\" Nvidia is the clear leader in hardware for AI infrastructure and is arguably the biggest benefactor from all that capital spending.\n\nNvidia's under-the-radar catalysts\n\nOn top of those tailwinds related to AI data center spending, Nvidia has other growing segments. Its gaming business generates the second most revenue. Gaming revenue has accelerated in each of the last three quarters, reaching its highest level since Nvidia's quarterly period ended May 1, 2022.\n\nIts professional visualization business provides a platform for creating industrial AI simulations and uses AI to drive efficiencies for industrial developers. That segment's revenue has increased in each of the past three quarters and has more than doubled in the last two years. Automotive and robotics revenue has also accelerated, with growth in each of the last five quarterly periods.\n\nThose other business segments use AI and also offer software solutions to Nvidia's AI hardware customers. That helps to provide a flywheel effect as its next-generation AI architecture continues to improve and be utilized in its various platform solutions.\n\nThe upcoming quarterly report could bring share price volatility, but investors should look to hold Nvidia for its long-term potential. A pullback that might come from the quarterly report would just provide an opportunity to buy more Nvidia stock.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "RTX 5090 GPU shortage could soon end if Nvidia adopts this strategy",
            "link": "http://www.msn.com/en-us/news/technology/rtx-5090-gpu-shortage-could-soon-end-if-nvidia-adopts-this-strategy/ar-AA1zeFNa?apiversion=v2&noservercache=1&domshim=1&renderwebcomponents=1&wcseo=1&batchservertelemetry=1&noservertelemetry=1",
            "snippet": "Online leakers explained that the RTX 5090 shortage began due to scalpers over-purchasing components, but it could soon end if Nvidia adopts this strategy.",
            "score": 0.93593829870224,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-16": {
        "0": {
            "title": "NVDA Stock Quote Price and Forecast",
            "link": "https://edition.cnn.com/markets/stocks/NVDA",
            "snippet": "View NVIDIA Corporation NVDA stock quote prices, financial information, real-time forecasts, and company news from CNN.",
            "score": 0.9209235906600952,
            "sentiment": null,
            "probability": null,
            "content": "1. How relevant is this ad to you?\n\nVideo player was slow to load content Video content never loaded Ad froze or did not finish loading Video content did not start after ad Audio on ad was too loud Other issues",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Where Will Nvidia Stock Be in 10 Years?",
            "link": "https://www.fool.com/investing/2025/02/16/where-will-nvidia-stock-be-in-10-years/",
            "snippet": "Long-term investing is the key to sustainable returns in the stock market because it smooths out short-term volatility and allows a company's fundamental...",
            "score": 0.9394428133964539,
            "sentiment": null,
            "probability": null,
            "content": "Long-term investing is the key to sustainable returns in the stock market because it smooths out short-term volatility and allows a company's fundamental value to shine through. Tech giant Nvidia (NVDA 5.27%) has been at the forefront of generative artificial intelligence (AI) growth. But as the hype inevitably fades and new opportunities like robotics and self-driving cars potentially have their turn in the sun, how will Nvidia respond?\n\nNvidia has a history of reinventing itself\n\nWith roughly 88% of revenue ($30.8 billion) coming from its data center segment, Nvidia is very overexposed to the market for generative AI hardware. This under-diversification puts the tech giant in a perilous position, especially as clients continue to lose money on their AI projects and new low-cost rivals like China's DeepSeek cause shareholders to question the industry's long-term profit potential.\n\nThe growing popularity of custom chips also allows key AI clients like OpenAI to bypass Nvidia and design their own hardware through partnerships with fab companies like Taiwan Semiconductor Manufacturing.\n\nBut while generative AI now dominates Nvidia's narrative, it wasn't always like this. The company has a history of shifting focus to tackle emerging trends. As recently as fiscal 2022, video game and crypto mining hardware (Nvidia's gaming segment) represented around 46% of total sales ($12.46 billion) compared to just 9.4% today as AI-related growth has overshadowed these once crucial opportunities.\n\nNvidia's rapid business transformation highlights the versatility of its core technology, the graphics processing unit (GPU), a type of computer chip that uses parallel processing to perform multiple calculations simultaneously. And over the next 10 years, investors should expect GPUs to continue finding new uses, even if demand related to generative AI slows down. Robotics and autonomous vehicles look promising.\n\nSelf-driving cars and robotics?\n\nAccording to analysts at McKinsey & Company, autonomous driving could create $300 billion to $400 billion in revenue by 2035 as automakers take advantage of the technology for software as a service (SaaS). And just like generative AI, self-driving cars need to process large amounts of data quickly and accurately, making this industry a potential gold mine for Nvidia and its industry-leading GPUs.\n\nTesla is a good example of the potential scale of the opportunity. Despite not offering a well-known generative AI large language model (LLM), the automaker is one of Nvidia's key customers. It is accumulating tens of thousands of GPUs to build its Dojo supercomputer, which is designed to be the brain behind its full self-driving (FSD) platform.\n\nNvidia also positions itself to play a direct role in the automotive industry through software like its Drive AGX, designed to synergize with its hardware to enable self-driving functions and process driving data.\n\nAs of the third quarter, Nvidia's automotive and robotics segment generated sales of $449 million. While this is a drop in the bucket compared to its total revenue of $35 billion, the segment grew by an impressive 72% year over year. And investors can expect that increase to accelerate over the coming decade.\n\nIs Nvidia stock a buy?\n\nThe generative AI boom is getting long in the tooth, and demand for Nvidia's GPUs may eventually slow as companies look for cheaper alternatives. However, over the long term, new opportunities like robotics and self-driving cars could pick up the slack.\n\nThat said, it is not a good idea to buy Nvidia at the height of its current hype cycle. Investors may want to wait for more information before considering a position in the stock.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "The Rower Turned Engineer Who Helped Make Nvidia a $3 Trillion Company",
            "link": "https://www.wsj.com/tech/the-rower-turned-engineer-who-helped-make-nvidia-a-3-trillion-company-99d72ecd",
            "snippet": "Jonah Alben uses lessons from his days as Stanford rowing coxswain to design AI chips and keep selling to China.",
            "score": 0.9059112668037415,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia RTX 5050, 5060, and 5060 Ti surface in Zotac EEC filings",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-rtx-5050-5060-and-5060-ti-surface-in-zotac-eec-filings",
            "snippet": "Zotac has registered a slew of Nvidia GeForce GPUs with the EEC (Eurasian Economic Commission), including two new entries: the RTX 5060 series and the RTX...",
            "score": 0.9300116896629333,
            "sentiment": null,
            "probability": null,
            "content": "Zotac has registered a slew of Nvidia GeForce GPUs with the EEC (Eurasian Economic Commission), including two new entries: the RTX 5060 series and the RTX 5050 (via Harukaze). Of the two, this is the first time we're hearing about an RTX 5050, succeeding the RTX 3050 as the RTX 40 series didn't feature a 50-class counterpart. In any case, we'll proceed with caution because a large number of SKUs listed at the EEC never actually see the light of day.\n\nThe newfound RTX 5050 could be Nvidia's first foray into the sub-$250 price bracket in a long time. This segment of the market has been primarily catered to by second-hand RTX 30 or Radeon RX 6000 series GPUs for the past few years. Lately, Intel's new Battlemage-based Arc B580 and Arc B570 have been a popular choice for budget gamers, though with a few caveats. As far as AMD is concerned, RDNA 4 will debut with the RX 9070 family (Navi 48) meaning budget RX 9060 or RX 9050 GPUs could take some time to materialize.\n\nTo reiterate, EEC listings never guarantee a GPU's release. OEMs typically register placeholder product names at these regulatory bodies, in case Nvidia launches said product in the future. With that in mind, it appears Zotac has filed several GPUs at the EEC spanning across multiple generations; from Kepler to Blackwell. The list includes the RTX 5060 Ti, RTX 5060, and the RTX 5050.\n\n(Image credit: EEC)\n\nNow, the RTX 5060 Ti is rumored to carry two variants: one with 8GB and the other with 16GB of VRAM. Hopefully, that's the only differentiating factor between the two, unlike the Pascal GTX 1060 series. Similarly, the same source claims that the RTX 5060 will offer only 8GB of memory and not much is known about the RTX 5050. Reports suggest the RTX 5060 family will debut next month, but alleged RTX 5070 delays might introduce some uncertainties.\n\nBudget gamers will likely have to wait until Computex in late May for potential RTX 5050 and RX 9060/9050 reveals. It is far too early to expect anything from Intel, given that Battlemage launched just two months ago. Celestial will arrive with Panther Lake CPUs in the second half of 2025, with dedicated GPUs potentially coming later.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "[News] Next-Gen HBM? NVIDIA Reportedly Teams up with Samsung/ SK hynix to Push SOCAMM for 2025 Production",
            "link": "https://www.trendforce.com/news/2025/02/17/news-next-gen-hbm-nvidia-reportedly-joins-forces-with-samsung-sk-hynix-to-push-socamm-for-2025-mass-production/",
            "snippet": "As NVIDIA collaborates closely with memory heavyweights on their latest HBM products for AI accelerators, the U.S. chip giant is said to be in talks w...",
            "score": 0.9169590473175049,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "NVIDIA\u2019s GeForce RTX 5050 And RTX 5060/5060 Ti GPUs Spotted At EEC Listing; More Mainstream GPUs On The Way?",
            "link": "https://wccftech.com/nvidia-geforce-rtx-5050-and-rtx-5060-5060-ti-gpus-spotted-at-eec-listing/",
            "snippet": "ZOTAC registers several NVIDIA RTX 50 series GPUs, including the upcoming RTX 5060, RTX 5060 Ti, and the RTX 5050 on EEC.",
            "score": 0.8713364005088806,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's GeForce RTX 5070, RTX 5060 Ti, RTX 5060, and RTX 5050 GPUs were registered at EEC by one of the major NVIDIA board partners, ZOTAC.\n\nGeForce RTX 5050 Exists, Confirms ZOTAC Registration of the GPU Alongside Several More Upcoming RTX 50 GPUs\n\nNVIDIA's board partner, ZOTAC, has recently registered several of its RTX 50 series GPUs at the Eurasian Economic Commission (EEC). The GPUs contain upcoming SKUs like GeForce RTX 5070 Ti, RTX 5070, and even 60-class cards such as RTX 5060 and RTX 5060 Ti.\n\nHowever, this is probably one of the rare sights of a 50-class card and that, too, a desktop variant. Apparently, NVIDIA's GeForce RTX 5050 exists, as can be seen from the list, which we thought to be non-existent at least in the desktop lineup, since the RTX 5050 mobile GPU exists, according to some leaks.\n\nThe registration date is 12th February 2025, which is very recent. It is evident that the RTX 5050 may be released after some time, probably a few weeks or months after the GeForce RTX 5060 series cards are launched. This is surprising since NVIDIA skipped the GeForce RTX 4050 completely from the desktop Ada lineup while launching it for only the mobile platform.\n\nEven though a Chinese company named Shangke did register an RTX 4050 in 2023, we didn't see the card in the mainstream market. Hopefully, the RTX 5050 can make it on the market as the current GPU market lacks affordable cards. Unlike the 60-class cards that are now retailing for $300+, there seems to be no sub-$300 or sub-$200 GPUs from the latest generation, either from NVIDIA or AMD.\n\nIt's likely that the RTX 5050 will be a sub $300 or sub $250 card, but a price tag of below $200 is unlikely. We don't know much about the RTX 5050, but according to some rumors, it's likely to bring 8 GB of VRAM. Furthermore, some reports suggested previously that it will feature GDDR6 memory, which seems unlikely as the RTX 5050 laptop variant is supposedly equipped with GDDR7 memory.\n\nAt the moment, the next RTX 50 series card is the GeForce RTX 5070 Ti, scheduled to launch on 20 February. It will be followed by the GeForce RTX 5070 on 5 March and the RTX 5060 Ti and RTX 5060 in April. The GeForce RTX 5050 may end up on the market in the second half of 2025, but that's just our speculation. Still, it's best to wait for more updates that can confirm the GPU's presence.\n\nNews Source: @harukaze5719",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Morgan Stanley says buy these stocks including Nvidia before earnings season wraps up",
            "link": "https://www.cnbc.com/2025/02/16/hurry-up-and-buy-nvidia-and-others-ahead-of-earnings-morgan-stanley-says.html",
            "snippet": "Morgan Stanley says there are still plenty of quality buying opportunities left ahead of earnings.",
            "score": 0.9037424325942993,
            "sentiment": null,
            "probability": null,
            "content": "Analysts at Morgan Stanley said there's still plenty of overweight-rated stocks to buy ahead of earnings as the reporting season wraps up. The firm says it's sticking with stocks like Nvidia heading into quarterly reports. Other stocks Morgan Stanley likes include: EQT , Arista Networks and Tuya. Arista Networks Analyst Meta Marshall is standing by shares of the networking stock ahead of earnings on Tuesday. \"[The] company has beaten revenue by ~300-400bps over the last 3-4 years, about in line with buyside expectations into the quarter,\" she wrote. Still, Marshall said the fourth-quarter results are not likely to move the needle. \"Further multiple expansion\" is on hold until the company shows a meaningful amount of new customers, she added. The company does, however, having an underappreciated opportunity in the data center space. \"Our checks were incrementally more positive on networking this quarter, with data center an area of focus and expectations of gradual improvement throughout the year,\" she said. Arista shares are up 61% over the last year. EQT Buy any dip in shares of the hydrocarbon exploration company, according to analyst Devin McDermott. \"The pullback creates an attractive entry point, with valuations now more compelling,\" he wrote in a recent note. EQT remains a top pick at the firm ahead of earnings on Wednesday. The stock has risen more than 60% over the past 12 months, and is already up 16% year to date. McDermott said the stock's recent choppiness is due to concerns around DeepSeek's AI model, which has the potential to lessen power demand. The firm acknowledged that those concerns may persist, but says it still sees a buying opportunity. \"Rising power consumption is a source of longer-term gas demand upside, but LNG [liquified natural gas] is a much larger near-term growth driver,\" he wrote. \"We continue to prefer gas over oil exposure within E & Ps and recommend buying the gas pullback,\" he went on to say. Tuya Tuya shares are up a whopping 66% this year, but the firm says the China internet of things and AI company has plenty more room to run. The firm reiterated its overweight rating in a recent note clients, saying there is \"strong revenue growth momentum\" that's likely to continue. However, analyst Yang Liu said there's a \"mismatch between fundamentals and share price,\" urging investors to remain calm. \"We think it leaves significant room for the share price to catch up with value,\" she added. The firm acknowledged some tariff risk, but says there's too many other positive catalysts to ignore right now. Liu also raised her price target to $3 from $2.30 with fourth-quarter earnings on deck on Feb. 26, which is the same day Nvidia reports its results. \"The robust top-line growth is likely to be sustained by strong overseas IoT demand and Tuya's market share gains,\" she in her quarterly preview. Nvidia \"While sentiment has worsened around potential longer term risks, near term business continues to firm, Blackwell supply visibility continues to build, customer desire to spend is clearly on display. ... .We believe that NVIDIA should trade at a premium given its higher probability of upward revisions in the near term.\" EQT \"We continue to prefer gas over oil exposure within E & Ps and recommend buying the gas pullback. ... .The pullback creates an attractive entry point, with valuations now more compelling. ... .Rising power consumption is a source of longer-term gas demand upside, but LNG [liquified natural gas] is a much larger near-term growth driver.\" Arista \"Company has beaten revenue by ~300-400bps over the last 3-4 years, about inline with buyside expectations into the quarter. ... .Further multiple expansion likely requires news of additional material customers or advancement on paused trials. ... .Our checks were incrementally more positive on networking this quarter, with data center an area of focus and expectations of gradual improvement throughout the year.\" Tuya \"We reiterate our OW rating & raise our PT to factor in sustained strong revenue growth driven by overseas IoT & good cost control. ... .The robust top-line growth is likely to be sustained by strong overseas IoT demand & Tuya's market share gains. ... .Strong revenue growth momentum to be sustained. ... .Mismatch between fundamentals & share price. ... .We think it leaves significant room for the share price to catch up with value.\"",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Could Power Up This Under-The-Radar Healthcare Stock",
            "link": "https://www.barchart.com/story/news/30957828/nvidia-could-power-up-this-under-the-radar-healthcare-stock",
            "snippet": "Ever since the artificial intelligence (AI) frenzy began, Nvidia (NVDA) has been king, influencing almost every other industry in the process.",
            "score": 0.8272302150726318,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Armis integrates Centrix with NVIDIA BlueField-3 for enhanced security",
            "link": "https://www.sdxcentral.com/articles/news/armis-integrates-centrix-with-nvidia-bluefield-3-for-enhanced-security/2025/02/",
            "snippet": "Armis integrates its Centrix platform with NVIDIA BlueField-3 DPUs to secure critical infrastructure while ensuring operational continuity.",
            "score": 0.679258406162262,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "Earthquakes might be to blame for NVIDIA GeForce RTX 50-series shortages",
            "link": "https://www.tweaktown.com/news/103248/earthquakes-might-be-to-blame-for-nvidia-geforce-rtx-50-series-shortages/index.html",
            "snippet": "TL;DR: The RTX 50-series GPUs are scarce due to a combination of factors: a 6.4 magnitude earthquake in Taiwan damaged TSMC's facilities, affecting wafer...",
            "score": 0.689414918422699,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-15": {
        "0": {
            "title": "Better Artificial Intelligence (AI) Buy for 2025: AMD or Nvidia Stock?",
            "link": "https://www.fool.com/investing/2025/02/15/better-artificial-intelligence-ai-buy-for-2025-amd/",
            "snippet": "Nvidia (NVDA 2.63%) has essentially owned the data center computing market, which is a huge deal, considering the hundreds of billions of dollars being...",
            "score": 0.9313293099403381,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) has essentially owned the data center computing market, which is a huge deal, considering the hundreds of billions of dollars being spent on artificial intelligence (AI) infrastructure. Nvidia is one of the primary benefactors of this investment, although some of its competitors, like AMD (AMD 2.92%), are also benefiting.\n\nHowever, the lead that Nvidia has surmounted looks unassailable, but all it takes is one innovation, and AMD could be neck and neck with Nvidia.\n\nSo, which of these two is the better AI stock for 2025?\n\nNvidia's lead in the data center market may be insurmountable\n\nNvidia's GPUs and software have become the industry standard in data centers. While AMD's GPUs look like they can compete on paper, Nvidia's software, CUDA, sets it apart. This software allows GPUs to process multiple calculations simultaneously and handle the sheer computing tasks that AI computing requires.\n\nBecause the industry has essentially adopted CUDA software versus AMD's ROCm, it's unlikely that AMD will ever be able to surmount Nvidia's lead in the data center race. The switching costs of moving from one supplier to another when the infrastructure is already set up for one is a massive consideration and is the primary hurdle for anyone switching.\n\nNvidia's lead over AMD can be seen in both of their financials. Each has a data center division, and Nvidia's lead is quite impressive. In Q4 2024, AMD's data center revenue was $3.9 billion, up 69% year over year. Nvidia hasn't reported its Q4 results yet, as its financial calendar is shifted by one month. As a result, using AMD's Q3 results provides a better comparison.\n\nIn Q3, AMD's data center revenue was $3.5 billion, up 122% year over year. These are impressive results by themselves, yet pale in comparison to Nvidia's.\n\nIn Q3 FY 2025 (ending Oct. 27), Nvidia's data center revenue was $30.8 billion, up 112% year over year. That indicates Nvidia's data center business is around 10 times the size of AMD's, which is a massive lead. We'll learn more about Nvidia's Q4 results when it reports on Feb. 26, but with all the talk from big tech companies about AI spending, it's likely to report great numbers.\n\nNvidia has built a massive moat with huge switching costs, which essentially blocks AMD from taking a meaningful amount of its data center business. However, if AMD is substantially cheaper as a stock, the discount could be enough of a reason to invest in AMD, as its data center business is still growing strongly, being much smaller than Nvidia.\n\nAMD doesn't have a discount to Nvidia\n\nBecause both companies are fully profitable, using an earnings-based metric like the price-to-earnings (P/E) ratio makes sense.\n\nFrom this perspective, AMD's stock looks far more expensive than Nvidia's, which it is. However, both companies are undergoing strong growth, and AMD's profit picture is set to improve throughout 2025, so using a forward P/E ratio is also a good idea.\n\nFrom this perspective, AMD is cheaper than Nvidia. However, the discrepancy between these two valuation levels can largely be attributed to the company-wide growth rate. Considering that Nvidia is expected to grow revenue by 52% in FY 2026 (ending January 2026) and AMD is expected to grow at a 24% pace, this difference seems reasonable.\n\nNvidia is growing faster and dominating the most important computing market right now. While AMD is still a fine company, I don't think there's any reason to own AMD over Nvidia. Best-in-class stocks usually outperform their peers by a wide margin, especially when they start from a similar valuation point.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia H100 purse hits the market for $65,000",
            "link": "https://www.tomshardware.com/pc-components/gpus/flaunt-your-style-with-this-usd65-000-nvidia-h100-purse",
            "snippet": "GPU Purses is selling an Nvidia H100 shoulder bag that costs $65536.",
            "score": 0.8379912972450256,
            "sentiment": null,
            "probability": null,
            "content": "GPU Purses made news a few months after it converted a $20 Nvidia GeForce GT 730 GPU into a thousand-dollar plastic handbag that looked like it came from Temu. It\u2019s now back with a more premium offering, putting an Nvidia H100 AI GPU (or at least pieces of it) on the same plastic casing, calling it the H100 Purse.\n\nHowever, the purse doesn\u2019t look like it features a complete graphics card\u2014instead, you\u2019ll see the massive chip labeled GH100 at the center of the bag, flanked by several LR22 and LR33 inductors. The listing shows just one picture, and the description only says, \u201cPurse that has a rare one of a kind gpt-4 training gpu.\u201d It also mentions, \u201cThis purse is subject to export controls,\u201d even though we\u2019re unsure if it will run or if it\u2019s even an actual AI GPU.\n\nThe listing priced the H100 Purse at $65,536, adding quite a premium on Nvidia\u2019s Hopper GPU. Although it does not have over a 50x price increase with the GT 730 GPU Purse, it\u2019s still over twice the asking price for a working H100 AI GPU, currently priced at around $25,000 apiece. And if you tack on $5,000 on the final price, you might be able to purchase a working next-generation Nvidia GB200 Blackwell GPU.\n\nLuxury fashion items are known for their inflated prices, but they\u2019re at least usually classy or stylish. We don\u2019t see the quality of these exorbitant GPU purses (and we\u2019re unlikely to purchase one, anyway), but the photos of the item, its product page, and the entire website do not inspire confidence.\n\nGiven its absurd prices, we\u2019re unsure if GPU Purses has sold anything or if it\u2019s even a legitimate site. Nevertheless, it\u2019s pretty interesting how GPU technologies have entered popular culture, where they are now being made into fashion pieces.\n\nThese GPUs aren\u2019t the first time we\u2019ve seen PC components turned into fashion accessories, though. You could quickly get CPU keychains online, and Etsy has a healthy \u2018CPU jewelry\u2019 marketplace. So, if you want to wear something that came from a computer, why not spend on those $20 trinkets instead of plunking thousands of dollars on a questionable site?",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Retail fave SoundHound AI pummeled by Nvidia exit",
            "link": "https://sherwood.news/markets/retail-fave-soundhound-ai-pummeled-by-nvidia-exit/",
            "snippet": "This follows an earlier report saying that TSMCTSM $203.79 (1.02%) is considering building an advanced packaging plant in the US amid pressure from Trump.",
            "score": 0.9582239985466003,
            "sentiment": null,
            "probability": null,
            "content": "D-Wave Quantum leads massive rally in quantum computing stocks as its revenue outlook goes parabolic\n\nD-Wave Quantum is ripping higher, leading an across-the-board surge in pure-play quantum computing stocks, after its Q4 results indicated a rapid improvement in its sales outlook.\n\nBookings, which are the value of customer orders received that are expected to be converted into revenues in the future, soared. At $18.3 million, these bookings are now higher than all the revenue the company has generated over the past two years.\n\nPeers IonQ, Rigetti Computing, and Quantum Computing are all up double digits as of midday.\n\n\u201cOur mission is unwavering: to help organizations realize the benefits of quantum computing now,\u201d CEO Alan Baratz said. \u201cWith record bookings, a record cash position, and an unequivocal demonstration of our quantum system outperforming classical on a real-world problem, our progress toward achieving that mission is clear.\u201d\n\nThe hype around the possibility of quantum computing in the wake of Alphabet\u2019s Willow chip sparked a run-up in the shares of companies operating in the space. But investor focus shifted to the lack of commercial utility of the technology to date after Nvidia CEO Jensen Huang said it would be decades before quantum computing would be \u201cvery useful.\u201d (The chip designer is hosting a \u201cQuantum Day\u201d to recognize and celebrate the progress in this industry at its conference next week.) Quantum computing stocks proceeded to tumble thereafter, with losses piling up during the market\u2019s recent rout.\n\nIn an interview with Sherwood News following Huang\u2019s remarks, Baratz pointed to the different strategy D-Wave takes in developing the technology (preferring annealing to a gate-based approach) as a critical differentiating factor that enables the company to be \u201ccommercial and delivering useful results today.\u201d\n\nAhead of its earnings, D-Wave recently announced \u201cthe world\u2019s first and only demonstration of quantum computational supremacy on a useful, real-world problem.\u201d\n\nThat progress, and the surge in bookings, serve as green shoots for the commercial viability of quantum computing, even though we\u2019re still talking fairly small numbers here.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "NVIDIA GeForce RTX 5090 supply will be 'stupidly high soon' leaving scalpers to 'cry so hard'",
            "link": "https://www.tweaktown.com/news/103308/nvidia-geforce-rtx-5090-supply-will-be-stupidly-high-soon-leaving-scalpers-to-cry-so-hard/index.html",
            "snippet": "TL;DR: NVIDIA's GeForce RTX 5090 graphics card faces high demand but limited stock. To address this, NVIDIA is using excess TSMC yields to produce more RTX...",
            "score": 0.9663903117179871,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "You may have sold Nvidia for the wrong reason. 3 reasons to consider buying this AI stock",
            "link": "https://www.usatoday.com/story/money/2025/02/15/why-buy-nvidia-stock/78652185007/",
            "snippet": "The good part to Nvidia's poor start to the year is investors have a window to buy this fast-growing company on the dip.",
            "score": 0.8004403710365295,
            "sentiment": null,
            "probability": null,
            "content": "You may have sold Nvidia for the wrong reason. 3 reasons to consider buying this AI stock\n\nShow Caption Hide Caption Trump, China, and the AI Summit: A global power struggle The AI Summit in Paris has drawn world leaders, tech executives, and researchers to shape the future of artificial intelligence. While many seek global cooperation, the event highlights deep geopolitical divides, particularly between the US and China. unbranded - Newsworthy\n\nNvidia's stock price has declined in early 2025 due to concerns about competition and US government restrictions.\n\nThe emergence of a Chinese AI model claiming to be cheaper to train raised concerns about demand for Nvidia's chips.\n\nHowever, analysis suggests the Chinese model's costs may be higher than initially claimed, and Nvidia's stock remains a good long-term investment opportunity.\n\nIncreased demand for AI applications and continued investment in AI infrastructure by major companies suggest a positive outlook for Nvidia.\n\nNvidia (NASDAQ: NVDA) has had a forgettable start to 2025 as shares of the semiconductor giant are down more than 3% as of this writing, and multiple factors out of the company's control have played a part in its decline.\n\nFor instance, the previous Biden administration proposed wide-ranging restrictions on sales of Nvidia's chips to foreign customers, but the impact of those restrictions was mitigated to some extent by the recent announcement of the Stargate project that could see $500 billion being poured into artificial intelligence (AI) infrastructure in the U.S. This development gave Nvidia stock a shot in the arm, but the chip designer would witness another sell-off very soon.\n\nNvidia stock fell thanks to DeepSeek, but investors may have jumped the gun\n\nChinese AI start-up DeepSeek released its R1 reasoning model and claimed that it was trained for a paltry $5.6 million. DeepSeek's model was good enough to compete with the o1 reasoning model from OpenAI, a company that has been spending billions to build its AI infrastructure using chips from Nvidia. So, the low-cost nature and efficiency of the Chinese company's model sent Nvidia stock packing.\n\nInvestors were worried about a potential drop in demand for its graphics cards that are being used for AI training and inference by major cloud computing companies and governments. The semiconductor giant shed almost $600 billion of its market cap in a single day on Jan. 27 following DeepSeek's purported breakthrough. However, a report from semiconductor industry analysis company SemiAnalysis (via Tom's Hardware) suggests that DeepSeek may not have revealed the actual cost of training its AI model.\n\nWhat is DeepSeek? How a small Chinese startup shook up the AI sector\n\nSemiAnalysis points out that DeepSeek reportedly incurred $1.6 billion in hardware expenses. It also adds that the Chinese start-up has access to 50,000 of Nvidia's previous-generation Hopper graphics processing units (GPUs), including 10,000 units of the flagship H100 processor. SemiAnalysis further points out that the $6 million figure highlighted by DeepSeek only refers to the potential money spent on training the model.\n\nIt doesn't consider other costs associated with research, data processing, fine-tuning the model, and infrastructure expenses. Given that DeepSeek has reportedly spent over $500 million on AI funding since its inception in 2023 and has its own data centers, there is a good chance that the cost of training the R1 model that sent Nvidia stock plunging was actually higher than what's being touted by the Chinese company.\n\nIf that's indeed the case, then investors may have hit the panic button for the wrong reason. However, the good part is that Nvidia's poor start to the year means that investors have a window to buy this fast-growing company on the dip. Here are three reasons doing that could turn out to be a smart move.\n\nThree reasons to buy Nvidia right now\n\nThe first reason to buy Nvidia is its valuation. The stock's expensive valuation following its tremendous rally over the past couple of years has been a cause for concern for investors and analysts. However, it is now trading at quite attractive levels. Though Nvidia's trailing price-to-earnings (P/E) ratio of 51 is higher than the tech-laden Nasdaq-100 index's 33.4, the forward earnings multiple of 30 is lower than that.\n\nThe second reason why investors should consider buying Nvidia is related to DeepSeek. Assuming that DeepSeek's claims are indeed true and it has managed to develop a low-cost model, it could lead to an increase in the demand for AI applications.\n\nBritish economist William Stanley Jevons observed in 1865 that the increased efficiency in coal consumption wouldn't reduce the demand for coal. It would instead spur the usage of coal in more industries. This concept, known as the Jevons Paradox, can be seen in many other applications. For instance, the increasing fuel efficiency of vehicles has reportedly led to an increase in distance traveled while the arrival of low-cost LED bulbs hasn't necessarily brought down electricity bills as people have tended to install more lights because of reduced costs.\n\nSo, efficient AI models could lead to an increase in their demand, and that's why the need for Nvidia's chips is likely to remain solid. As such, DeepSeek's purported breakthrough isn't necessarily a bad thing for Nvidia.\n\nThe final reason to buy Nvidia stock following its recent pullback is that cloud computing giants are set to keep spending more money on AI infrastructure. President Donald Trump's $500 billion AI infrastructure push that's being driven by the likes of Oracle, OpenAI, SoftBank, and Abu Dhabi-based AI investment vehicle MGX to build AI data centers will require more chips. Meanwhile, recent announcements from the CEOs of Meta Platforms and Microsoft supporting higher spending on AI also point toward healthy AI chip demand.\n\nFinally, a spike in bookings for the advanced machines sold by Dutch semiconductor equipment giant ASML provides further evidence that the appetite for advanced chips to support AI workloads isn't going away. So, it won't be surprising to see Nvidia's fortunes turn around, which is why investors should consider using the recent pullback in this AI stock given the healthy growth that it is capable of delivering in the long run.\n\nRandi Zuckerberg, a former director of market development and spokeswoman for Facebook and sister to Meta Platforms CEO Mark Zuckerberg, is a member of The Motley Fool's board of directors. Harsh Chauhan has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends ASML, Meta Platforms, Microsoft, Nvidia, and Oracle. The Motley Fool recommends the following options: long January 2026 $395 calls on Microsoft and short January 2026 $405 calls on Microsoft. The Motley Fool has a disclosure policy.\n\nThe Motley Fool is a USA TODAY content partner offering financial news, analysis and commentary designed to help people take control of their financial lives. Its content is produced independently of USA TODAY.\n\nShould you invest $1,000 in Nvidia right now?\n\nOffer from the Motley Fool: Before you buy stock in Nvidia, consider this:\n\nThe Motley Fool Stock Advisor analyst team just identified what they believe are the 10 best stocks for investors to buy now\u2026 and Nvidia wasn\u2019t one of them. The 10 stocks that made the cut could produce monster returns in the coming years.\n\nConsider when Nvidia made this list on April 15, 2005... if you invested $1,000 at the time of our recommendation, you\u2019d have $829,128!*\n\nStock Advisor provides investors with an easy-to-follow blueprint for success, including guidance on building a portfolio, regular updates from analysts, and two new stock picks each month. TheStock Advisorservice has more than quadrupled the return of S&P 500 since 2002*.\n\nLearn more \u00bb\n\n*Stock Advisor returns as of February 7, 2025",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA GB202 GPU supply for RTX 5090 set to increase, retailer has 147 RTX 5080 Cards but not selling them separately",
            "link": "https://videocardz.com/newz/nvidia-gb202-gpu-supply-for-rtx-5090-set-to-increase-retailer-has-147-rtx-5080-cards-but-not-selling-them-separately",
            "snippet": "According to MEGAsizeGPU, a person with very good ties to NVIDIA board partners and responsible for many GeForce leaks in the past, there is a significant...",
            "score": 0.6308721303939819,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Adobe's AI videos, PIN's AI app, and a Nvidia challenger gets millions: This week's AI launches",
            "link": "https://qz.com/adobe-video-pin-ai-app-synthesia-positron-chips-groq-1851763957",
            "snippet": "Each week, Quartz rounds up product launches, updates, and funding news from artificial intelligence-focused startups and companies. Advertisement.",
            "score": 0.9131541848182678,
            "sentiment": null,
            "probability": null,
            "content": "Adobe (ADBE) released a beta version of its Firefly Video Model to the public this week \u2014 the market\u2019s first commercially safe generative AI model for video, according to the company. The AI video model is available through Adobe\u2019s new Firefly app and Premiere Pro with Generative Extend.\n\nAdvertisement\n\nThe Firefly app integrates with Adobe\u2019s other applications such as Photoshop and Express, so creators can use it for text-to-video generation, video storyboards, and more.\n\n\u201cFirefly is designed for creative professionals looking for unmatched creative control and IP-friendly tools that can be used safely and effectively in both ideation and production,\u201d David Wadhwani, president of Adobe\u2019s digital media business, said in a statement. \u201cWe\u2019ve been thrilled to hear from beta customers who\u2019ve found it a game-changer for ideating concepts and producing stunning videos, and we can\u2019t wait to see how the creative community uses it to bring their stories to the world.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Earnings: What To Know About The Stock Before The Upcoming Report",
            "link": "https://www.forbes.com/sites/investor-hub/article/nvidia-earnings-what-to-know-stock-report-q1-2025/",
            "snippet": "Preparing for Nvidia's earnings? Explore important information about the stock's performance and upcoming report.",
            "score": 0.9338162541389465,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "NVIDIA GeForce RTX 5070 Ti (16 GB) Launches on February 20, Delivering \"2X\" the Performance of the GeForce RTX 4070 Ti",
            "link": "http://www.msn.com/en-us/lifestyle/shopping/nvidia-geforce-rtx-5070-ti-16-gb-launches-on-february-20-delivering-2x-the-performance-of-the-geforce-rtx-4070-ti/ar-AA1yMHar?item=flightsprg-tipsubsc-v1a%3Futm_source&apiversion=v2&noservercache=1&domshim=1&renderwebcomponents=1&wcseo=1&batchservertelemetry=1&noservertelemetry=1",
            "snippet": "The GeForce RTX 5070 Ti, a new $749 GPU based on the \"Blackwell\" architecture that NVIDIA says is twice as fast the GeForce RTX 4070 Ti, will be released on...",
            "score": 0.894567608833313,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "NVIDIA\u2019s GeForce RTX 50 Series Supply Might Increase Drastically In The Future; Team Green Might Dedicate AI Production Lines Towards Consumer GPUs",
            "link": "https://wccftech.com/nvidias-geforce-rtx-50-series-supply-might-increase-drastically-in-the-future/",
            "snippet": "The supply of NVIDIA's GeForce RTX 50 GPUs may improve drastically, as Team Green might dedicate some of its AI production lines.",
            "score": 0.8974723219871521,
            "sentiment": null,
            "probability": null,
            "content": "The supply of NVIDIA's GeForce RTX 50 series GPUs may improve drastically over the upcoming weeks, as Team Green might dedicate some of its AI production lines to consumer GPUs.\n\nNVIDIA's GeForce RTX 5090 Might See Higher Retail Availability In The Future, As Team Green Might Prioritize RTX 50 Production\n\nIt seems like NVIDIA's attention towards consumer GPUs might start to increase amid the massive demand it is seeing in the markets. Since the launch of NVIDIA's RTX Blackwell GPUs, we have seen global retailers reporting tremendous demand, and not only this, but Team Green's inventory levels were said to be pretty low this time, creating a whole launch fiasco.Accordingg to MEGAsizeGPU, Team Green's RTX 50 series supply is expected to improve drastically since the firm is seeing a slowed-down demand for AI products like the Blackwell B200 GPUs.\n\nImagine you are Nvidia and have purchased shit loads of TSMC yields for B200, but now the market doesn\u2019t want that much B200, and RTX40 is retired\u2026\u2026The only solution is to make as much RTX50 as possible to cover the unused yield of B200 https://t.co/GFXAAdCzOl \u2014 MEGAsizeGPU (@Zed__Wang) February 15, 2025\n\nWhile this might be speculation for now, it isn't false about the fact that NVIDIA's Blackwell AI products have seen a slowed down market adoption, mainly since companies are now referring to custom ASIC designs, and with the unutilized semiconductor wafers NVIDIA has on its hands, the company might try to utilize them for catering to the demand of RTX 50 series GPUs. This might be a dream come true for gamers looking to get their hands on RTX Blackwell SKUs, but for now, take this rumor with a grain of slot.\n\nThe demand is obviously there for NVIDIA's RTX 50 series GPUs, and given that the firm is slated to release mainstream GPUs such as the GeForce RTX 5070 and the GeForce RTX 5060, Team Green will see high demand in the future. Since they have been occupied catering to the AI supply chain for months now, it was imminent for NVIDIA to see a step back in consumer GPU inventory levels; however, given the reception it has seen from the markets, the wise choice would be to ramp up production numbers.\n\nAMD's RX 9070 series GPUs are right around the corner, and it wouldn't be a better time for NVIDIA other than now to toughen up the market competition by coming in with higher retail availability.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-02-14": {
        "0": {
            "title": "Nvidia Clears Key Level As Pricing Power Seen Dominating AI Server Market; Is The Stock A Buy Now?",
            "link": "https://www.investors.com/research/nvda-stock-is-nvidia-a-buy-february-2025/",
            "snippet": "Nvidia Clears Key Level As Pricing Power Seen Dominating AI Server Market; Is The Stock A Buy Now? ... Nvidia (NVDA) rose modestly Friday amid more bullish...",
            "score": 0.6797205209732056,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) rose 3% early Friday after Foxconn, formally known as Hon Hai Precision Industry, projected higher artificial intelligence sales in its fourth-quarter results, potentially benefitting Nvidia stock. Foxconn, an Apple (AAPL) partner, expects first-quarter AI server revenue to double from both a year ago and from the prior quarter, signaling a strong demand for AI chips. But is Nvidia\u2026",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia cuts stake in Arm Holdings, discloses position in China's WeRide",
            "link": "https://www.reuters.com/technology/nvidia-cuts-stake-arm-holdings-invests-chinas-weride-2025-02-14/",
            "snippet": "Nvidia reduced its stake in British chip firm Arm Holdings by about 44% and exited its holdings in Serve Robotics and SoundHound AI in the fourth quarter,...",
            "score": 0.48190969228744507,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "You May Have Sold Nvidia for the Wrong Reason. Here Are 3 Reasons Why You Should Be Buying This Artificial Intelligence Stock Once Again.",
            "link": "https://www.fool.com/investing/2025/02/14/you-may-have-sold-nvidia-for-the-wrong-reason-here/",
            "snippet": "Nvidia (NVDA 2.63%) has had a forgettable start to 2025 as shares of the semiconductor giant are down more than 3% as of this writing, and multiple factors...",
            "score": 0.7883887887001038,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) has had a forgettable start to 2025 as shares of the semiconductor giant are down more than 3% as of this writing, and multiple factors out of the company's control have played a part in its decline.\n\nFor instance, the previous Biden administration proposed wide-ranging restrictions on sales of Nvidia's chips to foreign customers, but the impact of those restrictions was mitigated to some extent by the recent announcement of the Stargate project that could see $500 billion being poured into artificial intelligence (AI) infrastructure in the U.S. This development gave Nvidia stock a shot in the arm, but the chip designer would witness another sell-off very soon.\n\nNvidia stock fell thanks to DeepSeek, but investors may have jumped the gun\n\nChinese AI start-up DeepSeek released its R1 reasoning model and claimed that it was trained for a paltry $5.6 million. DeepSeek's model was good enough to compete with the o1 reasoning model from OpenAI, a company that has been spending billions to build its AI infrastructure using chips from Nvidia. So, the low-cost nature and efficiency of the Chinese company's model sent Nvidia stock packing.\n\nInvestors were worried about a potential drop in demand for its graphics cards that are being used for AI training and inference by major cloud computing companies and governments. The semiconductor giant shed almost $600 billion of its market cap in a single day on Jan. 27 following DeepSeek's purported breakthrough. However, a report from semiconductor industry analysis company SemiAnalysis (via Tom's Hardware) suggests that DeepSeek may not have revealed the actual cost of training its AI model.\n\nSemiAnalysis points out that DeepSeek reportedly incurred $1.6 billion in hardware expenses. It also adds that the Chinese start-up has access to 50,000 of Nvidia's previous-generation Hopper graphics processing units (GPUs), including 10,000 units of the flagship H100 processor. SemiAnalysis further points out that the $6 million figure highlighted by DeepSeek only refers to the potential money spent on training the model.\n\nIt doesn't consider other costs associated with research, data processing, fine-tuning the model, and infrastructure expenses. Given that DeepSeek has reportedly spent over $500 million on AI funding since its inception in 2023 and has its own data centers, there is a good chance that the cost of training the R1 model that sent Nvidia stock plunging was actually higher than what's being touted by the Chinese company.\n\nIf that's indeed the case, then investors may have hit the panic button for the wrong reason. However, the good part is that Nvidia's poor start to the year means that investors have a window to buy this fast-growing company on the dip. Here are three reasons why doing that could turn out to be a smart move.\n\nThree reasons to buy Nvidia right now\n\nThe first reason to buy Nvidia is its valuation. The stock's expensive valuation following its tremendous rally over the past couple of years has been a cause for concern for investors and analysts. However, it is now trading at quite attractive levels. Though Nvidia's trailing price-to-earnings (P/E) ratio of 51 is higher than the tech-laden Nasdaq-100 index's 33.4, the forward earnings multiple of 30 is lower than that.\n\nThe second reason why investors should consider buying Nvidia is related to DeepSeek. Assuming that DeepSeek's claims are indeed true and it has managed to develop a low-cost model, it could lead to an increase in the demand for AI applications.\n\nBritish economist William Stanley Jevons observed in 1865 that the increased efficiency in coal consumption wouldn't reduce the demand for coal. It would instead spur the usage of coal in more industries. This concept, known as the Jevons Paradox, can be seen in many other applications. For instance, the increasing fuel efficiency of vehicles has reportedly led to an increase in distance traveled while the arrival of low-cost LED bulbs hasn't necessarily brought down electricity bills as people have tended to install more lights because of reduced costs.\n\nSo, efficient AI models could lead to an increase in their demand, and that's why the need for Nvidia's chips is likely to remain solid. As such, DeepSeek's purported breakthrough isn't necessarily a bad thing for Nvidia.\n\nThe final reason to buy Nvidia stock following its recent pullback is that cloud computing giants are set to keep spending more money on AI infrastructure. President Donald Trump's $500 billion AI infrastructure push that's being driven by the likes of Oracle, OpenAI, SoftBank, and Abu Dhabi-based AI investment vehicle MGX to build AI data centers will require more chips. Meanwhile, recent announcements from the CEOs of Meta Platforms and Microsoft supporting higher spending on AI also point toward healthy AI chip demand.\n\nFinally, a spike in bookings for the advanced machines sold by Dutch semiconductor equipment giant ASML provides further evidence that the appetite for advanced chips to support AI workloads isn't going away. So, it won't be surprising to see Nvidia's fortunes turn around, which is why investors should consider using the recent pullback in this AI stock given the healthy growth that it is capable of delivering in the long run.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "These Tech Stocks Soared Friday After Nvidia Disclosed Stakes",
            "link": "https://www.investopedia.com/these-tech-stocks-soared-friday-after-nvidia-disclosed-stakes-11680063",
            "snippet": "Shares of Chinese autonomous driving company WeRide and AI infrastructure company Nebius popped Friday after Nvidia disclosed stakes in the companies.",
            "score": 0.9336823225021362,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Shares of Chinese autonomous driving company WeRide and AI infrastructure company Nebius popped Friday after Nvidia disclosed stakes in the companies.\n\nA filing showed Nvidia dumped its holdings of SoundHound AI, Serve Robotics, and Nano-X Imaging, sending those stocks lower.\n\nNvidia also trimmed its stake in chip designer Arm.\n\nShares of WeRide (WRD) soared Friday along with other tech stocks after Nvidia (NVDA) disclosed a stake in the Chinese autonomous driving company.\n\nWeRide shares nearly doubled in value during Friday's session, rising more than 80$ after a filing Thursday showed Nvidia held 1.74 million shares as of Dec. 31. WeRide, which operates driverless vehicles in 30 cities across nine countries, made its debut on the Nasdaq in October.\n\nThe filing showed Nvidia added a stake in Nebius Group (NBIS), an AI infrastructure company, as well. Shares of Nebius were up nearly 7%. Nvidia rose almost 3%.\n\nShares of AI voice technology company SoundHound AI (SOUN), delivery robot developer Serve Robotics (SERV), and medical technology firm Nano-X Imaging (NNOX) tumbled as the filing showed Nvidia divested its holdings in those companies. SoundHound shares dropped almost 30%, Serve Robotics shares plunged 40%, and Nano-X fell 11%.\n\n\n\nNvidia also reduced its stake in chip designer Arm Holdings (ARM) to roughly 1.1 million shares from 1.96 million. Its shares finished Friday 5% down more than 3%.\n\n",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "All Systems Go: NVIDIA Engineer Takes NIMble Approach to Innovation",
            "link": "https://blogs.nvidia.com/blog/nvidia-life-maggie-zhang/",
            "snippet": "Maggie Zhang, a software engineer at NVIDIA, is passionate about how systems can help efficiently solve the world's greatest challenges.",
            "score": 0.8098397254943848,
            "sentiment": null,
            "probability": null,
            "content": "Maggie Zhang has never met a system they didn\u2019t like.\n\nWhether it\u2019s designing test automation systems for NVIDIA NIM microservices or researching how city infrastructure works in their free time, Zhang is fascinated by complex systems and how they can be used to efficiently solve the world\u2019s greatest challenges.\n\nZhang and the NIM factory team maintain tools that facilitate the compliance, performance and accuracy of NIM microservices, which are designed to enable the secure, reliable deployment of high-performance AI model inferencing across data centers, workstations and the cloud. The team has introduced a standard to measure NIM microservice performance that makes the evaluation process more scalable and efficient.\n\n\u201cI don\u2019t consider myself a specialist in one software type or product. As a systems engineer, I love working with different pieces of the puzzle and figuring out how they fit together,\u201d said Zhang, whose role as a technical lead requires them to collaborate with teams across the company, from hardware to software, while leading a team of engineers responsible for factory platform development.\n\nZhang first joined NVIDIA as an intern in the summer of 2020, during the early stages of the COVID-19 pandemic. They were inspired by the way that NVIDIA quickly pivoted to offer remote internships, aiming to give interns the best possible experience in spite of the circumstances.\n\n\u201cI knew from then on that there was something special about this company,\u201d Zhang said. \u201cFirst and foremost, NVIDIA cares about us as human beings, not just the work we do.\u201d\n\nAfter spending their first summer with the hardware infrastructure team as a software tools architecture intern, Zhang returned for a second internship with the NVIDIA Clara team, working on an open-source tool that helped measure resource utilization of AI solutions for the healthcare industry.\n\nFollowing their graduation from the University of Waterloo with a bachelor\u2019s degree in biomedical engineering, Zhang joined NVIDIA full time in 2022 as part of the NVIDIA Holoscan team, working remotely from Waterloo, Canada.\n\nZhang and their team in 2023 partnered with the NVIDIA BioNeMo team and biopharmaceutical company Recursion to help launch the open-source Phenom-Beta model, which is used to embed cellular microscopy images and provide researchers with insights about cell function to aid drug discovery. The project and its positive impact on biomedical research made it some of Zhang\u2019s proudest work at NVIDIA.\n\nEver passionate about systems, Zhang views cross-team collaboration as a powerful engine for success. One of their favorite parts about working at NVIDIA is how open and collaborative the culture is.\n\n\u201cTogether, we need to systematically understand how teams work together, and why one team\u2019s piece of the puzzle is critical to the bigger picture,\u201d Zhang said. \u201cWhen people understand the \u2018why,\u2019 we can accomplish anything together \u2014 it\u2019s one of the things that makes NVIDIA so special.\u201d\n\nFollow @nvidialife on Instagram and learn more about NVIDIA life, culture and careers.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia Stock Rises. This Is When the Next Big AI Sales Boost Will Come.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-today-ai-chips-5e9f0797",
            "snippet": "Nvidia stock was rising early Friday. Data-center sales could disappoint in the first half of the year but that's not a reason to sell the stock,...",
            "score": 0.5904681086540222,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia Invests in These 2 Artificial Intelligence (AI) Stocks and Sells These 3 Others",
            "link": "https://www.nasdaq.com/articles/nvidia-invests-these-2-artificial-intelligence-ai-stocks-and-sells-these-3-others",
            "snippet": "In this video, I will talk about the three artificial intelligence stocks Nvidia (NASDAQ: NVDA) sold and the two it bought at the end of 2024.",
            "score": 0.9390670657157898,
            "sentiment": null,
            "probability": null,
            "content": "In this video, I will talk about the three artificial intelligence stocks Nvidia (NASDAQ: NVDA) sold and the two it bought at the end of 2024. Watch the short video to learn more, consider subscribing, and click the special offer link below.\n\n*Stock prices used were from the trading day of Feb. 14, 2025. The video was published on Feb. 14, 2025.\n\nWhere to invest $1,000 right now? Our analyst team just revealed what they believe are the 10 best stocks to buy right now. Learn More \u00bb\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nEver feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia: if you invested $1,000 when we doubled down in 2009, you\u2019d have $350,809 !*\n\nif you invested $1,000 when we doubled down in 2009, !* Apple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $45,792 !*\n\nif you invested $1,000 when we doubled down in 2008, !* Netflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $562,853!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nLearn more \u00bb\n\n*Stock Advisor returns as of February 3, 2025\n\nNeil Rozenbaum has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Nebius Group, Nvidia, and Serve Robotics. The Motley Fool has a disclosure policy. Neil is an affiliate of The Motley Fool and may be compensated for promoting its services. If you choose to subscribe through his link, he will earn some extra money that supports his channel. His opinions remain his own and are unaffected by The Motley Fool.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "SoundHound\u2019s stock sinks as Nvidia sells off stake. What comes next?",
            "link": "https://www.marketwatch.com/story/soundhounds-stock-sinks-as-nvidia-sells-off-stake-what-comes-next-3cc41e0d",
            "snippet": "Nvidia's latest 13-F filing also indicated that it sold its stake in Serve Robotics, and shares of the self-driving-technology company were also sliding...",
            "score": 0.5930373072624207,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia Announces Release Dates for RTX 5070\u2014and Really, It Could Be Worse",
            "link": "https://www.vice.com/en/article/nvidia-announces-release-dates-for-rtx-5070/",
            "snippet": "NVIDIA announces the GeForce RTX 5070 Ti for February 20 and the GeForce RTX 5070 for March 5, confirming rumors of a (slight) delay.",
            "score": 0.9269053339958191,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA took to X early in the morning of February 13 to announce that the GeForce RTX 5070 Ti would become available for sale at 9 PM EST on February 20, with the GeForce RTX 5070 following on March 5.\n\nIt\u2019s been nearly two years since its predecessor, the GeForce RTX 4070 series, launched. Not only do we finally get a successor, though, but both new cards are $50 cheaper than their respective ancestors were at the time of launch, too.\n\nVideos by VICE\n\nWho\u2019d have thought that when we can\u2019t get a cheap egg anymore, one of the only things that\u2019d be getting cheaper would be a pair of high-powered graphics cards?\n\nperformance comparison of rtx 5070 Ti vs last-generation rtx 4070 Ti \u2013 credit: nvidia/screenshot by matt jancer\n\nfortunately, no ominous rumors of low stock\n\nThe RTX 5070 will retail for $549, and the slightly higher spec RTX 5070 Ti will retail for $749, although with demand wildly outstripping supply of the already released RTX 5080 and RTX 5090, there\u2019s no guarantee retailers won\u2019t jack up the price beyond the suggested retail.\n\nLuckily for us, we haven\u2019t heard the kinds of warnings of low stock from retailers about the RTX 5070 that we heard before the launch of the other two graphics cards. When the RTX 4070 and RTX 4070 Ti launched in 2023, they were priced at $599 and $799, respectively.\n\nWhile there will be an NVIDIA-produced Founders Edition of the RTX 5070, there won\u2019t be one of the RTX 5070 Ti, a curious omission that means the only RTX 5070 Ti units you\u2019ll be able to buy will be those produced by third parties. And of course, you\u2019ll also be able to buy an RTX 5070 built by a third-party, too.\n\nperformance comparison of rtx 5070 vs last-generation rtx 4070 \u2013 credit: nvidia/screenshot by matt jancer\n\nwill the good news last?\n\nWith a self-inflicted slapfight trade war looming, who knows whether the welcome price drop and sufficient availability will last, though? In an exchange on X with the electronics retailer Newegg, a user asked \u201cDid the pricing go up on 5080s?\u201d\n\nThe retailer responded simply, \u201cYeah, tariffs.\u201d Then it later added, \u201cTo add clarity, some graphics card prices have increased due to a number of factors that are, unfortunately, out of our control.\n\n\u201cWe know the situation has been confusing and frustrating, but it\u2019s important to know we are doing everything in our power to bring stability to the situation. Additionally, we are also building a new drawing system for hot items, which we will provide more details on when available. We are constantly restocking cards, so be sure to follow us for the latest updates.\u201d\n\nAnd then it deleted those messages, leaving a gaping hole in the record, except for a slew of headlines such as \u201cNewegg Blames Tariffs for RTX 5090 and RTX 5080 Price Hikes, but Then Walks It Back.\u201d\n\nFingers crossed we don\u2019t have to add graphics cards to the list of screwovers we\u2019ll have to endure this year, right next to eggs.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Read This Before You Buy Nvidia Stock in February 2025",
            "link": "https://moneymorning.com/2025/02/14/read-this-before-you-buy-nvidia-stock-in-february-2025/",
            "snippet": "Nvidia's stock has slowed down significantly over the past few months and has slid into negative territory year-to-date. Should you still buy ahead of...",
            "score": 0.9488869905471802,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-13": {
        "0": {
            "title": "Avowed & Indiana Jones and the Great Circle\u2122 Game Ready Driver Released",
            "link": "https://www.nvidia.com/en-us/geforce/news/avowed-indiana-jones-great-circle-geforce-game-ready-driver/",
            "snippet": "Also includes Game Ready support for Sid Meier's Civilization VII, and Wuthering Waves' DLSS Frame Generation update.",
            "score": 0.9232457280158997,
            "sentiment": null,
            "probability": null,
            "content": "Our newest GeForce Game Ready Driver features day-one support for Avowed, Indiana Jones and the Great Circle\u2019s\u2122 DLSS 4 with Multi Frame Generation update, Sid Meier's Civilization VII, and Wuthering Waves\u2019 DLSS Frame Generation update.\n\nAdditionally, there\u2019s support for 5 new G-SYNC Compatible gaming displays, and 4 new Optimal Playable Settings profiles.\n\nDownload and install the new Game Ready Driver from the Drivers tab of NVIDIA app or GeForce.com, and read on to learn more.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia: GeForce RTX 5070 Ti Launches Feb. 20, But 5070 Delayed Until March",
            "link": "https://www.pcmag.com/news/nvidia-geforce-rtx-5070-ti-launches-feb-20-but-5070-delayed-until-march",
            "snippet": "GPU buyers, get ready for another product launch. Nvidia will start selling the GeForce RTX 5070 Ti on Feb. 20. Following leaks that also mentioned the Feb.",
            "score": 0.9123075008392334,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Chip-Demand Optimism Lifts Nvidia Stock",
            "link": "https://www.investopedia.com/chip-demand-optimism-lifts-nvidia-stock-11679413",
            "snippet": "Nvidia shares were up about 3% in intraday trading Thursday amid optimism about strong demand for its AI chips.",
            "score": 0.9200106263160706,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock was up about 3% in intraday trading Thursday amid optimism about strong demand for its artificial intelligence chips.\n\nThe stock, up less than 1% so far this year, has had a difficult start to 2025. Nvidia lost nearly $600 billion in market capitalization in a single day late last month when a low-cost AI model from Chinese start-up DeepSeek shocked Wall Street and raised doubts about future demand for Nvidia\u2019s most advanced (and expensive) chips.\n\nSome analysts were quick to call the DeepSeek sell-off overblown and reiterate their bullish views on Nvidia and other AI stocks. Global shipments of AI servers are expected to grow nearly 30% this year, according to research firm TrendForce. That would be a slowdown from the 46% growth recorded last year, but Nvidia is expected to remain the leader in the space.\n\nNvidia's Blackwell Products Expected To Fuel Demand\n\nHP Enterprise (HPE) on Thursday said it had shipped its first Nvidia Blackwell family product, an Nvidia GB200 NVL72. Blackwell, Nvidia\u2019s most sophisticated line of chips, has faced some setbacks but, as the most advanced offering on the market, is expected to be in demand this year.\n\nMorgan Stanley analysts, in a note late Wednesday, argued the market currently underappreciates the value of Nvidia\u2019s offering compared with nascent ASIC\u2014or custom silicon\u2014offerings.\n\nNvidia chips, they said, generally offer a better cost-performance ratio than custom silicon from the likes of Amazon (AMZN) and Google (GOOG). As such, their base case was that Nvidia would retain its dominant market share.\n\nThe greatest risk to the stock, they said, was the possibility DeepSeek would prompt the U.S. to enforce tighter semiconductor export controls.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "HPE Announces Shipment of Its First NVIDIA Grace Blackwell System",
            "link": "https://www.businesswire.com/news/home/20250213323241/en/HPE-Announces-Shipment-of-Its-First-NVIDIA-Grace-Blackwell-System",
            "snippet": "Hewlett Packard Enterprise announces its first shipment of NVIDIA GB200 NVL72 with direct liquid cooling.",
            "score": 0.8230352997779846,
            "sentiment": null,
            "probability": null,
            "content": "HOUSTON--(BUSINESS WIRE)--Hewlett Packard Enterprise (NYSE:HPE) announced today that it has shipped its first NVIDIA Blackwell family-based solution, the NVIDIA GB200 NVL72. This rack-scale system by HPE is designed to help service providers and large enterprises quickly deploy very large, complex AI clusters with advanced, direct liquid cooling solutions to optimize efficiency and performance.\n\n\u201cAI service providers and large enterprise model builders are under tremendous pressure to offer scalability, extreme performance, and fast time-to-deployment,\u201d said Trish Damkroger, senior vice president and general manager of HPC & AI Infrastructure Solutions, HPE. \u201cAs builders of the world\u2019s top three fastest systems with direct liquid cooling, HPE offers customers lower cost per token training and best-in-class performance with industry-leading services expertise.\u201d\n\nThe NVIDIA GB200 NVL72 features shared-memory, low-latency architecture with the latest GPU technology designed for extremely large AI models of over a trillion parameters, in one memory space. GB200 NVL72 offers seamless integration of NVIDIA CPUs, GPUs, compute and switch trays, networking, and software, bringing together extreme performance to address heavily parallelizable workloads, like generative AI (GenAI) model training and inferencing, along with NVIDIA software applications.\n\n\u201cEngineers, scientists and researchers need cutting-edge liquid cooling technology to keep up with increasing power and compute requirements,\u201d said Bob Pette, vice president of enterprise platforms at NVIDIA. \u201cBuilding on continued collaboration between HPE and NVIDIA, HPE\u2019s first shipment of NVIDIA GB200 NVL72 will help service providers and large enterprises efficiently build, deploy and scale large AI clusters.\u201d\n\nWith escalating power requirements and data center density dynamics, HPE has five decades of liquid cooling expertise that uniquely positions the company to help customers bring fast deployment and an extensive infrastructure support system for complex liquid-cooled environments. This experience has enabled HPE to deliver eight of the top 15 supercomputers on the Green500 list, which ranks the world\u2019s most energy-efficient supercomputers. HPE is recognized as a leader in direct liquid cooling technology, having built seven of the top 10 world\u2019s fastest supercomputers.\n\nFeatures of NVIDIA GB200 NVL72 by HPE:\n\n72 NVIDIA Blackwell GPUs and 36 NVIDIA Grace CPUs interconnected via high-speed NVIDIA NVLink\n\nUp to 13.5 TB total HBM3e memory with 576 TB/sec bandwidth\n\nHPE direct liquid cooling technology\n\nIndustry Leading Services and Support:\n\nHPE is able to deliver AI solutions at a global scale, with proven ability to support massive, custom AI clusters with superior serviceability including expert on-site support, customized services, sustainability services and more. HPC & AI Custom Support Services are tailored to meet customer needs. With several levels of SLA coverage, HPE provides enhanced incident management with proactive support through dedicated remote engineers, ensuring rapid installation and faster time-to-value. Available services include:\n\nOnsite engineering resources: Comprehensive on-site support through highly trained resident engineers who work closely with a customer\u2019s IT teams to ensure optimal system performance and availability.\n\nPerformance and benchmarking engagements: Industry-leading team of experts to fine tune solutions throughout the life of a system.\n\nSustainability services: Energy and emissions reporting, sustainability workshops, and resource monitoring to reduce environmental impact.\n\nThe newly shipped NVIDIA GB200 NVL72 by HPE is one of a wide array of high-performance computing and supercomputing systems that address every use case for GenAI, scientific discovery, and other compute-intensive workloads. Learn more about our compute and supercomputing systems and other solutions in the NVIDIA AI Computing by HPE portfolio.\n\nAbout Hewlett Packard Enterprise\n\nHewlett Packard Enterprise (NYSE: HPE) is a global technology leader focused on developing intelligent solutions that allow customers to capture, analyze, and act upon data seamlessly. The company innovates across networking, hybrid cloud, and AI to help customers develop new business models, engage in new ways, and increase operational performance. For more information, visit: www.hpe.com.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Is NVIDIA Corporation (NVDA) the Best Major Stock to Buy According to Hedge Funds?",
            "link": "https://finance.yahoo.com/news/nvidia-corporation-nvda-best-major-195833394.html",
            "snippet": "We recently published a list of 10 Best Major Stocks to Buy According to Hedge Funds. In this article, we are going to take a look at where NVIDIA...",
            "score": 0.9101999998092651,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of 10 Best Major Stocks to Buy According to Hedge Funds. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against other best major stocks to buy according to hedge funds.\n\nAfter a notable growth of 25% for the broader market in 2024, the S&P 500 index gained a little under 4% in the first month of 2025. The stock market had a mixed start to 2025 after the U.S. tech companies took a massive hit from the launch of Deepseek\u2019s R1 AI model. Despite a major blow, the tech-heavy NASDAQ 100 index has gained over 4.50% year-to-date.\n\nREAD ALSO: 10 Large-Cap Stocks Insiders Are Selling Recently\n\nInflation and Tariff Concerns\n\nU.S. stocks fell on February 12 following the release of January inflation data. The consumer-price index (CPI) soared 3% in January from a year ago, exceeding economists\u2019 estimates. The data has somewhat stoked investors\u2019 concerns about price pressures and the worry that interest rates might not come down as expected.\n\nJanuary CPI usually indicates big price adjustments made by businesses at the start of the year. Moreover, the beginning of a new administration has an impact on businesses. According to Goldman Sachs Research\u2019s chief US equity strategist David Kostin, every five-percentage-point increase in the US tariff rate is estimated to reduce S&P 500 EPS by roughly 1-2%.\n\nTherefore, if the U.S. administration sustains the proposed tariff rates, a 25% tariff on imported goods from Mexico and Canada and an additional 10% tariff on imports from China would reduce S&P 500 EPS forecasts by nearly 2-3%, as per Goldman\u2019s Research.\n\nHowever, the tariff policy doesn\u2019t slow down the AI investment by the U.S. tech giants as they continue to expand their AI-related services and products. Four out of the Big Five companies are projected to invest over $300 billion in 2025 building data centers to fuel the AI boom.\n\nOur Methodology\n\nWe have listed the top 10 best major stocks based on hedge fund sentiment, according to Insider Monkey\u2019s database. The best major stocks are ranked in ascending order of the number of hedge fund holders, as of Q3 2024.\n\nWhy do we care about what hedge funds do? The reason is simple: our research has shown that we can outperform the market by imitating the top stock picks of the best hedge funds. Our quarterly newsletter\u2019s strategy selects 14 small-cap and large-cap stocks every quarter and has returned 275% since May 2014, beating its benchmark by 150 percentage points (see more details here).\n\nIs NVIDIA Corporation (NVDA) the Best Major Stock to Buy According to Hedge Funds?\n\nA close-up of a colorful high-end graphics card being plugged in to a gaming computer.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia investors should brace for some Blackwell \u2018growing pains\u2019 to show up with earnings report",
            "link": "https://www.marketwatch.com/story/ahead-of-nvidia-earnings-brace-for-some-blackwell-growing-pains-be169388",
            "snippet": "A Mizuho analyst sees \u201cgrowing pains\u201d ahead for Nvidia Corp. as the company pursues the big opportunities afforded by its new Blackwell graphics processing...",
            "score": 0.786901593208313,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia delays the RTX 5070 till after AMD\u2019s reveal",
            "link": "https://www.theverge.com/news/612650/nvidia-delays-rtx-5070-after-amd-9070",
            "snippet": "As always, the most important Nvidia graphics card is the one you can actually buy, and Nvidia's talked a big game for its RTX 5070, making the dubious but...",
            "score": 0.8921810388565063,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and founding member of The Verge who covers gadgets, games, and toys. He spent 15 years editing the likes of CNET, Gizmodo, and Engadget.\n\nAs always, the most important Nvidia graphics card is the one you can actually buy, and Nvidia\u2019s talked a big game for its RTX 5070, making the dubious but nuanced claim it can deliver RTX 4090 performance for just $549. On February 28th, AMD will get its chance to intercept with the Radeon RX 9070 and 9070 XT, in a streaming event it just announced today. But Nvidia has now made its own wiggle room, delaying the launch of the RTX 5070 from February to March 5th, its product page reveals today.\n\nNvidia will ship its $749 RTX 5070 Ti ahead of AMD\u2019s event, though, on February 20th, a week from today.\n\nAMD has telegraphed that it won\u2019t be competing with Nvidia\u2019s latest and greatest cards, so price is the one big lever that AMD can potentially pull in order to compete. (The AMD Radeon 9070 cards appeared to be targeting Nvidia 4070 Ti and 4070 Super levels of performance, not necessarily higher.) But Nvidia, a company that can now make $20 billion in pure profit in a single quarter, could theoretically counter that if it feels it needs to, and now has more room to do so.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Is DeepSeek's Breakthrough Really a Disaster For Nvidia Stock?",
            "link": "https://www.fool.com/investing/2025/02/13/is-deepseeks-breakthrough-a-disaster-for-nvidia/",
            "snippet": "We are constantly reminded not to get too comfortable in the world of investing. When things seem to chug along on autopilot, an innovation shakes things up...",
            "score": 0.7126988768577576,
            "sentiment": null,
            "probability": null,
            "content": "We are constantly reminded not to get too comfortable in the world of investing. When things seem to chug along on autopilot, an innovation shakes things up. A recent example is the late 2022 release of the original ChatGPT, which thrust artificial intelligence (AI) into the forefront and set off a race in an industry worth trillions of dollars. A breakthrough from a Chinese company called DeepSeek may be shaking things up again (or there may be more to the story).\n\nThere are several layers to this onion. Here's what to know.\n\nWhat is DeepSeek?\n\nDeepSeek is a Chinese tech company that created DeepSeek-R1 to compete with ChatGPT-4 and other large language models (LLMs), like Alphabet's (GOOG 1.75%) (GOOGL 1.68%) Google Gemini and Llama 3 created by Meta Platforms (META 2.96%). But that isn't the headline-inspiring story. DeepSeek \"trained\" its model with $6 million and just 2,000 somewhat outdated Nvidia (NVDA 5.27%) graphics processing units (GPUs). This is a startling claim when competing programs reportedly cost hundreds of millions of dollars and many thousands of top-shelf GPUs. For example, xAI's Colossus uses 200,000 GPUs, with plans to expand to 1 million.\n\nThe news crushed Nvidia's stock. It was down more than 20% from its recent all-time high at one point, as investors worried about what this breakthrough would do to GPU demand.\n\nBut is this breakthrough exactly what it seems? Maybe not. Industry experts have said they believe the actual cost of DeepSeek-R1 is $1.6 billion and that the company has 50,000 Nvidia GPUs. DeepSeek may have exaggerated its triumph because of U.S. export controls on high-powered GPUs, preferring to avoid the ire of regulators, or perhaps to garner more attention. Either way, it doesn't look like the U.S. tech giants will stop buying thousands of Nvidia GPUs anytime soon.\n\nAlphabet and Amazon (AMZN 2.09%) announced massive 2025 budgets for capital expenditures (CapEx) on their recent earnings calls. Amazon spent $26 billion in the fourth quarter and expects this to continue in 2025, while Alphabet dished out $14 billion with plans to spend $75 billion in 2025. Much of this will go toward data centers, servers, and GPUs.\n\nIs Nvidia stock a buy now?\n\nIt was going to be difficult to top Nvidia's incredibly successful fiscal 2024, which featured $61 billion in sales on 126% growth. This was led by staggering growth in data center sales that hit $48 billion on 217% growth. However, fiscal 2025 is also incredible. Through three quarters, sales are $91 billion, led by another massive increase in the data center segment.\n\nEven better, operating income through the third quarter of fiscal 2025 is $61 billion. The 67% operating margin, versus 61% in fiscal 2024, shows that demand is accelerating as customers are willing to pay steep prices to obtain Nvidia products.\n\nRather than a disaster, the drop in Nvidia stock caused by DeepSeek looks like a terrific opportunity for long-term investors. As shown below, the forward price-to-earnings (P/E) ratio dropped sharply along with the stock price.\n\nAs you can see on the chart, the sudden drop in valuation isn't unique. However, it is rare, having happened only once in 2023 and 2024. Both times were excellent opportunities for investors to buy the stock. With DeepSeek's claims in question, Big Tech confirming that capital investments will be robust, and Nvidia's spectacular results and lower-than-usual valuation, long-term investors should consider purchasing the stock now.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "New Data Show Strong Chip Demand. Nvidia Stock Rises.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-ai-chips-c6eba0fc",
            "snippet": "Nvidia stock is set to benefit from strong growth in AI servers but the use of in-house chips is a threat to keep an eye on.",
            "score": 0.7932266592979431,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "Create the future with AI: Join Microsoft at NVIDIA GTC",
            "link": "https://venturebeat.com/ai/create-the-future-with-ai-join-microsoft-at-nvidia-gtc/",
            "snippet": "NVIDIA GTC is the best opportunity to explore the future of AI, and the powerful partnership of Microsoft Azure and NVIDIA AI solutions.",
            "score": 0.9312502145767212,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-12": {
        "0": {
            "title": "Safety First: Leading Partners Adopt NVIDIA Cybersecurity AI to Safeguard Critical Infrastructure",
            "link": "https://blogs.nvidia.com/blog/cybersecurity-ai-critical-infrastructure/",
            "snippet": "Leading OT providers today showcased how they're adopting the NVIDIA cybersecurity AI platform for real-time threat detection and protection.",
            "score": 0.5775058269500732,
            "sentiment": null,
            "probability": null,
            "content": "The rapid evolution of generative AI has created countless opportunities for innovation across industry and research. As is often the case with state-of-the-art technology, this evolution has also shifted the landscape of cybersecurity threats, creating new security requirements. Critical infrastructure cybersecurity is advancing to thwart the next wave of emerging threats in the AI era.\n\nLeading operational technology (OT) providers today showcased at the S4 conference for industrial control systems (ICS) and OT cybersecurity how they\u2019re adopting the NVIDIA cybersecurity AI platform to deliver real-time threat detection and critical infrastructure protection.\n\nArmis, Check Point, CrowdStrike, Deloitte and World Wide Technology (WWT) are integrating the platform to help customers bolster critical infrastructure, such as energy, utilities and manufacturing facilities, against cyber threats.\n\nCritical infrastructure operates in highly complex environments, where the convergence of IT and OT, often accelerated by digital transformation, creates a perfect storm of vulnerabilities. Traditional cybersecurity measures are no longer sufficient to address these emerging threats.\n\nBy harnessing NVIDIA\u2019s cybersecurity AI platform, these partners can provide exceptional visibility into critical infrastructure environments, achieving robust and adaptive security while delivering operational continuity.\n\nThe platform integrates NVIDIA\u2019s accelerated computing and AI, featuring NVIDIA BlueField-3 DPUs, NVIDIA DOCA and the NVIDIA Morpheus AI cybersecurity framework, part of the NVIDIA AI Enterprise. This combination enables real-time threat detection, empowering cybersecurity professionals to respond swiftly at the edge and across networks.\n\nUnlike conventional solutions that depend on intrusive methods or software agents, BlueField-3 DPUs function as a virtual security overlay. They inspect network traffic and safeguard host integrity without disrupting operations. Acting as embedded sensors within each server, they stream telemetry data to NVIDIA Morpheus, enabling detailed monitoring of host activities, network traffic and application behaviors \u2014 seamlessly and without operational impact.\n\nDriving Cybersecurity Innovation Across Industries\n\nIntegrating Armis Centrix, Armis\u2019 AI-powered cyber exposure management platform, with NVIDIA cybersecurity AI helps secure critical infrastructure like energy, manufacturing, healthcare and transportation.\n\n\u201cOT environments are increasingly targeted by sophisticated cyber threats, requiring robust solutions that ensure both security and operational continuity,\u201d said Nadir Izrael, chief technology officer and cofounder of Armis. \u201cCombining Armis\u2019 unmatched platform for OT security and cyber exposure management with NVIDIA BlueField-3 DPUs enables organizations to comprehensively protect cyber-physical systems without disrupting operations.\u201d\n\nCrowdStrike is helping secure critical infrastructure such as ICS and OT by deploying its CrowdStrike Falcon security agent on BlueField-3 DPUs to boost real-time AI-powered threat detection and response.\n\n\u201cOT environments are under increasing threat, demanding AI-powered security that adapts in real time,\u201d said Raj Rajamani, head of products at CrowdStrike. \u201cBy integrating NVIDIA BlueField-3 DPUs with the CrowdStrike Falcon platform, we\u2019re extending industry-leading protection to critical infrastructure without disrupting operations \u2014 delivering unified protection at the edge and helping organizations stay ahead of modern threats.\u201d\n\nDeloitte is driving customers\u2019 digital transformation, enabled by NVIDIA\u2019s cybersecurity AI platform, to help meet the demands of breakthrough technologies that require real-time, granular visibility into data center networks to defend against increasingly sophisticated threats.\n\n\u201cProtecting OT and ICS systems is becoming increasingly challenging as organizations embrace digital transformation and interconnected technologies,\u201d said Dmitry Dudorov, an AI security leader at Deloitte U.K. \u201cHarnessing NVIDIA\u2019s cybersecurity AI platform can enable organizations to determine threat detection, enhance resilience and safeguard their infrastructure to accelerate their efforts.\u201d\n\nA Safer Future, Powered by AI\n\nNVIDIA\u2019s cybersecurity AI platform, combined with the expertise of ecosystem partners, offers a powerful and scalable solution to protect critical infrastructure environments against evolving threats. Bringing NVIDIA AI and accelerated computing to the forefront of OT security can help organizations protect what matters most \u2014 now and in the future.\n\nLearn more by attending the NVIDIA GTC global AI conference, running March 17-21, where Armis, Check Point and CrowdStrike cybersecurity leaders will host sessions about their collaborations with NVIDIA.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "Nvidia Stock Warning: Trump Could Further Curb Nvidia\u2019s Chip Sales",
            "link": "https://www.barchart.com/story/news/30902292/nvidia-stock-warning-trump-could-further-curb-nvidias-chip-sales",
            "snippet": "Down 11% from all-time highs, Nvidia stock trades at a reasonable valuation in 2025. However, the chip maker continues to wrestle with multiple headwinds...",
            "score": 0.9699532985687256,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Researchers Find New Exploit Bypassing Patched NVIDIA Container Toolkit Vulnerability",
            "link": "https://thehackernews.com/2025/02/researchers-find-new-exploit-bypassing.html",
            "snippet": "CVE-2025-23359 (CVSS 8.3) lets attackers bypass NVIDIA's container security, gaining host access via a TOCTOU flaw. Fix available in v1.17.4.",
            "score": 0.5287162661552429,
            "sentiment": null,
            "probability": null,
            "content": "Cybersecurity researchers have discovered a bypass for a now-patched security vulnerability in the NVIDIA Container Toolkit that could be exploited to break out of a container's isolation protections and gain complete access to the underlying host.\n\nThe new vulnerability is being tracked as CVE-2025-23359 (CVSS score: 8.3). It affects the following versions -\n\nNVIDIA Container Toolkit (All versions up to and including 1.17.3) - Fixed in version 1.17.4\n\nNVIDIA GPU Operator (All versions up to and including 24.9.1) - Fixed in version 24.9.2\n\n\"NVIDIA Container Toolkit for Linux contains a Time-of-Check Time-of-Use (TOCTOU) vulnerability when used with default configuration, where a crafted container image could gain access to the host file system,\" the company said in an advisory on Tuesday.\n\n\"A successful exploit of this vulnerability might lead to code execution, denial of service, escalation of privileges, information disclosure, and data tampering.\"\n\nCloud security firm Wiz, which shared additional technical specifics of the flaw, said it's a bypass for another vulnerability (CVE-2024-0132, CVSS score: 9.0) that was addressed by NVIDIA in September 2024.\n\nIn a nutshell, the vulnerability enables bad actors to mount the host's root file system into a container, granting them unfettered access to all files. Furthermore, the access can be leveraged to launch privileged containers and achieve full host compromise via the runtime Unix socket.\n\nWiz researchers security researchers Shir Tamari, Ronen Shustin, and Andres Riancho said their source code analysis of the container toolkit found that the file paths used during mount operations could be manipulated using a symbolic link such that it makes it possible to mount from outside the container (i.e., the root directory) into a path within \"/usr/lib64.\"\n\nWhile the access to the host file system afforded by the container escape is read-only, this limitation can be circumvented by interacting with the Unix sockets to spawn new privileged containers and gain unrestricted access to the file system.\n\n\"This elevated level of access also allowed us to monitor network traffic, debug active processes, and perform a range of other host-level operations,\" the researchers said.\n\nBesides updating to the latest version, users of the NVIDIA Container Toolkit are recommended to not disable the \"--no-cntlibs\" flag in production environments.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "What's Going On With Nvidia Stock?",
            "link": "https://www.fool.com/investing/2025/02/12/whats-going-on-with-nvidia-stock/",
            "snippet": "In this video, I will talk about recent updates regarding Nvidia (NVDA 2.63%). Watch the short video to learn more, consider subscribing, and click the...",
            "score": 0.6424580216407776,
            "sentiment": null,
            "probability": null,
            "content": "In this video, I will talk about recent updates regarding Nvidia (NVDA 5.27%). Watch the short video to learn more, consider subscribing, and click the special offer link below.\n\n*Stock prices used were from the trading day of Feb. 7, 2025. The video was published on Feb. 10, 2025.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia Shares No Longer Bulletproof as DeepSeek Fears Linger",
            "link": "https://finance.yahoo.com/news/nvidia-shares-no-longer-bulletproof-121436604.html",
            "snippet": "(Bloomberg) -- Nvidia Corp. investors have typically rushed to buy the stock on any dips. But the mood since the DeepSeek-driven rout has been different,...",
            "score": 0.6929557919502258,
            "sentiment": null,
            "probability": null,
            "content": "(Bloomberg) -- Nvidia Corp. investors have typically rushed to buy the stock on any dips. But the mood since the DeepSeek-driven rout has been different, signaling that fears of a slowdown in AI spending aren\u2019t going away.\n\nMost Read from Bloomberg\n\nNvidia shares slumped 17% in a single day, erasing about $590 billion from the company\u2019s market capitalization, after the Chinese AI startup claimed high performance at a lower cost. The stock has since regained some ground, but it\u2019s still more than 12% below its January record high. That\u2019s despite key customers Amazon.com Inc., Alphabet Inc., Meta Platforms Inc. and Microsoft Corp. planning a combined $300 billion in capital expenditures this year.\n\nDip buyers didn\u2019t step in until Nvidia shares had fallen more than 21% from their peak, a phenomenon that\u2019s happened only a handful of times in recent years. It points to increasing investor caution about AI spending \u2014 especially because DeepSeek claimed to use fewer chips for its AI model.\n\n\u201cThere\u2019s this underlying concern about when the party is going to end and I think DeepSeek was a wake up call that that may come faster than people think,\u201d said Gene Munster, managing partner and cofounder of Deepwater Asset Management. \u201cThe psychology within a day shifted from being essentially an impenetrable, bulletproof story to one that can viciously change.\u201d\n\nNvidia shares fell as much as 2.8% in early trading Wednesday.\n\nThe negative sentiment has created a very different setup heading into Nvidia\u2019s earnings, due on Feb. 26. Almost every quarterly report in the last two years has been positively anticipated, with shares trading at or near record highs ahead of the results. This time, the company needs to convince investors who may have started to doubt how much further the stock can run.\n\nRead: Big Tech\u2019s Grip on Market Shows Cracks as Earnings Fall Flat\n\n\u201cThe negative stock reaction has become the story, and in many ways frames the biggest risk from here,\u201d Morgan Stanley analysts led by Joseph Moore wrote in a note last week. With investor sentiment having soured, they wrote that \u201cthe cynicism is overwhelming. It remains to be seen if revenue acceleration can mitigate that concern; we think that it can, but it remains a debate.\u201d\n\nThe wobble in investor confidence is occurring as Nvidia comes up against past quarters where it saw exponential growth, making year-over-year comparisons difficult to top. The company is expected to report revenue growth of 73%, down from 94% last quarter and significantly lower than the 265% growth in the same quarter last year, according to estimates compiled by Bloomberg.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Three reasons Nvidia\u2019s stock is still compelling, and why it could rise 30% in the next year",
            "link": "https://www.marketwatch.com/story/three-reasons-nvidias-stock-is-still-compelling-and-why-it-could-rise-30-in-the-next-year-ea639ffe",
            "snippet": "A year ago, a screen of the S&P 500 placed Nvidia Corp. at the top of the list of companies that were expected to show the most rapid increases in revenue...",
            "score": 0.8825812935829163,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "CrowdStrike, WWT And Others Tap Nvidia's Cybersecurity AI Platform To Shield OT Systems",
            "link": "https://www.crn.com/news/security/2025/wwt-crowdstrike-and-others-tap-nvidia-s-cybersecurity-ai-platform-to-shield-ot-systems",
            "snippet": "Nvidia said its cybersecurity AI platform has been adopted by World Wide Technology, CrowdStrike, Check Point and Armis to protect critical infrastructure.",
            "score": 0.6916128396987915,
            "sentiment": null,
            "probability": null,
            "content": "CrowdStrike, WWT And Others Tap Nvidia's Cybersecurity AI Platform To Shield OT Systems\n\nAccording to Nvidia, what makes its cybersecurity AI platform different from conventional offerings is that instead of relying on \u2018intrusive methods or software agents\u2019 to protect against threats, the platform\u2019s BlueField-3 data processing unit acts as a \u2018virtual security overlay.\u2019\n\nNvidia said its cybersecurity AI platform has been adopted by solution provider powerhouse World Wide Technology along with security trailblazers CrowdStrike, Check Point Software Technologies and Armis to protect critical infrastructure.\n\nThe AI computing giant said that the partners were expected to present Wednesday at the S4 conference how they are taking advantage of the Nvidia cybersecurity AI platform to \u201cdeliver real-time threat detection and critical infrastructure protection.\u201d\n\n[Related: Nvidia Seeks To Tap Into Global Real Estate Firm\u2019s Data Center Business]\n\nCritical infrastructure in this case ranges from industrial control systems (ICS) to operational technologies (OT) that are increasingly intersecting with IT systems.\n\nWWT, CrowdStrike, Check Point and Armis are helping energy, utility and manufacturing customers protect against cyber threats with the Nvidia cybersecurity AI platform, which combines Nvidia\u2019s BlueField-3 data processing units (DPUs) with its DOCA programming framework and Nvidia Morpheus AI cybersecurity framework, Nvidia said.\n\nAccording to Nvidia, what makes its cybersecurity AI platform different from conventional offerings is that instead of relying on \u201cintrusive methods or software agents\u201d to protect against threats, the BlueField-3 DPU acts as a \u201cvirtual security overlay.\u201d\n\n\u201cThey inspect network traffic and safeguard host integrity without disrupting operations. Acting as embedded sensors within each server, they stream telemetry data to Nvidia Morpheus, enabling detailed monitoring of host activities, network traffic and application behaviors\u2014seamlessly and without operational impact,\u201d Nvidia wrote in a blog post.\n\nArmis is integrating Nvidia\u2019s cybersecurity AI platform into Armis Centrix, a cyber exposure management platform that is designed to protect critical infrastructure in industries like energy, manufacturing, health care and transportation, according to Nvidia.\n\n\u201cOT environments are increasingly targeted by sophisticated cyber threats, requiring robust solutions that ensure both security and operational continuity,\u201d said Nadir Izrael, co-founder and CTO of Armis, in a statement.\n\n\u201cCombining Armis\u2019 unmatched platform for OT security and cyber exposure management with Nvidia BlueField-3 DPUs enables organizations to comprehensively protect cyber-physical systems without disrupting operations,\u201d he added.\n\nCrowdStrike, on the other hand, plans to deploy its CrowdStrike Falcon security agent on BlueField-3 DPUs \u201cto boost real-time AI-powered threat detection and response.\u201d\n\n\"By integrating Nvidia BlueField-3 DPUs with the CrowdStrike Falcon platform, we\u2019re extending industry-leading protection to critical infrastructure without disrupting operations\u2014delivering unified protection at the edge and helping organizations stay ahead of modern threats,\u201d said Raj Rajamani, head of products at CrowdStrike, in a statement.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Armis to Protect Cyber-Physical Systems with NVIDIA Cybersecurity AI",
            "link": "https://www.businesswire.com/news/home/20250212724803/en/Armis-to-Protect-Cyber-Physical-Systems-with-NVIDIA-Cybersecurity-AI",
            "snippet": "Armis, the cyber exposure management & security company, today announced that its Armis Centrix\u2122 platform will be enabled by NVIDIA BlueField-3 da.",
            "score": 0.8711539506912231,
            "sentiment": null,
            "probability": null,
            "content": "SAN FRANCISCO--(BUSINESS WIRE)--Armis, the cyber exposure management & security company, today announced that its Armis Centrix\u2122 platform will be enabled by NVIDIA BlueField-3 data processing units (DPUs) and, in the future, the NVIDIA Morpheus cybersecurity AI framework to comprehensively protect critical infrastructure while maintaining operational continuity.\n\nThese integrations provide a secure, isolated environment for critical infrastructure sectors without impacting performance or operations. Armis Centrix\u2122, Armis\u2019 cyber exposure management platform, with NVIDIA BlueField-3 DPUs enables customers to safeguard critical OT systems at both the host and network level.\n\n\u201cFrom the convergence of IT and OT environments to sophisticated cyber threats targeting cyber-physical systems, organizations face growing challenges in securing attack surfaces and managing their cyber risk exposure,\u201d said Nadir Izrael, CTO and Co-Founder at Armis. \u201cThis integration with NVIDIA technologies extends the reach of Armis\u2019 comprehensive, best-in-class platform for OT security and cyber exposure management so that more organizations can see, protect and manage their critical assets with operational resiliency.\u201d\n\nArmis Centrix\u2122 deployed on NVIDIA BlueField-3 DPUs fortifies security from the ground to the cloud by leveraging Armis\u2019 AI-powered asset intelligence to detect and mitigate threats in real time. This proactive approach to reducing risk keeps systems functional and secure, preserving business continuity and operational integrity across all asset types.\n\nWith its seamless deployment, the integration of Armis\u2019 capabilities into the NVIDIA BlueField-3 DPU platform allows for compatibility and scalability across a wide range of OT environments without compromising performance or resiliency.\n\n\u201cSecuring critical infrastructure amid today\u2019s dynamic threat landscape is more important than ever,\u201d said Ofir Arkin, Senior Distinguished Architect for Cybersecurity at NVIDIA. \u201cThe NVIDIA cybersecurity AI platform allows innovators like Armis to develop breakthrough technologies for cyber exposure management and security, delivering a powerful, scalable and flexible solution tailored to the needs of modern OT environments.\u201d\n\nRead more about Armis\u2019 integration with the NVIDIA cybersecurity AI platform here.\n\nArmis is showcasing its Armis Centrix\u2122 platform this week at S4x25 in Tampa, Florida. Learn more here and to book a meeting with Armis executives.\n\nAbout Armis\n\nArmis, the cyber exposure management & security company, protects the entire attack surface and manages the organization\u2019s cyber risk exposure in real time. In a rapidly evolving, perimeter-less world Armis ensures that organizations continuously see, protect and manage all critical assets \u2013 from the ground to the cloud. Armis secures Fortune 100, 200 and 500 companies as well as national governments, state and local entities to help keep critical infrastructure, economies and society stay safe and secure 24/7. Armis is a privately held company headquartered in California.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia Shares No Longer Bulletproof as DeepSeek Fears Linger",
            "link": "https://www.bloomberg.com/news/articles/2025-02-12/nvidia-shares-no-longer-bulletproof-as-deepseek-fears-linger",
            "snippet": "Nvidia Corp. investors have typically rushed to buy the stock on any dips. But the mood since the DeepSeek-driven rout has been different, signaling that...",
            "score": 0.6929557919502258,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "What's Going On With Nvidia Stock?",
            "link": "https://www.nasdaq.com/articles/whats-going-nvidia-stock",
            "snippet": "In this video, I will talk about recent updates regarding Nvidia (NASDAQ: NVDA). Watch the short video to learn more, consider subscribing, and click the...",
            "score": 0.6424580216407776,
            "sentiment": null,
            "probability": null,
            "content": "In this video, I will talk about recent updates regarding Nvidia (NASDAQ: NVDA). Watch the short video to learn more, consider subscribing, and click the special offer link below.\n\n*Stock prices used were from the trading day of Feb. 7, 2025. The video was published on Feb. 10, 2025.\n\nWhere to invest $1,000 right now? Our analyst team just revealed what they believe are the 10 best stocks to buy right now. Learn More \u00bb\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nEver feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia: if you invested $1,000 when we doubled down in 2009, you\u2019d have $344,352 !*\n\nif you invested $1,000 when we doubled down in 2009, !* Apple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $44,103 !*\n\nif you invested $1,000 when we doubled down in 2008, !* Netflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $543,649!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nLearn more \u00bb\n\n*Stock Advisor returns as of February 3, 2025\n\nNeil Rozenbaum has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Nvidia. The Motley Fool has a disclosure policy. Neil is an affiliate of The Motley Fool and may be compensated for promoting its services. If you choose to subscribe through his link, he will earn some extra money that supports his channel. His opinions remain his own and are unaffected by The Motley Fool.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-11": {
        "0": {
            "title": "What Are Foundation Models?",
            "link": "https://blogs.nvidia.com/blog/what-are-foundation-models/",
            "snippet": "Foundation models are AI neural networks trained on massive unlabeled datasets to handle a wide variety of jobs from translating text to analyzing medical...",
            "score": 0.9232577085494995,
            "sentiment": null,
            "probability": null,
            "content": "Foundation models are AI neural networks trained on massive unlabeled datasets to handle a wide variety of jobs from translating text to analyzing medical images.\n\nEditor\u2019s note: This article, originally published on March 13, 2023, has been updated.\n\nThe mics were live and tape was rolling in the studio where the Miles Davis Quintet was recording dozens of tunes in 1956 for Prestige Records.\n\nWhen an engineer asked for the next song\u2019s title, Davis shot back, \u201cI\u2019ll play it, and tell you what it is later.\u201d\n\nLike the prolific jazz trumpeter and composer, researchers have been generating AI models at a feverish pace, exploring new architectures and use cases. According to the 2024 AI Index report from the Stanford Institute for Human-Centered Artificial Intelligence, 149 foundation models were published in 2023, more than double the number released in 2022.\n\nThey said transformer models, large language models (LLMs), vision language models (VLMs) and other neural networks still being built are part of an important new category they dubbed foundation models.\n\nFoundation Models Defined\n\nA foundation model is an AI neural network \u2014 trained on mountains of raw data, generally with unsupervised learning \u2014 that can be adapted to accomplish a broad range of tasks.\n\nTwo important concepts help define this umbrella category: Data gathering is easier, and opportunities are as wide as the horizon.\n\nNo Labels, Lots of Opportunity\n\nFoundation models generally learn from unlabeled datasets, saving the time and expense of manually describing each item in massive collections.\n\nEarlier neural networks were narrowly tuned for specific tasks. With a little fine-tuning, foundation models can handle jobs from translating text to analyzing medical images to performing agent-based behaviors.\n\n\u201cI think we\u2019ve uncovered a very small fraction of the capabilities of existing foundation models, let alone future ones,\u201d said Percy Liang, the center\u2019s director, in the opening talk of the first workshop on foundation models.\n\nAI\u2019s Emergence and Homogenization\n\nIn that talk, Liang coined two terms to describe foundation models:\n\nEmergence refers to AI features still being discovered, such as the many nascent skills in foundation models. He calls the blending of AI algorithms and model architectures homogenization, a trend that helped form foundation models. (See chart below.)\n\nThe field continues to move fast.\n\nA year after the group defined foundation models, other tech watchers coined a related term \u2014 generative AI. It\u2019s an umbrella term for transformers, large language models, diffusion models and other neural networks capturing people\u2019s imaginations because they can create text, images, music, software, videos and more.\n\nGenerative AI has the potential to yield trillions of dollars of economic value, said executives from the venture firm Sequoia Capital who shared their views in a recent AI Podcast.\n\nA Brief History of Foundation Models\n\n\u201cWe are in a time where simple methods like neural networks are giving us an explosion of new capabilities,\u201d said Ashish Vaswani, an entrepreneur and former senior staff research scientist at Google Brain who led work on the seminal 2017 paper on transformers.\n\nThat work inspired researchers who created BERT and other large language models, making 2018 \u201ca watershed moment\u201d for natural language processing, a report on AI said at the end of that year.\n\nGoogle released BERT as open-source software, spawning a family of follow-ons and setting off a race to build ever larger, more powerful LLMs. Then it applied the technology to its search engine so users could ask questions in simple sentences.\n\nIn 2020, researchers at OpenAI announced another landmark transformer, GPT-3. Within weeks, people were using it to create poems, programs, songs, websites and more.\n\n\u201cLanguage models have a wide range of beneficial applications for society,\u201d the researchers wrote.\n\nTheir work also showed how large and compute-intensive these models can be. GPT-3 was trained on a dataset with nearly a trillion words, and it sports a whopping 175 billion parameters, a key measure of the power and complexity of neural networks. In 2024, Google released Gemini Ultra, a state-of-the-art foundation model that requires 50 billion petaflops.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "NVIDIA AI vulnerability: Deep Dive into CVE 2024-0132",
            "link": "https://www.wiz.io/blog/nvidia-ai-vulnerability-deep-dive-cve-2024-0132",
            "snippet": "Technical details on a critical severity vulnerability (CVE-2024-0132) in NVIDIA Container Toolkit and GPU Operator, affecting cloud service providers .",
            "score": 0.8927385807037354,
            "sentiment": null,
            "probability": null,
            "content": "Executive summary\n\nIn September of last year, Wiz Research uncovered a critical security vulnerability, tracked as CVE-2024-0132, in the widely used NVIDIA Container Toolkit, which provides containerized AI applications with access to GPU resources. Our initial blog post was purposely vague because the vulnerability was under embargo for an extended period, allowing both NVIDIA and cloud providers to address the issue. As we detailed in our initial blog post, this vulnerability affects any AI application\u2014whether in the cloud or on-premises\u2014that is running the vulnerable container toolkit. Today, we are ready to release the technical details of the vulnerability.\n\nThe vulnerability enables attackers who control a container image executed by the vulnerable toolkit to escape from the container\u2019s isolation and gain full access to the underlying host, posing a serious risk to sensitive data and infrastructure.\n\nWe withheld specific technical details of the vulnerability because the NVIDIA PSIRT team identified that the original patch did not fully resolve the issue. We worked closely with the NVIDIA team to ensure proper mitigation of both the original vulnerability and the bypass. The bypass is tracked under a separate CVE, CVE-2025-23359. We strongly encourage everyone to update to the latest version of the NVIDIA Container Toolkit, 1.17.4, which addresses both vulnerabilities.\n\nWe want to thank the entire NVIDIA team for their transparency, responsiveness, and collaboration throughout the disclosure process\u2014we truly appreciate their support. We also want to thank the gVisor team for reviewing this blog.\n\nWe appreciate Wiz Research\u2019s help in identifying this vulnerability and working closely with our product security team to address it. We remain committed to fostering a robust security ecosystem to protect our customers and the industry. NVIDIA Corporation\n\nThe vulnerability in a nutshell\n\nThe vulnerability enables a malicious adversary to mount the host\u2019s root filesystem into a container, granting unrestricted access to all of the host\u2019s files. Moreover, with access to the host\u2019s container runtime Unix sockets, attackers can launch privileged containers and achieve full host compromise. In our demo, we exploit this by mounting the host\u2019s filesystem within the container and then leveraging access to docker.sock to launch a privileged container and fully compromise the host.\n\nOur research identified multiple vulnerable Cloud Service Providers where NVIDIA\u2019s Container Toolkit was exploitable. In some cases, the vulnerability allowed for the full compromise of a Kubernetes cluster shared across multiple tenants. In this blog post, we cover the technical details of the vulnerability and discuss different ways to exploit it against Docker and gVisor.\n\nMitigation & Detection\n\nAll the issues have been addressed in version 1.17.4 of NVIDIA Container Toolkit. We recommend users to:\n\nUpdate to the latest version. Do not disable the --no-cntlibs flag in production environments.\n\nWiz customers can use the following to detect vulnerable instances in their cloud environment:\n\nHow we found CVE-2024-0132\n\nArchitecture\n\nThis diagram provides a high-level overview of the process of creating a container in most environments:\n\nIn this setup, Docker receives user inputs to build the container configuration, including resource allocations and environment settings. Docker then communicates with containerd, which manages the container lifecycle, handles image transfers, and sets up networking. containerd utilizes runc to create and run the container according to the Open Container Initiative (OCI) specifications. runc sets up namespaces and cgroups to isolate the container environment and starts the main process within the container.\n\nEnter NVIDIA\u2019s Container Toolkit\n\nThe NVIDIA Container Toolkit is a set of open-source software libraries and tools that configure containers to consume GPU resources. NVIDIA\u2019s toolkit is widely used by AI SaaS vendors, companies running AI models, and endpoint devices.\n\nThe NVIDIA Container Toolkit supports multiple container runtimes such as Docker, Containerd (Kubernetes), CRI-O, and Podman. This blog post will focus on Docker but other including Containerd and CRI-O are affected by CVE-2024-0132 . Note that since the use of the Container Device Interface (CDI) bypasses the affected code entirely, environments where this is used are not affected. This includes Podman, for example, which offers native support for requesting devices using CDI.\n\nLet\u2019s add the NVIDIA Container Toolkit to the mix:\n\nDuring installation, NVIDIA Container Toolkit changes the Docker daemon configuration and sets itself as the default container runtime.\n\nWhen a user executes docker run , Docker will invoke the nvidia-container-runtime binary to create and start the container. The container definition generated by docker is modified by nvidia-container-runtime to add a new prestart hook and is then passed to the system\u2019s runc .\n\nrunc executes the standard container initialization steps and then calls the previously added prestart hook.\n\nnvidia-container-runtime-hook is responsible for mounting devices, libraries and binaries into the container process and uses the nvidia-container-cli ( libnvidia-container ) to perform most of these actions.\n\nAfter the prestart hook is complete the control is returned to runc, which continues with the container initialization, sets security boundaries and finally calls the container\u2019s ENTRYPOINT.\n\nNVIDIA Container Toolkit attack surface\n\nDuring the execution of nvidia-container-runtime-hook, the container is in an early initialization phase, meaning key security controls and OS restrictions that apply when the container is fully running are not yet in effect.\n\nSignificant and risky operations occur on the container\u2019s filesystem, where a potential attacker could manipulate files and settings. Furthermore, these operations are executed from the host. This setup implies that if a filesystem vulnerability were exploited during these operations, an attacker could potentially gain direct access to execute actions on the host filesystem itself.\n\nWhile reviewing the NVIDIA Container Toolkit source code, we discovered a Time of Check/Time of Use (TOC/TOU) vulnerability in the way libnvidia-container mounts files into the container. The exploit we outline in the next sections tricks libnvidia-container into mounting directories from the host inside our container, effectively performing a container escape.\n\nInteresting mounts\n\nWhen running a container with the NVIDIA Container Toolkit runtime, we observed several interesting mounts:\n\nExamining the output from the mount command, we observed that the NVIDIA Container Toolkit had mounted several libraries into the container. Further investigation of the container toolkit debug logs revealed related operations that appeared promising:\n\nSource code analysis\n\nLet\u2019s review the source code to better understand how nvidia-container-cli mounts files into the guest container. The main logic lies in the nvc_driver_mount function, which is quite large and performs most of the mounts by calling mount_files .\n\nMounts can be split into two main groups: mounting resources from the host into the container and mounting resources from the container to itself ( cnt->cfg.libs_dir ). We decided to focus on the latter, as we suspected that we might be able to control the mount source ( cnt->lib ) by modifying files, paths, or symbolic links inside the container\u2019s filesystem. It turns out this mechanism is used for backward compatibility. This process is tricky to implement, as the host must handle filesystem operations on behalf of the guest container while ensuring that both the source and destination locations remain within the container\u2019s permissible namespace.\n\nSince the mount operations are performed from the host, the destination root ( cnt->cfg.rootfs ) appears as follows: /var/lib/docker/overlay2/<container_id>/merged. If we manage to mount a source outside of this path and set a destination within this path, we can read files outside of the container\u2019s namespace.\n\nBasically, nvc_driver_mounts copies cnt->libs , filters them using filter_libraries , and calls mount_files to mount them in the appropriate locations. filter_libraries is not very strict; it simply looks for specific version numbers of the libraries in their filenames.\n\nThe mount\u2019s source path, cnt->libs , is set during initialization ( nvc_driver_info_new ) by find_library_paths . It uses glibc\u2019s glob to search for paths matching the pattern: /usr/local/cuda/compat/lib*.so.* . These paths are later mounted by nvc_driver_mount , as described above.\n\nSince we can control the container's filesystem, we can also control the paths returned by the xglob function. Interestingly, there is no check to ensure that the paths returned by xglob are regular files, even though the regex clearly expects only files to be returned. This oversight allows us to plant directories instead, which will be useful later when we discuss the exploit.\n\nThis function also calls path_resolve , which follows symlinks and dot-dot (..) entries to return an absolute path. However, this path is not a host path (it doesn\u2019t start with cnt->rootfs ); rather, it is a guest container path\u2014for example, \u201c/\u201d refers to the container root. These paths are then returned and eventually stored in cnt->libs .\n\nThe vulnerability\n\nWe noticed that path resolution occurs during initialization, while we can still manipulate the filesystem layout between mount operations. Could this be exploited somehow?\n\nLet\u2019s examine the mount_files function and explore our options:\n\nSimplified version of mount_files function\n\nWhen nvc_driver_mount calls this function, the arguments are as follows:\n\nroot \u2013 The root directory of the container on the host (/var/lib/docker/overlay2/<container_id>/merged). dir \u2013 Depends on the container image architecture; for x64, it will be /usr/lib64 . paths \u2013 An array of file paths, which we can control in the /usr/local/cuda/compat directory, that need to be mounted to dir .\n\nThe function essentially mounts /usr/local/compat/lib*.so.* to the same filenames in /usr/lib64 , all within our container. If we could manipulate the paths using a symbolic link or ../../../../../ , we would be able to mount from outside the container into a path within /usr/lib64 . However, this shouldn\u2019t be possible because find_library_paths already normalizes all paths.\n\nThe final source mount path is set as root + paths[i] . We can take advantage of the loop and modify the filesystem between iterations. Since we control the entire filesystem, we can plant a mount destination to replace the next source between operations and ensure that the next paths[i] is a symbolic link.\n\nAnalysis of the exploit\n\nLet\u2019s look at the exploit and understand how it works:\n\nWhen running this exploit, we start the container with the /usr/local/cuda/compat directory structured as follows:\n\n1.libnvidia-ml.so.6 \u2013 A directory\n\nlibnvidia-ml.so.7 \u2013 A symbolic link pointing to ../../../../../../ (outside the container filesystem)\n\n2. libnvidia-ml.so.7 \u2013 A regular file\n\nInitially, when find_library_paths traverses /usr/local/cuda/compat/ , it only detects two entries: a directory named libnvidia-ml.so.6 and libnvidia-ml.so.7 , an empty regular file.\n\nWhen we enter mount_files , the first iteration mounts /usr/local/cuda/compat/libnvidia-ml.so.6 to /usr/lib64/libnvidia-ml.so.6 . We strategically planted this destination path as a specially crafted symbolic link pointing back to /usr/local/cuda/compat/ , which the kernel follows. As a result, the /usr/local/cuda/compat/libnvidia-ml.so.6 directory gets mounted over /usr/local/cuda/compat/ , effectively replacing its contents. At this point, the filesystem has changed compared to what f ind_library_paths originally observed. Now, our libnvidia-ml.so.6 directory overrides the compat directory with new entries.\n\nIn the second iteration of the loop, the function attempts to mount /usr/local/cuda/compat/libnvidia-ml.so.7 into /usr/lib64/libnvidia-ml.so.7 . However, libnvidia-ml.so.7 is no longer a regular file! It is now a symbolic link, and it is not the same file that find_library_paths originally detected during initialization.\n\nAs the function mounts /usr/local/cuda/compat/libnvidia-ml.so.7 , the Linux kernel resolves any symbolic links in the path. In this case, libnvidia-ml.so.7 effectively points to the host\u2019s filesystem root ( ../../../../../../ ). This forces the kernel to mount / from the host into /usr/lib64/libnvidia-ml.so.7 within the container, effectively breaking container isolation!\n\nPrivilege escalation & RCE\n\nOne frustrating limitation of this container escape vulnerability is that the host filesystem is mounted as read-only.\n\nDespite the read-only restriction, we could still interact with Unix sockets from the host, as they are not affected by this limitation. By opening the host\u2019s docker.sock , we can spawn new privileged containers. This became the final step in our exploit implementation:\n\nOnce we ran this image and spawned the privileged container, we gained unrestricted access to the host\u2019s filesystem, including full write capabilities. This elevated level of access also allowed us to monitor network traffic, debug active processes, and perform a range of other host-level operations.\n\nImpact on gVisor\n\nGoogle\u2019s gVisor is the de facto standard for container isolation. It also supports the NVIDIA Container Toolkit, enabling the use of GPU containers in a more secure manner. However, we confirmed that the vulnerability does impact gVisor! This allows files and directories from the host to be mounted into the sandboxed container, potentially leading to a container escape.\n\nThat said, in our testing, we found that the same privilege escalation via /run/docker.sock may not work as-is on certain Linux distributions with gVisor. The exploit requires adjustments to be effective in these environments.\n\nTakeaways\n\nImplementing container security is challenging. Even developers with deep experience in Linux security can miss or misconfigure key protections, leaving systems vulnerable to critical exploits. The layered nature of container isolation means that a single overlooked detail can have significant security implications.\n\nIt\u2019s clear that these layers are complex and require far more security research\u2014something we are committed to at Wiz Research. More scrutiny is needed in this area. Containers are not a strong security barrier and should not be relied upon as the sole means of isolation. As we have seen, such vulnerabilities can even impact stronger alternatives like gVisor.\n\nWhen designing applications, especially multi-tenant applications, we should always \u201cassume a vulnerability\u201d and ensure at least one strong isolation barrier, such as virtualization (as explained in the PEACH framework ). Wiz Research has extensively documented similar issues, and you can read more about them in our previous research blogs on Alibaba Cloud , IBM , Azure , Hugging Face , Replicate , and SAP .\n\nWe once again want to thank the NVIDIA PSIRT team and the gVisor security team for their collaboration and responsiveness.\n\nStay in touch!",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "The unwelcome workaround for Nvidia's RTX 50-series black screen issues is to hobble your gaming monitor with a 60 Hz refresh rate",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/today-i-found-a-potential-solution-to-your-black-screening-rtx-50-series-graphics-card-problems-though-youre-not-going-to-like-it/",
            "snippet": "During all of my initial review testing and overclocking of the RTX 5090 and RTX 5080 graphics cards, I had no issues with the black screening problem that...",
            "score": 0.6360903382301331,
            "sentiment": null,
            "probability": null,
            "content": "During all of my initial review testing and overclocking of the RTX 5090 and RTX 5080 graphics cards, I had no issues with the black screening problem that we've seen cropping up in various forums and Reddit threads. My Founders Edition cards have worked beautifully and not once set fire to the wooden cabin tinderbox in which I do all my performance testing.\n\nBut today I hit a wall. That is how I am going to refer to the MSI RTX 5090 Suprim, a wall, because boy, that thing is chonk with a capital OMG.\n\nThis is my first third-party RTX 50-series card, and it is towering over my test rig right now, and kinda terrorising it, if truth be told. Because now, I too, have fallen victim to the black screen effect we've read about. Nvidia has said it is investigating the issue but hasn't been able to help me through the struggles with the card.\n\nBut I have found a solution\u2026 in part. But it's not a solution I would want to live with, just something that I could put up with until Nvidia comes out with a proper fix which stops this $2,700 card from blacking out when it's put under pressure.\n\nBasically, you have to hobble your high refresh rate monitor. Thanks Reddit.\n\nIt's horrible, and I don't want to have to do it, but this way I'm able to get Cyberpunk 2077 or DaVinci Resolve to run without crashing my entire rig, and the only way I've managed to get through most of our GPU benchmarking suite is by dropping my glorious 4K 240 Hz OLED monitor down to a lowly 60 Hz refresh.\n\nIt's still not allowed me to get through a full 3DMark Time Spy run, but you can't have everything. Even if you spend this much on a brand new graphics card, it seems.\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nLet me count the other things I tried that have failed:\n\nAs always, I used Display Driver Uninstaller to clean my old drivers for a fresh start\n\nRolled back, clean uninstall, and installed the pre-released drivers Nvidia supplied for the review\n\nTried both MSI's 'Silent' and 'Gaming' BIOS settings\n\nUsed different power cables\n\nPlugged it in and out of the PCAT power testing module\n\nReseated the RAM (always worth a try)\n\nSwapping between HDMI and DP cables\n\nChanged Nvidia Control Panel power modes\n\nLeft the room while I booted 3DMark (it used to work with games on tape with the Commodore 64)\n\nTried 120 Hz \ud83d\ude2d\n\nIt is worth noting that I have so far only tested the card on the PCG GPU test rig. This is the one which has had zero issues with the other RTX 50-series cards, on indeed any graphics card I've tested in the past 12 months.\n\nBut it is the one which did give me horrendous coil whine on the RTX 5090 Founders Edition, so I will be switching machines now I have completed testing on this overclocked MSI card to see if it works within another PC.\n\nBut yes, there you have it, run your monitor like it's 2007 and you can at least play some games on your RTX 50-series GPU.\n\nYou're welcome.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia Seeks To Tap Into Global Real Estate Firm\u2019s Data Center Business",
            "link": "https://www.crn.com/news/ai/2025/nvidia-seeks-to-tap-into-global-real-estate-firm-s-data-center-business",
            "snippet": "Nvidia told CRN that it has recently partnered with global real estate firm CBRE to tap into the company's data center solutions business for AI expansion...",
            "score": 0.5501333475112915,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia Seeks To Tap Into Global Real Estate Firm\u2019s Data Center Business\n\nAn Nvidia spokesperson tells CRN that the AI computing giant recently partnered with global real estate firm CBRE to tap into the company\u2019s data center solutions business for AI expansion opportunities in the United States, Europe, the Middle East and Africa.\n\nAs Nvidia expands its roster of partners this year, the AI computing giant recently added one that doesn\u2019t fit the mold of solution providers, OEMs or cloud providers but has evolved in recent years to service growing opportunities around AI data centers.\n\nThat new partner is global real estate firm CBRE, which has joined the Nvidia Partner Network (NPN) as a solutions advisor consultant, said Rob Cooper, head of CBRE\u2019s data center advisory in Europe, the Middle East and Africa (EMEA), in a LinkedIn post last week.\n\n[Related: Nvidia Channel Chief Lists 2025 Priorities For Growing North American Partner Roster]\n\nIn his LinkedIn post announcing the partnership, Cooper wrote that CBRE is \u201cuniquely positioned to guide clients in identifying and securing optimal data centre solutions and deploying accelerated AI infrastructure at scale.\u201d\n\nAn Nvidia spokesperson told CRN via email that its NPN relationship with CBRE is based around the firm\u2019s data center solutions business and not CBRE\u2019s commercial real estate business.\n\n\u201cThe CBRE data center consulting business oversees significant data center capacity in regions including EMEA and the U.S. to support enterprises that would like to expand their Nvidia AI operations. CBRE\u2019s commercial real estate business is outside the scope of our current NPN relationship,\u201d the Nvidia representative said.\n\nCBRE declined to comment.\n\nAccording to a web page on CBRE\u2019s United Kingdom website that was linked in Cooper\u2019s LinkedIn post, the firm\u2019s data center advisory business can help customers find colocation facilities with the proper power, cooling and network capacity for AI-ready data centers. The practice also possesses \u201cunique operational and logistics capabilities\u201d when it comes to \u201csourcing, inventory management and installation\u201d for GPU deployments.\n\nCBRE\u2019s partner type in the NPN partner program is a solution advisor, which according to Nvidia\u2019s website, is a company \u201cwhose primary business model is to provide consultation services and expert advice to customers looking to implement Nvidia products, Nvidia-based solutions and technologies.\u201d\n\nA senior executive at a top U.S. Nvidia channel partner told CRN that the AI computing giant\u2019s new partnership with CBRE is a \u201csmart move\u201d because three of the biggest challenges with building next-generation AI data centers are \u201cpower, cooling and space,\u201d especially when considering Nvidia\u2019s recently launched Blackwell-based systems.\n\n\u201cThe amount of power draw requires a specialized type of power infrastructure, which then requires a specialized type of real estate and facility to be able to handle that,\u201d said Andy Lin, CTO at Houston-based Mark III Systems.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Chinese GPUs outdo Nvidia chips nearly tenfold in supercomputer task",
            "link": "https://www.scmp.com/news/china/science/article/3298226/chinese-gpus-outdo-nvidia-chips-nearly-tenfold-supercomputer-simulation-study",
            "snippet": "Innovative parallel computing design using domestic hardware underscores Beijing's broader strategy to blunt 'chokepoint' risks in critical tech.",
            "score": 0.9084574580192566,
            "sentiment": null,
            "probability": null,
            "content": "Computer researchers in China using domestically made graphics processors have achieved a near-tenfold boost in performance over powerful US supercomputers that rely on Nvidia \u2019s cutting-edge hardware, according to a peer-reviewed study.\n\nAdvertisement\n\nThe accomplishment points to possible unintended consequences of Washington\u2019s escalating tech sanctions while challenging the dominance of American-made chips, long considered vital for advanced scientific research.\n\nThe researchers said that innovative software optimisation techniques enabled them to improve efficiency gains in computers powered by Chinese-designed graphics processing units (GPUs) to outperform US supercomputers in certain scientific computations.\n\nWhile sceptics caution that software tweaks alone cannot bridge hardware gaps indefinitely, the development underscores Beijing\u2019s broader strategy to mitigate \u201c chokepoint \u201d risks in critical technologies.\n\nScientists often rely on simulations to model real-world circumstances, such as designs to defend against flooding or urban waterlogging. But such reproductions, especially large-scale, high-resolution simulations, demand substantial time and computational resources, limiting the broader application of such an approach.\n\nAdvertisement\n\nThe challenge for Chinese scientists is even more daunting. For hardware, production of advanced GPUs like the A100 and H100 are dominated by foreign manufacturers. On the software side, US-based Nvidia has restricted its CUDA software ecosystem from running on third-party hardware, thus hindering the development of independent algorithms.\n\nIn search of a breakthrough, Professor Nan Tongchao with the State Key Laboratory of Hydrology-Water Resources and Hydraulic Engineering at Hohai University in Nanjing, began exploring a \u201cmulti-node, multi-GPU\u201d parallel computing approach based on domestic CPUs and GPUs. The results of their research were published in the Chinese Journal of Hydraulic Engineering on January 3.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Prediction: Nvidia Stock Will Underwhelm After Feb. 26 for 3 Very Specific Reasons",
            "link": "https://www.fool.com/investing/2025/02/11/prediction-nvidia-stock-underwhelm-after-feb-26/",
            "snippet": "The best days may be in the rearview mirror for the face of the artificial intelligence (AI) revolution.",
            "score": 0.9333083629608154,
            "sentiment": null,
            "probability": null,
            "content": "The best days may be in the rearview mirror for the face of the artificial intelligence (AI) revolution.\n\nRoughly three decades ago, the advent of the internet and its mainstream proliferation began changing the business world forever. Though this transformation didn't occur overnight, investors have been waiting quite some time for the next game-changing innovation to come along and bolster the long-term growth prospects of corporate America. After an extensive wait, artificial intelligence (AI) appears to have answered the call.\n\nSoftware and systems are being empowered by AI to make decisions, reason, and evolve, all without the aid of human intervention. With use cases in most industries around the globe, it's perhaps no surprise that the analysts at PwC are forecasting a $15.7 trillion benefit to the global economy from AI by 2030.\n\nNo public company has taken the bull by the horns more during the early stages of the AI revolution than Nvidia (NVDA 5.27%). The company's Hopper (H100) and Blackwell graphics processing units (GPUs) have quickly become the standard for high-compute enterprise data centers responsible for training large language models and running generative AI solutions.\n\nBut with lofty growth expectations already baked into Nvidia's nearly $3.2 trillion market cap, there are three very specific reasons to believe Nvidia stock is poised to underwhelm following the release of its fiscal 2025 fourth-quarter operating results after the closing bell on Feb. 26.\n\nWaning AI-GPU scarcity\n\nTo give credit where credit is due, no chip company is particularly close to matching the computing speed of Nvidia's successor Blackwell chip, or even unseating the Hopper GPU at this point.\n\nHowever, the company has also benefited immensely from AI-GPU scarcity. Overwhelming demand for Nvidia's hardware, coupled with limited supply, has helped it book orders well in advance, as well as charge a premium price for its AI-GPUs. Whereas Advanced Micro Devices was netting in the neighborhood of $10,000 to $15,000 for its Instinct MI300X AI-accelerating chips in early 2024, Nvidia was commanding up to $40,000 for its Hopper chip. The result was a gross margin that peaked at 78.4% in the first quarter of fiscal 2025.\n\nThe concern for Nvidia on and after Feb. 26 is that computing speed isn't everything in the AI space. It's dominated because it's been able to supply the in-demand hardware businesses are asking for. But as AI-GPU scarcity wanes, so will the company's pricing power and its supercharged gross margin.\n\nWhile most investors have focused on direct competitors, such as AMD, the bigger threat is the possibility of losing out on valuable data-center real estate from its top customers by net sales. Most members of the \"Magnificent Seven\" are internally developing AI chips of their own. Even though these GPUs are unlikely to surpass the computing potential of Nvidia's Hopper and/or Blackwell chips, they're substantially cheaper and not backlogged. This is a recipe for future orders of Nvidia's hardware from America's most-influential businesses to disappoint.\n\nWe've already seen Nvidia's gross margin retrace by 380 basis points from its all-time high of 78.4% over the previous two quarters. Don't be surprised if growing internal and external competition, along with waning AI-GPU scarcity, weighs on Nvidia's gross margin and its stock.\n\nTariffs and trade limitations\n\nIf there's one thing Wall Street tends to reward, regardless of valuation, it's predictability. Unfortunately, various uncertainties regarding tariffs and export limitations may lead to Nvidia's outlook containing more unknowns than usual.\n\nWhile on the campaign trail, then-candidate Donald Trump was forthcoming about his willingness to use tariffs if elected to promote American interests. The idea behind tariffs is that imposing them on select imports can help domestic manufacturers be more competitive on price. Last week, Trump instituted a 10% tariff on select goods from China.\n\nHowever, a December-released analysis from Liberty Street Economics, which publishes research for the Federal Reserve Bank of New York, found that the stock of public companies exposed to Trump's China tariffs in 2018 and 2019 performed notably worse on the days tariffs were announced than companies that had no exposure. These underperforming businesses also, on average, saw their profits, employment, sales, and labor productivity decline from 2019 to 2021.\n\nWhile Nvidia doesn't import products from China, the world's No. 2 economy is one of its largest hardware purchasers. Strained trade relations between the U.S. and China could jeopardize billions of dollars in quarterly sales for Nvidia.\n\nTo make matters worse, the Biden administration restricted the export of Nvidia's high-powered AI chips to China for three consecutive years (2022 through 2024). Although President Trump and former President Joe Biden don't see eye-to-eye on much, protecting U.S. AI interests is one of those rare shared points. Trump or his administration are unlikely to loosen the regulations surrounding AI-GPU exports to China.\n\nThese limitations are likely to be reflected in Nvidia's outlook and may result in cautious commentary from its management team.\n\nHistoric precedent\n\nThe third very specific reason Nvidia stock can underwhelm after reporting its fiscal 2025 operating results on Feb. 26 is historic precedent. History has a flawless track record when it comes to next-big-thing innovations, and that's bad news for the face of the artificial intelligence movement.\n\nOne of the more prevalent concerns for Nvidia is that every game-changing technological innovation for three decades, including the internet, has navigated its way through a bubble-bursting event. Bubbles form because investors have a terrible habit of overestimating how quickly a new technology will be adopted and/or gain mainstream utility. Although there are plenty of use cases, on paper, for AI, most businesses lack well-defined blueprints for optimizing the technology and meaningfully improving their sales and profits from it.\n\nIf there's a silver lining for Nvidia, it's the company's well-established business segments that pre-date the AI revolution. If the proverbial AI bubble were to burst, Nvidia stock would be partially buoyed by GPU demand for gaming and cryptocurrency mining, as well as demand for its virtualization software.\n\nThe other historic battle Nvidia will be fighting is against its pricey valuation. In June-July 2024, Nvidia stock surpassed a price-to-sales (P/S) ratio of 40, which has historically been a level that's signaled a top for other market-leading businesses of next-big-thing innovations. Even though Nvidia has backed off its P/S ratio high of last summer, its stock remains pricey -- especially given the laundry list of challenges described above.\n\nEven though every Wall Street analyst expects Nvidia stock to rally in 2025, the table appears to be set for it to underperform.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Nvidia Stock Edges Higher. What to Watch Today.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-openai-chips-34823b35",
            "snippet": "Nvidia is likely to be closely watching developments around ChatGPT-developer OpenAI after a consortium headed by Elon Musk offered almost $100 billion for...",
            "score": 0.8762643337249756,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Analyst Sees \u2018Massive Tailwinds\u2019 For Nvidia (NVDA) Despite Valuation Concerns",
            "link": "https://finance.yahoo.com/news/analyst-sees-massive-tailwinds-nvidia-220055731.html",
            "snippet": "We recently published a list of Top 10 AI Stocks to Watch In February. In this article, we are going to take a look at where NVIDIA Corporation...",
            "score": 0.8076679110527039,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of Top 10 AI Stocks to Watch In February. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against other top AI stocks to watch in February.\n\nBarry Bannister, Stifel\u2019s chief equity strategist, said in a latest program on CNBC that the macroeconomic factors are finally catching up to the AI-led bull market. He expects inflation to remain sticky and no further rate cuts from the Federal Reserve in the short term. The analyst also rejected the notion that the massive tech selloff after the launch of DeepSeek was a buying opportunity.\n\n\u201cOver 30 years ago, we used to joke about how technology was such a displacement event business where new competitors would come in and destroy the entrenched stocks, that it deserved a lower multiple because of that. It\u2019s a short life cycle business that\u2019s got a very short competitive advantage period. But investors forgot about that. They bid up the stocks. The growth relative to value large-cap total return, one divided by the other on a 10-year compound basis, reached the absolute outer limits of the past 90 years. And that exact limit line is exactly where it peaked\u2014the price earnings multiple and the outperformance of growth. So, for us, it\u2019s just a very bubbly market that\u2019s just gotta take some air out of it.\u201d\n\nREAD ALSO 7 Best Stocks to Buy For Long-Term and 8 Cheap Jim Cramer Stocks to Invest In\n\nFor this article, we chose 10 AI stocks currently making moves in the market. With each stock, we have mentioned the number of hedge fund investors. Why are we interested in the stocks that hedge funds pile into? The reason is simple: our research has shown that we can outperform the market by imitating the top stock picks of the best hedge funds. Our quarterly newsletter\u2019s strategy selects 14 small-cap and large-cap stocks every quarter and has returned 275% since May 2014, beating its benchmark by 150 percentage points (see more details here).\n\nAnalyst Sees \u2018Massive Tailwinds\u2019 For Nvidia (NVDA) Despite Valuation Concerns\n\nNVIDIA Corp (NASDAQ:NVDA)\n\nNumber of Hedge Fund Investors: 193\n\nMegan Brantley from LikeFolio\u202c said in a latest program on Schwab Network that she sees \u201cmassive tailwinds\u201d for Nvidia, especially after the Stargate project announcement despite valuation concerns.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia GeForce RTX 4090 and 5090 prototypes exposed \u2014 development and testing cards had four 16-pin power connectors",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-geforce-rtx-4090-and-5090-prototypes-exposed-development-and-testing-cards-had-four-16-pin-power-connectors",
            "snippet": "A leaker managed to get ahold of some of Nvidia's latest-generation development/bring-up boards. The prototype hardware features extra power interfaces,...",
            "score": 0.9360913038253784,
            "sentiment": null,
            "probability": null,
            "content": "A leaker from the Chipell forums (via HXL) published images of Nvidia-designed PCBs (printed circuit boards) that were used to bring up the new Blackwell RTX 50-series GPUs while the company was developing its latest graphics cards. The boards feature debugging interfaces alongside overwhelming power delivery circuitry, with the latter allowing designers to better understand how particular GPUs work and respond to various factors.\n\n\ud83e\udd72nvidia engineering samplesAD102: 4x 16pinGB202: 4x 16pinproduction card4090\uff1a1x 16pin5090\uff1a1x 16pinhttps://t.co/c4RAWWfO4u pic.twitter.com/ReybcTsdeUFebruary 11, 2025\n\nAs an example of this, development/bring up boards for Nvidia's GeForce RTX 4090 (AD102) and RTX 5090 (GB202) \u2014 according to blogger Panzerlized \u2014 feature four 16-pin 12VHPWR and 12V-2x6 power connectors, respectively. By contrast, development cards for the GeForce RTX 3090 (GA102) and RTX 3080 (GA104) featured four or three 8-pin power connectors. So it's not just the latest GPUs, as older-generation products also featured an 'excessive' number of power connectors.\n\n(Image credit: Chiphell)\n\nThe higher number of power connectors on development/bring-up boards is not surprising or indicative of any underlying problems. Engineers tend to power up such graphics cards in stages, including core voltages, uncore voltages, memory voltages, I/O rails, and so on. This is done to verify that all power the rails initialize correctly, and that no short-circuit or out-of-spec behavior occurs.\n\n\n\nThe cards also feature debugging interfaces, test points, and instrumentation headers to allow for in-depth measurement and monitoring. All these are present on the cards pictured. Finally, there are plenty of jumpers on the PCBs to reconfigure various things.\n\n\n\nAs an added bonus, these boards can deliver an excessive amount of power to the GPUs. This can help Nvidia and its partners determine the exact capabilities and limits of the processors, as well as testing their performance under various conditions. For example, a prototype GeForce RTX 4090 had as many as 45 power phases along with four 12VHPWR power connectors. That's enough to deliver up to 2400 watts to the card, which far exceeds the amount of power required by the end product. (And we suspect Nvidia never actually tried to feed 2400W into a prototype.)\n\n(Image credit: Chiphell)\n\nSome extreme overclockers would probably be excited to acquire such a graphics cards. The excessive power delivery setup could help with liquid nitrogen cooling, as one possibility. However, these development/bring up boards usually come with older BIOS versions that only work with select GPUs, and due to their early nature there's a good chance that they won't fully work or behave as expected. Getting drivers that support such cards could also be difficult.\n\n\n\nRegardless, it's interesting to see the development boards used by Nvidia and its graphics cards partners. Designing, prototyping, testing, debugging, and finalizing modern computer components can be a very complex process, and even with all the work that goes into the launch of a new GPU, issues can crop up once cards are in the hands of thousands of gamers with a wide variety of hardware configurations.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "AI chip startup Positron raises $23.5 million seed round to take on Nvidia",
            "link": "https://www.reuters.com/technology/artificial-intelligence/ai-chip-startup-positron-raises-235-million-seed-round-take-nvidia-2025-02-11/",
            "snippet": "Positron, a startup chip maker that aims to compete with Nvidia , said Tuesday it raised $23.5 million to scale production of its U.S.-made artificial...",
            "score": 0.6278677582740784,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-02-10": {
        "0": {
            "title": "META_TITLE_QUOTE",
            "link": "https://finance.yahoo.com/quote/NVDA/",
            "snippet": "319,896.36% \u00b7 PREV_CLOSE 138.85 \u00b7 OPEN 141.25 \u00b7 BID 139.98 x 100 \u00b7 ASK 140.06 x 200 \u00b7 DAYS_RANGE 138.98 - 143.44 \u00b7 FIFTY_TWO_WK_RANGE 66.25 - 153.13 \u00b7 VOLUME...",
            "score": 0.9248377680778503,
            "sentiment": null,
            "probability": null,
            "content": "Tariffs Could Hit Differently this Time The second Trump presidency has moved faster than the first to enact significant policy changes that are reshaping the federal government, immigration policy, and a myriad of other areas. During his campaign, the president promised new tariffs. Some of these are now being rolled out, while others appear subject to negotiation. Because the tariffs are mainly in a pending phase, economists have focused on other risks and opportunities from new policies. Immigration has been a major source of labor force growth in recent years, but fewer immigrants might keep unemployment low. The shake-out in federal employment has been devastating to those caught in the cross-hairs, but even big cuts in the federal workforce would return it to levels seen just a few years ago. Tariffs risk rekindling inflation, many economists agree, while noting that inflation is not a certainty. If companies are willing to absorb most of the higher costs, consumers may feel only moderate pain. If companies pass along most of the higher costs, consumers will see higher prices almost immediately. Proponents note that the first round of tariffs enacted beginning in 2017 did not cause immediate inflation. Economists and scholars will long argue whether that first round had anything to do with the supply-chain crisis that ultimately contributed to inflation. The COVID-19 pandemic sits smack in the middle of this timeline and will forever complicate any clear analysis of the chain of events that ultimately led to the worst inflation in 40 years. The focus on the risks of tariffs triggering new inflation may be missing a larger point. U.S. consumers are still in the grip of existing inflation, and many consumers particularly in the lower tiers of the economy, appear tapped out. If tariff-sensitive companies decide to pass along the bulk of these costs rather than absorb them, instead of (or alongside of) rising inflation, we may see a slowdown in consumer spending. And with the consumer driving approximately 70% of U.S. GDP, that could send total GDP growth below the 2% level of 4Q24. The Pre-2017 U.S. Consumer Argus President John Eade has identified the passage of Brexit, even more than the election of nation-first world leaders, as marking a key turning point in global relations and world trade. Prior to Brexit, much of the world was operating in a period of globalization that stretched back to the millennial turn or even before that. In this period, relatively free trade enabled developed countries such as the U.S. to effectively 'import deflation.' U.S. consumers benefited from relatively inexpensive goods, with prices that often were lower than those for the goods they displaced. The other price paid was steep, as high-quality jobs were lost to overseas relocation. The consumer price index captures this benign pricing environment. In 2000, the Consumer Price Index was 172.2 (the baseline was established at 100 in 1982). By 2024, the CPI was 313.7. Using a simple compound average annual growth rate (CAGR) calculation, the CPI appreciated at an average 2.5% annual rate over the 2000-2024 period. In the decade preceding the first tariffs in 2017, the consumer really had it easy. From 215.3 in 2008 to 245.1 in 2017, the CPI grew at a 1.5% CAGR. The Great Recession is at the beginning of this period and, like COVID-19, introduces a knotty complication into the calculation. The main point is that inflation in this period averaged a full half-percentage point below the Fed's 2% long-term target range. As noted, the period following 2017 included the supply-chain crisis and a global pandemic, culminating in withering inflation. From 245.1 in 2018 to 313.8 in 2024, the U.S. consumer price index grew at a 3.6% CAGR. That includes some relatively benign years such as 2020 (1.2% CPI growth) and some really nasty periods such as 2022 (8.0% CPI growth). The point is that goods and services prices are up substantially from the last time America embarked on a significant round of tariffs. The 2025 Consumer Every U.S. consumer has felt the pressure of inflation over the past five or six years. Those on the bottom are feeling it more, however. The bifurcation between the upper half of consumers and the lower half has never been more pronounced. And the culprit once again is inflation. Any Economics 101 textbook likely explains inflation by examining what it does to goods prices. The inflation that kicked into high gear in 2022 has been characterized by particularly severe and sticky services inflation. Rents have gone up severely (and continue to rise), representing a larger and larger chunk of renters' take-home pay. Prices for all kinds of insurance, including vehicle insurance, have appreciated more and faster than overall inflation. Price for food and other necessities are also up sharply. Lower-tier consumers are severely strained by this burden and have less and less available for discretionary spending, from restaurant meals to laptops. If you go to Walmart for a first-hand glimpse of this reality, you might be surprised at all the nice new cars in the parking lot. Walmart has been gaining higher income shoppers in recent years, and the trend appears to be accelerating. In May 2024, Walmart CEO John Rainey noted that the chain was seeing growth across income cohorts, 'with upper-income households continuing to account for the majority of share gains.' Walmart defines upper-income as households with more than $100,000 in income. That trend was also evident when the company reported 2024 holiday-quarter sales in mid-February 2025. Walmart management also warned that it expected fiscal-year profit growth to slow, as the company would not be 'immune' from tariffs on Mexico and Canada. Upper-income consumers flocking to Walmart is a sign that no consumer is immune to the damage done by inflation. Enactment of new 10%-25% or even higher tariffs on major trading partners risks causing a slowing in consumer spending across the board. Conclusion While there is no certainty on the timing and magnitude of tariffs here and worldwide, we expect impacted companies to respond with agility and flexibility as a means of maintaining business activity levels and retaining customer relationships. The practical implication is that companies will 'eat' some portion of tariffs, to the detriment of margins. Multiple factors could mitigate the worst impacts, however. The first round of tariffs, the pandemic, and the supply-chain crisis all awakened Americans to the need to lessen reliance on outsourcing and to bring production of goods closer to home. The CHIPS & Science Act has encouraged the on-shoring of U.S. technology production, and the Inflation Reduction Act has used government resources to encourage domestic production of green energy solutions. We continue to monitor risks and opportunities in the economy related to meaningful policy changes. Our base case remains that the economy will continue to grow in the low-2% range, unemployment will remain relatively close to current levels, and S&P 500 earnings growth will accelerate in 2025 from 2024 levels. Putting that all together, we believe the S&P 500 can appreciate in the 'normal' 8%-12% range in 2025, with leadership potentially shifting away from traditional growth sectors and toward cyclical, income-sensitive, and defensive sectors.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "There is one thing making me excited about the new Nvidia RTX 5070 Ti and no, it isn't Multi Frame Generation",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/there-is-one-thing-making-me-excited-about-the-new-nvidia-rtx-5070-ti-and-no-it-isnt-multi-frame-generation/",
            "snippet": "If the GPU at the heart of the RTX 5070 Ti can deliver the same level of overclocking it does with the RTX 5080 we could have a champ on our hands.",
            "score": 0.8917370438575745,
            "sentiment": null,
            "probability": null,
            "content": "The next family of cards in the new RTX Blackwell series of Nvidia GeForce GPUs is going to be an interesting one. Theoretically, the lower we go down the RTX 50-series stack, the less exciting the graphics cards look to be on paper. The generational differences in silicon look slight, and there's a general expectation that Nvidia is going to be leaning even harder on the substantial veneer of Multi Frame Generation performance to make them look good. But the binary RTX 5070 family of cards may still end up giving us the most PC enthusiast of GPUs in this entire generation.\n\nI'm talking here about overclocking, more in the classic PC gaming sense than the sort of rather pointless number-chasing it's become. Y'know, that old school method of wringing every last drop of gaming performance out of your silicon because the price of stepping up to higher spec hardware is utterly punitive.\n\nBefore we get too deep into that, however, I do want to acknowledge that obviously the $749 RTX 5070 Ti is not mid-range silicon, not some sort of middle-order GPU for the masses. That used to be the price of a GeForce Titan card, ffs. But it is arguably the more affordable face of high-end PC graphics.\n\nAnd, from my time overclocking the RTX 5080, there is a chance the RTX 5070 Ti, with the same GPU, could be a hell of a strong contender for the best overclocking card we've seen in an age. Already the RTX 5080 has delivered some pretty stunning performance on that count, allowing me to push the GPU clock well beyond the 3 GHz mark with a stable overclock, which didn't require me to drive a ton of extra voltage through the GB203 chip, either.\n\nWith the RTX 5070 Ti specs showing a clock speed well below what the RTX 5080 is delivering from its own Boost clock, I feel there really ought to be some serious headroom in that cut-down chip.\n\nAs I say, both cards are using the GB203 GPU\u2014the RTX 5080 is taking the full chip, utilising all available 84 streaming multiprocessors (SMs), while the RTX 5070 Ti is taking hold of 70 SMs. That equates to a difference of some 1,792 CUDA cores between them, though interestingly only 768 between the RTX 5070 Ti and the old RTX 4080 of the previous generation.\n\nSwipe to scroll horizontally RTX 5070 Ti vs RTX 5080 Header Cell - Column 0 RTX 5070 Ti RTX 5080 GPU GB203 GB203 SMs 70 84 CUDA cores 8960 10752 Boost clock (GHz) 2.45 2.62 Base clock (GHz) 2.30 2.3 Tensor core TOPS (FP16) 176 225 Ray tracing core TFLOPS 133 171 Memory 16 GB GDDR7 16 GB GDDR7 Memory bus width 256-bit 256-bit Memory bandwidth (GB/s) 896 960 Total Graphics Power (watts) 300 360 Required system power (PSU wattage) 750 850 Power connector 1x 300 W PCIe Gen 5 OR 2x PCIe 8-pin adapter 1x 450 W PCIe Gen 5 OR 3x PCIe 8-pin adapter Price $749 $999\n\nIn terms of the Boost clock, the RTX 5080 comes in at a rated 2,617 MHz (though that is a moveable feast given the dynamic nature of GPU frequencies these days), and the standard RTX 5070 Ti is rated at 2,452 MHz.\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nThat's a pretty healthy difference in clock speed there and, seeing as we were able to squeeze a +525 MHz offset just using a simple MSI Afterburner tweak with the RTX 5080, it's not unreasonable to think we ought to be able to spike that sub-2,500 MHz figure a lot higher, too. And it's not like the RTX 5070 Ti is being specifically limited in terms of power draw, either. Compared with the previous generation, the RTX Blackwell card has a higher power rating, at 300 W, which is already mighty close to the RTX 5080's 360 W rating.\n\nI've not had the pleasure of slotting an RTX 5070 Ti into the PC Gamer test rig as yet, so I cannot talk from direct experience, this is all just tentatively excited expectation born of what I've seen the other card using that same GB203 silicon doing.\n\nIf we can get the same 3 GHz+ stable clock speed out of the RTX 5070 Ti then I would expect another 10%+ in terms of gaming performance out of the card. Given that Nvidia has already primed us to expect a 20% gen-on-gen performance gain for the card, slapping another 10% on top of that will put us in the same sort of frame rate bump territory as the RTX 5090 delivers.\n\nThat's the card offering the biggest generational performance increase of the RTX Blackwell cards, with a 30% 4K gen-on-gen gaming hike. Being able to push the lower spec RTX 5070 Ti card to match that percentage boost will make it a far more tempting card.\n\nImage 1 of 1 (Image credit: Future)\n\nObviously, there are things which could put a blocker in all this excitement of mine. There's a good chance there may be something in the vBIOS which stops the card from clocking so high, or there could be some limitations put on the power delivery. If there's any chance of the RTX 5070 Ti being able to be overclocked to come near the RTX 5080 in terms of gaming performance you can bet there will be some limits put in place.\n\nIt's also worth noting my exceptional overclocking numbers of the RTX 5080 came from the over-engineered, loss-leading Founders Edition version of the card, which has been specifically created with high-performance power componentry.\n\nThere is no Founders Edition card for the RTX 5070 Ti, however, and whether third-party PCBs are going to be capable of driving the GB203 GPU stably at those frequencies is still one that's up for debate. Nvidia certainly suggested I'd won the silicon lottery hitting those figures with our RTX 5080 card.\n\nA key hint to what we might be able to expect to see from overclocking the RTX 5070 Ti would be at what level the AIBs are setting their own factory overclocked versions. With the RTX 5080, the likes of Asus and Gigabyte are setting confidently high overclocked clock speeds on their retail cards. As yet, however, we don't have details of what those companies are setting their overclocked versions at for the RTX 5070 Ti. Right now, all we get is a wee \"TBD\" when it comes to the final clock speed specs of these cards.\n\nBut the RTX 5070 Ti is coming out this month, in a scant few weeks. So we shall know for sure whether there is reason to be cheerful about the third-tier of the RTX Blackwell cards very soon. If it's the OC king, the clock-happy mac daddy, and you had any hope of buying one, it could be the best-value card of the lot. That's a lot of maybes, I'll grant you, but it's certainly not beyond the realms of possibility.\n\nYes, Nvidia might accidentally create a great tinkerers GPU and let us really tweak the twangers off it.\n\nAnd what of the RTX 5070? Well, it's a smaller chip being offered a lot more power, maybe that's going to give it something to offer us via its ickle GB205 GPU. Though I am definitely less convinced about that as a possibility right now.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Stock Rallies, Shakes Off Concerns About OpenAI Moves",
            "link": "https://www.investors.com/news/technology/nvidia-stock-openai-concerns/",
            "snippet": "Nvidia stock came under pressure from a report that ChatGPT creator OpenAI is pushing ahead with plans to develop its own AI chips.",
            "score": 0.4500487446784973,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) appeared to come under pressure Monday from a report that ChatGPT creator OpenAI is pushing ahead with plans to develop its own AI chips to reduce its reliance on Nvidia processors for artificial intelligence applications. But Nvidia stock rallied despite the news.\n\nOpenAI plans to finalize the design for its first in-house chip in the next few months, Reuters reported. Taiwan Semiconductor Manufacturing (TSM) is set to make the AI chip using its 3-nanometer process technology. OpenAI wants to mass produce the chips at TSMC in 2026, Reuters said.\n\nNvidia stock has traded lower in recent weeks after China's DeepSeek announced a low-cost AI computing system. That news caused investors to question whether hyperscale cloud computing providers would pause their costly AI data center buildouts.\n\nBut Google parent Alphabet (GOOGL), Amazon.com (AMZN), Facebook parent Meta Platforms (META) and Microsoft (MSFT) have reiterated their commitments to increase their capex.\n\nMeanwhile, Evercore ISI analyst Mark Lipacis on Sunday added a \"tactical outperform\" rating to Nvidia stock ahead of the company's fiscal fourth-quarter earnings report. Nvidia is scheduled to post its fiscal Q4 results on Feb. 26.\n\nLipacis said his firm's channel checks are constituent with its bullish long-term thesis. He expects a positive report and outlook from the company. Lipacis has a price target on Nvidia stock of 190.\n\nOn the stock market today, Nvidia stock rose 2.9% to close at 133.57.\n\nNvidia Stock On Tech Leaders List\n\nOther semiconductor stocks moving on news Monday included TSMC, NXP Semiconductors (NXPI), Onsemi (ON) and Semtech (SMTC).\n\nTSMC warned that its first-quarter revenue will be at the lower end of its prior forecast range due to the impact from a recent earthquake and aftershocks. However, it reiterated its full-year outlook. TSM stock rose 0.9% to close at 207.95 on Monday.\n\nNXP announced an agreement to buy Kinara, a maker of high-performance, energy-efficient and programmable discrete neural processing units. Those NPUs will enable a wide range of edge AI applications, NXP said. The $307 million all-cash deal is expected to close in the first half of 2025. NXP stock fell 1.4% to 210.05.\n\nOnsemi stock dropped 8.2% to 47.04 after the chipmaker delivered disappointing fourth-quarter results and guidance. The maker of chips for automotive and industrial applications missed its sales and earnings targets for Q4 and guided well below views for the current quarter.\n\nSemtech stock plunged 31% to 37.60 on Monday after it revealed late Friday in a regulatory filing that sales of its CopperEdge products will be lower than previously anticipated. CopperEdge products are used in active copper cables for data center applications. Piper Sandler said the lower sales estimate suggests a change in rack architecture at Nvidia.\n\nNvidia and TSMC are on the IBD Tech Leaders stock list.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nArm Beats Estimates, Gives In-Line Outlook. Stock Falls.\n\nAMD Stock Craters On Soft AI Data Center Business\n\nSee Stocks On The List Of Leaders Near A Buy Point\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia Leads Chip Stocks Higher Monday as Markets Rebound",
            "link": "https://www.investopedia.com/nvidia-leads-chip-stocks-higher-monday-as-markets-rebound-8789002",
            "snippet": "Nvidia shares gained Monday, leading other chip stocks higher in a boost to the tech sector as the major U.S. indexes looked to rebound from Friday's...",
            "score": 0.924728274345398,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia shares gained Monday, leading other chip stocks higher.\n\nThe surge helped lift the broader tech sector and the major U.S. indexes, after losing ground Friday.\n\nThe gains for Nvidia came after partner TSMC reported a jump in sales, and Evercore ISI analysts issued a bullish note Monday ahead of the chipmaker's earnings later in the month.\n\nNvidia (NVDA) shares gained Monday, leading other chip stocks higher in a boost to the tech sector as the major U.S. indexes looked to rebound from Friday's losses.\n\nShares of Nvidia were up close to 3% at $133.56 in intraday trading, making it one of the best-performing stocks on the Dow Jones Industrial Average. Shares of several Nvidia partners like Micron Technology (MU), Dell (DELL), and Supermicro (SMCI) also climbed, along with other chip stocks including Broadcom (AVGO), Advanced Micro Devices (AMD), Qualcomm (QCOM), Western Digital (WDC), and Intel (INTC).\n\nThe surge helped lift the broader tech sector, with several of the other mega-cap members of the \"Magnificent 7\" stocks also advancing. The tech-heavy Nasdaq was up over 1%, while the S&P 500 gained 0.6%, and the Dow edged 0.2% higher.\n\nThe gains for Nvidia came after partner TSMC (TSM) reported a jump in sales, and Evercore ISI analysts on Monday added the chipmaker to their \"tactical outperform\" list ahead of Nvidia's earnings report on Feb. 26, as a potential catalyst for the stock.\n\nThe analysts issued a $190 price target, more than 40% above Monday's intraday price. The average target from analysts polled by Visible Alpha, who have remained overwhelmingly bullish on the stock despite recent pressure amid concerns about the pace of Big Tech's AI spending, is roughly $174, suggesting about 30% upside.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Buy Nvidia\u2019s stock ahead of earnings? This analyst makes the case.",
            "link": "https://www.marketwatch.com/story/buy-nvidias-stock-ahead-of-earnings-this-analyst-makes-the-case-459abcfd",
            "snippet": "Evercore ISI analyst Mark Lipacis is shrugging off Wall Street's concerns about Nvidia Corp.'s stock as he recommends buying it ahead of the company's...",
            "score": 0.9243146181106567,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Is Nvidia the Greatest Artificial Intelligence (AI) Stock?",
            "link": "https://www.fool.com/investing/2025/02/10/is-nvidia-the-greatest-ai-stock/",
            "snippet": "Artificial intelligence (AI) has probably been the hottest investment theme on the planet over the past year. Companies involved in this high-growth area...",
            "score": 0.8981859683990479,
            "sentiment": null,
            "probability": null,
            "content": "Artificial intelligence (AI) has probably been the hottest investment theme on the planet over the past year. Companies involved in this high-growth area have seen their share prices climb in the triple digits and have fueled gains in the three major benchmarks -- the S&P 500, the Dow Jones Industrial Average, and the Nasdaq.\n\nInvestors have piled into AI stocks because of the technology's potential to streamline processes and develop game-changing new products and services. Today's $200 billion AI market is forecast to reach more than $1 trillion by the end of the decade, offering significant growth opportunities for companies involved -- and investors who get in on these growth stories early.\n\nAnd one particular growth story that's attracted a lot of attention is Nvidia (NVDA 5.27%), a stock that's soared 1,900% over the past five years. The company has gone from primarily serving the video gaming market with its chips to dominating the AI chip market -- and revenue has exploded higher, climbing in the double or triple digits quarter after quarter. But is Nvidia really the greatest AI stock around? Let's find out.\n\nHow AI is transforming businesses\n\nSo, first, let's talk a bit about AI in general. The technology, by leveraging the power of large language models (LLMs), is helping companies, researchers, and even you and me in our daily lives on a variety of tasks. Some of these uses are more important than others, and certain can be game-changing. For example, AI is streamlining processes for companies, identifying risks to their businesses, and powering the development of new life-saving drugs and medical devices.\n\nMany types of companies could generate earnings growth thanks to AI: those designing the tools for the development of LLMs and AI platforms, cloud providers offering customers access to these tools, companies including AI in the products and services they sell to others, and companies applying AI to their businesses to gain in efficiency and reach new accomplishments. That's a lot of potential AI winners.\n\nNow, let's move on to consider Nvidia's position in the AI market. The company is the dominant player in AI chips thanks to the quality of its graphics processing units (GPUs). These chips are the fastest around, powering key tasks like the training and inferencing of models. Inferencing is a particularly important function because it involves the reasoning the model goes through to answer complex questions -- and here, it's key to use many high-powered GPUs. So, this represents a growth area for Nvidia.\n\nNvidia's ecosystem of AI\n\nBut Nvidia isn't only about chips. The company has built an entire ecosystem of AI products and services, including enterprise software -- and this is allowing Nvidia to remain involved in every stage of a customer's AI development and serve every type of customer.\n\nYou can go to Nvidia for GPUs and networking options, for example, or you can sign on for enterprise software to streamline the deployment of an entire AI platform. Nvidia even is set to play a big role in the next wave of AI growth: agentic AI, or AI-driven software that can reason and solve problems. The company and its partners have designed AI blueprints to help customers design their own AI agents to be used through the Nvidia Enterprise system.\n\nPotential headwinds?\n\nSome investors have worried about rivals challenging Nvidia's dominance, but the company's leadership position today and commitment to innovation will make it difficult for another player to unseat it. Another concern, which arose recently when DeepSeek said it trained a model for less than $6 million, was that Nvidia customers may follow that route -- and cut their training budgets.\n\nI don't see that happening. First, experts already are saying DeepSeek's figures aren't accurate -- and the Chinese start-up actually spent a lot more than it announced to train its model. And second, if indeed it is possible to train a model for a very low price, more and more companies would launch AI programs -- offering Nvidia the opportunity to gain on volume. Finally, it's important to remember that Nvidia GPUs could see significant growth as inferencing takes off.\n\nSo, let's get back to our question: Is Nvidia really the greatest AI stock? There are many very strong AI players out there that make wonderful investments. But, if you're looking for just one AI stock to buy right now, Nvidia is the name to snap up. I consider this company the ultimate AI stock because it benefits from every stage of AI development -- from the infrastructure buildout to the actual application of AI to real world situations. Across those stages, customers will flock to Nvidia, and that should result in fantastic earnings growth -- and share price performance -- over the long run.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "I Tested Nvidia\u2019s AI Tool for Making Your Webcam Better, and Oof",
            "link": "https://lifehacker.com/tech/i-tested-nvidia-ai-tool-for-making-your-webcam-better-and-oof",
            "snippet": "The latest version of Nvidia Broadcast comes with some extra enhancements that try using AI to turn bargain bin recording devices into professional-quality...",
            "score": 0.7505618333816528,
            "sentiment": null,
            "probability": null,
            "content": "Credit: Mark Knapp\n\nWhen Nvidia introduced the free Nvidia Broadcast app in 2020, it promised to use the AI capabilities of RTX GPUs to boost the video and sound quality of users' recordings, presuming they had the right hardware. This could be useful when a laptop\u2019s built-in webcam and microphones may not capture the best looking footage or the richest audio, and could potentially help streamers get by with a cheaper setup. With version 2.0 of the Nvidia Broadcast app, released at the tail end of January, the capabilities are stepping up even higher...perhaps a little too close to the sun.\n\nIn Nvidia Broadcast 2.0, microphones still have options for noise removal and room echo removal, but there\u2019s now also a studio voice effect in beta that \u201cenhances the quality of your mic to simulate a high end recording studio.\u201d For video, the app can still make tweaks to your background (replacing, blurring, or essentially green-screening it), remove noise from grainy footage, zoom in and automatically keep you in frame, and strangely enough, make it look like your eyes are looking at the camera. But new with the update is virtual lighting, to better highlight you in your video.\n\nSome of these new features call for powerful GPUs. Studio voice, video noise removal, and virtual key light all call for a \u201chigh-end GPU\u201d and aren\u2019t recommended for use while gaming or using other GPU-intensive applications. All features require RTX hardware, meaning you\u2019ll need at least an RTX 2060 or above to try them, but for the features that require a \u201chigh-end GPU,\u201d Nvidia says an RTX 4080, 5080, or higher is \u201crequired.\u201d That said, I was able to run both features on an RTX 4060 mobile GPU.\n\nNvidia Broadcast in action\n\nThe idea behind these AI features is cool, but how well they actually work is still in question. For one thing, they may really be as demanding as Nvidia says. Running either the virtual key light or studio voice feature on my RTX 4060-powered laptop showed the GPU was absolutely slammed by the process. Nvidia\u2019s built-in GPU Utilization monitor was showing red, with the RTX 4060 all but maxed out and the performance overlay showing it drawing 60 watts. My laptop\u2019s fans even ramped up as if I were gaming at full throttle. So just from an economics standpoint, these features are costly no matter how you look at them. You\u2019ll need to have powerful hardware to run them, and then run that hardware hard. Plan on using these features on a desktop computer or with your laptop plugged in.\n\nThen there\u2019s the even more crucial matter of how they really look and sound. Let\u2019s start with video.\n\nCredit: Mark Knapp\n\nThe eye contact tool, despite being available before Broadcast 2.0, has now come out of beta. But I\u2019m not convinced it should have. Sure, enabling it makes it look like I\u2019m staring into the camera in video footage. But in my testing, it invariably gave me big blue eyes that made me look like a White Walker right out of Game of Thrones. For reference, I do not have blue eyes. Even when I was making eye contact with the camera, Nvidia Broadcast still insisted on editing my eyes and making them blue.\n\nCredit: Mark Knapp\n\nThe virtual key light did what it said. It created artificial lighting to boost brightness on me without bumping up the brightness on the whole video. The results failed to impress me, though. With it enabled, I simply look like I\u2019ve gone radioactive. The lighting is very unnatural.\n\nAs for the audio, at first blush, it sounds fairly impressive. The mics on my laptop are not very good. Even in a quiet room, they put out audio that has me sounding far away and slightly muffled. With studio voice enabled, my voice ends up much fuller and clearer sounding. But listening closely, there\u2019s an odd digitization going on. It\u2019s hard to characterize, but it doesn\u2019t sound like it\u2019s really my voice. It sounds more like a recording of my voice was used in a voice cloner, and then that repeated everything I said. It\u2019s all just a little stilted and quavering. Listen below:\n\nThe studio voice feature also can\u2019t save the mic from a bad recording environment. Testing in a small room with a box fan running at full blast, the audio was a dramatic improvement in clarity compared to the raw recording from the microphones, but it was still audibly processed, and the efforts to subdue the background noise made my voice sound especially odd.\n\nIf you have a half-decent microphone, studio voice might even make it worse. I made additional test recordings using the built-in boom mic on the Audeze Maxwell headset with it directly wired into my laptop. In both a quiet and loud room, it provided a loud, clear, and full recording of my voice without studio voice enabled. In both cases, turning on studio voice then introduced hard-to-miss digitization that not only made the audio sound worse but also made it harder to comprehend.\n\nCan Nvidia AI replace a proper streaming setup?\n\nGiven the hardware requirements, performance demands, and quality of the results, the stars really need to align for these newer Nvidia Broadcast features to feel truly worthwhile. If you have an Nvidia-powered system, by all means, play with the tool. Some of the features can come in handy, like the auto-framing one. But I wouldn\u2019t recommend shelling out for a new Nvidia GPU just so you can save money on audio and video recording gear, especially if you want to get anywhere close to professional quality. And don\u2019t forget that the power draw of the GPU trying to run these features will add up over time.\n\nThe audio quality I got from studio voice \u2014 perhaps limited by the RTX 4060 in my system \u2014 wasn\u2019t something I\u2019d want to share with any kind of audience on a regular basis, and it paled in comparison to the quality I could get just from having a headset with a boom mic. I\u2019ve tested a lot of gaming headsets, and even very cheap wired headsets with a boom mic are leagues better than what I heard from studio voice.\n\nThe eye contact feature failed to be anything other than unsettling, and I don\u2019t think it\u2019s going to fool anyone into believing you\u2019re actually making eye contact with them. And the virtual key light, much like studio voice, doesn\u2019t appear to be a quality substitute for a real key light, especially when affordable LED lights are a dime a dozen.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Handful of users claim new Nvidia GPUs are melting power cables again",
            "link": "https://arstechnica.com/gadgets/2025/02/handful-of-users-claim-new-nvidia-gpus-are-melting-power-cables-again/",
            "snippet": "Here we (maybe) go again: Reports from a handful of early adopters of Nvidia's new GeForce RTX 5090 graphics card are reporting that their power cables are...",
            "score": 0.9269448518753052,
            "sentiment": null,
            "probability": null,
            "content": "Here we (maybe) go again: Reports from a handful of early adopters of Nvidia's new GeForce RTX 5090 graphics card are reporting that their power cables are melting (so far, there's at least one report on YouTube and one on Reddit, as reported by The Verge). This recalls a similar situation from early in the RTX 4090's life cycle, when power connectors were melting and even catching fire, damaging the GPUs and power supplies.\n\nAfter much investigation and many guesses from Nvidia and other testers, the 4090's power connector issues ended up being blamed on what was essentially user error; the 12VHPWR connectors were not being inserted all the way into the socket on the GPU or were being bent in a way that created stress on the connection, which caused the connectors to run hot and eventually burst into flames.\n\nThe PCI-SIG, the standards body responsible for the design of the new connector, claimed that the design of the 12VHPWR connector itself was sound and that any problems with it should be attributed to the manufacturers implementing the standard. Partly in response to the 4090 issues, the 12VHPWR connector was replaced by an updated standard called 12V-2x6, which uses the same cables and is pin-compatible with 12VHPWR, but which tweaked the connector to ensure that power is only actually delivered if the connectors are firmly seated. The RTX 50-series cards use the 12V-2x6 connector.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia\u2019s RTX 5090 power connectors are melting",
            "link": "https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues",
            "snippet": "Nvidia's new RTX 5090 GPUs are experiencing issues with the 12VHPWR power connector melting. It's similar to reports of RTX 40-series connectors melting.",
            "score": 0.9379948377609253,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years.\n\nAh shit, here we go again. Two owners of Nvidia\u2019s new RTX 5090 Founders Edition GPUs have reported melted power connectors and damage to their PSUs. The images look identical to reports of RTX 4090 power cables burning or melting from two years ago. Nvidia blamed the issue on people not properly plugging the 12VHPWR power connection in fully and the PCI standards body blamed Nvidia.\n\nA Reddit poster upgraded from an RTX 4090 to an RTX 5090 and noticed \u201ca burning smell playing Battlefield 5,\u201d before turning off their PC and finding the damage. The images show burnt plastic at both the PSU end of the power connector and the part that connects directly to the GPU. The cable is one from MODDIY, a popular manufacturer of custom cables, and the poster claims it was \u201csecurely fastened and clicked on both sides (GPU and PSU).\u201d\n\nThe melted power connector and damaged RTX 5090 Founders Edition. Image: ivan6953 (Reddit)\n\nWhile it\u2019s tempting to blame the MODDIY cable, Spanish YouTuber Toro Tocho has experienced the same burnt cable (both at the GPU and PSU ends) with an RTX 5090 Founders Edition while using a cable supplied by PSU manufacturer FSP. Plastic has also melted into the PCIe 5.0 power connector on the power supply. MODDIY also responded in a Reddit thread, ruling out the \u201cpossibility of a defective cable or manufacturing error\u201d and offering to cover the cost of repair if Nvidia and Asus don\u2019t honor their warranties.\n\nYouTuber der8auer has also examined the Reddit poster\u2019s equipment in person, and ruled out any form of user error in the process. He\u2019s also found that this could be related to a current distribution problem with RTX 5090 Founders Edition models instead. Either way, nobody should be blaming 12VHPWR issues on end users.\n\nNvidia originally introduced the 12VHPWR power connector on its RTX 40-series GPUs, and power supplies also debuted to support the new standard. The RTX 4090 Founders Edition was able to draw 450 watts over the 12VHPWR connector, while the new RTX 5090 draws up to 575 watts over a cable that\u2019s rated up to 600 watts. After early issues with RTX 4090 connectors melting, PCI-SIG, the standards organization responsible for the 12VHPWR connector, has now updated it to a new 12V-2x6 connector on the GPU side and in some cases the PSU side, too.\n\nThe 12V-2x6 connector has shorter sensing pins and longer conductor terminals, to improve reliability. \u201cThis might not sound like a huge difference, but it matters in ensuring that the power cable has been properly connected to whatever device is going to be pulling power from your system\u2019s power supply,\u201d explains Corsair.\n\nNvidia uses the 12V-2x6 connector on its RTX 50-series GPUs, but you can still use existing 12VHPWR cables. \u201cTo be clear, this is not a new cable, it is an updated change to the pins in the socket, which is referred to as 12V-2x6,\u201d says Corsair. PSU manufacturers like Corsair and MSI have adopted colored pins on their 12VHPWR cables so that if you can still see the yellow or grey pins it means the connector isn\u2019t seated properly.\n\nDamage to a PSU and the 12VHPWR power connector. Image: El Chapuzas Infomatico\n\nWhile Intel and AMD are both members of the PCI-SIG group that helped develop the 12VHPWR power connector, only Nvidia has adopted the standard so far for consumer GPUs. Even AMD\u2019s upcoming Radeon RX 9070-series are using existing 8-pin PCIe connections instead. AMD even suggested the 12VHPWR connector was a fire hazard in late 2022, when the company\u2019s gaming marketing director Sasa Marinkovic tweeted \u201cStay safe this holiday season\u201d alongside a picture of 8-pin connectors.\n\n12VHPWR has been branded a \u201cdumpster fire,\u201d thanks to design oversights that make it relatively easy for end users to not properly connect the cable securely. Cablemod was also forced to recall its 12VHPWR GPU power adapters last year after reports of melted adapters.\n\nWe reached out to Nvidia to comment on these latest reports of RTX 5090 power connector issues, but the company refused to comment.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "An AI giant poses a new threat to Nvidia",
            "link": "https://www.thestreet.com/investing/nvidia-stock-slips-amid-report-of-openai-plan-for-ai-chip-development",
            "snippet": "Updated at 9:37 AM EST. Nvidia shares bumped higher in early Monday trading but remain in negative territory for the year, following a report suggesting...",
            "score": 0.916420578956604,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-09": {
        "0": {
            "title": "#176 Nvidia\u2019s Rise: A Story of Vision, Not Chance",
            "link": "https://substack.com/home/post/p-156821118?utm_campaign=post&utm_medium=web",
            "snippet": "Last week, I was reading The Nvidia Way, and coincidentally, the stock market saw one of its biggest drops ever in the industry\u2014driven by Nvidia's 15%...",
            "score": 0.9044244289398193,
            "sentiment": null,
            "probability": null,
            "content": "Last week, I was reading The Nvidia Way, and coincidentally, the stock market saw one of its biggest drops ever in the industry\u2014driven by Nvidia's 15% decline after DeepSeek went viral. Of course, no company is invincible. But being deep into Nvidia\u2019s story and learning about Jensen Huang, I realized something fundamental: unlike many tech companies that rely on trends, Nvidia creates them. They didn\u2019t just capitalize on the AI boom; they made it possible. And with the rise of edge computing, self-driving cars, and even the metaverse, their vision still has plenty of room to grow.\n\nI\u2019ve read quite a few books about great companies and their founders\u2014Elon Musk\u2019s autobiography, Shoe Dog by Phil Knight, and others. One common theme in these stories is luck. Right place, right time, right connections. Luck always played some role in their success. But Nvidia\u2019s story just feels different to me. It wasn\u2019t luck that made Nvidia what it is today\u2014it was Jensen Huang\u2019s vision and hard work. He didn\u2019t just stumble into an opportunity. He saw the future before anyone else and built a company to meet it.\n\nWhen the industry norm dictated a two-year cycle for chip manufacturing, Jensen pushed boundaries and reduced the cycle to just six months. This wasn\u2019t a minor improvement\u2014it was a complete paradigm shift that allowed Nvidia to outpace competitors and deliver cutting-edge technology at an unprecedented speed. Jensen also recognized early on that GPUs could be more than just gaming hardware. While most companies were focused on traditional computing, he saw an opportunity in academia and high-performance computing. Nvidia poured resources into developing CUDA, a programming model that allowed GPUs to be used for deep learning and scientific research in spite of facing the pressure from investors. This investment played a key role in enabling breakthroughs like Ilya Sutskever\u2019s famous ImageNet paper, which unlocked the power of compute.\n\nReading the book I realized even the company\u2019s name has an interesting story. The name \u201cNvidia\u201d comes from the Latin word Invidia, which means \u201cenvy.\u201d The idea was that Nvidia\u2019s graphics would be so advanced, competitors would be envious. It\u2019s a bold statement\u2014one that the company has certainly lived up to over the years.\n\nToday, Nvidia is at the center of the AI revolution. Companies like OpenAI, Google, and Meta rely on Nvidia\u2019s chips to train their massive AI models. Their dominance in AI computing isn\u2019t an accident\u2014it\u2019s a result of decades of preparation. While other companies scrambled to pivot into AI, Nvidia had already built the foundation. It wasn\u2019t just about making powerful GPUs; they created the software ecosystem around it, ensuring that AI researchers and developers had the tools they needed. CUDA, TensorRT, and other Nvidia technologies have become the backbone of modern AI advancements today.\n\nSure, timing matters, but more than anything, I feel Jensen\u2019s ability to see the future and position Nvidia accordingly made all the difference. Many companies get a lucky break but fail to capitalize on it. Others try to follow trends but arrive too late. Nvidia didn\u2019t wait for luck. It built the future it wanted to see.\n\nAnd that\u2019s The Nvidia Way\u2014vision and hard work over luck, every time.\n\nNvidia is Jensen. Jensen is Nvidia. Jensen is the company.\n\nCurated Picks for the Week",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Stock Investors Just Got Great News From Amazon and Google-Parent Alphabet",
            "link": "https://www.fool.com/investing/2025/02/09/nvidia-stock-investors-great-news-amazon-google/",
            "snippet": "In January, Chinese start-up DeepSeek introduced an artificial intelligence (AI) chatbot that quickly became the most downloaded free mobile application in...",
            "score": 0.8223106265068054,
            "sentiment": null,
            "probability": null,
            "content": "In January, Chinese start-up DeepSeek introduced an artificial intelligence (AI) chatbot that quickly became the most downloaded free mobile application in the U.S. Importantly, the base large language model behind the DeepSeek application allegedly cost $6 million to train and reportedly outperforms top U.S. models on certain benchmarks.\n\nThat shook Wall Street because OpenAI spent over $100 million training its GPT-4 model and supposedly had access to more advanced chips than DeepSeek. Investors rushed to the conclusion that U.S. companies have been overspending on AI infrastructure, and Nvidia (NVDA 5.27%) shares fell 17% on the news, erasing $600 billion in market value. That is the largest single-day loss in U.S. history.\n\nNvidia shares are still down by about 9% since DeepSeek rattled the market, and the stock is currently 13% off its all-time high. But Nvidia shareholders just got some good news from Amazon (AMZN 2.09%) and Alphabet (GOOGL 1.68%). Read on to learn more.\n\nSome Wall Street analysts are skeptical about the DeepSeek narrative\n\nMany analysts have praised DeepSeek for its engineering breakthroughs. The company reduced expenses by cutting down on data processing with innovative training techniques. However, several analysts also question whether DeepSeek has been completely forthcoming about its infrastructure and the associated costs.\n\nFor instance, Dan Ives at Wedbush Securities wrote, \"Saying DeepSeek was built for $6 million with no Nvidia next generation hardware is likely a fictional story.\" Likewise, research company SemiAnalysis reports that DeepSeek not only had next-generation Nvidia GPUs but also incurred about $1.6 billion in expenses while training its model.\n\nImportantly, cost efficiencies could actually boost demand for Nvidia graphics processing units (GPUs) by enabling faster diffusion of artificial intelligence (AI) through the economy. Put differently, if novel training methods reduce model development costs, more software companies will build AI products, increasing the total demand for Nvidia GPUs.\n\nAmazon and Google plan to invest even more aggressively in AI infrastructure\n\nLast week, Amazon and Google parent Alphabet reported fourth-quarter financial results. Both companies said capital expenditures would increase substantially in 2025, as they are currently supply-constrained where AI infrastructure is concerned.\n\nAmazon CEO Andy Jassy predicted costs associated with developing AI models will continue to fall. But he also said, \"I think it will make it much easier for companies to be able to infuse all their applications with inference and with generative AI.\" And CFO Brian Olsavsky told analysts that capital spend may exceed $100 billion in 2025, up from $83 billion in 2024, due to demand for AI infrastructure.\n\nAlphabet CEO Sundar Pichai provided similar insights. He said a greater percentage of capital expenses are shifting toward inference as training becomes less expensive, implying DeepSeek's breakthroughs would not slow AI spending. Indeed, CFO Ana Ashkenazi told analysts that capital expenditures would total $75 billion in 2025, up from $52 billion in 2024, due primarily to investments in data centers, servers, and networking.\n\nThe capital spending guidance from Amazon and Google parent Alphabet corroborates the narrative that demand for Nvidia GPUs could actually increase as training costs decline because companies will shift their resources toward inference. Andy Jassy compared the situation to how cloud computing sales have actually increased over time despite cloud services becoming less expensive.\n\n\"What happens is companies will spend a lot less per unit of infrastructure, and that is very, very useful for their businesses. But then they get excited about what else they could build that was always cost-prohibitive before, and they usually end up spending a lot more in total on technology once you make the per unit cost less. I think that is very much what's going to happen here in AI.\"\n\nWall Street still expects Nvidia's earnings to grow very quickly\n\nWall Street estimates Nvidia's adjusted earnings will grow at 52% annually through fiscal 2026, which ends in January 2026. That consensus makes the current valuation of 50 times adjusted earnings look cheap. Admittedly, analysts have set a very high bar, so the stock could fall sharply if Nvidia misses those expectations.\n\nHere is the bottom line: Nvidia shareholders worried about the DeepSeek drama can take a deep breath. Many analysts think more efficient AI training techniques will lead to greater demand for Nvidia GPUs. And evidence from the largest hyperscale cloud companies supports that conclusion.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Nvidia (NVDA): Analysts Bullish as AI Inference Becomes Key Growth Driver",
            "link": "https://finance.yahoo.com/news/nvidia-nvda-analysts-bullish-ai-171309650.html",
            "snippet": "We recently published a list of 10 AI News Updates You Should Not Miss. In this article, we are going to take a look at where NVIDIA Corporation...",
            "score": 0.9121724367141724,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of 10 AI News Updates You Should Not Miss. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against other AI news updates you should not miss.\n\nEurope is putting itself on the artificial map with France and the United Arab Emirates having agreed to a framework accord for a 1 gigawatt data center dedicated to artificial intelligence. The French presidency said in a statement that the center will be the core of a new AI \u201ccampus\u201d and will have up to 1 gigawatt of capacity, \u201cwhich represents investments of 30 to 50 billion euros\u201d.\n\nREAD ALSO: Top 10 AI Stocks Trending On Wall Street and 12 High-Flying AI Stocks This Week\n\nThe move is said to be a part of a larger AI agreement signed between French President Emmanuel Macron and his UAE counterpart Sheikh Mohamed bin Zayed Al Nahyan in Paris. President Emmanuel Macron hosted his Emirati counterpart on Thursday.\n\n\u201cThe two leaders expressed their desire to create a strategic partnership in the field of AI and committed to exploring collaborations on projects and investments supporting the development of the AI \u200b\u200bvalue chain\u201d.\n\nAccording to the leaders, the investments would go into both French and Emirati AI and would include buying advanced chips, building data centers, talent development, and establishing virtual data embassies to secure AI and cloud systems in both countries.\n\nFrance is also going to be hosting the Artificial Intelligence (AI) Action Summit at the Grand Palais on February 10-11, 2025. The country has deemed the summit a \u201cwake-up call\u201d for Europe in the global race for AI.\n\n\u201cThe summit comes at exactly the right time for this wake-up call for France and Europe, and to show we are in position\u201d to take advantage of the technology, an official in Macron\u2019s office told reporters.\n\nAttendees at the summit will include Sam Altman, head of OpenAI, Google CEO Sundar Pichai, and Nobel Prize winner Demis Hassabis, who leads the company\u2019s DeepMind AI research unit, amongst many other industry leaders, government officials, and similar figures from around the world.\n\nFor this article, we selected AI stocks by going through news articles, stock analysis, and press releases. These stocks are also popular among hedge funds.\n\nWhy are we interested in the stocks that hedge funds pile into? The reason is simple: our research has shown that we can outperform the market by imitating the top stock picks of the best hedge funds. Our quarterly newsletter\u2019s strategy selects 14 small-cap and large-cap stocks every quarter and has returned 275% since May 2014, beating its benchmark by 150 percentage points (see more details here).",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Dear Nvidia Stock Fans, Mark Your Calendars For February 26",
            "link": "https://www.barchart.com/story/news/30841907/dear-nvidia-stock-fans-mark-your-calendars-for-february-26",
            "snippet": "With earnings day approaching, Nvidia fans await Feb. 26 to see if Wall Street's optimism holds strong or if the rally has truly lost steam amid fresh...",
            "score": 0.9351723194122314,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "\u2018Don\u2019t Pull the Trigger,\u2019 Says Investor About Nvidia Stock",
            "link": "https://www.tipranks.com/news/dont-pull-the-trigger-says-investor-about-nvidia-stock",
            "snippet": "Nvidia (NASDAQ:NVDA) is betting on sustained hyperscaler capex spending to fuel its revenue and margins, but the game may be shifting. The DeepSeek news has...",
            "score": 0.8799440860748291,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NASDAQ:NVDA) is betting on sustained hyperscaler capex spending to fuel its revenue and margins, but the game may be shifting. The DeepSeek news has shaken the industry, raising questions about whether efficiency, rather than brute-force computing power, will dictate the next wave of AI dominance.\n\nIt\u2019s no surprise that the idea of more efficient models has rattled Nvidia\u2019s stronghold. After a steep plunge following the DeepSeek news, the stock has been on a rollercoaster as investors scramble to comprehend what this shift could mean for the chip giant\u2019s dominance.\n\nOne investor, known by the pseudonym Bluesea Research, believes that despite Nvidia\u2019s tremendous track record, the writing on the wall doesn\u2019t look good at the moment.\n\n\u201cDespite great products, Nvidia stock could see a correction in the near term as the market finds the right balance between computing power and efficiency,\u201d the investor opined.\n\nBluesea spots evidence that the short-term focus among hyperscalers will be on building better AI efficiencies, noting that both Amazon and Microsoft have already started to offer DeepSeek models to their customers in an effort to keep costs down.\n\n\u201cAt least in the near term, we could see some adjustment in the investment runway for big tech AI spenders,\u201d predicts Bluesea, adding that \u201cthis will hurt the demand for the latest chips from Nvidia.\u201d\n\nAnother pressure point that Bluesea expects to bring down NVDA\u2019s margins is the advent of unique chips that hyperscalers are in the process of developing. Companies like Google, Amazon, and Microsoft have already made significant strides in designing custom AI chips tailored to their specific workloads, reducing their dependence on Nvidia\u2019s GPUs. Given that hyperscalers account for roughly 40% of Nvidia\u2019s revenue, Bluesea warns that this dual threat \u2013 a shift to in-house solutions and a shrinking customer base \u2013 poses a serious risk to the company\u2019s profitability.\n\nBluesea is also concerned by the large variance between high- and low-revenue and EPS estimates among analysts. The investor mentions that for FY 2027, the high end of the revenue and EPS estimates are roughly double that of the lower projections. This dynamic is fairly unusual among Nvidia\u2019s big tech peers, states Bluesea, and further points to the uncertainty surrounding prospects for the company.\n\nThe bottom line? Bluesea isn\u2019t convinced the risks are fully baked into the lofty valuation \u2013 and calls NVDA shares a Sell. (To watch Bluesea Research\u2019s track record, click here)\n\nWall Street, however, sees things differently. With 37 Buy against just 3 Hold rating, NVDA owns a solid Strong Buy consensus. Analysts peg the stock\u2019s 12-month price target at $178.71 \u2014 a projection that implies a ~38% upside from current levels. (See NVDA stock forecast)\n\nTo find good ideas for stocks trading at attractive valuations, visit TipRanks\u2019 Best Stocks to Buy, a tool that unites all of TipRanks\u2019 equity insights.\n\nDisclaimer: The opinions expressed in this article are solely those of the featured investor. The content is intended to be used for informational purposes only. It is very important to do your own analysis before making any investment.\n\nQuestions or Comments about the article? Write to editor@tipranks.com",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Economics: NVIDIA Stock Falls Due To The Thing -- Look Man, I Have An MA In Journalism -- I Don't\u2026",
            "link": "https://medium.com/@maurasateriale/economics-nvidia-stock-falls-due-to-the-thing-look-man-i-have-an-ma-in-journalism-i-dont-7d1b9aa6f97c",
            "snippet": "On this. the 20th anniversary of Lil' Jon's seminal work, \u201cGet Low\u201d, we apply critical eye to the meaning of skeet. May 23, 2022.",
            "score": 0.9194433093070984,
            "sentiment": null,
            "probability": null,
            "content": "New York, NY -- There\u2019s chaos on Wall Street this week as NVIDIA stock prices tumble due to new AI --- stuff ---look man, I\u2019m a freelance writer making $0.75 a word. I don\u2019t get why the open source algorithms from China are somehow making the computer chips less valuable. I literally just learned it\u2019s \"open source\" and not \"open sores\". Sounded gross to me, but what do I know? There\u2019s an AI company called \"Hugging Face\"? Apparently? None of this stuff makes any sense. I\u2019m just picking up freelance assignments until I get a full time writing position in politics. I didn\u2019t know I needed a goddamn PhD in computer science to talk about senate hearings but here we are. Ok, the stock is up, the word count is 300. We\u2019re done here. Good day",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Nvidia and AMD trade blows over who is faster on DeepSeek AI benchmarks, so which team is telling the truth?",
            "link": "https://www.techradar.com/pro/nvidia-and-amd-trade-blows-over-who-is-faster-on-deepseek-ai-benchmarks-so-which-team-is-telling-the-truth",
            "snippet": "DeepSeek is the new AI darling \u2013 for now at least \u2013 and Nvidia and AMD have been arguing over which of them runs it fastest. AMD began the spat by releasing...",
            "score": 0.6233795285224915,
            "sentiment": null,
            "probability": null,
            "content": "AMD\u2019s benchmarks show RX 7900 XTX beating RTX 4090, 4080 Super in AI tests\n\nNvidia hit back, claiming RTX 5090 is 2.2x faster than AMD\u2019s GPU\n\nBenchmarks differ, but AMD\u2019s RX 7900 XTX is far cheaper than Nvidia\u2019s cards\n\nDeepSeek is the new AI darling \u2013 for now at least \u2013 and Nvidia and AMD have been arguing over which of them runs it fastest.\n\nAMD began the spat by releasing benchmarks showing its RX 7900 XTX high-end graphics card outperforming Nvidia\u2019s RTX 4090 and RTX 4080 Super in DeepSeek R1 tests. Nvidia \u2013 inevitably - responded with its own results which show the opposite. Because of course they do.\n\nAccording to David McAfee on X, AMD\u2019s RDNA3-based GPU was up to 13% faster than the RTX 4090 and 34% ahead of the RTX 4080 Super. The RX 7900 XTX performed best against the RTX 4090 using DeepSeek R1 Distill Qwen 7B, where it led by 13%. AMD also tested Distill Llama 8B and Distill Qwen 14B, with its GPU being 11% and 2% faster, respectively. The RTX 4090 had one advantage, it was 4% ahead in Distill Qwen 32B.\n\nAgainst the RTX 4080 Super, AMD's GPU showed larger leads. The RX 7900 XTX was 34% faster using DeepSeek R1 Distill Qwen 7B, 27% ahead with Distill Llama 8B, and 22% faster using Distill Qwen 14B. Obviously, we can\u2019t be sure quite how Nvidia\u2019s GPUs were configured for the tests and, as AMD ran them, it\u2019s fair to say that Team Red may not have gone out of its way to optimize things for Team Green's hardware.\n\nNvidia fights back\n\nAs Tom\u2019s Hardware points out, the RX 7900 XTX is not widely used for AI, but its RDNA3 architecture includes AI processing capabilities and AMD has marketed this aspect under the \u201cAI Accelerator\u201d label.\n\nShortly after AMD\u2019s benchmarks went live, Nvidia countered in a blog post with its own results, also reported by Tom's Hardware, claiming the RTX 5090 is up to 2.2 times faster than the RX 7900 XTX. Using Qwen 32B, Nvidia reported a 124% advantage, while the RTX 4090 was 47% ahead. With Llama 8B, the RTX 5090 was 106% faster, and the RTX 4090 led by 47%.\n\nThe back-and-forth inevitably highlights the importance of treating manufacturer-released benchmarks with caution. Different optimizations, driver versions, and testing conditions can result in different outcomes.\n\nAre you a pro? Subscribe to our newsletter Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nIt\u2019s also important to note that even if Nvidia\u2019s benchmarks are true, AMD still comes out on top in one very important aspect \u2013 price. Its GPU is significantly cheaper than both of Nvidia\u2019s powerhouse offerings.\n\n(Image credit: Nvidia)",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Super Micro Lost Market Share In 2024 - Nvidia May Have Moved On (NASDAQ:SMCI)",
            "link": "https://seekingalpha.com/article/4756467-super-micro-lost-market-share-in-2024-nvidia-may-have-moved-on",
            "snippet": "Super Micro faces increased competitive pressures, losing 300 bp of market share, with Nvidia increasingly shifting its partnership to Foxconn,...",
            "score": 0.9268219470977783,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia, Microsoft take different approach to President Donald Trump than other tech giants. Here's how",
            "link": "https://m.economictimes.com/news/international/us/nvidia-microsoft-take-different-approach-to-president-donald-trump-than-other-tech-giants-heres-how/articleshow/118094624.cms",
            "snippet": "While Microsoft and Nvidia share a quieter approach to President Donald Trump, their footprints in Washington are the polar opposite.",
            "score": 0.8936349153518677,
            "sentiment": null,
            "probability": null,
            "content": "FAQs\n\n\n\n\n\n(You can now subscribe to our\n\n(You can now subscribe to our Economic Times WhatsApp channel\n\nOn a recent Friday, Jensen Huang, CEO of chipmaker Nvidia , slipped into the White House to meet President Donald Trump for the first time. There was no fanfare, and he left without a single public photo taken of the two, NYT News Service reported.Two weeks earlier, Microsoft CEO Satya Nadella had lunch with Trump at his Mar-a-Lago estate in Florida . They dined with little fuss and also barely made the news, as per a NYT News Service report.Neither executive joined his big tech contemporaries who stood ramrod behind Trump at his inauguration.While Microsoft and Nvidia share a quieter approach to Trump, their footprints in Washington are the polar opposite. Microsoft, on the cusp of its 50th anniversary and schooled by its antitrust fight more than two decades ago, is arguably tech's savviest player on policy issues, with a strong lobbying arm and executives who have nurtured contacts in both political parties.Nvidia is a rookie in Washington. Its profile has grown rapidly in the last few years, thanks to its overwhelming control of the chips that other tech companies need to build big artificial intelligence systems.The stakes for both companies are high. The new administration has to finalize rules about the sale of Nvidia chips and building Microsoft data centers overseas. Microsoft has an array of other concerns, including the electricity demands to power data centers. After a breakthrough development by the startup DeepSeek, Nvidia also faces a risk that the administration could further curtail sales of its chips to China.Nvidia has been beefing up its Washington presence. Last Monday, the company, which opened its first office there last year, brought on board a Republican government affairs executive, Stewart Barber, who worked as an adviser to Ivanka Trump, said two people familiar with Nvidia's office.The company has applied to become a member of the Information Technology Industry Council, a policy group that represents most of the leading tech companies. It has also hired American Global Strategies, led by Robert C. O'Brien, who served as national security adviser for two years under Trump.In a little over a year, Nvidia's market value has ballooned by $2 trillion, making it one of the world's three most valuable public companies, with Microsoft and Apple.A1. In a little over a year, Nvidia's market value has ballooned by $2 trillion, making it one of the world's three most valuable public companies, with Microsoft and Apple.A2. Donald Trump resides at his Mar-a-Lago estate in Florida.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Tech giants Microsoft and Nvidia taking a quieter approach to Trump",
            "link": "https://www.japantimes.co.jp/business/2025/02/09/companies/microsoft-nvidia-trump/",
            "snippet": "Under Biden, the United States curbed the sale of Nvidia chips to China and capped the sales of its AI chips to more than 100 other countries.",
            "score": 0.8371515870094299,
            "sentiment": null,
            "probability": null,
            "content": "On a recent Friday afternoon, Jensen Huang, CEO of the chipmaker Nvidia, slipped into the White House to meet U.S. President Donald Trump for the first time. There was no fanfare and he left without a single public photo taken of the two.\n\nTwo weeks earlier, Microsoft CEO Satya Nadella had a lengthy lunch with Trump at his Mar-a-Lago estate in Florida. They dined with little fuss and also barely made the news.\n\nNeither executive joined his big tech contemporaries who stood ramrod behind Trump at his inauguration. Instead, the two were on entirely different continents: Nadella was traveling to the World Economic Forum in Davos, Switzerland, while Huang was wrapping up a visit to see suppliers and employees in Taiwan and China.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-08": {
        "0": {
            "title": "Microsoft and Nvidia: The Tech Giants Taking a Quieter Approach to Trump",
            "link": "https://www.nytimes.com/2025/02/08/technology/microsoft-nvidia-trump.html",
            "snippet": "Friday afternoon last week, Jensen Huang, the chief executive of the chipmaker Nvidia, slipped into the White House to meet President Trump for the first...",
            "score": 0.9143983125686646,
            "sentiment": null,
            "probability": null,
            "content": "Friday afternoon last week, Jensen Huang, the chief executive of the chipmaker Nvidia, slipped into the White House to meet President Trump for the first time. There was no fanfare, and he left without a single public photo taken of the two.\n\nTwo weeks earlier, Microsoft\u2019s chief executive, Satya Nadella, had a lengthy lunch with Mr. Trump at his Mar-a-Lago estate in Florida. They dined with little fuss and also barely made the news.\n\nNeither executive joined his big tech contemporaries who stood ramrod behind Mr. Trump at his inauguration. Instead, the two were on entirely different continents: Mr. Nadella was traveling to the World Economic Forum in Davos, Switzerland, while Mr. Huang was wrapping up a visit to see suppliers and employees in Taiwan and China.\n\nThe absence at the inauguration of the chief executives of two of the world\u2019s most valuable companies was perhaps the most visual sign that some companies were trying a lower-key approach as Mr. Trump returned to Washington, even as some peers took to flamboyant displays of courtship.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "The \"Dean of Valuation\" Says Nvidia Stock Could Plunge by 31%. Here's My Contrarian Take on Why DeepSeek Could Fuel It to New Highs, Instead.",
            "link": "https://www.fool.com/investing/2025/02/08/the-dean-of-valuation-says-nvidia-stock-could-plun/",
            "snippet": "Finance professor Aswath Damodaran just slapped a $78 price target on Nvidia stock -- implying over 30% downside from current levels.",
            "score": 0.8110639452934265,
            "sentiment": null,
            "probability": null,
            "content": "Finance professor Aswath Damodaran just slapped a $78 price target on Nvidia stock -- implying over 30% downside from current levels.\n\nAswath Damodaran is an accomplished professor at New York University's Stern School of Business. In particular, Damodaran specializes in valuation -- having written several books on the topic, and often publishing his models and forecasts to the public. Over the years, Damodaran has become known as the \"Dean of Valuation\" among financial journalists and media personalities.\n\nLast week, Damodaran published a new forecast around Nvidia (NVDA 5.27%) -- calling for a 37% drop in share price from current levels (as of Feb. 5).\n\nBelow, I'm going to detail Damodaran's logic to help explain why he's calling for such a drop. From there, I'll give my take on why I'm not fully aligned with his bearish forecast.\n\nDeepSeek changed the chip narrative...or did it?\n\nBy now, you're probably familiar with AI's newest talking point -- namely, a Chinese start-up called DeepSeek. DeepSeek is the latest company to emerge in the AI realm, claiming it's developed game-changing applications for a fraction of the cost used to build mainstream models from OpenAI or Anthropic.\n\nIn Damodaran's analysis, he states that DeepSeek has \"changed the AI story\" that will \"create a bifurcated AI market, with a segment of low-grade AI products that is commoditized and highly competitive and a segment of premium products.\"\n\nOn the surface, I understand what Damodaran is getting at. If (key word \"If\") DeepSeek has built a platform on par with or superior to existing AI models and did so with less costly infrastructure, Nvidia's position as the king of the chip realm would appear jeopardized.\n\nTo me, the above contention is still more of a theory than anything. It seems that each hour, more stories are publishing about DeepSeek -- many of which are now alleging the start-up was funded with much more than the initial $6 million it claimed. If that's the case, then Nvidia has less to worry about.\n\nBut in a world where DeepSeek was built for far less than funding compared to what was plowed into OpenAI and its cohorts, I still don't see such a notion as a bad thing for Nvidia. The reason actually lines up with Damodaran's point of chipware becoming commoditized.\n\nRight now, it's well known that many of Nvidia's largest customers include cloud hyperscalers such as Microsoft, Alphabet, and Amazon. Moreover, big tech giants such as Meta Platforms and Tesla are also some of Nvidia's biggest adopters. What is also known is that many of these companies are investing heavily into internal chipware and working with lower-cost providers, such as Advanced Micro Devices.\n\nThe rationale behind these investments is not that Nvidia's chips are falling short of expectations, but rather because these businesses are seeking ways to diversify their own platforms and create cost-saving opportunities in the process. As more chips enter the market, these products would become somewhat commoditized anyway. In my mind, DeepSeek doesn't change the narrative of chips becoming a commodity hardware product at all -- it's reinforcing the idea.\n\nPrecise figures may be more blurry, but the big picture tells the same story\n\nThe one area that I will concede looks a bit blurry right now is Nvidia's growth trajectory. I think DeepSeek's arrival is causing investors to consider the inconvenient (but likely) idea that Nvidia's growth could start decelerating at a meaningful pace someday.\n\nWhile such concerns are legitimate, big tech still appears to be first in line at Nvidia's doorstep for now. Recent comments from Meta CEO Mark Zuckerberg as well as comments from Microsoft's leadership both indicate that investment in AI infrastructure is going to continue for the foreseeable future.\n\nIt's difficult to determine precisely how much of that spending will be designated for Nvidia, but I am highly confident that the leading chip manufacturer will remain central to the world's top AI businesses in the future.\n\nIs now a good time to buy Nvidia stock?\n\nWhat's ironic is that even while Nvidia's largest customers have publicly stated that their capital expenditure (capex) budgets remain robust, shares are still selling off.\n\nIn all honesty, I wouldn't be surprised if Nvidia stock continues experiencing drops until the company reports earnings on Feb. 26. By then, I think investors and analysts will have sufficient detail that could signal what AI spend is going to look like across both near- and long-term horizons.\n\nMy contrarian take is that during Nvidia's fourth-quarter call, the company's leadership will drive one point above all else: Demand for its chips -- including the latest and most expensive architectures -- remains strong and should continue that way for some time.\n\nAs such, I wouldn't be surprised to see shares of Nvidia begin turning around in an epic fashion. For now, I see dips in Nvidia stock as incredible buying opportunities and think the stock will soar much higher from where it is today.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "NVIDIA is investigating reported GeForce RTX 50 issues",
            "link": "https://videocardz.com/newz/nvidia-is-investigating-reported-geforce-rtx-50-issues",
            "snippet": "There's a new development regarding the NVIDIA RTX 50 black screen issue, with some users claiming their cards are permanently damaged, while others suggest...",
            "score": 0.8974994421005249,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia is investigating RTX 5090 and RTX 5080 instability issues",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-is-investigating-rtx-5090-and-rtx-5080-instability-issues",
            "snippet": "Some stopgap solutions exist while we wait for Nvidia's official fix.",
            "score": 0.8777922987937927,
            "sentiment": null,
            "probability": null,
            "content": "Many users reported stability issues with the newly released GeForce RTX 5090 and RTX 5080 graphics cards. At the time, users could not determine the cause of the black screen and GPU initialization issues after installing the latest driver, fearing the problem was permanent. Nvidia acknowledged this issue, started collecting feedback, and began investigating.\n\nAn Nvidia representative told PC Gamer, \"We are investigating the reported issues with the RTX 50 series. \"\n\nAs a quick recap, the issue happens when users upgrade to Nvidia's newly released GeForce Game Ready 572.16 WHQL driver. There are some stopgap solutions to fix the problem. Some users got their RTX 5080 graphics cards to work by limiting the interface to PCIe 2.0, while some lowered the monitor refresh rate to 60Hz or switched to a single display with the Windows 11 23H2 build. One user reportedly used a third-party Display Driver Uninstaller (DDU) utility to remove all traces of the GeForce driver before reinstalling with the GPU limited to PCIe 4.0 in the motherboard BIOS.\n\nBeing an early adopter has its adventures, and the newly released Blackwell series is no exception. The number of users who experienced the black screen issue extended to Youtubers who reported on this matter. Judging by the number of problems and stopgap solutions, the issue is likely related to the latest driver. Interestingly, some RTX 40-series (Ada Lovelace) GPU users reported the same issue involving the GeForce Game Ready 572.16 WHQL driver.\n\nA hotfix and explanation will likely be released, but for now, using the DDU and switching to PCIe 4.0 in the BIOS seems to be one of the better mitigation solutions.\n\nThese issues also extended to previous-generation GPUs, so narrowing this issue is challenging. Acquiring an RTX 5080 or RTX 5090 is a strenuous affair because of limited availability, hiked prices, and scalpers, so we feel for owners experiencing this pesky issue.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia RTX 5090 and 5080 GPUs are causing some PCs to crash \u2013 and driver issues could be the cause",
            "link": "https://www.techradar.com/computing/gpu/nvidia-is-investigating-reports-of-crashes-plaguing-rtx-5090-and-5080-gpus-with-possible-driver-issues-maybe-hitting-rtx-4000-models-too",
            "snippet": "Nvidia's RTX 5090 and 5080 graphics cards are experiencing nasty issues; This could be tied in to the latest drivers, given problems are seemingly also...",
            "score": 0.953410267829895,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia\u2019s RTX 5090 and 5080 graphics cards are experiencing nasty issues\n\nThis could be tied in to the latest drivers, given problems are seemingly also affecting RTX 4000 GPUs\n\nNvidia is \u201cinvestigating the reported issues\u201d and hopefully we\u2019ll hear more from Team Green before long\n\nNvidia is investigating reports of problems with its new RTX 5000 GPUs which is causing some graphics cards to crash. The issues appear to be occurring repeatedly for some, in a variety of circumstances, potentially leaving Team Green with a major headache around its new cards.\n\nThe troubles began when Nvidia pushed out a new graphics driver (version 572.16) and people started experiencing crashes with some PC games. After that came reports of the RTX 5090 misfiring badly, including crashes happening, or the GPU not being recognized by the host PC. (And even the odd tale of a Blackwell flagship being bricked, but we must be very careful around those rare assertions).\n\nThe problems now seem more widespread, and are apparently affecting both RTX 5090 and 5080 models, with repeated crashes occurring as noted \u2013 often freezes leaving you staring at a black screen.\n\nPart of the difficulty here is the wide range of circumstances that these different problems are manifesting in. From apparently non-functional GPUs to issues with multi-monitor setups, there\u2019s a bewildering array of nuances to the individual complaints from RTX 5090 and 5080 owners.\n\nNvidia told PC Gamer that it is now \u201cinvestigating the reported issues with the RTX 50-series,\u201d and I\u2019ve got a feeling that this presumably deep dive into what\u2019s going on here is going to take some time.\n\n(Image credit: Nvidia)\n\nAnalysis: Clean install to cure driver blues?\n\nClearly, the volume of reports across the usual forums (Reddit, and Nvidia\u2019s own message board) has prompted Nvidia to let us know it is indeed taking action on this front. Until we hear further feedback from Team Green, all we can do is trawl through a whole pile of suggested fixes, some of which work for some folks, but not for others.\n\nHowever, this can of worms does appear to be related to the latest driver, an idea backed up by a scattering of reports of problems with RTX 4000 graphics cards and this most recent GPU driver. As PC Gamer theorizes, these gremlins might pertain to old driver files kicking about in the background, clashing with the new setup somehow after the user has installed this latest driver.\n\nGet daily insight, inspiration and deals in your inbox Sign up for breaking news, reviews, opinion, top tech deals, and more. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nThat\u2019s why our sister site advises a full driver wipe (using Display Driver Uninstaller, or DDU, to fully remove every bit of the old graphics driver; this doesn\u2019t happen with a normal install \u2013 tiny bits of driver detritus can remain) before setting up your shiny new RTX 5000 GPU.\n\nSo, driver-wise, your choices are to roll back to the previous Nvidia graphics driver before this latest release, or do a full wipe (using DDU as mentioned) before reinstalling the latest driver, keeping your fingers firmly crossed.\n\nOr, just live with your gaming PC as it is \u2013 if the crashing isn\u2019t excessively annoying in your particular case \u2013 in the hope that Nvidia deploys a hotfix soon enough. It\u2019s possible this could happen, as one recently arrived to solve the crashing issues with a couple of games in the latest driver.\n\nMeanwhile, if you\u2019re in \u2018live with it\u2019 mode, there are some common workarounds you can try which are simple and have done the trick for some folks. They include turning off HDR in Windows 11 (assuming you have it on, of course) and reducing the refresh rate of your monitor to 60Hz.\n\nThe latter appears to have worked for a fair few people to resolve some, if not all, of the crashing. So that\u2019s one to have a shot with, definitely. It might also help to explain why multi-monitor setups are seemingly potentially more wonky, as if there are monitor-related issues in the driver, then obviously they\u2019re more likely to happen in that scenario.\n\nBear in mind that as advised in the past, some RTX 5090 owners have found a solution in going into the BIOS and dropping down their graphics card from using PCIe 5.0 to PCIe 4.0. That will mean the GPU runs slower, but not in a major way, and if it means a crash-free experience then clearly that\u2019s a much better path forward for now, as a temporary fix, until Nvidia comes out with its report (and hopefully a fix).\n\nAs already noted, I\u2019ve a feeling that the solution might be a tricky one here, so I\u2019m not overly optimistic about a quick hotfix \u2013 but you never know.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "NVIDIA Responds To Bricked RTX 5090/5090D GPUs, Says It Is \u201cInvestigating\u201d The Reported Issues",
            "link": "https://wccftech.com/nvidia-responds-to-bricked-rtx-5090-5090d-gpus-says-it-is-investigating-issues/",
            "snippet": "After several reports of supposedly bricked GeForce RTX 5090 GPUs emerged, NVIDIA started investigating the matter.",
            "score": 0.7316707968711853,
            "sentiment": null,
            "probability": null,
            "content": "After several reports of supposedly bricked GeForce RTX 5090 GPUs emerged, NVIDIA started investigating the matter.\n\nAmid Reports of Bricked RTX 5090 and RTX 5090D GPUs, NVIDIA Has Launched an Investigation\n\nThe reports of bricked RTX 5090 GPUs and the Chinese variant RTX 5090D have been piling up for a few days. With no clear solution in sight, many have tried troubleshooting the issue through various methods, only to find the issue being persistent in most cases.\n\nWhile some have claimed to mitigate the issue through unusual methods, the vast majority who are currently facing this haven't had good luck with their GPUs. Since the problem isn't uncommon now, it has reached NVIDIA, which has finally responded. PC Gamer reports that NVIDIA has told them that,\n\nWe are investigating the reported issues with the RTX 50 series. NVIDIA Rep to PCGamer\n\nAnd that's it! Nothing in detail has been forwarded, but it does show that NVIDIA has finally taken notice of the matter. If you aren't aware of the problem, then you should refer to the article we published a few days ago, detailing the issue. Simply put, several RTX 5090 and RTX 5090D were unable to power up the monitor display and were only showing black screens once the driver was updated.\n\nThis was common across most reports and as soon as the users installed the latest NVIDIA driver. The issue has become quite complex on various levels to the point that even after rolling back to the previous driver version, the GPU couldn't be recognized on the computer. Some users have reported that even resetting the BIOS couldn't do anything and the GPU would simply refuse to show up in Device Manager or even BIOS as well.\n\nThe root cause is unknown, but some reports suggest that this is due to architectural or driver compatibility issues and not related to the hardware itself. We will have to wait for NVIDIA's response on this since nobody has found the solution to the problem that could fix the bricking of RTX 5090/5090D permanently.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Hurry up and buy stocks like 'compelling' Nvidia ahead of earnings, Bank of America says",
            "link": "https://www.cnbc.com/2025/02/08/nvidia-is-compelling-ahead-of-earnings-bank-of-america-says.html",
            "snippet": "Bank of America said there's still plenty of quality buying opportunities ahead of next week's earnings reports.",
            "score": 0.8490768671035767,
            "sentiment": null,
            "probability": null,
            "content": "There's still plenty of stocks to buy ahead of earnings, according to Bank of America. The firm named several companies it says are \"compelling\" such as Nvidia. The other buy-rated stocks include: JD.com, Block and Toronto-Dominon. Toronto-Dominion Toronto-Dominion Bank was recently upgraded to buy from neutral by analyst Ebrahim Poonawala. The Canadian bank had been under scrutiny for failing to properly maintain its anti-money-laundering unit, but Poonawala said the company is turning a corner following a series of fines and penalties imposed by the U.S. Department of Justice. New CEO Raymond Chun took the helm on Feb. 1, bolstering Poonawala's confidence in the stock. He expects the new executive will help drive the franchise \"toward improved profitability,\" he wrote. Shares are up 8% this year and remain attractive, he said. \"We believe the stock is more than adequately discounting downside risks, while giving little credit for improved execution,\" Poonawala wrote. Toronto-Dominion will report earnings in late February. JD.com The China-based e-commerce company really is firing on all cylinders, according to analyst Joyce Ju and team. JD shares are up almost 75% over the past year with plenty more room to run, the firm says. \"Direct sales revenues are estimated to grow 10.5% YoY, driven by 10.6% growth in electronic and home appliances sales and 10.3% growth in general merchandise sales,\" she wrote. Further, Ju estimates that other services revenue like logistics will be markedly higher. The firm likes JD's direct sales model as well as its third-party marketplace capabilities. \"JD.com should grow moderately faster than the industry average off a lower base, driven by diversification of product categories and expansion in business models,\" she wrote. JD is scheduled to report earnings in early March. Block Analyst Jason Kupferberg is standing by Block this year as a slew of positive metrics shows the fintech payment company is extremely well positioned. \"We are bullish on [Block's] full-fledged dual-sided ecosystem,\" he wrote referring to its financial apps, Cash App and Square. The latter is primarily used by businesses. The firm says the stock is just not getting enough credit from investors. \"[Block's] combination of top-line growth and profitability (best among large-caps) is underappreciated in our view...,\" he added. Kupferberg acknowledged the company's Feb. 20 earnings report might not be a significant event for the stock as shares are up almost 25% over the last 12 months. Still, the firm says it sees more upside ahead. Nvidia \"Expect Q4 eps call to reassure on CY25 outlook. Reiterate Buy, top pick ahead of NVDA's FQ4'25 (Jan) earnings call scheduled for 26-Feb. We expect modest beat/inline sales guidance and lower GM in FQ1 (Apr) given Blackwell product transition/China restrictions.\" Block \"[ Block ]'s combination of top-line growth and profitability (best among large-caps) is underappreciated in our view, and as a US-centric re-acceleration story, we believe shares can outperform in '25. ... .We are bullish on [Block's] full-fledged dual-sided ecosystem. We believe the stock is not being given enough credit for the general resilience the business has shown to date as well as its opex discipline.\" Toronto-Dominion \"We are upgrading our rating on (Toronto-Dominion) TD Bank-TD to Buy from Neutral on increased confidence that new leadership under CEO Raymond Chun can fix the US AML issues while driving the franchise toward improved profitability relative to our current forecast. ... .We believe the stock is more than adequately discounting downside risks, while giving little credit for improved execution.\" JD.com \"Direct sales revenues are estimated to grow 10.5% YoY, driven by 10.6% growth in electronic and home appliances sales and 10.3% growth in general merchandise sales. ... .JD.com should grow moderately faster than the industry average off a lower base, driven by diversification of product categories and expansion in business models.\"",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "The Nvidia panic, America's Bitcoin Reserve, and Trump's trade war hits stocks: Markets news roundup",
            "link": "https://qz.com/nvidia-stock-bitcoin-reserve-trump-trade-war-tariffs-1851758149",
            "snippet": "The Nasdaq and other stock market indexes began Monday morning by dropping sharply in the face of President Donald Trump's new tariffs, which are poised to...",
            "score": 0.9030559659004211,
            "sentiment": null,
            "probability": null,
            "content": "With rising prices, an affordable housing crisis, and the specter of looming tariffs, Americans are looking to save money just about everywhere.\n\nAdvertisement\n\nAnd with nine in 10 American households owning at least one car, many are looking at their vehicles as a place to save.\n\nSelf, a credit-building toolkit, looked at the 50 best-selling cars between 2022 and 2024 and ran the numbers to find the most and least expensive ones to run. It didn\u2019t consider the vehicle\u2019s purchase or lease price and instead looked at average annual gas costs, maintenance, insurance, fees, and taxes.\n\nRead More",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia stock set to soar? Bank of America calls it a \u2018Compelling\u2019 buy ahead of earnings \u2013 Here\u2019s why inves",
            "link": "https://m.economictimes.com/news/international/us/nvidia-stock-set-to-soar-bank-of-america-calls-it-a-compelling-buy-ahead-of-earnings-heres-why-investors-are-paying-attention/articleshow/118070900.cms",
            "snippet": "Nvidia stock is gaining attention as Bank of America (BofA) calls it a \u201ccompelling\u201d buy ahead of its earnings report. With soaring demand for AI chips,...",
            "score": 0.6614296436309814,
            "sentiment": null,
            "probability": null,
            "content": "Why Bank of America recommends Nvidia?\n\n\n\nNvidia\u2019s financial strength\n\n\n\nWhat makes Nvidia a strong investment?\n\n\n\nFAQs:\n\n\n\n\n\n\n\n(You can now subscribe to our\n\n(You can now subscribe to our Economic Times WhatsApp channel\n\nBank of America (BofA) is urging investors to buy Nvidia stock ahead of its upcoming earnings report. The bank\u2019s analysts have labeled Nvidia as a \u201ccompelling\u201d investment, citing its strong position in the artificial intelligence (AI) sector, robust financial performance, and future growth potential.Nvidia has become the leader in AI-driven technology, with its advanced graphics processing units (GPUs) powering AI applications worldwide. The demand for its AI chips has surged, with companies like Microsoft, Meta, and Google relying on Nvidia\u2019s hardware for their AI advancements.According to BofA analyst Vivek Arya, Nvidia\u2019s earnings potential remains strong, and the market is underestimating the impact of its latest AI chips. He has raised the stock\u2019s price target from $165 to $190, indicating a potential upside of nearly 39% from its current level (CNBC).Nvidia has delivered remarkable stock performance over the past year, tripling its value as the AI sector expands. Analysts expect the company to generate at least $200 billion in free cash flow over the next two years, strengthening its ability to invest in research and development.The company\u2019s upcoming earnings report will be crucial. Wall Street estimates suggest Nvidia could continue its growth streak, with revenue expected to surpass $30 billion this quarter, reflecting the high demand for AI-powered GPUs.Nvidia dominates the AI chip market, holding over 80% of the GPU segment for AI workloads.Companies investing in AI, including OpenAI and Amazon, use Nvidia\u2019s chips.Nvidia is projected to quintuple its earnings per share by 2027, making it a long-term growth stock.Out of 64 analysts surveyed, none have a sell rating on Nvidia.Bank of America cites AI chip demand, strong cash flow, and growth potential.BofA raised it to $190, predicting a 39% upside.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "The Ultimate NVIDIA Valuation Guide",
            "link": "https://www.nanalyze.com/videos/the-ultimate-nvidia-valuation-guide/",
            "snippet": "NVIDIA stock has been making headlines after DeepSeek caused a massive panic. Does the emergence of a low-cost large language model spell doom for NVDA...",
            "score": 0.9467375874519348,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-07": {
        "0": {
            "title": "AI-Designed Proteins Take on Deadly Snake Venom",
            "link": "https://blogs.nvidia.com/blog/ai-designed-proteins-snake-venom/",
            "snippet": "AI-driven medicine could deliver life-saving snakebite treatments to the world's most vulnerable.",
            "score": 0.9171749949455261,
            "sentiment": null,
            "probability": null,
            "content": "Every year, venomous snakes kill over 100,000 people and leave 300,000 more with devastating injuries \u2014 amputations, paralysis and permanent disabilities. The victims are often farmers, herders and children in rural communities across sub-Saharan Africa, South Asia and Latin America. For them, a snakebite isn\u2019t just a medical crisis \u2014 it\u2019s an economic catastrophe.\n\nTreatment hasn\u2019t changed in over a century. Antivenoms \u2014 derived from the blood of immunized animals \u2014 are expensive, difficult to manufacture and often ineffective against the deadliest toxins. Worse, they require refrigeration and trained medical staff, making them unreachable for many who need them most.\n\nNow, a team led by Susana V\u00e1zquez Torres, a computational biologist working in Nobel Prize winner David Baker\u2019s renowned protein design lab at the University of Washington, has used AI to create entirely new proteins that neutralize lethal snake venom in laboratory tests \u2014 faster, cheaper and more effectively than traditional antivenoms. Their research, published in Nature, introduces a new class of synthetic proteins that successfully protect animals from otherwise lethal doses of snake venom toxins.\n\nHow AI Cracked the Code on Venom\n\nFor over a century, antivenom production has relied on animal immunization, requiring thousands of snake milkings and plasma extractions. Torres and her team hope to replace this with AI-driven protein design, compressing years of work into weeks.\n\nUsing NVIDIA Ampere architecture and L40 GPUs, the Baker Lab used its deep learning models, including RFdiffusion and ProteinMPNN, to generate millions of potential antitoxin structures \u2018in silico,\u2019 or in computer simulations. Instead of screening a vast number of these proteins in a lab, they used AI tools to predict how the designer proteins would interact with snake venom toxins, rapidly homing in on the most promising designs.\n\nThe results were remarkable:\n\nNewly designed proteins bound tightly to three-finger toxins (3FTx), the deadliest components of elapid venom, effectively neutralizing their toxic effects.\n\nLab tests confirmed their high stability and neutralization capability.\n\nMouse studies showed an 80-100% survival rate following exposure to lethal neurotoxins.\n\nThe AI-designed proteins were small, heat-resistant and easy to manufacture \u2014 no cold storage required.\n\nA Lifeline for the Most Neglected Victims\n\nUnlike traditional antivenoms, which cost hundreds of dollars per dose, it may be possible to mass-produce these AI-designed proteins at low cost, making life-saving treatment available where it\u2019s needed most.\n\nMany snakebite victims can\u2019t afford antivenom or delay seeking care due to cost and accessibility barriers. In some cases, the financial burden of treatment can push entire families deeper into poverty. With an accessible, affordable and shelf-stable antidote, millions of lives \u2014 and livelihoods \u2014 could be saved.\n\nBeyond Snakebites: The Future of AI-Designed Medicine\n\nThis research isn\u2019t just about snakebites. The same AI-driven approach could be used to design precision treatments for viral infections, autoimmune diseases and other hard-to-treat conditions, according to the researchers.\n\nBy replacing trial-and-error drug development with algorithmic precision, researchers using AI to design proteins are working to make life-saving medicines more affordable and accessible worldwide.\n\nTorres and her collaborators \u2014 including researchers from the Technical University of Denmark, University of Northern Colorado and Liverpool School of Tropical Medicine \u2014 are now focused on preparing these venom-neutralizing proteins for clinical testing and large-scale production.\n\nIf successful, this AI-driven advancement could save lives, and uplift families and communities around the world.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "I'm genuinely stunned by the overclocking performance of the RTX 5080, and curious as to why Nvidia left so much headroom",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/im-genuinely-stunned-about-the-overclocking-performance-of-the-rtx-5080-and-curious-why-nvidia-left-so-much-headroom/",
            "snippet": "Maybe I won the silicon lottery, but the GB203 sure looks like there was a lot of performance left on the table.",
            "score": 0.5908571481704712,
            "sentiment": null,
            "probability": null,
            "content": "The Nvidia RTX 5080 has been a much-maligned graphics card since its launch. It is a smaller, more power-hungry GPU than its Ada predecessor, and doesn't really offer a whole lot more silicon than the RTX 4080 Super. Yes, it's ostensibly the same price, but is only delivering around 15% higher frame rates at 4K, and less at lower resolutions.\n\nOh boy, I was not expecting this. I've not seen this level of overclocking headroom in a released GPU in years.\n\nThat hasn't stopped them from all selling out the instant they went on sale. After all, the impressive performance of Multi Frame Generation and its $999 sticker price does make it much cheaper than the $2,000+ RTX 5090 and still able to offer a healthy generational uplift in DLSS 4 supporting games.\n\nI've felt pretty uninspired by the second-tier RTX Blackwell GPU, if truth be told. It will be a great basis for high-end gaming PCs going forward, but in raw silicon terms the needle has barely moved from Ada to Blackwell.\n\nWell, I was pretty uninspired until I started overclocking this thing. And oh boy, I was not expecting this. I've not seen this level of overclocking headroom in a released GPU in years. There is a ton of raw performance left on the engineering room floor, and it makes what was a pretty middling performance hike over the RTX 4080 Super rather more telling.\n\nI'm seeing over 500 MHz extra performance to play with, which is giving me practically another 10% higher frame rates. And all without really taxing the power or thermal performance of the Founders Edition board.\n\nSince we got into the realms of boost clocks, and smart GPUs essentially altering their clock speeds on the fly depending on the thermal and power headroom available to it, there hasn't been a lot left for overclockers.\n\nSure, you could bump up the frequency offset a touch, but rarely by a meaningful amount, especially not to a level that would register as anything more than the sort of gaming frame rate variance you'd see just from standard testing. Even factory overclocked cards barely offered anything particularly exciting. Go look at an overclocked Asus RTX 4080 Super and you're lucky if you get another 90 MHz out of it.\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nWhat's different here, with the RTX 5080, is that even those factory overclocked cards are offering something different. The Asus ROG Astral RTX 5080 is offering a +173 MHz overclock and the Gigabyte Aorus RTX 5080 a +188 MHz overclock. We've not seen that level of confidence from a manufacture when it comes to their OC cards in a long while.\n\nNow, as much as people might suggest I've won the silicon lottery\u2014and as much as Nvidia wants to point out that users should not expect every board to be able to hit the same 0.5 GHz boost clock bump that I'm seeing\u2014it's obvious to me there is a lot of headroom in the GB 203 GPU right here.\n\n(Image credit: Future)\n\nI am not some super special overclocker. Though I have been professionally messing around with GPUs for the best part of 20 years, I am a very basic overclocker. MSI Afterburner is my jam, and I don't go too deep. I did play around with the OC Scanner, which tries to tailor the voltage curve to the capabilities of your GPU, and that can give you higher stable performance, but it didn't really move the needle too much with the RTX 5080.\n\nJust going buckwild with the MHz sliders, however, sure did. With the Black Myth Wukong benchmark running, I was quickly able to go through the gears and hit a solid +500 MHz offset. That wasn't quite giving the 3.1 GHz clock I wanted, however, so I squeezed the GPU some more and hit +525 MHz.\n\nIt very much would not go to +550 MHz, though. That way lay catastrophic failure, black screens, and a GPU that was rather tentative about going quite so high again when I rebooted. Having let it lie for a wee while I then went back to +525 MHz and it has remained stable since.\n\nI was rather cavalier with the memory clocks and went all the way up to +1,000 MHz, where I just drew the line out of fear.\n\nAs you can see, the performance increase is there. Pretty much a 10% bump at 4K and not far off at 1440p (which is important, as if you're running DLSS at Quality mode at 4K that's the resolution you're really running).\n\nCombine that with the gen-on-gen bump of the RTX 5080 over the RTX 4080 Super and you're now talking about a 25% increase instead of a 15% bump. That's a far more compelling figure, and makes the RTX Blackwell card a more tempting option, especially as it's the same price and also comes with Multi Frame Gen as an extra performance panacea.\n\nOf course, it's still well below the 50%+ frame rate increase you saw going from the RTX 3080 up to the RTX 4080, but it's a better look than the second-tier RTX Blackwell card has presented so far, and much closer to the 30% boost the RTX 5090 gets over the RTX 4090. And it also actually delivers a few occasions where the RTX 5080 is able to match the raw rendering power of that top Ada GPU.\n\nIt's barely shifted the thermals or performance demands of the GPU.\n\nOne other thing that has impressed me, and made me feel like this is more than just a numbers game, but an overclock I could legitimately live with fulltime, is that it's barely shifted the thermals or performance demands of the GPU. I was half expecting an exponential increase to get that extra 10%, which would have explained why Nvidia had been so conservative on the boost clocks. But no, it's barely changed.\n\nQuite why there is so much performance being left behind as standard, I'm not sure. I only have this one RTX 5080 Founders Edition to hand right now, so it will be interesting to see what other manufacturers' cards deliver on that front, but I would expect to still see some performance in there to play with.\n\nWhat's interesting here is that the upcoming RTX 5070 Ti also uses the same GB 203 GPU. Sure, it's going to be a fairly cut down version, but it's also got a default boost clock well below that of the RTX 5080. If there is the same headroom in that card it could well be the overclocking king of the upper mid-range.\n\nStill, for now, I'm going to bask in the glory of winning the silicon lottery. And carry on making it run Football Manager 2024 anyways. I mean, it's going to have to keep doing that for a while now the 2025 version's been cancelled, eh \ud83e\uded7.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Will DeepSeek's Artificial Intelligence Model Destroy Nvidia's Valuation?",
            "link": "https://www.fool.com/investing/2025/02/07/will-deepseeks-artificial-intelligence-model-destr/",
            "snippet": "DeepSeek claims it has made an artificial intelligence model for less than $6 million, and that it's comparable to ChatGPT.",
            "score": 0.7304636240005493,
            "sentiment": null,
            "probability": null,
            "content": "Did DeepSeek's artificial intelligence (AI) model really cost less than $6 million to make? If that's the case, it makes you wonder what big tech plans to spend tens of billions of dollars on this year, not to mention the massive $500 billion Stargate project that President Trump announced last month.\n\nDeepSeek's numbers may be grossly underestimated, however, with a recent report suggesting that the company may have spent well over $500 million just on its hardware. It'll inevitably take time before investors get a good grasp on just how concerning of a problem DeepSeek's AI development is or isn't for the tech sector.\n\nIn the meantime, one stock that's been declining on these developments is chipmaking giant and AI figurehead Nvidia (NVDA 5.27%). The stock has been synonymous with AI development, as its chips have been viewed as crucial for companies developing next-gen models. The news related to DeepSeek has already resulted in some sizable losses to Nvidia's market cap, but could this be just the start -- is more of a sell-off coming?\n\nWhy I wouldn't be too worried about the DeepSeek AI model\n\nThe best and brightest minds in tech work in the U.S., for top tech companies such as Nvidia, Microsoft, Apple, and other well-known names. To suggest a Chinese start-up company that launched in 2023 has put to shame some of the most successful and most valuable businesses in the world is just not a scenario I'd consider highly plausible.\n\nOdds are, DeepSeek's costs to develop its AI model are significantly understated. After all, it's not as if investors have audited financial statements they can look at to assess the true costs. ChatGPT-maker OpenAI is also alleging that DeepSeek used its AI models in creating the new chatbot. \"We are aware of and reviewing indications that DeepSeek may have inappropriately distilled our models.\" If DeepSeek did rely on OpenAI's model to help build its own chatbot, that would certainly help explain why it might cost a whole lot less and why it could achieve similar results.\n\nFor now, however, I wouldn't rush to assume that DeepSeek is simply much more efficient and that big tech has just been wasting billions of dollars.\n\nThis doesn't mean Nvidia's stock is out of the woods\n\nIf DeepSeek's AI model does indeed prove to be too good to be true and cost much more than the company said it did, it still may not necessarily lead to a significant rebound in Nvidia's valuation. What the news relating to DeepSeek has done is shined a light on AI-related spending and raised a valuable question of whether companies are being too aggressive in pursuing AI projects.\n\nThis could lead to companies reevaluating their tech needs and determining whether all that spending is justifiable. And a time when the threat of tariffs is weighing on the economy, it may be tempting for businesses to scale back their AI-related expenditures given the uncertainty ahead.\n\nFor Nvidia, a company that has soared in value due to its impressive growth, any slowdown in demand could make the chipmaker's stock vulnerable to more of a correction.\n\nIs Nvidia's stock still a good buy?\n\nAs of Monday, Nvidia's stock was down 12% to start the new year. And based on analyst projections, it's now trading at 28 times its future profits, which isn't all that expensive for a top tech company. But if those projections come down, then the stock's valuation won't look nearly as attractive as it does today.\n\nHowever, if you're buying the stock for the long haul, it may not be a bad idea to load up on it today. The company's impressive profit margins, strong market position, and reduced valuation could make now an optimal time to add Nvidia's stock to your portfolio since it still has a bright future ahead.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "AI Chip Stocks Rise On Latest Data Center Spending Plans",
            "link": "https://www.investors.com/news/technology/nvidia-ai-chip-stocks-rise-capex-plans/",
            "snippet": "Leading AI chip stocks got a lift this week from hyperscale cloud computing companies reiterating their hefty data center spending plans.",
            "score": 0.6109048128128052,
            "sentiment": null,
            "probability": null,
            "content": "Leading AI chip stocks got a lift this week from hyperscale cloud computing companies reiterating their hefty data center spending plans. The reassuring reports came after concerns last week that companies might pause their spending following news that China's DeepSeek built a low-cost AI system.\n\nNvidia (NVDA), Broadcom (AVGO) and Marvell Technology (MRVL) moved higher on capex reports from Amazon.com (AMZN), Google parent Alphabet (GOOGL), Facebook parent Meta Platforms (META) and Microsoft (MSFT).\n\nAmazon said late Thursday it will spend about $100 billion on capital expenditures this year, up 29% over last year's capex.\n\nEarlier in the week, Alphabet said it plans to spend $75 billion on capex in 2024, a 43% lift over 2024. Microsoft has said it will spend $80 billion building artificial intelligence data centers in its fiscal year ending June. And Meta said it plans to spend as much as $65 billion on capex in 2025.\n\nDeepSeek's success with its budget AI system is not causing hyperscalers to rethink their plans to invest heavily in cutting-edge data centers for artificial intelligence.\n\nThe top dog among AI chip stocks, Nvidia, earned a supportive report from investment Morgan Stanley on Thursday. Analyst Joseph Moore reiterated his \"top pick\" buy rating on Nvidia stock with a price target of 152.\n\nOn the stock market today, Nvidia stock climbed 0.9% to close at 129.84.\n\n\"Capex commentary from Nvidia's largest customers reaffirmed investment trajectories, and emphasized a continued near-term supply-demand mismatch,\" Moore said in a client note.\n\nThe top three AI chip stocks \u2014 Nvidia, Broadcom and Marvell \u2014 are on the IBD Tech Leaders list.\n\nRambus Joins AI Chip Stocks\n\nMeanwhile, semiconductor companies posting quarterly results this week saw mixed responses from investors.\n\nAI-exposed AMD (AMD) and Arm (ARM) topped estimates but disappointed with their current-quarter guidance.\n\nChips stocks tied to cyclical industries like automotive, industrial and personal electronics were a mixed bag.\n\nGainers post-earnings included Cirrus Logic (CRUS), InterDigital (IDCC), Monolithic Power Systems (MPWR), Rambus (RMBS) and Silicon Labs (SLAB).\n\nDecliners included Impinj (PI), Macom Technology Solutions (MTSI), Microchip Technology (MCHP), Qualcomm (QCOM), SiTime (SITM), Skyworks Solutions (SWKS) and Synaptics (SYNA).\n\nSeveral analysts noted that Rambus is an AI play as it is benefiting from the AI infrastructure buildout with its memory-to-processor interface chips.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nArm Beats Estimates, Gives In-Line Outlook. Stock Falls.\n\nAMD Stock Craters On Soft AI Data Center Business\n\nSee Stocks On The List Of Leaders Near A Buy Point\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia Competitors: Who Are the AI Chip Alternatives?",
            "link": "https://www.nerdwallet.com/article/investing/nvidia-competitors",
            "snippet": "Nvidia's major competitors. Nvidia controls more than four-fifths of the market for add-on GPUs. But there are two other companies with some market share:...",
            "score": 0.9375274777412415,
            "sentiment": null,
            "probability": null,
            "content": "NerdWallet, Inc. is an independent publisher and comparison service, not an investment advisor. Its articles, interactive tools and other content are provided to you for free, as self-help tools and for informational purposes only. They are not intended to provide investment advice. NerdWallet does not and cannot guarantee the accuracy or applicability of any information in regard to your individual circumstances. Examples are hypothetical, and we encourage you to seek personalized advice from qualified professionals regarding specific investment issues. Our estimates are based on past market performance, and past performance is not a guarantee of future performance.\n\nWe believe everyone should be able to make financial decisions with confidence. And while our site doesn\u2019t feature every company or financial product available on the market, we\u2019re proud that the guidance we offer, the information we provide and the tools we create are objective, independent, straightforward \u2014 and free.\n\nSo how do we make money? Our partners compensate us. This may influence which products we review and write about (and where those products appear on the site), but it in no way affects our recommendations or advice, which are grounded in thousands of hours of research. Our partners cannot pay us to guarantee favorable reviews of their products or services. Here is a list of our partners.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "See CG benchmarks for NVIDIA\u2019s GeForce RTX 5090 and 5080 GPUs",
            "link": "https://www.cgchannel.com/2025/02/see-cg-benchmarks-for-nvidias-geforce-rtx-5090-and-5080-gpus/",
            "snippet": "Discover how NVIDIA's Blackwell GPUs performed in CG software like 3ds Max, Arnold, Blender, Maya, UE5 and V-Ray in our early tests.",
            "score": 0.9342164397239685,
            "sentiment": null,
            "probability": null,
            "content": "The first NVIDIA GeForce RTX 50 Series GPUs are now available. Ahead of a longer review, CG Channel\u2019s hardware expert Jason Lewis sets out his early benchmark test results for the GeForce RTX 5090 and 5080 in CG apps including 3ds Max, Blender, Maya and Unreal Engine.\n\n\n\nLast week saw the launch of the GeForce RTX 5090 and GeForce RTX 5080, the first cards from the GeForce RTX 50 Series, NVIDIA\u2019s new consumer GPUs based on its Blackwell architecture.\n\nSince then, there have been quite a few online reviews of the new GPUs, most focused on gaming, and a few on AI workloads. But how do they perform in DCC and graphics workflows?\n\nWell, I have also begun testing the GeForce RTX 5090 and 5080 with a handful of popular DCC applications, and have some quick-and-dirty early test results that I can share with you here.\n\nTechnical specifications\n\nI tested the new GPUs against the equivalent cards from the Ada-generation GeForce RTX 40 Series GPUs, the GeForce RTX 4090 and GeForce RTX 4080 Super.\n\nHere\u2019s how the GeForce RTX 5090 and GeForce RTX 5080 stack up on paper against their previous-gen counterparts.\n\nNVIDIA\u2019s GeForce RTX 50 Series GPUs and previous-gen counterparts RTX 5090 RTX 4090 RTX 5080 RTX 4080\n\nSuper Architecture Blackwell Ada Lovelace Blackwell Ada Lovelace CUDA cores 21,760 16,384 10,752 10.240 Tensor cores* 680 512 336 320 RT cores * 170 128 84 80 Base clock (GHz) 2.01 2.23 2.30 2.29 Boost clock (GHz) 2.41 2.52 2.62 2.55 Compute performance\n\nFP32 (Tflops)* 104.8 82.6 56.3 52.2 GPU memory 32 GB GDDR7 24GB\n\nGDDR6X 16 GB GDDR7 16GB\n\nGDDR6X TGP 575W 450W 360W 320W Release date 2025 2022 2025 2024 MSRP at launch $1,999 $1,599 $999 $999\n\n*Data taken from third-party websites.\n\n\n\nTesting process\n\nFor my early tests, I used the same test rig and benchmark scenes as my most recent group test of GeForce RTX 40 Series GPUs, updated to the current versions of the software.\n\nBefore we get to the actual test results, I want to reiterate something that many of you will already know: that hardware development almost always outpaces software development.\n\nProfessional graphics applications evolve quite slowly in comparison to games and other consumer applications, and it takes some time for CG software to reach the level of optimization that really allows it to take advantage of new GPU architectures.\n\n\n\nBenchmarks and performance in DCC applications\n\nAs you can see, the early test results are all over the place. In some tests, the GeForce RTX 5090 and 5080 are significantly faster than their previous-generation counterparts, but in some they are only slightly faster, and in a few, they are actually slower.\n\nSo what does this mean? Most other early reviewers have concluded that both NVIDIA\u2019s GPU drivers and the software they are used with require updates before we can assess the true performance of the new GPUs, and I believe that we\u2019re in the same boat.\n\nI will be doing much more extensive testing of these new GPUs in the coming weeks, then providing a comprehensive review, and I hope to have a clearer picture by then.\n\nEarly availability\n\nOne last thing I want to talk about is the current availability of the GeForce RTX 5090 and 5080.\n\nMany reviewers, myself included, believe that last week was really a paper launch for the GPUs, since there is currently very little availability in retail channels.\n\nSome retailers estimate that it could take several months for supply to catch up with demand, and while gamers are understandably upset about this, it\u2019s much less of an issue for CG artists.\n\nBy the time the new GPUs are back in full supply, the drivers should have matured, and graphics software should be able to take better advantage of their potential.\n\nMany DCC software developers release major new versions of their software in mid-spring, so it will be interesting to see how those new versions fare with the Blackwell GPUs.\n\nFind specifications for the GeForce RTX 50 Series GPUs on NVIDIA\u2019s website\n\n\n\nHave your say on this story by following CG Channel on Facebook, Instagram and X (formerly Twitter). As well as being able to comment on stories, followers of our social media accounts can see videos we don\u2019t post on the site itself, including making-ofs for the latest VFX movies, animations, games cinematics and motion graphics projects.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Should You Buy the Dip on Nvidia Stock This February?",
            "link": "https://moneymorning.com/2025/02/07/should-you-buy-the-dip-on-nvidia-stock-this-february/",
            "snippet": "Nvidia (NASDAQ:NVDA) doesn't really need an introduction. It now has an 80% hold over the global GPU market and no other company comes close to it when it...",
            "score": 0.9132172465324402,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "How Nvidia\u2019s New AI-Optimised GPUs Transform Gen AI on PCs",
            "link": "https://technologymagazine.com/articles/how-nvidias-new-ai-optimised-gpus-transform-gen-ai-on-pcs",
            "snippet": "Nvidia launches GeForce RTX 5090 & 5080 chips with AI acceleration hardware, ML capabilities and development tools that enable AI models to run on PCs.",
            "score": 0.8982419371604919,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia Stock Is Climbing. Amazon\u2019s Earnings Were a Big Moment for the Chip Maker.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-87ccceac",
            "snippet": "Nvidia stock was rising early Friday. A solid week for the artificial-intelligence chip maker is coming to a close. As of Thursday's close, the stock was up...",
            "score": 0.9455337524414062,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "UPDATE: NVIDIA says it's \"investigating the reported issues with the RTX 50-series\" as reports of faulty GPUs pile up",
            "link": "https://www.windowscentral.com/hardware/cpu-gpu-components/nvidia-rtx-5090d-driver-issues",
            "snippet": "UPDATE: Just a few days after the initial coverage by Wccftech, PCGamer reached out to NVIDIA for some answers. In response, NVIDIA said it's \"investigating...",
            "score": 0.964438259601593,
            "sentiment": null,
            "probability": null,
            "content": "The new RTX 5090 and 5090D seems to be having some driver issues.\n\nUPDATE: Just a few days after the initial coverage by Wccftech, PCGamer reached out to NVIDIA for some answers. In response, NVIDIA said it's \"investigating the reported issues with the RTX 50-series.\"\n\nIt's never easy to pinpoint an issue like this, especially with hardware that's so new on the market. It gets worse when there are so many conflicting claims popping up from those who already have the RTX 5090 and RTX 5080 in their hands. Some users have been able to fix the issue on their own, while others are left with no video output and hardware that's unrecognized by Windows.\n\nIt still appears that the root of the problem comes from NVIDIA's 572.16 driver which was released alongside the RTX 50-series launch. Considering some RTX 40-series users are reporting similar issues with the older hardware, it's not likely an issue with the new hardware itself.\n\nI'll continue to keep on top of the situation, and I'll hopefully be able to offer a concrete solution very soon.\n\nRecent updates\n\nNVIDIA's launch of new RTX 5090 and RTX 5080 GPUs has so far been rocky, and it doesn't appear to be getting any better. Amid RTX 5000 stock shortages, mediocre performance gains, and sky-high prices from scalpers, some cards could be outright failing after updating to NVIDIA's latest drivers.\n\nAs reported by Wccftech, it appears that some owners of the RTX 5090D \u2014 a variant of the RTX 5090 made for Chinese markets \u2014 are struggling with their new GPUs following the recommended NVIDIA driver update. These reports come from Chinese forums and social media hotspots like Baidu, Chiphell, and Bilibili; it looks like Colorful, Gigabyte, and Manli brands are affected.\n\nSomeone isn't having a good time with their new RTX 5090. (Image credit: Future | Reddit)\n\nHowever, the issue doesn't appear to be exclusive to the 5090D variants. A member of the r/ASUS subreddit made a post two days ago with the title \"RTX 5090 not detected\" with an issue similar to the one experienced by Chinese users. The Reddit user, unable to find a solution, sent the RTX 5090 back to NVIDIA to hopefully get some answers.\n\nGet the Windows Central Newsletter All the latest news, reviews, and guides for Windows and Xbox diehards. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nNVIDIA RTX 5090/D symptoms to watch for\n\nAre the RTX 5090/D's new drivers causing major issues? (Image credit: Windows Central | Ben Wilson)\n\nUsers who took to forums to detail their RTX 5090/D issues all have the same common complaints. Following an update to NVIDIA's latest driver that unlocks DLSS 4, some cards refuse to output any video or even be recognized by Windows. Swapping between HDMI and DisplayPort outputs doesn't seem to fix anything.\n\nIt remains unclear if all RTX 5090 and RTX 5090D GPUs are affected by the problem, but I feel like we'd be hearing more reports if this were the case. Despite a severely limited launch in which many major retailers had only a handful of cards to sell, plenty have made it into the wild.\n\nA Little More Performance but a Lot More PCIe Issues - RTX 5080 FE Review - YouTube Watch On\n\nWhat's interesting is that reviewer der8auer mentions PCIe issues in his RTX 5080 Founders Edition review. Testing the GPU with the PC's BIOS set to PCIe 5.0 resulted in frequent crashes and black screens, which sounds eerily similar to what folks are experiencing with the RTX 5090 and 5090D. Changing the motherboard's BIOS to force PCIe 4.0 alleviated der8auer's issues, but that doesn't mean it's the right fix for whatever is happening to the RTX 5090/D cards.\n\nWindows Central Senior Editor Ben Wilson tested the RTX 5090 Founders Edition on PCIe 5.0 successfully, though it was with an advance version of the drivers. Furthermore, using standard release drivers after launch, he's had no issues expanding RTX 5080 testing for DLSS 4 in Cyberpunk 2077.\n\nThis all seems to point to some sort of driver issue on NVIDIA's end that only affects some hardware. While it's usually AMD receiving flak for its underbaked GPU support system, NVIDIA's rush to launch its new Blackwell hardware appears to have resulted in some unfixed bugs. I wish I could offer a solid fix right now, but we'll have to wait and see if NVIDIA addresses the problem and what it suggests as a solution.\n\nRTX 5000 GPUs are sold out everywhere\n\nThe last thing early RTX 5090/D adopters want to discover is a major bug like this one apparently affecting some users. Dropping $2,000+ on a single piece of hardware, only to discover that it's not functional, will undoubtedly tarnish NVIDIA's reputation (assuming these issues are valid and widespread).\n\nAnd it all comes on the heels of a shaky release that some are referring to as a \"paper launch\" due to a lack of actual inventory to sell. It could be several months before any meaningful RTX 5090 numbers hit shelves, and it's not looking much better for the RTX 5080.\n\nNVIDIA is still planning to launch its RTX 5070 Ti and RTX 5070 in February, and AMD's RDNA Radeon GPUs are expected in March. If you didn't manage to snag a new RTX 5090 on launch day, I wouldn't worry too much. You might be avoiding some next-gen blues while saving your money for something that better fits your system.\n\nYou can always check out my roundup of pre-built RTX 5090 and RTX 5080 PCs should you not want to wait for a restock on standalone cards.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-06": {
        "0": {
            "title": "Medieval Mayhem Arrives With \u2018Kingdom Come: Deliverance II\u2019 on GeForce NOW",
            "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-kingdom-come-deliverance-2/",
            "snippet": "The epic medieval game from Warhorse Studios leads 7 new games joining the over 2000 games in the GeForce NOW cloud gaming library.",
            "score": 0.9005826711654663,
            "sentiment": null,
            "probability": null,
            "content": "It\u2019s part of seven games coming to the cloud this week, kicking off GeForce NOW\u2019s fifth anniversary.\n\nGeForce NOW celebrates its fifth anniversary this February with a lineup of five major releases. The month kicks off with Kingdom Come: Deliverance II. Prepare for a journey back in time \u2014 Warhorse Studios\u2019 newest medieval role-playing game (RPG) comes to GeForce NOW on its launch day, bringing 15th-century Bohemia to devices everywhere.\n\nExperience the highly anticipated sequel\u2019s stunning open world at GeForce RTX quality in the cloud, available to stream across devices at launch. It leads seven games joining the GeForce NOW library of over 2,000 titles.\n\nChainmail Meets the Cloud\n\nKingdom Come: Deliverance II continues the epic, open-world RPG saga set in the brutal and realistic medieval world of Bohemia. Continue the story of Henry, a blacksmith\u2019s son turned warrior, as he navigates political intrigue and warfare. Explore a world twice the size of the original, whether in the bustling streets of Kuttenberg or the picturesque Bohemian Paradise.\n\nThe game builds on its predecessor\u2019s realistic combat system by introducing crossbows, early firearms and a host of new weapons, while refining its already sophisticated melee combat mechanics. Navigate a complex narrative full of difficult decisions, forge alliances with powerful figures, engage in tactical large-scale battles and face moral dilemmas that impact both the journey and fate of the kingdom \u2014 all while experiencing a historically rich environment faithful to the period.\n\nThe game also features enhanced graphics powered by GeForce RTX, making it ideal to stream on GeForce NOW even without a game-ready rig. Experience all the medieval action at up to 4K and 120 frames per second with eight-hour sessions using an Ultimate membership, or 1440p and 120 fps with six-hour sessions using a Performance membership. Enjoy seamless gameplay, stunning visuals and smooth performance throughout the vast, immersive world of Bohemia.\n\nSound the Alarm for New Games\n\nExperience every aspect of a paramedic\u2019s life in Ambulance Life: A Paramedic Simulator from Nacon Games. Quickly reach the accident site, take care of the injured and apply first aid. Each accident is different. It\u2019s up to players to adapt and make the right choices while being fast and efficient. Explore three different Districts containing a variety of environments. At each accident site, analyze the situation to precisely determine the right treatment for each patient. Build a reputation, unlock new tools and get assigned to new districts with thrilling new situations.\n\nLook for the following games available to stream in the cloud this week:\n\nKingdom Come: Deliverance II (New release on Steam, Feb. 4)\n\n(New release on Steam, Feb. 4) Sid Meier\u2019s Civilization VII (New release on Steam and Epic Games Store, Advanced access on Feb. 5)\n\n(New release on Steam and Epic Games Store, Advanced access on Feb. 5) Ambulance Life: A Paramedic Simulator (New Release on Steam, Feb. 6)\n\n(New Release on Steam, Feb. 6) SWORN (New release on Steam, Feb. 6)\n\n(New release on Steam, Feb. 6) Alan Wake (Xbox, available on the Microsoft Store)\n\n(Xbox, available on the Microsoft Store) Ashes of the Singularity: Escalation (Xbox, available on the Microsoft Store)\n\n(Xbox, available on the Microsoft Store) Far Cry: New Dawn (New release on PC Game Pass, Feb. 4)\n\nWhat are you planning to play this weekend? Let us know on X or in the comments below.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "3 Reasons Why I'm Still Loading Up on Nvidia Shares",
            "link": "https://www.fool.com/investing/2025/02/06/3-reasons-why-im-still-loading-up-on-nvidia-shares/",
            "snippet": "I've got three reasons why Nvidia stock is still a buy right now, and investors would be smart to take advantage of it while it's on sale.",
            "score": 0.8882441520690918,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's (NVDA 5.27%) stock has been beaten down over the past two weeks thanks to the release of DeepSeek's efficient generative AI model. Because DeepSeek reportedly trained its artificial intelligence (AI) model for less than $6 million, many investors are assuming that companies like Nvidia that supply this computing power are in trouble, as the AI hyperscalers won't spend as much because more efficient computing methods are possible.\n\nWhile this is a reasonable train of thought, I don't think it's valid, as there are multiple indications that Nvidia will be fine. I've got three reasons why Nvidia stock is still a buy right now, and investors would be smart to take advantage of it while it's on sale.\n\n1. Nvidia's largest clients are still spending big\n\nThe biggest fallacy from investors assuming that Nvidia is toast due to more efficient AI models becoming available is the premise that U.S. AI hyperscalers are concerned with efficiency. The reason why DeepSeek chose to make its model more efficient is because it had to.\n\nDeepSeek is limited to what computing power it has access to, as the U.S. imposed strict export restrictions on Nvidia's hardware to China. As a result, DeepSeek had to train on H800 GPUs versus the more powerful H100 counterparts that are normally used in the U.S. Because it had to focus on efficiency to make the model, the result was naturally efficient.\n\nDomestic AI companies don't have these same limitations, as they have access to near-unlimited computing power without any regulations to deal with. So, they haven't focused on efficiency but are instead building out as much computing capacity as possible to produce the most powerful AI models.\n\nThis is why initiatives like the $500 billion Stargate Project or companies like Meta Platforms are spending $60 billion to $65 billion on capital expenditures this year, most of which will be directed toward AI computing power. These companies will still be spending truckloads of money with Nvidia throughout 2025 and beyond, so there shouldn't be fears of Nvidia's sales slowing down in 2025.\n\n2. Nvidia's Blackwell GPU production has yet to meet demand\n\nAnother innovation Nvidia has up its sleeve is its next-gen Blackwell architecture. The Blackwell architecture far exceeds the performance of the previous-generation Hopper architecture, including being able to train AI models four times faster. These GPUs are an incredible performance boost, and many companies are trying to get their hands on them. But Nvidia can't keep up right now.\n\nDuring its Q3 conference call (which occurred on Nov. 20), Nvidia CFO Colette Kress stated, \"Blackwell demand is staggering, and we are racing to scale supply to meet the incredible demand customers are placing on us.\" Nvidia is still realizing this growth catalyst, which bodes well for the stock in 2025.\n\n3. Nvidia's stock can almost be considered cheap\n\nDuring Nvidia's run, it has rarely looked cheap. However, I'm willing to state that it has nearly reached that level.\n\nBecause of Nvidia's rapid growth, using trailing earnings metrics isn't a great way to assess the stock. As a result, I'll use Nvidia's forward price-to-earnings (P/E) ratio. Thanks to the massive drop-off, Nvidia's stock trades for just 26 times forward earnings.\n\nFor the growth that Nvidia is putting up, that's dirt cheap. For comparison, the S&P 500 trades for 22.3 times forward earnings, so Nvidia really doesn't hold that much of a premium to the broader market. This is despite multiple growth catalysts and a generational technological shift that Nvidia is at the center of.\n\nWith Nvidia's stock down over 20% from its all-time high, I think it's a smart move for investors to take a position now, as it's rare that Nvidia goes on sale like this.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "I Review GPUs for a Living\u2014Here's Why (and How) I'm Buying a GeForce RTX 5080",
            "link": "https://www.pcmag.com/opinions/i-review-gpus-for-a-living-heres-why-and-how-im-buying-a-geforce-rtx-5080",
            "snippet": "Nvidia's new $999 card earned our Editors' Choice award on the basis of its pure muscle, but I have a few very specific reasons I'm vying to buy one.",
            "score": 0.932673990726471,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Don't panic, Nvidia will actually be a DeepSeek winner, JPMorgan says",
            "link": "https://finance.yahoo.com/news/dont-panic-nvidia-actually-deepseek-134700195.html",
            "snippet": "DeepSeek sparked a global tech stock sell-off that cost Nvidia $600 billion in market value. But JPMorgan (JPM) Chase says the AI chipmaker is bound to...",
            "score": 0.9348574876785278,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang during an event in Taipei, Taiwan on June 2, 2024. - Photo: Annabelle Chih/Bloomberg (Getty Images)\n\nDeepSeek sparked a global tech stock sell-off that cost Nvidia $600 billion in market value. But JPMorgan (JPM) Chase says the AI chipmaker is bound to benefit from the Chinese startup.\n\nThe Hangzhou-based artificial intelligence startup sent shockwaves through both Silicon Valley and Wall Street last month after raising questions about Big Tech\u2019s big spending on AI infrastructure.\n\nBut despite the AI-driven stock rally losing $1 trillion in value, JPMorgan U.S. Equity Research analysts said in a new report that DeepSeek will likely have a positive impact on companies such as Amazon (AMZN), Meta (META), and Google (GOOGL) parent Alphabet, as well as chipmaking leader Nvidia (NVDA).\n\nThe Chinese startup launched its open-source DeepSeek-R1 reasoning models in January that performed on par with similar models from OpenAI and Anthropic, while its open-source DeepSeek-V3 model released in December also performed competitively with AI models from the U.S.-based companies \u2014 for far less money and less advanced chips.\n\nAccording to the V3 technical paper, the model cost $5.6 million to train and develop on just under 2,050 of Nvidia\u2019s reduced-capability H800 chips. U.S. firms, meanwhile, are spending billions on tens of thousands of Nvidia\u2019s more powerful H100 chips, which are not allowed to be sold to China under U.S. export controls.\n\nDeepSeek\u2019s demonstration of cost-efficiency and AI innovation will lead to \u201cstrong demand\u201d for higher performance graphics processing units, or GPUs, JPMorgan analysts said Wednesday. \u201cTherefore, Nvidia\u2019s leadership in advanced AI chips \u201cshould enable them to unlock new use-cases.\u201d Other chip firms such as Broadcom (AVGO), Marvell (MRVL), and Micron (MU) are also likely to benefit from DeepSeek, according to JPMorgan.\n\nChip pioneer Intel (INTC), however, stands to lose out from DeepSeek, analysts said, because demand for its central processing units, or CPUs, will decrease as amid the shift to accelerated computing.\n\nJPMorgan analysts also said they \u201cbelieve the broader Internet ecosystem should benefit\u201d following the open-source AI model advancements demonstrated by DeepSeek and China\u2019s Alibaba (BABA). The improved cost efficiency \u201cshould accelerate\u201d AI development and adoption, leading to more consumption, the analysts said, adding that they \u201cexpect heavy capital expenditures investment\u201d by Amazon, Alphabet, and Meta to continue in the medium term.\n\nMeta is expected to benefit from increased open-source model adoption as it builds its next-generation open-source Llama 4 model, JPMorgan said. During the company\u2019s fourth-quarter earnings call, Meta chief executive Mark Zuckerberg, who touts open-source AI models as \u201cgood for the world,\u201d said DeepSeek\u2019s breakthrough shows the need for a global open-source standard led by the U.S.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "The Best Graphics Card (GPU) Deals for February 2025",
            "link": "https://www.extremetech.com/gaming/the-best-graphics-card-deals",
            "snippet": "New GPU models from Nvidia mean chaos in the marketplace. Here are the best discounts on graphics cards for every PC.",
            "score": 0.8066052198410034,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Trying to sign up for Nvidia GeForce? You won\u2019t be able to for another two weeks",
            "link": "https://www.polygon.com/news/520359/nvidia-geforce-now-sold-out-return-new-users",
            "snippet": "The PC game streaming service isn't accepting new members due to a payment provider transition, but Day Passes will be the first to allow them.",
            "score": 0.9007336497306824,
            "sentiment": null,
            "probability": null,
            "content": "If you try to sign up for Nvidia GeForce Now, the GPU maker\u2019s game streaming service that lets you stream your PC games on other devices, you currently can\u2019t. Every tier of service has a \u201csold out\u201d notice above it, with no indication as to when people will be able to sign up again. But don\u2019t fret, it\u2019s coming back soon \u2014 well, part of it is, anyway.\n\nNvidia spokesperson Stephenie Ngo told Polygon that it expects to be able to welcome new sign-ups \u201cin roughly two weeks with Day Passes.\u201d So, you still won\u2019t be able to sign up for the one-month or six-month plans, but you\u2019ll at least be able to pay to play games one day at a time.\n\nAs for why the service currently isn\u2019t accepting new members, Ngo told Polygon that Nvidia is \u201ccurrently transitioning to a new payment provider.\u201d During that transition, it\u2019s pausing new sign-ups, Day Passes, and gift card purchases. The company posted on the GeForce Now subreddit in late January, saying the full transition is expected to take \u201ca minimum of 5 weeks.\u201d Ngo says that current members aren\u2019t affected by these changes.\n\nUnlike Xbox Game Pass, which has a curated catalog of games you stream in full, or download to an Xbox or PC, GeForce Now lets you stream only the games you own on PC platforms, plus some free-to-play games. It can pull in many (but not all) games from Steam, the Epic Games Store, and the Xbox game store. And notably, it lets you stream them in high quality through one of its Nvidia RTX-powered virtualized rigs. In my experience, games look better through GeForce Now than Xbox Cloud Gaming, since Nvidia\u2019s service has a tier that goes up to 4K at 240 frames per second.\n\nGeForce Now is available on numerous devices, ranging from handhelds to TVs. Nvidia announced at CES 2025 that a native app will be coming to the Steam Deck sometime in 2025. So, at least it\u2019s getting the payment provider transition out of the way before that arrives.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia Stock Rises. Why the Chip Maker Is Getting a Much-Needed Boost.",
            "link": "https://www.barrons.com/articles/nvidia-stock-rises-ai-chip-google-de411ab3",
            "snippet": "Nvidia stock climbed again on Thursday after racking up gains the day before on Google parent Alphabet's artificial intelligence spending plans.",
            "score": 0.8798636794090271,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Analyst reboots Nvidia's stock price target ahead of earnings",
            "link": "https://www.thestreet.com/investing/stocks/analyst-reboots-nvidias-stock-price-target-ahead-of-earnings",
            "snippet": "This is what could happen next to Nvidia shares.",
            "score": 0.9233600497245789,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "CoreWeave brings Nvidia GB200 NVL72 instances to its cloud platform",
            "link": "https://www.datacenterdynamics.com/en/news/coreweave-brings-nvidia-gb200-nvl72-instances-to-its-cloud-platform/",
            "snippet": "CoreWeave is now offering Nvidia GB200 NVL72 instances on its cloud platform. The instances are generally available via CoreWeave's Kubernetes Service,...",
            "score": 0.7521481513977051,
            "sentiment": null,
            "probability": null,
            "content": "The instances are generally available via CoreWeave's Kubernetes Service, Slurm oN Kubernetes (SUNK), and Mission Control platform.\n\nCoreWeave is now offering Nvidia GB200 NVL72 instances on its cloud platform.\n\nThe GB200 NVL72-based instances on CoreWeave connect 36 Nvidia Grace CPUs and 72 Nvidia Blackwell GPUs in a liquid-cooled, rack-scale design and are available as bare-metal instances through CoreWeave Kubernetes Service, and are scalable up to 110,000 GPUs.\n\nThe instances use Nvidia Quantum-2 InfiniBand networking with 400Gbps of bandwidth per GPU, and deploy the chip vendor's Quantum-2's SHARP In-Network Computing technology to reduce latency further.\n\nThe instances are currently available via the US-West-01 region.\n\nCoreWeave claims that it is the first cloud provider to make Nvidia GB200 NVL72 instances generally available.\n\nThe company displayed a demonstration of a GB200 NVL72 system at one of its data centers in November of last year. The cluster delivered up to 1.4 exaFLOPS of AI compute.\n\nIn January 2025, it was reported that IBM would be using CoreWeave's cloud platform to access the Nvidia GB200 clusters, featuring the GB200 NVL72 systems for training its next generation of Granite AI models.\n\nLast month, Lambda deployed two GB200 NVL72 racks - one at an EdgeCloudLink data center and another at a Pegatron facility. Microsoft was the first cloud provider to deploy Nvidia\u2019s new GB200 GPUs in its AI cloud servers, although, according to reports, that rack configuration was not a GB200 NVL72. Google also gave a sneak peek of its upcoming Nvidia Blackwell GB200 NVL racks in October 2024.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Stock Market Today: Dow Jones Dives As Nvidia Rises On DeepSeek Ban Report; Cathie Wood Stock Crashes On Earnings (Live Coverage)",
            "link": "https://www.investors.com/market-trend/stock-market-today/dow-jones-sp500-nasdaq-nvidia-nvda-stock-6/",
            "snippet": "Volatility was the market watchword Thursday as major indexes wavered in the last hour of trading and investors awaited a crucial jobs report. Nvidia (NVDA)...",
            "score": 0.9546301364898682,
            "sentiment": null,
            "probability": null,
            "content": "On The Hunt For Magnificent Earnings Growth? Check These 7 Stocks.\n\n3/14/2025 None of our picks is Nvidia or Amazon or Tesla. Investors may wish to widen the aperture to find the...\n\n3/14/2025 None of our picks is Nvidia or Amazon or Tesla....",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-02-05": {
        "0": {
            "title": "How GeForce RTX 50 Series GPUs Are Built to Supercharge Generative AI on PCs",
            "link": "https://blogs.nvidia.com/blog/rtx-ai-garage-blackwell-nim-blueprints-pc/",
            "snippet": "NVIDIA's GeForce RTX 5090 and 5080 GPUs \u2014 which are based on the groundbreaking NVIDIA Blackwell architecture \u2014offer up to 8x faster frame rates with NVIDIA...",
            "score": 0.9006508588790894,
            "sentiment": null,
            "probability": null,
            "content": "With NVIDIA Blackwell, NIM microservices and AI Blueprints, developers and enthusiasts can tap into powerful local AI performance.\n\nNVIDIA\u2019s GeForce RTX 5090 and 5080 GPUs \u2014 which are based on the groundbreaking NVIDIA Blackwell architecture \u2014offer up to 8x faster frame rates with NVIDIA DLSS 4 technology, lower latency with NVIDIA Reflex 2 and enhanced graphical fidelity with NVIDIA RTX neural shaders.\n\nThese GPUs were built to accelerate the latest generative AI workloads, delivering up to 3,352 AI trillion operations per second (TOPS), enabling incredible experiences for AI enthusiasts, gamers, creators and developers.\n\nTo help AI developers and enthusiasts harness these capabilities, NVIDIA at the CES trade show last month unveiled NVIDIA NIM and AI Blueprints for RTX. NVIDIA NIM microservices are prepackaged generative AI models that let developers and enthusiasts easily get started with generative AI, iterate quickly and harness the power of RTX for accelerating AI on Windows PCs. NVIDIA AI Blueprints are reference projects that show developers how to use NIM microservices to build the next generation of AI experiences.\n\nNIM and AI Blueprints are optimized for GeForce RTX 50 Series GPUs. These technologies work together seamlessly to help developers and enthusiasts build, iterate and deliver cutting-edge AI experiences on AI PCs.\n\nNVIDIA NIM Accelerates Generative AI on PCs\n\nWhile AI model development is rapidly advancing, bringing these innovations to PCs remains a challenge for many people. Models posted on platforms like Hugging Face must be curated, adapted and quantized to run on PC. They also need to be integrated into new AI application programming interfaces (APIs) to ensure compatibility with existing tools, and converted to optimized inference backends for peak performance.\n\nNVIDIA NIM microservices for RTX AI PCs and workstations can ease the complexity of this process by providing access to community-driven and NVIDIA-developed AI models. These microservices are easy to download and connect to via industry-standard APIs and span the key modalities essential for AI PCs. They are also compatible with a wide range of AI tools and offer flexible deployment options, whether on PCs, in data centers, or in the cloud.\n\nNIM microservices include everything needed to run optimized models on PCs with RTX GPUs, including prebuilt engines for specific GPUs, the NVIDIA TensorRT software development kit (SDK), the open-source NVIDIA TensorRT-LLM library for accelerated inference using Tensor Cores, and more.\n\nMicrosoft and NVIDIA worked together to enable NIM microservices and AI Blueprints for RTX in Windows Subsystem for Linux (WSL2). With WSL2, the same AI containers that run on data center GPUs can now run efficiently on RTX PCs, making it easier for developers to build, test and deploy AI models across platforms.\n\nIn addition, NIM and AI Blueprints harness key innovations of the Blackwell architecture that the GeForce RTX 50 series is built on, including fifth-generation Tensor Cores and support for FP4 precision.\n\nTensor Cores Drive Next-Gen AI Performance\n\nAI calculations are incredibly demanding and require vast amounts of processing power. Whether generating images and videos or understanding language and making real-time decisions, AI models rely on hundreds of trillions of mathematical operations to be completed every second. To keep up, computers need specialized hardware built specifically for AI.\n\nIn 2018, NVIDIA GeForce RTX GPUs changed the game by introducing Tensor Cores \u2014 dedicated AI processors designed to handle these intensive workloads. Unlike traditional computing cores, Tensor Cores are built to accelerate AI by performing calculations faster and more efficiently. This breakthrough helped bring AI-powered gaming, creative tools and productivity applications into the mainstream.\n\nBlackwell architecture takes AI acceleration to the next level. The fifth-generation Tensor Cores in Blackwell GPUs deliver up to 3,352 AI TOPS to handle even more demanding AI tasks and simultaneously run multiple AI models. This means faster AI-driven experiences, from real-time rendering to intelligent assistants, that pave the way for greater innovation in gaming, content creation and beyond.\n\nFP4 \u2014 Smaller Models, Bigger Performance\n\nAnother way to optimize AI performance is through quantization, a technique that reduces model sizes, enabling the models to run faster while reducing the memory requirements.\n\nEnter FP4 \u2014 an advanced quantization format that allows AI models to run faster and leaner without compromising output quality. Compared with FP16, it reduces model size by up to 60% and more than doubles performance, with minimal degradation.\n\nFor example, Black Forest Labs\u2019 FLUX.1 [dev] model at FP16 requires over 23GB of VRAM, meaning it can only be supported by the GeForce RTX 4090 and professional GPUs. With FP4, FLUX.1 [dev] requires less than 10GB, so it can run locally on more GeForce RTX GPUs.\n\nOn a GeForce RTX 4090 with FP16, the FLUX.1 [dev] model can generate images in 15 seconds with just 30 steps. With a GeForce RTX 5090 with FP4, images can be generated in just over five seconds.\n\nFP4 is natively supported by the Blackwell architecture, making it easier than ever to deploy high-performance AI on local PCs. It\u2019s also integrated into NIM microservices, effectively optimizing models that were previously difficult to quantize. By enabling more efficient AI processing, FP4 helps to bring faster, smarter AI experiences for content creation.\n\nAI Blueprints Power Advanced AI Workflows on RTX PCs\n\nNVIDIA AI Blueprints, built on NIM microservices, provide prepackaged, optimized reference implementations that make it easier to develop advanced AI-powered projects \u2014 whether for digital humans, podcast generators or application assistants.\n\nAt CES, NVIDIA demonstrated PDF to Podcast, a blueprint that allows users to convert a PDF into a fun podcast, and even create a Q&A with the AI podcast host afterwards. This workflow integrates seven different AI models, all working in sync to deliver a dynamic, interactive experience.\n\nWith AI Blueprints, users can quickly go from experimenting with to developing AI on RTX PCs and workstations.\n\nNIM and AI Blueprints Coming Soon to RTX PCs and Workstations\n\nGenerative AI is pushing the boundaries of what\u2019s possible across gaming, content creation and more. With NIM microservices and AI Blueprints, the latest AI advancements are no longer limited to the cloud \u2014 they\u2019re now optimized for RTX PCs. With RTX GPUs, developers and enthusiasts can experiment, build and deploy AI locally, right from their PCs and workstations.\n\nNIM microservices and AI Blueprints are coming soon, with initial hardware support for GeForce RTX 50 Series, GeForce RTX 4090 and 4080, and NVIDIA RTX 6000 and 5000 professional GPUs. Additional GPUs will be supported in the future.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Why Nvidia Stock Rallied on Wednesday",
            "link": "https://www.fool.com/investing/2025/02/05/why-nvidia-stock-rallied-on-wednesday/",
            "snippet": "Shares of Nvidia (NVDA 2.63%) gained ground on Wednesday, climbing as much as 5%. As of 12:42 p.m. ET, the stock was still up 4.4%.",
            "score": 0.911461353302002,
            "sentiment": null,
            "probability": null,
            "content": "The good news continues to pour in for the artificial intelligence (AI) chipmaker.\n\nShares of Nvidia (NVDA 5.27%) gained ground on Wednesday, climbing as much as 5%. As of 12:42 p.m. ET, the stock was still up 4.4%.\n\nThe catalyst that sent the artificial intelligence (AI) chipmaker higher was news that strong demand for its processors will likely continue.\n\nIt's all about the data center\n\nAlphabet (GOOGL 1.68%) (GOOG 1.75%) released its fourth-quarter financial report after market close on Tuesday, and investors were keen to understand how demand for AI was progressing. The company reported revenue grew 12% year over year to $96 billion, generating diluted earnings per share (EPS) of $2.15, an increase of 31%.\n\nThe results were largely in line with Wall Street's expectations, but some investors were troubled by results in the company's cloud segment. Revenue from Google Cloud grew 30% year over year to $12 billion, slightly below consensus estimates of $12.2 billion.\n\nManagement explained that the shortfall was the result of tough comps, as it began its AI deployment in the year-ago quarter. Additionally, Alphabet had greater demand for AI than it could provide, so the company is scrambling to bring more capacity online.\n\nWhat does this have to do with Nvidia?\n\nAlphabet is limited by the number of data centers it has and is investing significantly to rectify that situation. CFO Anat Ashkenazi said that the company planned to spend $75 billion on capital expenditures in 2025, noting the majority of that spending would go toward \"technical infrastructure, which includes servers and data centers.\"\n\nAs the principle supplier of the graphics processing units (GPUs) used to power AI in data centers, Nvidia stands to directly benefit from this increased spending. This follows an announcement by Microsoft (MSFT 2.58%) that it plans to spend $80 billion on data centers this year, also benefiting Nvidia.\n\nAnd at just 28 times next year's earnings, Nvidia stock is attractively priced.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Prediction: Nvidia Stock Is Going to Soar After Feb. 26",
            "link": "https://finance.yahoo.com/news/prediction-nvidia-stock-going-soar-124300040.html",
            "snippet": "Nvidia (NASDAQ: NVDA) has gotten off to a bad start on the stock market in 2025, losing more than 10% of its value as of this writing.",
            "score": 0.494581401348114,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has been at the forefront of the AI revolution with its powerful graphics processing units (GPUs) based on the Ampere architecture that helped OpenAI train ChatGPT. It has kept pushing the envelope in the AI accelerator market, churning out more powerful chips in the past three years based on its Hopper and Blackwell architectures.\n\nIn a post announcing the Stargate project, OpenAI pointed out that the initial funding will be provided by SoftBank, OpenAI, Oracle, and Abu Dhabi's AI-focused investment company, MGX. The post further highlighted that Nvidia is going to be among the \"initial technology partners\" in Stargate.\n\nStargate's first AI data center is already under construction in Texas, according to Oracle chairman Larry Ellison. The joint venture is expected to construct 20 data centers, creating an estimated 100,000 jobs.\n\nOn Tuesday, Jan. 21, Trump announced at the White House that SoftBank , OpenAI, and Oracle (NYSE: ORCL) are forming a joint venture that plans to invest $100 billion in AI infrastructure. The joint venture, known as Stargate, is eventually planning to spend up to a whopping $500 billion on building AI infrastructure in the U.S. over the next four years.\n\nOver the last few years, tech giants and governments around the world have poured a lot of money into the development of AI infrastructure, and President Donald Trump gave AI spending prospects a massive boost last month.\n\nLet's look at why Nvidia could offer a bright update about the state of AI spending later this month along with its quarterly report.\n\nDeepSeek's low-cost model sparked concerns about major cloud computing companies and governments reducing their demand for the AI chips Nvidia sells. However, a closer look at recent developments in the AI space suggests that the spending on AI chips could continue to head higher, opening the possibility of Nvidia stock regaining its mojo once it releases its fiscal 2025 fourth-quarter results on Feb. 26.\n\nWhen DeepSeek claimed that it spent just $6 million to train its R1 reasoning model that's capable of competing with OpenAI's o1 reasoning model, AI stocks in the U.S. took a big beating. Nvidia stock was one of the biggest victims of the sell-off, dropping 17% on Jan. 27 after it emerged that DeepSeek overtook ChatGPT's downloads on the Apple app store in the U.S.\n\nNvidia (NASDAQ: NVDA) has gotten off to a bad start on the stock market in 2025, losing more than 10% of its value as of this writing, with Chinese artificial intelligence (AI) start-up DeepSeek's launch of a low-cost but capable AI model playing a key role in the semiconductor giant's troubles.\n\nStory Continues\n\nThis explains why Nvidia has maintained a solid grip on the AI chip market with an estimated share of 90%. Given that Nvidia has created a technology advantage over rivals in the AI chip market, it could remain the dominant force in this space. As Nvidia's GPUs are the basic building blocks of AI data centers given their ability to perform massive calculations simultaneously, allowing companies to train and deploy AI models quickly, Stargate's ambitious investment plan should ideally help improve the chipmaker's addressable market.\n\nFor instance, Oracle has been relying on Nvidia's GPUs to create AI infrastructure to rent out to customers so that they can train AI models in the cloud. In September last year, Ellison remarked that one of Oracle's largest data centers \"is 800 megawatts, and it will contain acres of NVIDIA GP clusters able to train the world's largest AI models.\" This was followed by a remark from Oracle CEO Safra Catz on the December 2024 earnings conference call that the company \"delivered the world's largest and fastest AI supercomputer, scaling up to 65,000 Nvidia H200 GPUs.\"\n\nOracle is planning to deploy an additional 35 cloud regions around the globe in addition to the 17 it already has. It won't be surprising to see the company's appetite for Nvidia's GPUs increasing. More importantly, as Nvidia has been working with its supply chain partners to increase the output of its AI GPUs in 2025, it should be in a position to meet the higher demand for its chips that's likely to arise following Stargate.\n\nMeanwhile, the likes of Meta Platforms and Microsoft aren't going to curtail their spending on AI infrastructure following DeepSeek's breakthrough. Both companies believe that heavy AI investments are required to support the growing demand for AI applications, thanks to the potential arrival of more efficient models as demonstrated by DeepSeek. Dutch semiconductor equipment giant ASML suggested something similar after the company witnessed solid growth in orders and received way more bookings than Wall Street was anticipating.\n\nAll this suggests that the AI spending environment could remain robust, and that could help Nvidia deliver solid results and guidance later this month.\n\nStronger-than-expected growth could lead to more upside\n\nAnalysts are currently expecting Nvidia's revenue in fiscal 2026 (which has just begun) to increase 52% to just over $196 billion, followed by a 21% increase in fiscal 2027.\n\nHowever, expect to see those estimates head higher in light of the above discussion, paving the way for more upside in Nvidia stock. It is worth noting that Nvidia's 12-month price target of $175, according to 66 analysts covering the stock, points toward 46% gains from current levels.\n\nNvidia's revenue estimates for both fiscal years 2026 and 2027 have jumped higher of late, a trend that could continue thanks to continued investments in AI. As a result, Nvidia's price target could also witness upward revisions.\n\nSo, investors who have been on the sidelines and are wondering if it is a good idea to buy shares of Nvidia following the stellar returns that the stock has delivered in the past couple of years can consider buying it right away as it can regain its mojo. The stock's forward earnings multiple of 23 is very attractive considering that the tech-laden Nasdaq-100 index has a forward earnings multiple of 27. That's why buying Nvidia right now could turn out to be a smart move as its healthy earnings growth momentum is likely to continue.\n\nShould you invest $1,000 in Nvidia right now?\n\nBefore you buy stock in Nvidia, consider this:\n\nThe Motley Fool Stock Advisor analyst team just identified what they believe are the 10 best stocks for investors to buy now\u2026 and Nvidia wasn\u2019t one of them. The 10 stocks that made the cut could produce monster returns in the coming years.\n\nConsider when Nvidia made this list on April 15, 2005... if you invested $1,000 at the time of our recommendation, you\u2019d have $714,954!*\n\nNow, it\u2019s worth noting Stock Advisor\u2019s total average return is 895% \u2014 a market-crushing outperformance compared to 176% for the S&P 500. Don\u2019t miss out on the latest top 10 list.\n\nLearn more \u00bb\n\n*Stock Advisor returns as of February 3, 2025\n\nRandi Zuckerberg, a former director of market development and spokeswoman for Facebook and sister to Meta Platforms CEO Mark Zuckerberg, is a member of The Motley Fool's board of directors. Harsh Chauhan has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends ASML, Apple, Meta Platforms, Microsoft, Nvidia, and Oracle. The Motley Fool recommends the following options: long January 2026 $395 calls on Microsoft and short January 2026 $405 calls on Microsoft. The Motley Fool has a disclosure policy.\n\nPrediction: Nvidia Stock Is Going to Soar After Feb. 26 was originally published by The Motley Fool",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Why Nvidia, Broadcom, and Other Chip Stocks Are Surging Wednesday",
            "link": "https://www.investopedia.com/why-nvidia-broadcom-and-other-chip-stocks-are-surging-wednesday-8786442",
            "snippet": "Nvidia and Broadcom shares surged Wednesday after Google parent Alphabet, a customer of both chipmakers, said it plans to ramp up spending on artificial...",
            "score": 0.6711499691009521,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia and Broadcom shares surged Wednesday after Google parent Alphabet, a customer of both chipmakers, said it plans to ramp up spending on artificial intelligence.\n\nAlphabet said Tuesday that it plans to spend as much as $75 billion in capital expenditures this year, with most of the funds set to go toward expanding its AI infrastructure.\n\nAlphabet CEO Sundar Pichai also told investors on the company\u2019s earnings call that the tech giant intends to continue its \u201cstrong relationship\u201d with Nvidia.\n\nNvidia (NVDA) shares surged Wednesday, after Google parent Alphabet (GOOGL), a buyer of Nvidia\u2019s chips, said it plans to ramp up spending on artificial intelligence (AI).\n\nAlphabet said Tuesday that it plans to spend as much as $75 billion in capital expenditures this year, with most of the funds set to go toward expanding its AI infrastructure, including servers and data centers, to meet demand for AI.\n\nThat could benefit Nvidia, with Alphabet Chief Executive Officer Sundar Pichai telling investors on the company\u2019s earnings call that the tech giant intends to continue its \u201cstrong relationship\u201d with the chipmaker, after announcing its first customer running on Nvidia\u2019s Blackwell platform last week.\n\nThe announcement also comes just days after Meta Platforms (META) said it plans to spend $60 billion to $65 billion this year, and Microsoft (MSFT) said it expects to spend $80 billion on infrastructure in fiscal 2025.\n\nBroadcom Shares Also Advance on Alphabet Plans\n\nBroadcom (AVGO), which designs custom AI chips for several large U.S. tech firms including Alphabet and Meta, also experienced a share-price jump Wednesday following the news.\n\nShares of Broadcom were up nearly 6% at $235.69 in intraday trading Wednesday, while Nvidia jumped about 4% to $123.43. Several other chip stocks and shares of Nvidia partners, including chip designer Arm Holdings (ARM) and manufacturer TSMC (TSM), were higher as well.\n\nArm is also set to report earnings after the bell Wednesday, along with chipmaker Qualcomm (QCOM).",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Supermicro Ramps Full Production of NVIDIA Blackwell Rack-Scale Solutions with NVIDIA HGX B200",
            "link": "https://www.prnewswire.com/news-releases/supermicro-ramps-full-production-of-nvidia-blackwell-rack-scale-solutions-with-nvidia-hgx-b200-302368747.html",
            "snippet": "Supermicro Offers Next-Generation Air-Cooled and Liquid-Cooled Architecture for NVIDIA Blackwell Platform. SAN JOSE, Calif., Feb. 5, 2025 /PRNewswire/...",
            "score": 0.892304003238678,
            "sentiment": null,
            "probability": null,
            "content": "Supermicro Offers Next-Generation Air-Cooled and Liquid-Cooled Architecture for NVIDIA Blackwell Platform\n\nSAN JOSE, Calif., Feb. 5, 2025 /PRNewswire/ -- Supermicro, Inc. (NASDAQ: SMCI), a Total IT Solution Provider for AI/ML, HPC, Cloud, Storage, and 5G/Edge, is announcing full production availability of its end-to-end AI data center Building Block Solutions\u00ae accelerated by the NVIDIA Blackwell platform. The Supermicro Building Block portfolio provides the core infrastructure elements necessary to scale Blackwell solutions with exceptional time to deployment. The portfolio includes a broad range of air-cooled and liquid-cooled systems with multiple CPU options. These include superior thermal design supporting traditional air cooling, liquid-to-liquid (L2L) and liquid-to-air (L2A) cooling. In addition, a full data center management software suite, rack-level integration, including full network switching and cabling and cluster-level L12 solution validation can be delivered as turn-key offering with global delivery, professional support, and service.\n\nSupermicro Ramps Full Production of NVIDIA Blackwell Rack-Scale Solutions with NVIDIA HGX B200\n\n\"In this transformative moment of AI, where scaling laws are pushing the limits of data center capabilities, our latest NVIDIA Blackwell-powered solutions, developed through close collaboration with NVIDIA, deliver outstanding computational power,\" said Charles Liang, president and CEO of Supermicro. \"Supermicro's NVIDIA Blackwell GPU offerings in plug-and-play scalable units with advanced liquid cooling and air cooling are empowering customers to deploy an infrastructure that supports increasingly complex AI workloads while maintaining exceptional efficiency. This reinforces our commitment to providing sustainable, cutting-edge solutions that accelerate AI innovation.\"\n\nFor more information, please visit www.supermicro.com/AI\n\nSupermicro's NVIDIA HGX B200 8-GPU systems utilize next-generation liquid-cooling and air-cooling technology. The newly developed cold plates and the new 250kW coolant distribution unit (CDU) more than double the cooling capacity of the previous generation in the same 4U form factor. Available in 42U, 48U, or 52U configurations, the rack-scale design with the new vertical coolant distribution manifolds (CDM) no longer occupy valuable rack units. This enables 8 systems, comprising 64 NVIDIA Blackwell GPUs in a 42U rack, and all the way up to 12 systems with 96 NVIDIA Blackwell GPUs in a 52U rack.\n\nThe new air-cooled 10U NVIDIA HGX B200 system features a redesigned chassis with expanded thermal headroom to accommodate eight 1000W TDP Blackwell GPUs. Up to 4 of the new 10U air-cooled systems can be installed and fully integrated in a rack, the same density as the previous generation, while providing up to 15x inference and 3x training performance.\n\nThe new SuperCluster designs incorporate NVIDIA Quantum-2 InfiniBand or NVIDIA Spectrum-X Ethernet networking in a centralized rack, enabling a non-blocking, 256-GPU scalable unit in five racks or an extended 768-GPU scalable unit in nine racks. This architecture \u2014 purpose-built for NVIDIA HGX B200 systems with native support for the NVIDIA AI Enterprise software platform for developing and deploying production-grade, end-to-end agentic AI pipelines \u2014 combined with Supermicro's expertise in deploying the world's largest liquid-cooled data centers delivers exceptional efficiency and time-to-online for today's most ambitious AI data center projects.\n\nLiquid-cooled or air-cooled: Supermicro NVIDIA HGX B200 Systems\n\nThe new liquid-cooled 4U NVIDIA HGX B200 8-GPU system features newly developed cold plates and advanced tubing design that further enhance the efficiency and serviceability of the predecessor that was used for the NVIDIA HGX H100/H200 8-GPU system. Complemented by a new 250kW cooling distribution unit, more than doubling the cooling capacity of the previous generation while maintaining the same 4U form factor, the new rack-scale design with the new vertical coolant distribution manifolds (CDM) enables denser architecture with flexible configuration scenarios used for various data center environments. Supermicro offers 42U, 48U, or 52U rack configurations for liquid-cooled data centers. The 42U or 48U configuration provides 8 systems and 64-GPU in a rack, and 256-GPU scalable unit in five racks. The 52U rack configuration allows 96-GPU in a rack and enables 768-GPU scalable unit in nine racks for the most advanced AI data center deployments. Supermicro also offers an in-row CDU option for large deployments, as well as liquid-to-air cooling rack solution that doesn't require facility water.\n\nSupermicro's NVIDIA HGX B200 systems natively support NVIDIA AI Enterprise software to accelerate time to production AI. NVIDIA NIM microservices allow organizations to access the latest AI models for fast, secure, and reliable deployment on NVIDIA accelerated infrastructure anywhere \u2014 whether in data centers, the cloud or workstations.\n\nFor traditional data centers, the new 10U air-cooled NVIDIA B200 8-GPU system is also available, with a redesigned modular GPU tray to house the NVIDIA Blackwell GPUs in an air-cooled environment. The air-cooled rack design follows the proven, industry-leading architecture of the previous generation, four systems and 32 GPUs in a 48U rack, while providing NVIDIA Blackwell performance. All Supermicro NVIDIA HGX B200 systems are equipped with a 1:1 GPU-to-NIC ratio supporting NVIDIA BlueField-3 SuperNICs or NVIDIA ConnectX-7 NICs for scaling across a high-performance compute fabric.\n\nSupermicro provides support for systems included in the NVIDIA-Certified Systems program. This program incorporates NVIDIA GPUs, CPUs, and high-speed, secure networking technologies into systems from leading NVIDIA partners, ensuring configurations that are validated for optimal performance, reliability, and scalability. By choosing an NVIDIA-Certified System, enterprises can confidently select hardware solutions to power their accelerated computing workloads. NVIDIA has certified Supermicro systems with NVIDIA H100 and H200 GPUs.\n\nEnd-to-end Liquid-cooling Solution for NVIDIA GB200 NVL72\n\nSupermicro's SuperCluster solution, based on the NVIDIA GB200 NVL72 system, represents a breakthrough in AI computing infrastructure, combining Supermicro's end-to-end liquid-cooling technology. The system integrates 72 NVIDIA Blackwell GPUs and 36 NVIDIA Grace CPUs in a single rack, delivering exascale computing capabilities through NVIDIA's most extensive NVLink network to date, achieving 130 TB/s of GPU communications.\n\nThe 48U solution's versatility supports both liquid-to-air and liquid-to-liquid cooling configurations, accommodating various data center environments. Additionally, Supermicro's SuperCloud Composer software provides management tools for monitoring and optimizing liquid-cooled infrastructure, delivering a complete solution from proof of concept to full-scale deployment.\n\nEnd-to-end Data Center Solution and Deployment Services for NVIDIA Blackwell\n\nFrom proof-of-concept (PoC) to full-scale deployment, Supermicro serves as a comprehensive one-stop solution provider with global manufacturing scale, delivering all necessary components, data center-level solution design, liquid-cooling technologies, networking solutions, cabling, management software, testing and validation, and onsite installation services. Its in-house liquid-cooling ecosystem offers a complete, custom-designed thermal management solution, featuring optimized cold plates for GPUs, CPUs, and memory modules, along with versatile coolant distribution unit form factors and capacities, manifolds, hoses, connectors, cooling towers, and sophisticated monitoring and management software. With production facilities across San Jose, Europe, and Asia, Supermicro offers unmatched manufacturing capacity for liquid-cooled rack systems, ensuring timely delivery, reduced total cost of ownership (TCO) and environmental impact, and consistent quality.\n\nAbout Super Micro Computer, Inc.\n\nSupermicro (NASDAQ: SMCI) is a global leader in Application-Optimized Total IT Solutions. Founded and operating in San Jose, California, Supermicro is committed to delivering first to market innovation for Enterprise, Cloud, AI, and 5G Telco/Edge IT Infrastructure. We are a Total IT Solutions provider with server, AI, storage, IoT, switch systems, software, and support services. Supermicro's motherboard, power, and chassis design expertise further enables our development and production, enabling next generation innovation from cloud to edge for our global customers. Our products are designed and manufactured in-house (in the US, Taiwan, and the Netherlands), leveraging global operations for scale and efficiency and optimized to improve TCO and reduce environmental impact (Green Computing). The award-winning portfolio of Server Building Block Solutions\u00ae allows customers to optimize for their exact workload and application by selecting from a broad family of systems built from our flexible and reusable building blocks that support a comprehensive set of form factors, processors, memory, GPUs, storage, networking, power, and cooling solutions (air-conditioned, free air cooling or liquid cooling).\n\nSupermicro, Server Building Block Solutions, and We Keep IT Green are trademarks and/or registered trademarks of Super Micro Computer, Inc.\n\nAll other brands, names, and trademarks are the property of their respective owners.\n\nSOURCE Super Micro Computer, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia Stock Rises. Why AMD, Alphabet Earnings Are Boosting the AI Chip Maker.",
            "link": "https://www.barrons.com/articles/nvidia-stock-amd-alphabet-ai-chips-2054cbe7",
            "snippet": "Nvidia stock was gaining after Google parent Alphabet updated on its AI spending plans and AMD stock tumbled.",
            "score": 0.9470316171646118,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia CEO Jensen Huang Says You Should Ask Yourself These 2 Questions to Prepare for an AI-Filled Future",
            "link": "https://www.inc.com/jessica-stillman/nvidia-ceo-jensen-huang-says-you-should-ask-yourself-these-2-questions-to-prepare-for-an-ai-filled-future/91141688",
            "snippet": "Anxious about AI? Nvidia's co-founder and CEO has some tips for how to find your feet in this new landscape.",
            "score": 0.9108116626739502,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Jumps on Super Micro Saying Blackwell-Based System Ready",
            "link": "https://www.bloomberg.com/news/articles/2025-02-05/nvidia-jumps-on-super-micro-saying-blackwell-based-system-ready",
            "snippet": "Shares of Nvidia Corp. gained on Wednesday after a key partner, Super Micro Computer Inc., said its new AI data center systems powered by Nvidia's advanced...",
            "score": 0.4919634759426117,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Nvidia DLSS4, MFG, and full ray tracing tested on RTX 5090 and RTX 5080",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-dlss4-mfg-and-full-ray-tracing-tested-on-rtx-5090-and-rtx-5080",
            "snippet": "Nvidia's new GeForce RTX 5090 and GeForce RTX 5080 have arrived, and coupled with a new testbed and a revised test suite, not to mention new drivers and a...",
            "score": 0.854268491268158,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's new GeForce RTX 5090 and GeForce RTX 5080 have arrived, and coupled with a new testbed and a revised test suite, not to mention new drivers and a host of other changes, our initial benchmarks had to gloss over a few areas. One of the biggest selling points for the Blackwell RTX 50-series GPUs \u2014 according to Nvidia, at least \u2014 is DLSS4 with Multi Frame Generation (MFG), an AI-based technology that offers further \"performance\" improvements over DLSS3 and framegen. But there are other changes as well.\n\n\n\nAs we discussed prior to the Blackwell hardware paper launch, DLSS4 and neural rendering technologies make direct comparisons between the new RTX 50-series GPUs and their 40-series predecessors a bit more complex. And by \"complex\" we mean you can't just take any of the published numbers at face value. MFG in particular requires a far more nuanced approach.\n\n\n\nAs pointed out in our RTX 5090 testing in Cyberpunk 2077, there's far more going on than a simple \"bigger number is better\" approach can convey. The easiest way to show this is to think about what some theoretical performance numbers might mean.\n\n\n\nTake a game running at 50 FPS baseline, without frame generation. That's a decent result but not totally smooth. Now turn on a perfectly executed framegen algorithm and say it gets 100 FPS. That's twice as many \"frames\" delivered to your monitor each second, but user input sampling happens at the same 50 FPS as before. It looks smoother but it doesn't actually feel much different. Double that again with MFG4X mode and it's 200 FPS, still with the same 50 input samples per second.\n\n(Image credit: Nvidia)\n\nAs with the original framegen, it's more about smoothing out the visuals than providing a true boost to performance. And on some level, games can and do feel better with a big enough increase in the number of frames being sent to your display. Visual smoothness and \"feel\" are linked in our brains (or at least, they are in my brain), so a game spitting out 100 frames in a second but sampling just 25 times per second can still feel better than the same game running at 30 FPS with 30 input samples per second. Usually, anyway \u2014 it varies by game and other factors still play a role.\n\n\n\nHow much better framegen is compared to non-framegen traditional rendering is a much harder question to answer. And not only does it vary by game, but it also varies by individual. One person might find framegen delivers a totally acceptable experience, while another might hate that exact same experience.\n\n\n\nPersonally, what I find is that single frame generation \u2014 meaning, what we had with DLSS3 and FSR3 \u2014 needs to boost the \"FPS\" by at least 50% to have a decent chance at feeling better. That would mean as an example taking 40 FPS native and turning it into 60 FPS or more with framegen. And there are also limits to how far you need to go. Boosting 100 FPS to 150 FPS via framegen doesn't represent a true 50% improvement in performance, but the latter gets beyond a 144Hz display refresh rate and is so fast that, even though the input sampling rate is lower (75 compared to 100 samples per second), it can still look and feel \"better\" to some people.\n\n\n\nBut what if the base framerate is 100 FPS and framegen only increases that to 130 FPS? That's something we've seen in certain cases. Or what if it's a change from 50 FPS to 65 FPS? The latter in particular can feel worse, as the input sampling rate drops from 50 down to just 32.5, 35% slower, and that can be quite sluggish as far as responsiveness goes.\n\n\n\nThen we have to add in MFG now, interpolating multiple frames between two rendered frames. With a perfect algorithm, MFG4X can double the frames to monitor rate compared to framegen (now MFG2X on 50-series GPUs), or quadruple the native framerate. Does it feel twice or four times as fast? No. Not even close. It can still feel better \u2014 again, that's subjective and varies by game and user \u2014 but how much becomes a far more difficult proposition to quantify.\n\nThat's the preamble to a far more fuzzy and indistinct topic than raw performance. It's why we focus primarily on native non-upscaled, non-framegen performance for our reviews. It's one thing to say that 50 FPS is faster than 40 FPS when using traditional rendering. It's quite a different story if we're talking about 50 FPS with framegen versus 40 FPS without, or 80 FPS with MFG versus 40 FPS native. In both of those scenarios, I would almost certainly prefer the non-framegen experience.\n\n\n\nFor this deeper dive into MFG and DLSS4, and regular framegen as well, I've picked five of the most demanding ray tracing games currently available, three of which now have native support for Nvidia's DLSS4 and MFG. All of the games also feature higher levels of ray tracing, with four of the games supporting full ray tracing (aka \"path tracing\"). This should in theory provide some of the best-case scenarios for the RTX 5090 and RTX 5080 to strut their stuff.\n\n\n\nThe test hardware is the same as we used before, with a Ryzen 7 9800X3D processor and all the bells and whistles. We have 32GB of low latency DDR5-6000 memory, a 4TB PCIe 5.0 SSD, a high-end motherboard, and a powerful 1500W PSU. Full specs are in the boxout.\n\n\n\nLet's get to the test results, in alphabetical order. We have four charts for each game: 4K ultra, 4K ultra plus framegen, and then mostly for reference we have 1440p and 1080p ultra as well. And \"ultra\" means various things here, but you'll get that from the charts. Note that we are not running strictly apples to apples comparisons here, as the AMD GPU doesn't support Nvidia's DLSS algorithm, nor does it support Ray Reconstruction. It's present mostly as a point of reference, and the image quality of FSR3 is provably inferior. The 40-series GPUs also don't support MFG but should otherwise be similar in capabilities and image quality to the 50-series GPUs.\n\nAlan Wake 2: DLSS4 and MFG\n\n(Image credit: Remedy)\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nAlan Wake 2 kicks things off with a brutally punishing full RT mode that brings lesser GPUs crashing to their knees. The 4080 Super, even with DLSS Quality mode upscaling \u2014 and we're using the new Transformers mode here \u2014 only manages 26 FPS. The \"new and improved\" RTX 5080 bumps that all the way up to... 29 FPS. Yup, it's not much faster in terms of raw performance potential. And AMD's RX 7900 XTX stutters along at just 12.5 FPS.\n\n\n\nThe 4090 manages a more respectable 37 FPS, which is playable if not amazingly smooth. RTX 5090 kicks that up to 48 FPS, a significantly better overall experience. They're both fast enough at base performance that frame generation could actually prove useful. So what does framegen and MFG do to the standings?\n\n\n\nFirst, let's note that Alan Wake 2 only supports FSR2, so AMD doesn't get any framegen help. <Sigh> This is one of the continually frustrating things with upscaling and frame generation: There are a lot of games that primarily support only one vendor solution. Even those that do support AMD FSR, Intel XeSS, and Nvidia DLSS don't always treat them equally, as seen here. FSR3.1 should be basically drop-in compatible with FSR2, at least for upscaling, and it also offers framegen as a potential help. It won't look as good as DLSS, but just about anything is better than 12.5 FPS at 4K on AMD's fastest GPU.\n\n\n\nLooking to Nvidia's 40-series parts, the RTX 4080 Super gets a bump to 49 FPS, an 86% improvement over the baseline FPS. That means that, even though it does have a slightly lower input sampling rate of 24.4 instead of 26.2, overall it does feel better in my opinion. It also helps that Alan Wake 2 tends to be a more methodical game so having a super responsive experience isn't required. The 4090 sees a similar bump of 85% to 68 FPS, which again is a better overall experience in my opinion.\n\n\n\nWhat about the RTX 50-series GPUs? The 5080 gets an 87% improvement with MFG2X, about the same as the 40-series. But then it also has options for MFG3X that's 2.71X faster than the baseline rendering, and MFG4X that's 3.49X faster than baseline. Each progressively higher level of frame generation does increase latency slightly, but the scaling is otherwise nearly linear.\n\n\n\nThe 5090 sees similar gains: 1.84X with MFG2X, 2.66X for MFG3X, and 3.44X using MFG4X. Those are slightly lower in all three cases than the 5080, so perhaps other factors like CPU speed are slightly limiting performance, but each additional frame generated by MFG provides about an 80% boost to the frames to monitor rate.\n\n\n\nBut here's the important bit: MFG4X doesn't feel like it's running anywhere near twice as fast as MFG2X. It looks smoother, and feels a bit better subjectively, but if you were to hide the settings and ask me what the game feels like? I'd guess somewhere in the 65~80 FPS range. That's better than the 42 samples per second on the input, but far lower than the 166 \"FPS\" results would suggest. MFG2X feels like it's running at perhaps 55~65 FPS, but definitely not as responsive as what you'd get from native rendering at 89 FPS.\n\n\n\nAnd again, these numbers are all very fuzzy. We can measure how many frames get sent to the monitor, and use that to calculate how many input samples are taken, but the actual feel lands somewhere in between. Where exactly it lands? That's far more subjective and will depend on the game and user. Alan Wake 2 also happens to be the best-case scenario for framegen and MFG in my experience, as it's far slower paced. The main characters have two speeds: a meandering saunter and a slightly faster brisk walk. It's not a fast-paced game, in other words.\n\nBlack Myth: Wukong: DLSS3 and Framegen\n\n(Image credit: Game Science)\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nNext up is Black Myth: Wukong, also running full RT effects but this time without DLSS4 or ray reconstruction on Nvidia GPUs. That means no MFG, for now \u2014 though we suspect an upgrade and patch are in the works, or at least an Nvidia App override. (There's no option to override Black Myth: Wukong settings right now.)\n\n\n\nThe baseline performance at 4K with quality mode upscaling is again quite terrible on AMD's 7900 XTX, at just 9.2 FPS. The 4080 Super is already almost four times as fast, but the 5080 is only 13% faster than its predecessor. The 4090 sits at 48 FPS, and the 5090 beats it by 31% at 62 FPS.\n\n\n\nThis time, we have FSR3 support for framegen and that gives AMD a 92% improvement to the frames to monitor rate \u2014 which at just under 18 FPS is still far from playable (and still feels like 9 FPS). The 4080 Super gets a 62% boost via DLSS3 framegen, and there's a similar improvement for other RTX cards. The 5080 gets a 65% improvement, 4090 nets a 62% increase, and the 5090 improves by 67%.\n\n\n\nThis time, the benefits of frame generation are less pronounced. DLSS3 relies on fixed function hardware and doesn't tend to scale as well as the new DLSS4 algorithms, so the 4090 as an example goes from a playable 40 FPS with input sampling also happening 40 times per second, up to 66 FPS with framegen but with input sampling dropping to 33 times per second. It looks smoother but feels less responsive, and the same goes for the 5080 and 4080 Super.\n\n\n\nThe 5090 benefits by simply being more powerful. Yes, input sampling sees a similar drop, from 62 samples per second without framegen down to 52 samples with framegen. Both results are high enough that, at least for me, it feels \"smooth enough\" that I'm not bothered by the experience. But I do look forward to seeing MFG and DLSS4 support, as that should hopefully provide a similar 80~85 percent improvement on all the 40-series and 50-series GPUs, with MFG4X potentially providing even better frame smoothing.\n\nCyberpunk 2077: DLSS4 and MFG\n\n(Image credit: CD Projekt Red)\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nCyberpunk 2077 was the first game to get official native DLSS4 support. It also lets you choose between the old DLSS CNN models and the newer DLSS Transformer models for upscaling, if you're okay with sacrificing quality to get more of a performance boost. For these tests, however, I stuck to using DLSS Transformers in all cases.\n\n\n\nThe RT-Overdrive preset uses \"path tracing\" (full RT) and again destroys all but the fastest Nvidia GPUs at 4K. Ray Reconstruction may or may not provide much of a performance benefit, but it does look better than the default path tracing mode, so we used that for all the Nvidia GPUs \u2014 so again, not apples to apples.\n\n\n\nAMD's 7900 XTX chokes at 4K with quality mode upscaling at just 16 FPS. The 4080 Super doubles that with 32 FPS, and the 5080 again shows only a minor 12% improvement over its direct predecessor. The 4090 is playable at 43 FPS, while the 5090 delivers 37% higher performance at 59 FPS.\n\nAMD 7900 XTX running FSR3.1 framegen, with serious artifacts visible. (Image credit: Tom's Hardware)\n\nWith frame generation, AMD gets a nearly perfect 94% improvement to 31 FPS, though it's still not particularly playable. You can also see the very noticeable artifacts caused by FSR3 framegen in the above image. The 4080 Super and 4090 see an 85% and 90% increase in frames to monitor rates, nearly matching AMD's benefit \u2014 and this is yet again why DLSS4 feels much better now than DLSS3.\n\n\n\nAnd then there's the 50-series parts. The 5080 gets a 1.83X boost with MFG2X, 2.64X with 3X, and 3.38X with MFG4X. The 5090 sees improvements of 1.82X, 2.61X, and 3.30X. Input latency does increase slightly at each additional level of MFG, but it's not terrible: 40ms without MFG on the 5090, 47ms with 2X, 50 with 3X, and 53 with 4X. It's also not better. The 5080 went from 55ms latency, increasing to 67ms with MFG2X, 70 with 3X, and 74ms with 4X.\n\n\n\nDoes MFG make the 5080 as an example feel faster than an RTX 4090? The frames to monitor rates suggest it's much faster \u2014 up to 122 \"FPS\" compared to only 82 \"FPS.\" But the input sampling rate of 30.5 samples per second versus 40.8 samples per second makes things far less clear. Subjectively, I'd say it felt more like equal, at best, rather than a 50% advantage for the RTX 5080. I suspect a lot of gamers would end up preferring the 4090 2X framegen experience over the 5080 with MFG4X.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nHogwarts Legacy: DLSS4 and MFG\n\n(Image credit: Warner Bros. Games)\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nHogwarts Legacy also has a public update that enables DLSS4 with ray reconstruction and MFG support. If you thought it would be \"more of the same,\" relative to the above two MFG examples, it's not. And the problems are systemic for the game engine, as far as I can tell. Simply put: Maxing out the ray tracing options right now makes Hogwarts Legacy a worse experience. There's a lot of microstuttering, and the overall performance gets massively reduced compared to running the game without RT.\n\n\n\nThe issue seems to be inherent to virtually all Unreal Engine 4 games with lots of ray tracing. It just doesn't handle RT effects well, no matter the hardware. Hogwarts Legacy becomes CPU limited on the 9800X3D to around 60 FPS, give or take \u2014 slightly higher on the 7900 XTX than on the fastest Nvidia GPUs. And again, it's not apples to apples since we're running DLSS4 vs FSR2 \u2014 yes, FSR2 again, so no framegen for non-Nvidia GPUs.\n\n\n\nOn the one hand, being so CPU limited means we should see very good scaling from framegen and MFG for the Nvidia GPUs. The 4080 Super gets an 80% boost from framegen and the 4090 sees a similar 81% improvement. MFG2X yields an 82% increase on the 5080, and a \"better than perfect\" 106% boost on the 5090. Still, all the microstuttering remains a perceptible artifact, even after frame generation.\n\n\n\nThat goes for higher levels of MFG as well. The 5080 runs 2.61X faster with MFG3X and 3.23X faster with MFG4X. It does look smoother, and feels a bit better, but the irregular frametimes can still be felt. The routine hitching makes overall performance feel closer to the listed 1% low FPS rather than the average FPS.\n\n\n\nThe 5090 meanwhile continues to see exceptional scaling for whatever reason. It's 3.05X faster with MFG3X and 4.01X faster using MFG4X. It probably has something to do with the poorly running RT plus ray reconstruction (turning off ray reconstruction only boosts performance by around 10%), but framegen really shouldn't provide more than a 100% improvement at each level. Fundamentally, though, the inconsistent base performance creates a problem.\n\n\n\nThe 5090 starts with an average of 55 FPS but 1% lows of just 33 FPS. That's a pretty big disparity that's very noticeable when playing the game. You see similar average and minimum FPS results for all the RTX cards at 1440p and 1080p. MFG attempts to smooth things out, but the uneven pacing seems to throw the algorithm off, so that a single frametime spike in the base rendering still shows up as a spike with MFG enabled.\n\n\n\nTurn off ray tracing completely and baseline performance basically doubles at 4K, with significantly higher performance at 1440p and 1080p. That makes framegen and MFG less important to begin with, and certainly the benefits of MFG4X pulling over 300 FPS feel exaggerated. The 5080 as an example manages 115 FPS average, 62 FPS on the 1% lows, and an input latency of 24ms at 4K with DLSS Quality Transformers and no MFG. Turn on MFG4X and it gets 296 \"FPS\" with 1% lows of 113 FPS, and an input latency of 34ms \u2014 which means a base rendering speed of 74 FPS. The net result, again, is that even if MFG4X perhaps looks smoother, the overall experience isn't noticeably improved.\n\nIndiana Jones and the Great Circle: DLSS3 and Framegen\n\n(Image credit: Bethesda)\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nIndiana Jones and the Great Circle is our final test for DLSS upscaling and framegen. It's a Vulkan Ray Tracing game, using Nvidia RT extensions that apparently only fully work on RTX cards if you want to enable the full RT mode \u2014 which we did. That means no AMD card this time.\n\n\n\nAnd it's possible to use the Nvidia App to force the use of DLSS Transformers, but you can't force DLSS4 frame generation right now. That's not supported for this game. (Even if you could, as you can in about 75 other games, it's a pain as far as testing goes: Launch Nvidia App, set overrides, start and test the game, then exit the game and repeat the process. It's better if you're not trying to run benchmarks, of course.)\n\n\n\nThe baseline at 4K ultra with full RT using the maximum \"very high\" setting ends up being quite good for all four of the GPUs we tested. The 4080 Super gets 60 FPS, 69 FPS for the 5080, 81 FPS on the 4090, and 102 FPS on the 5090. That should provide for some useful improvements with framegen... except this ends up being a terrible result with DLSS3 framegen.\n\n\n\nThe 5090 \"improves\" to just 138 FPS, a 36% increase in the frames to monitor rate, but the base rendering rate drops to just 69 FPS. Going from a native 102 input samples per second to a framegen 69 samples per second is not an improved experience. Unfortunately, FrameView didn't give us an input latency result for Indiana Jones \u2014 probably for the best (from Nvidia's perspective) as it would certainly be substantially worse with framegen enabled. The RTX 4090 gets a 37% increase from framegen as well, going from 81 to 110 FPS, but with the base rendering rate dropping to 55 FPS.\n\n\n\nAs for the RTX 5080 and 4080 Super, with the current public build and without forcing DLSS4 through the Nvidia App, they both run out of VRAM and effectively fail to work at 4K with quality upscaling and framegen. (I didn't check if DLSS Transformers helped, but I suspect not.) The 5080 just locked up the game completely and we had to kill the process manually, while the 4080 Super dropped to a slideshow-like 18 FPS.\n\n\n\nThat's another \"thanks but no thanks\" to frame generation with the current public release of the game, if you're keeping track.\n\nNvidia sample of DLSS performance improvements over time. (Image credit: Nvidia)\n\nNvidia DLSS4 and MFG: Closing Thoughts\n\nAfter testing five of the heaviest games available right now, and also poking around at one game without using ray tracing, the net takeaway for DLSS4 MFG hasn't radically changed compared to DLSS3 framegen. That doesn't mean DLSS4 in general isn't important, but the MFG marketing for the 50-series wildly overstates the performance and end-user experience.\n\n\n\nDLSS4 Transformers rendering looks better, often correcting some of the most egregious rendering errors caused by upscaling. It's still not perfect but the artifacts are mostly things that can be ignored. With an RTX 40-series or 50-series GPU and a game that supports DLSS Transformers either natively or via Nvidia App overrides, it's a welcome addition. Performance is slightly slower but quality is improved, so that potentially you could use DLSS Transformers with Balanced (3X) upscaling to deliver comparable visuals and higher performance than DLSS CNN with Quality (2.25X) upscaling.\n\n\n\nRay Reconstruction sees similar improvements from the use of Transformers instead of a CNN network, but here the number of games that support DLSS 3.5 Ray Reconstruction is so limited as to make this a far less important feature. You can check the full Nvidia list of DLSS games, but here's the summary for DLSS 3.5 RR showing just five games.\n\n(Image credit: Nvidia)\n\nAs for MFG, it's not a bad option to have, but it's no universal panacea, certainly not in the current implementation. Of the games we tested that support the feature, the experience ranged from being akin to lipstick on a pig (Hogwarts Legacy, with or without ray tracing) to decent (Cyberpunk 2077) to pretty good (Alan Wake 2). What it mostly comes down to is the speed of the gameplay and the base level of performance.\n\n\n\nMFG4X certainly smooths out the frame pacing to your monitor. The best use case seems to be streaming at 120 FPS, where you could potentially have a game that only runs at 30~45 FPS get a boost to 120 for viewers. (Not that many streamers even use that mode.) It would still feel closer to the base framerate, but for those watching it would look smoother and more fluid. But it can't work miracles so if you have a game with inconsistent frame pacing (looking at you again, Hogwarts Legacy), even after MFG smoothing it will still feel inconsistent \u2014 a bit less so, but definitely still perceptible.\n\n\n\nIf on the other hand you have a game that already delivers a consistent experience, in terms of frame pacing, MFG and framegen tend to work better. And at the same time, they become less important to have. In other words, it's about taking a good experience and making it slightly better, rather than taking a mediocre or poor experience and making it good.\n\n\n\nAs Nvidia discussed in its presentation on Neural Rendering, there are three competing factors with rendering graphics: performance, latency, and image quality. (It called these smoothness, responsiveness, and image quality.) Pushing all three to higher levels is difficult and can require lots of expensive PC hardware.\n\nImage 1 of 4 (Image credit: Nvidia) (Image credit: Nvidia) (Image credit: Nvidia) (Image credit: Nvidia)\n\nFramegen and MFG are an alternative take to traditional rendering that compromises on latency in order to boost performance, with a hopefully only slight loss in image fidelity. But you can't just measure frames to monitor and latency to conclude whether something is \"better\" or not. Again, it's fuzzy math \u2014 what feels acceptable or even good to one person may not be good for another.\n\n\n\nTake a game running at 50 FPS without frame generation. It's a provably worse experience to get 50 FPS with framegen \u2014 you'd have 25 input samples per second (IPS) and higher input latency. Start increasing that base rendering level, to 30 IPS and 60 FPS, or 35 IPS and 70 FPS... and at some point the look and feel should surpass the baseline 50 FPS and IPS. Where is that crossover point? It varies, by person and game.\n\n\n\nNow do the same with MFG4X. 50 FPS and 12.5 IPS is going to be a terrible experience, so push that to 20 IPS and 80 FPS, or 30 IPS and 120 FPS. Again, eventually it will feel better than the baseline experience. It could be at 35 IPS and 140 FPS, or maybe it's a bit higher or lower. But somewhere along the curves, MFG should start to look and feel better.\n\n\n\nThat's just one discrete example, and it's glossing over a lot of the complexity. What happens if the baseline performance is 100 FPS instead of 50, even 150 FPS? For most of us, we reach a point of diminishing returns and so the crossover point might end up being quite a bit lower than when the base performance was only 50 FPS. A steady but lower framerate can also feel better than a higher but inconsistent framerate, as we discussed with Hogwarts Legacy.\n\nNvidia marketing slide showing RTX 5090 performance using MFG4X. (Image credit: Nvidia)\n\nReally, the biggest issue with frame generation technologies is the marketing. Look at the above chart showing RTX 5090 performance, compared to the 4090. Nine games were tested, two without support for MFG and seven with MFG. Nvidia used data like this to claim that the 5090 is often over twice as fast as the 4090. It's deceptive at best, greatly inflating the importance of MFG.\n\n\n\nI've used the phrase \"frame smoothing\" repeatedly throughout this analysis, and for good reason. The AI-powered \"generation\" of \"new\" frames is really just a more sophisticated take on temporal interpolation and frame smoothing, something we've seen in TVs for over a decade with varying levels of quality. Putting that technology into real-time games ends up being less beneficial than it is for passively viewing content like TV broadcasts and movies.\n\n\n\nNvidia has said that AI and frame generation technologies aren't going away, and we believe it. The company will continue to push for more efficient ways to boost the number of frames that get sent to your monitor. However, we are quickly approaching \u2014 and probably well past in the case of MFG4X \u2014 the point of diminishing returns. Inserting one generated frame between two rendered frames can smooth things out. Generating two or three frames? The experience definitely doesn't scale linearly.\n\n\n\nIt's disingenuous of marketing to pretend that 200 FPS using MFG4X is anything like 200 FPS with regular framegen, or 200 FPS without framegen. It's equally laughable to suggest that an RTX 5070 with MFG4X is \"as fast as an RTX 4090\" with normal framegen. It's not \u2014 and it also lacks the VRAM to keep pace in a variety of other workloads. In either case, the base rendering speed and input sampling rates are critical and interlinked factors.\n\n\n\nA base rendering rate of 200 FPS on a 200 Hz (with adaptive sync) monitor feels completely different than a base rendering rate of 50 FPS quadrupled to 200 \"FPS\" on the same display. If Nvidia continues with the hubris and tries for RTX 60-series GPUs in a couple of years that generate up to seven frames between two rendered frames, it will be laughably transparent. In such a scenario, \"400 FPS!\" while rendering at a base 50 FPS would ultimately still feel probably closer to 80~100 FPS, which is basically where MFG4X already lands.\n\n\n\nSubjectively, it's almost impossible to give an answer to how much better (or worse) things are with frame generation enabled, in all situations. Sometimes it works well, other times it works okay, and still other times it can look slightly better but end up feeling worse. And if MFG is like framegen, sometimes it will break down \u2014 as DLSS3 routinely does on the RTX 4060 at higher resolutions.\n\n\n\nUltimately, it's the combination of base rendering speed and latency that defines the experience. Get the base performance high enough (above 40 FPS in my general opinion) and keep the latency low enough and you end up with a decent generated result. The higher the base rendering speed, the less frame generation techniques make it feel sluggish \u2014 and the less necessary framegen is in the first place.\n\n\n\nWhat we really need for frame generation is a way to link it to new user input. Something like Reflex 2 with its warping and in-painting combined with frame projection \u2014 the prediction of future frames. That's more complex, but if that can be done, games could actually feel more responsive rather than merely looking smoother. And that's probably where we're headed with DLSS 5 in the future. When and if that can be made to work effectively remains to be seen.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Mark Zuckerberg Just Delivered Incredible News for Nvidia, AMD, and Micron Stock Investors",
            "link": "https://www.nasdaq.com/articles/mark-zuckerberg-just-delivered-incredible-news-nvidia-amd-and-micron-stock-investors",
            "snippet": "Last week, semiconductor stocks like Nvidia (NASDAQ: NVDA), Advanced Micro Devices (NASDAQ: AMD), and Micron Technology (NASDAQ: MU) plunged on news that a...",
            "score": 0.7468873858451843,
            "sentiment": null,
            "probability": null,
            "content": "Last week, semiconductor stocks like Nvidia (NASDAQ: NVDA), Advanced Micro Devices (NASDAQ: AMD), and Micron Technology (NASDAQ: MU) plunged on news that a Chinese start-up called DeepSeek had figured out how to train artificial intelligence (AI) models for a fraction of the cost of its American peers.\n\nInvestors were concerned that DeepSeek's innovative approach would trigger a collapse in demand for graphics processors (GPUs) and other data center components, which are key to developing AI. However, those concerns might be overblown.\n\nMeta Platforms (NASDAQ: META) is a huge buyer of AI chips from Nvidia and AMD. On Jan. 29, CEO Mark Zuckerberg made a series of comments that should be music to the ears of investors who own AI hardware stocks.\n\nDeepSeek background\n\nSuccessful Chinese hedge fund High-Flyer has been using AI to build trading algorithms for years. It established DeepSeek as a separate entity in 2023 to capitalize on the success of other AI research companies, which were rapidly soaring in value.\n\nLast week's stock market panic was triggered by DeepSeek's V3 large language model (LLM), which matches the performance of the latest GPT-4o models from America's premier AI start-up, OpenAI, across several benchmarks. That isn't a concern at face value, except DeepSeek claims to have spent just $5.6 million training V3, whereas OpenAI has burned over $20 billion since 2015 to reach its current stage.\n\nTo make matters more concerning, DeepSeek doesn't have access to the latest data center GPUs from Nvidia, because the U.S. government banned them from being sold to Chinese firms. That means the start-up had to use older generations like the H100 and the underpowered H800, indicating it's possible to train leading AI models without the best hardware.\n\nTo offset the lack of computational performance, DeepSeek innovated on the software side by developing more efficient algorithms and data input methods. Plus, it adopted a technique called distillation, which involves using a successful model to train its own smaller models. This rapidly speeds up the training process and requires far less computing capacity.\n\nInvestors are concerned that if other AI firms adopt DeepSeek's approach, they won't need to buy as many GPUs from Nvidia or AMD. That would also squash demand for Micron's industry-leading data center memory solutions.\n\nNvidia, AMD, and Micron power the AI revolution\n\nNvidia's GPUs are the most popular in the world for developing AI models. The company's fiscal year 2025 just ended on Jan. 31, and according to management's guidance, its revenue likely more than doubled to a record $128.6 billion (the official results will be released on Feb. 26). If recent quarters are anything to go by, around 88% of that revenue will have come from its data center segment thanks to GPU sales.\n\nThat incredible growth is the reason Nvidia has added $2.5 trillion to its market capitalization over the last two years. If chip demand were to slow down, a lot of that value would likely evaporate.\n\nAMD has become a worthy competitor to Nvidia in the data center. The company plans to launch its new MI350 GPU later this year, which is expected to rival Nvidia's latest Blackwell chips that have become the gold standard for processing AI workloads.\n\nBut AMD is also a leading supplier of AI chips for personal computers, which could become a major growth segment in the future. As LLMs become cheaper and more efficient, it will eventually be possible to run them on smaller chips inside computers and devices, reducing reliance on external data centers.\n\nFinally, Micron is often overlooked as an AI chip company, but it plays a critical role in the industry. Its HBM3E (high-bandwidth memory) for the data center is best in class when it comes to capacity and energy efficiency, which is why Nvidia uses it inside its latest Blackwell GPUs. Memory stores information in a ready state, which allows the GPU to receive it instantaneously when needed, and since AI workloads are so data intensive, it's an important piece of the hardware puzzle.\n\nMark Zuckerberg might have put recent concerns to bed\n\nMeta Platforms spent a whopping $39.2 billion on chips and data center infrastructure during 2024, and it plans to spend as much as $65 billion this year. Those investments are helping the company further advance its Llama LLMs, which are the most popular open-source models in the world, with 600 million downloads. Llama 4 is due to launch this year, and CEO Mark Zuckerberg thinks it could be the most advanced in the industry, outperforming even the best closed-source models.\n\nOn Jan. 29, Meta held a conference call with analysts about its fourth quarter of 2024. When Zuckerberg was quizzed about the potential impact of DeepSeek, he said it's probably too early to determine what it means for capital investments into chips and data centers. However, he said even if it results in less capacity requirements for AI training workloads, it doesn't mean companies will need fewer chips.\n\nInstead, he thinks capacity could shift away from training and toward inference, which is the process by which AI models process inputs from users and form responses. Many developers are moving away from training models by using endless amounts of data, and focusing on \"reasoning\" capabilities instead. This is referred to as test-time scaling, and it involves the model taking extra time to \"think\" before rendering an output, which results in higher-quality responses.\n\nReasoning requires more inference compute, so Zuckerberg thinks companies will still need the best data center infrastructure to maintain an advantage over the competition. Plus, most AI software products haven't achieved mainstream adoption yet, and Zuckerberg acknowledges that serving many users will also require additional data center capacity over time.\n\nSo, while it's hard to put exact numbers on how DeepSeek's innovations will reshape chip demand, Zuckerberg's comments suggest there isn't a reason for Nvidia, AMD, and Micron stock investors to panic. In fact, there is even a bullish case for those stocks over the long term.\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nEver feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia: if you invested $1,000 when we doubled down in 2009, you\u2019d have $307,661 !*\n\nif you invested $1,000 when we doubled down in 2009, !* Apple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $44,088 !*\n\nif you invested $1,000 when we doubled down in 2008, !* Netflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $536,525!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nLearn more \u00bb\n\n*Stock Advisor returns as of February 3, 2025\n\nRandi Zuckerberg, a former director of market development and spokeswoman for Facebook and sister to Meta Platforms CEO Mark Zuckerberg, is a member of The Motley Fool's board of directors. Anthony Di Pizio has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Advanced Micro Devices, Meta Platforms, and Nvidia. The Motley Fool has a disclosure policy.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-02-04": {
        "0": {
            "title": "NVIDIA Blackwell Now Generally Available in the Cloud",
            "link": "https://blogs.nvidia.com/blog/blackwell-coreweave-gb200-nvl72-instances-cloud/",
            "snippet": "CoreWeave has launched NVIDIA GB200 NVL72-based instances, becoming the first cloud service provider to make Blackwell generally available.",
            "score": 0.848275363445282,
            "sentiment": null,
            "probability": null,
            "content": "CoreWeave launches the first NVIDIA GB200 NVL72-cloud based instances to power the next era of AI reasoning.\n\nAI reasoning models and agents are set to transform industries, but delivering their full potential at scale requires massive compute and optimized software. The \u201creasoning\u201d process involves multiple models, generating many additional tokens, and demands infrastructure with a combination of high-speed communication, memory and compute to ensure real-time, high-quality results.\n\nTo meet this demand, CoreWeave has launched NVIDIA GB200 NVL72-based instances, becoming the first cloud service provider to make the NVIDIA Blackwell platform generally available.\n\nWith rack-scale NVIDIA NVLink across 72 NVIDIA Blackwell GPUs and 36 NVIDIA Grace CPUs, scaling to up to 110,000 GPUs with NVIDIA Quantum-2 InfiniBand networking, these instances provide the scale and performance needed to build and deploy the next generation of AI reasoning models and agents.\n\nNVIDIA GB200 NVL72 on CoreWeave\n\nNVIDIA GB200 NVL72 is a liquid-cooled, rack-scale solution with a 72-GPU NVLink domain, which enables the six dozen GPUs to act as a single massive GPU.\n\nNVIDIA Blackwell features many technological breakthroughs that accelerate inference token generation, boosting performance while reducing service costs. For example, fifth-generation NVLink enables 130TB/s of GPU bandwidth in one 72-GPU NVLink domain, and the second-generation Transformer Engine enables FP4 for faster AI performance while maintaining high accuracy.\n\nCoreWeave\u2019s portfolio of managed cloud services is purpose-built for Blackwell. CoreWeave Kubernetes Service optimizes workload orchestration by exposing NVLink domain IDs, ensuring efficient scheduling within the same rack. Slurm on Kubernetes (SUNK) supports the topology block plug-in, enabling intelligent workload distribution across GB200 NVL72 racks. In addition, CoreWeave\u2019s Observability Platform provides real-time insights into NVLink performance, GPU utilization and temperatures.\n\nCoreWeave\u2019s GB200 NVL72 instances feature NVIDIA Quantum-2 InfiniBand networking that delivers 400Gb/s bandwidth per GPU for clusters up to 110,000 GPUs. NVIDIA BlueField-3 DPUs also provide accelerated multi-tenant cloud networking, high-performance data access and GPU compute elasticity for these instances.\n\nFull-Stack Accelerated Computing Platform for Enterprise AI\n\nNVIDIA\u2019s full-stack AI platform pairs cutting-edge software with Blackwell-powered infrastructure to help enterprises build fast, accurate and scalable AI agents.\n\nNVIDIA Blueprints provides pre-defined, customizable, ready-to-deploy reference workflows to help developers create real-world applications. NVIDIA NIM is a set of easy-to-use microservices designed for secure, reliable deployment of high-performance AI models for inference. NVIDIA NeMo includes tools for training, customization and continuous improvement of AI models for modern enterprise use cases. Enterprises can use NVIDIA Blueprints, NIM and NeMo to build and fine-tune models for their specialized AI agents.\n\nThese software components, all part of the NVIDIA AI Enterprise software platform, are key enablers to delivering agentic AI at scale and can readily be deployed on CoreWeave.\n\nBringing Next-Generation AI to the Cloud\n\nThe general availability of NVIDIA GB200 NVL72-based instances on CoreWeave underscores the latest in the companies\u2019 collaboration, focused on delivering the latest accelerated computing solutions to the cloud. With the launch of these instances, enterprises now have access to the scale and performance needed to power the next wave of AI reasoning models and agents.\n\nCustomers can start provisioning GB200 NVL72-based instances through CoreWeave Kubernetes Service in the US-WEST-01 region using the gb200-4x instance ID. To get started, contact CoreWeave.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "I plugged an Nvidia RTX 5090 into a gaming handheld",
            "link": "https://www.theverge.com/pc-gaming/606006/nvidia-rtx-5090-egpu-gaming-handheld-oculink-test-pcie-minisforum-deg1",
            "snippet": "We took the Nvidia RTX 5090, the world's fastest GPU, and routed its power to a gaming handheld over an Oculink cable.",
            "score": 0.9290900230407715,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and founding member of The Verge who covers gadgets, games, and toys. He spent 15 years editing the likes of CNET, Gizmodo, and Engadget.\n\nTwo weeks ago, I showed you how the world\u2019s fastest graphics card works in a small form factor PC. To my surprise, Nvidia\u2019s RTX 5090 Founders Edition delivered the vast majority of its performance even in a 12.7-liter desktop with a five-year-old CPU.\n\nIt made me wonder: what if I plugged this card into a handheld gaming PC instead? So I did, and let me tell you: it\u2019s a wonder to behold. It\u2019s enough to make me believe in a rich future where handhelds get more powerful when you dock them at home.\n\nI started with the same $1,999 RTX 5090 FE and 1000-watt power supply from my desktop test, dropping them both onto a $99 Minisforum DEG1. It\u2019s an open-air external GPU that can connect to the Oculink port that\u2019s now shipping in a handful of portable gaming PCs, so long as you bring your own desktop GPU and power supply.\n\nI plugged that Oculink cable into a $1,000 GPD Win Max 2 handheld. And then, with just an AMD Ryzen 8840U mobile CPU and four lanes of PCIe 4.0 bandwidth, rather than the 16 lanes of PCIe 5.0 that Nvidia\u2019s GPU technically supports, my new Franken-desktop spit fire anyhow. I\u2019m talking over 100 frames per second in Cyberpunk 2077 at 4K resolution and Ultra settings. I\u2019m talking about playable frame rates in all the most intensive games at 4K and near-maximum specs.\n\nAcross our eight test games, it did run between 7 percent and 47 percent slower than when my colleague Tom Warren paired a 5090 with the fastest gaming CPU money can buy. But it was only 4 to 29 percent slower than that 5090 in my SFF desktop and far faster than when I stuck an RTX 3080 in that desktop. And it was four to 12 times faster than the handheld\u2019s Radeon 780M integrated GPU could manage on its own.\n\nStart with the \u201c5090 handheld\u201d column in my table below:\n\n5090 eGPU versus 5090 desktop @ 4K Game 3080 SFF 5090 Handheld 5090 SFF 5090 Bench 5090 Handheld vs. 5090 SFF 5090 Handheld vs. 5090 Bench 5090 Handheld vs. 3080 SFF Assassin\u2019s Creed Mirage (Ultra High, Native) 65 108 135 144 -20.00% -25.00% 66.15% Black Myth: Wukong (100% resolution) 22 55 63 62 -12.70% -11.29% 150.00% Black Myth: Wukong (DLSS + Frame Gen) N/A 132 147 146 -10.20% -9.59% N/A Call of Duty: Black Ops 6 (Extreme) 58 134 140 145 -4.29% -7.59% 131.03% Cyberpunk 2077 (Ultra, no RT) 40 101 108 109 -6.48% -7.34% 152.50% Cyberpunk 2077 (Ultra RT + DLSS Quality + FG) N/A 132 152 153 -13.16% -13.73% N/A Horizon Zero Dawn Remastered (Very High, Native) 59 80 112 153 -28.57% -47.71% 35.59% Horizon Zero Dawn (DLSS Quality + FG) N/A 151 206 237 -26.70% -36.29% N/A Metro Exodus Enhanced (Extreme) 33 85 92 91 -7.61% -6.59% 157.58% Returnal (Epic) 61 113 138 142 -18.12% -20.42% 85.25% Shadow of the Tomb Raider (Highest) 89 150 207 238 -27.54% -36.97% 68.54% Average frame rates, higher is better. DLSS Frame Gen unsupported on 3080.\n\nFor example, my GPD Win Max 2 review unit averages just 17fps in Call of Duty: Black Ops 6 at Extreme spec at the handheld\u2019s native 2560 x 1600 screen resolution \u2014 and if I set the game to the lowest, most potato graphics possible at a pixelated 640 x 480 resolution, the very best I can get is 113fps. Plug in the RTX 5090, and I get 131fps with the graphics at their highest settings. From potato to ultra with one plug \u2014 and a mortgage payment.\n\nBlack Myth: Wukong at the most potato settings I could muster.\n\nHorizon: Zero Dawn Remastered benchmark at utter potato settings.\n\nNot that you\u2019d want to play games that potent on a handheld\u2019s small internal screen, of course \u2014 and I saw the best results plugging in an external monitor, too, because you lose less performance shuffling data back and forth across that Oculink cable. My Call of Duty frame rate went from 131fps on the handheld\u2019s 1600p internal screen to 134fps on an external 4K monitor. Of our test games, only Black Myth: Wukong fell below 60 frames per second at native 4K.\n\nI don\u2019t want to oversell Oculink GPUs too much because they\u2019re not entirely ready for primetime, and they come with some major caveats compared to Thunderbolt and USB-C tech.\n\nFirst up, Oculink cables are not hot-swappable like USB products. It\u2019s so tempting to imagine playing a game handheld, then plugging it in and resuming with amazing graphics on a big screen, but that\u2019s not the reality. You have to shut down your PC every time you connect and disconnect or risk damage \u2014 the first time I tried an Oculink system a year ago, I apparently damaged its port for good. And before you power on your PC, you have to power on the eGPU first. (The Minisforum DEG1 I\u2019m using does attempt to sync with a connected PC and power on and off at the same time, but it hasn\u2019t been foolproof for me; the company says it\u2019s designed to sync with its own mini-PCs.)\n\nSecond, current Oculink cables themselves aren\u2019t as robustly designed as many USB products. While they\u2019re shaped like a smaller DisplayPort, with the same kind of locking connector, they seem far easier to accidentally dislodge, bend, or break.\n\nHandheld graphics versus eGPU graphics, internal versus external display Game 780M (1600p, internal screen) 5090 (1600p, internal screen) 5090 (4K, external screen) 5090 (1440p, external screen) 780M (potato settings) Assassin\u2019s Creed Mirage (Ultra High, Native) 18 106 108 109 89 Black Myth: Wukong (100% resolution) 6 72 55 81 76 Black Myth: Wukong (DLSS + Frame Gen) N/A 140 132 161 N/A Call of Duty: Black Ops 6 (Extreme) 17 131 134 140 113 Cyberpunk 2077 (Ultra, no RT) 12 107 101 116 101 Cyberpunk 2077 (Ultra RT + DLSS Quality + FG) N/A 153 132 164 N/A Horizon Zero Dawn Remastered (Very High, Native) 15 77 80 86 71 Horizon Zero Dawn (DLSS Quality + Frame Gen) N/A 138 151 158 N/A Metro Exodus Enhanced (Extreme) 7 94 85 98 51 Returnal (Epic) 15 110 113 113 52 Shadow of the Tomb Raider (Highest) 20 144 150 150 96 Average frame rates, higher is better. DLSS Frame Gen unsupported on 780M.\n\nThird, Oculink is not a single-cable docking solution like Thunderbolt or USB4 because it doesn\u2019t provide USB data, audio, or power to a connected laptop: I had to run a separate USB-C power cord to the handhelds I tested. That\u2019s a feature, not a bug, so there\u2019s nothing to get in the way of your GPU bandwidth.\n\nFourth, there simply aren\u2019t all that many devices with Oculink. GPD is one devotee, and I tested it working with an early sample of the Ayaneo 3, but the companies you\u2019ll find at your local Best Buy \u2014 Asus, Lenovo \u2014 are currently putting USB4 ports into their handhelds instead of Oculink.\n\nAnd fifth, Oculink is currently such an afterthought for companies like Nvidia that their drivers have apparently been bugged for years. When I first plugged the 5090 into the GPD Win Max 2 and the Ayaneo 3, they refused to tap into its power at all until I installed a community-developed \u201cerror43\u201d patch that worked like magic. Even so, I saw a couple of crashes that I couldn\u2019t pin down.\n\nSuccessfully running the error43 fixer on an Ayaneo 3. And yes, this handheld is beige. Photo by Sean Hollister / The Verge\n\nToday, prospective eGPU buyers have to pick between the reduced bandwidth and compatibility pains of Thunderbolt 4 and USB 4 or the cleaner but less-user-friendly Oculink, but hopefully best-of-both-worlds options won\u2019t take too long to materialize. Asus has already announced the first Thunderbolt 5 eGPU, even if the company doesn\u2019t currently have any smaller, weaker PCs with Thunderbolt 5 that could actually take advantage of it, and perhaps some future 80Gbps USB4 devices could offer the same without needing Intel\u2019s Thunderbolt certification.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Microsoft's Bill Gates issues this key reminder to Nvidia bulls",
            "link": "https://finance.yahoo.com/news/microsofts-bill-gates-issues-this-key-reminder-to-nvidia-bulls-125913525.html",
            "snippet": "Delivering on high expectations won't be getting any easier for market darling Nvidia, Bill Gates warns.",
            "score": 0.8866302967071533,
            "sentiment": null,
            "probability": null,
            "content": "Listen and subscribe to Opening Bid on Apple Podcasts, Spotify, YouTube, or wherever you find your favorite podcasts.\n\nStaying on top in the AI chip race won't be easy for market darling Nvidia (NVDA) and its founder Jensen Huang, reminds one fellow tech billionaire.\n\n\"The fact that Jensen doesn't even make his own chips \u2014 that everyone has Taiwan Semiconductor (TSM) available \u2014 is all the more credit to him at the design level, the way they have done things is pretty fantastic. I would say I wouldn't want to be Jensen necessarily because wow, other people are working on the same things,\" Microsoft (MSFT) co-founder Bill Gates told me on Yahoo Finance's Opening Bid podcast (video above; listen in below).\n\nAdded Gates, \"But so far he has stayed way ahead and Microsoft is an incredible customer and is always wanting to get as many of his chips as they possibly can. At the same time other big tech companies are working on their own AI chips.\"\n\nThis embedded content is not available in your region.\n\nCompetition for Nvidia is sprouting up, as Gates correctly points out.\n\nWith giants like Amazon (AMZN) announcing an $8 billion partnership with Anthropic to enter the AI chip space and Google (GOOG) dropping a supercomputer with an AI chip called Willow, it's evident Big Tech companies want in on the action.\n\nFurther, Broadcom (AVGO) and Marvell (MRVL) have released advanced custom chips.\n\nAt the same time, the long-term demand for Nvidia's powerful AI chips is being questioned arguably for the first time.\n\nDeepSeek is a Chinese company that has bursted onto the tech landscape seemingly out of nowhere.\n\nIt surprised markets and those trading hot AI names like Nvidia and AMD (AMD) after unveiling RI, its AI model that gave a ChatGPT-esque performance at a cheaper price tag. RI costs a reported $5.6 million to build a base model, compared to the hundreds of millions of dollars incurred at US-based companies such as OpenAI and Anthropic.\n\nWatch: Why billionaire investor Ray Dalio is concerned about tech valuations\n\nFears mounted instantly that US companies are overspending on AI infrastructure, which includes Nvidia chips.\n\n\"So what this [news] basically does is two things. One, it says there's still a lot of innovation left and many companies can aspire to train these models. But, two, it also raises the really interesting question of do you need to spend billions of dollars in order to train cutting-edge, world-class models? I think the jury is still out there for things like that, but it's basically upended our assumptions about where AI is going,\" Snowflake CEO Sridhar Ramaswamy told me on Opening Bid podcast.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Every Nvidia Investor Should Keep an Eye on This Number",
            "link": "https://www.fool.com/investing/2025/02/04/every-nvidia-investor-should-watch-this-number/",
            "snippet": "Artificial intelligence (AI) leader Nvidia (NVDA 0.87%) had a strong 2024. During the year, analysts debated how high its data center revenue could grow as...",
            "score": 0.9376355409622192,
            "sentiment": null,
            "probability": null,
            "content": "Artificial intelligence (AI) leader Nvidia (NVDA 5.27%) had a strong 2024. During the year, analysts debated how high its data center revenue could grow as it rolled out new products. Hyperscaler customers kept increasing capital spending to build out data center compute power. But things turned on a dime in January 2025 when Chinese hedge fund-owned DeepSeek dropped an AI bombshell regarding resources needed to do AI well.\n\nRather than wondering how Nvidia's latest AI chip sales were going, some investors cared more about how to invest in DeepSeek and its claims that it created a high-performing large language model (LLM) with just $6 million in capital. The news led to an 11% decline in Nvidia stock thus far in 2025. But questions remain about DeepSeek and its claims, and investors would be smart to focus on what Nvidia says in its next earnings report (due on Feb. 26).\n\nData centers and more\n\nBefore the DeepSeek news, all eyes were on Nvidia's unprecedented revenue growth -- specifically, in its data center segment. The unsustainable rate of growth was slowing, though, and Nvidia's share price had already rocketed higher as those sales soared.\n\nIf spending on generative AI infrastructure comes to a screeching halt, as some have surmised from the DeepSeek news, the quarter-over-quarter growth in Nvidia's data center revenue should quickly reverse. As seen below, it appeared to begin stabilizing at a mid-teens growth rate in the last quarter.\n\nQuarter-Over-Quarter Data Center Revenue Growth Q2 2024 Q3 2024 Q4 2024 Q1 2025 Q2 2025 Q3 2025 141% 41% 27% 23% 16% 17%\n\nThat's the number I'll continue to watch to see if Nvidia's share price can recoup recent losses and more -- at least in the short term. Nvidia also has other irons in the fire for the long term.\n\nThe company's auto and robotics segment will be the next area to watch. That revenue pales in comparison to the data center but could be the next impactful growth catalyst for Nvidia.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia: Thank You DeepSeek (NASDAQ:NVDA)",
            "link": "https://seekingalpha.com/article/4754377-nvidia-thank-you-deepseek",
            "snippet": "DeepSeek's claims may be overstated, and Nvidia's long-term growth prospects remain strong. Read why I remain bullish on NVDA stock.",
            "score": 0.8538247346878052,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "The Nvidia Shield TV Pro is down to $179.99. Grab this rare discount on the ultimate 4K HDR streaming box.",
            "link": "https://mashable.com/article/feb-4-nvidia-shield-tv-pro-deal",
            "snippet": "As of Feb. 4, Nvidia Shield TV Pro is on sale for $179.99 at Amazon, down from $199.99. This premium Android TV device offers Dolby Vision, AI upscaling,...",
            "score": 0.926600992679596,
            "sentiment": null,
            "probability": null,
            "content": "Deal pricing and availability subject to change after time of publication. Learn more about how we select deals\n\nSAVE $20: As of Feb. 4, the Nvidia Shield TV Pro is available for $179.99 at Amazon, down from $199.99. That\u2019s a $20 discount on one of the most versatile media streamers available, featuring Dolby Vision HDR, AI-enhanced upscaling, Plex media server functionality, and GeForce NOW cloud gaming.\n\nIf you want a streaming box that does it all, the Nvidia Shield TV Pro is the best does-it-all streaming box you can buy right now. At $179.99 on Amazon, a $20 saving over its $199.99 list price, this is a gaming and binge-worthy bargain. It's a home theater powerhouse, Plex media server, and cloud gaming rig wrapped into one sleek box.\n\nWith Dolby Vision and Dolby Atmos, the Shield TV Pro transforms your living room into a cinematic experience. You'll get crisp 4K visuals and immersive audio from services such as Netflix, Disney+, Apple TV+, or HBO Max (subscription-dependent). Older HD content gets treated to AI-enhanced upscaling that sharpens it to near-4K quality for a cleaner, crisper picture.\n\nOne of its biggest perks is built-in Plex media server support. If you have a collection of movies, TV shows, or music, the Shield TV Pro can store and stream them across your devices, giving you full control over your personal media library. Unlike regular streaming sticks, this device has USB 3.0 ports, letting you expand storage, connect a keyboard, or even use a USB webcam.\n\nGaming? No problem. With GeForce NOW, the Shield TV Pro turns into a powerful gaming system with no console required. Play AAA PC games like Cyberpunk 2077 and Fortnite directly from the cloud, with ray tracing and DLSS support for ultra-smooth gameplay. Just connect a controller, and you're ready to go.\n\nMashable Deals Want more hand-picked deals from our shopping experts? Sign up for the Mashable Deals newsletter. Loading... Sign Me Up By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy Thanks for signing up!\n\nRunning on Android TV 11, the Shield TV Pro supports all major streaming services, Google Chromecast 4K, and hands-free voice control with Google Assistant and Alexa.\n\nAt just $179.99, this deal is a no-brainer. Whether you need a high-end streaming device, a Plex media server, or a cloud gaming machine, the Shield TV Pro does it all \u2014 and this discount won't last long.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Nvidia Gets The Axe As Google, Meta Lead The Resistance",
            "link": "https://www.investors.com/research/how-to-find-the-best-stocks-to-buy/nvidia-stock-meta-google-applovin-ibd-leaderboard/",
            "snippet": "While Nvidia stock has dropped off IBD Leaderboard, Meta and Google show strength as Trump tariffs rattle the market.",
            "score": 0.8293789029121399,
            "sentiment": null,
            "probability": null,
            "content": "After gapping down below its 50-day moving average and failing to find support at its 200-day line, Nvidia (NVDA) has lost its spot on IBD Leaderboard. But fellow Magnificent Seven stocks Meta Platforms (META) and Alphabet (GOOGL) have shown strength. But Google stock is down late Tuesday after reporting earnings.\n\nMeanwhile, AppLovin (APP) \u2014 one of the best stocks of 2024 \u2014 looks to launch a new breakout. And Tesla (TSLA) and Axon Enterprise (AXON) join a group of four stocks on the IBD Leaderboard watchlist.\n\n\u2191 X How To Avoid Surprise Drops In Generative AI Growth Stocks See All Videos NOW PLAYING How To Avoid Surprise Drops In Generative AI Growth Stocks\n\nAppLovin Sets Up As Nasdaq Shakes Tariff Fight\n\nMeta continues to hold near its all-time high as Nvidia reinforces the eight \"secrets\" of when to sell stocks. After notching a record high Tuesday, Alphabet is down after hours on light revenue.\n\nAt the same time, AppLovin, Shopify (SHOP) ad Us Foods (USFD) are among five IBD Leaderboard names in or near a buy zone. HubSpot (HUBS), which was featured in the IBD Stock Analysis on Jan. 17, and Wix.com (WIX) round out this cadre of stocks to watch.\n\nNetflix (NFLX) joins IBD Leaderboard as the video streamer tests the very top of its buy range.\n\nAxon Takes Aim At Breakout As Tesla Looks On\n\nAmong the four stocks on the IBD Leaderboard watchlist, Axon stands closest to launching a breakout.\n\nThe Taser maker has formed a second-stage cup pattern with a 698.67 buy point.\n\nWhile Cava Group (CAVA) and On Holding (ONON) continue to work on new setups, both stocks have climbed back above their 50-day lines. After falling over 7% below a 60.12 entry to trigger a sell rule, On Holding is striding to retake that prior buy point.\n\nAfter reporting earnings and revenue that came in short of Wall Street expectations, Tesla stock retreated below its 50-day moving average on Monday. While the electric vehicle maker may be in the process of building a new base, its relative strength line has lagged since notching a 52-week high in December.\n\nFor the first time since August of 2023, Tesla just made this month's list of new buys by the best mutual funds, which comes out on Friday.\n\nFollow Matthew Galgani on X (formerly Twitter) at @IBD_MGalgani.\n\nYOU MAY ALSO LIKE:\n\nNvidia Got Spanked, But This AI Big Daddy Soars 194% \u2014 And Counting\n\nTwo Of 2024's Best Stocks Rule The Market Yet Again This Year\n\nDeepSeek-Driven Crash Makes Nvidia Investors Face 8 'Secrets' Of Selling. No. 2 Is Key.\n\nThree Key Factors For Stock Investing\n\nBuild Custom Stock Screens With MarketSurge",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Stock Rises. What Tariff Threats Mean for the AI Chip Maker.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-china-tariffs-taiwan-1357fce4",
            "snippet": "Nvidia stock was gaining early Tuesday as the threat of tariffs disrupting the chip sector appeared to fade.",
            "score": 0.555151641368866,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia stock faces fresh China concerns",
            "link": "https://www.thestreet.com/investing/nvidia-stock-faces-fresh-china-concerns",
            "snippet": "Nvidia generates nearly a third of its revenue from China and Taiwan.",
            "score": 0.9531718492507935,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Versatile 16-Phase, 2-Rail PMIC Supports NVIDIA GPU-Based Apps",
            "link": "https://www.electronicdesign.com/technologies/power/article/55265485/electronic-design-versatile-16-phase-2-rail-pmic-supports-nvidia-gpu-based-apps",
            "snippet": "Specifically designed to NVIDIA's latest OpenVreg16 Phase specifications, Alpha and Omega Semi's power-management IC delivers an efficient, flexible power...",
            "score": 0.8216478228569031,
            "sentiment": null,
            "probability": null,
            "content": "Alpha and Omega Semiconductor\u2019s (AOS) AOZ73016QI is a 16-phase, 2-rail controller specifically designed to the latest OpenVReg16 (OVR16) specifications from NVIDIA. The AOZ73016QI controller design is based on the company\u2019s high-performance, proprietary AOS Advanced Transient Modulator (A2TM) control scheme, which offers high-level current balance at all load conditions as well as during fast load transients.\n\nIn addition to supporting all basic requirements of the OVR16 specification, the new AOS power-management IC (PMIC) offers value-added features such as R DS(on) and DC resistance (DCR) sensing for current monitoring and current balance. These features enable AOS\u2019s controller to support both DrMOS and Smart Power Stages (SPS) to deliver a complete AI server and graphic-card power solution along with increased design flexibility.\n\nThe controller also supports phase doubling or tripling without an external phase multiplier, allowing for a single controller to provide up to 48 phases to its associated power stages.\n\nWhen paired with AOS\u2019s high-performance DrMOS and SPS power stages, designers can achieve high efficiency and thermal performance. This will significantly cut transient power demands by several hundred watts during the brief periods when the SoC draws peak power.\n\nThe AOZ73016QI offers full programmability via the PMBus interface and is AVSBus-compliant. The device features digitally programmable voltage- and current-regulation loops, minimizing the external components required to implement a solution. It supports electronic control system (ECS) programmability with the ability to update configuration in the field, and to pre-program up to six configuration settings with a pin-strap selection\n\nThe AOZ73016QI is immediately available in production quantities with a lead time of 12 to 16 weeks. Unit pricing starts at $4.00 in 1,000-piece quantities.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-03": {
        "0": {
            "title": "Lots of NVIDIA GeForce RTX 5090 & 5090D GPUs Are Getting Bricked, Possibly Due To Driver, BIOS or PCIe Issues",
            "link": "https://hardforum.com/threads/lots-of-nvidia-geforce-rtx-5090-5090d-gpus-are-getting-bricked-possibly-due-to-driver-bios-or-pcie-issues.2039565/",
            "snippet": "In other news, the PCMarket community on Facebook (via Videocardz & Uniko's Hardware) managed to do some testing with RTX 5090D, RTX 5080, and RTX 4080...",
            "score": 0.9625167846679688,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "Is Nvidia in Deep Trouble Due to DeepSeek?",
            "link": "https://www.fool.com/investing/2025/02/03/is-nvidia-in-deep-trouble-with-deepseek/",
            "snippet": "DeepSeek shook up the tech world with claims that it built an impressive new AI model for a small fraction of the cost of prior ones.",
            "score": 0.9483724236488342,
            "sentiment": null,
            "probability": null,
            "content": "DeepSeek, a Chinese artificial intelligence (AI) company that develops large language models (LLMs), turned the world of AI on its head recently when it claimed that it spent just $5.6 million (note this is million, not billion) on computing power to develop its base AI model. That would be a fraction of what U.S. companies have been spending on computing power to build their AI models. And demand for infrastructure to power AI software is expected to be immense.\n\nFor example, Microsoft plans to spend $80 billion building out AI-capable data centers this year. Historically, about half of the spending on data centers goes toward servers. Meta Platforms, meanwhile, announced it would spend $65 billion this year on AI development, while the recently announced Stargate project backed by Oracle, OpenAI, and Softbank has plans to spend $500 billion on AI infrastructure over the next several years.\n\nThe claim that DeepSeek could build an LLM so cheaply sent shock waves through the markets last week, and Nvidia (NVDA 5.27%) was the biggest loser. Nvidia's graphics processing units (GPUs) are central to the tech world's AI infrastructure buildout as they are the primary source of the specific type of rapid computing power that AI systems require. The market's logic was simple: If DeepSeek can create an LLM chatbot on par with (or better than) ChatGPT or Meta's Llama using far less processing power, that does not bode well for GPU demand.\n\nIn the U.S., tech companies have been using steadily more GPUs to develop each new iteration of their AI models. For example, Meta is deploying 160,000 GPUs to train Llama 4 -- 10 times as many as it used to train Llama 3. Elon Musk's xAI, meanwhile, used 20,000 GPUs to train its Grok 2 model, while for Grok 3, it used 100,000 GPUs for phase 1 of its training, then boosted it to 200,000 for phase 2.\n\nIf effective models can be built using much less computing power, that could potentially be bad news for Nvidia.\n\nQuestionable costs and potential theft\n\nWhile DeepSeek's chatbot is widely viewed as being very good, there is no verification about how much it actually spent on computing power, how many GPUs it used, or which particular models they were. The company claims it used a little more than 2,000 GPUs to train its model and that it had access to 10,000 older Nvidia A100 GPUs.\n\nSome experts do not believe it. Alexandr Wang, CEO of Scale AI, has said in interviews that it was his understanding that DeepSeek had access to about 50,000 more advanced Nvidia H100 chips, but that it can't say so publicly because U.S. regulations forbid their export to China. There is a belief that China is getting high-end Nvidia chips through Singapore. Nvidia's H100 chips cost $25,000 each, so 50,000 chips would have cost $1.25 billion. That's far higher than the asserted development price tag of $5.6 million.\n\nSemiAnalysis analyst Dylan Patel has estimated that DeepSeek and its parent company, Chinese hedge fund High-Flyer, have access to tens of thousands of Nvidia GPUs and have spent well north of $500 million on GPUs.\n\nMeanwhile, OpenAI has recently said that it has proof that DeepSeek was trained in part by extracting data from ChatGPT's model. Microsoft, an OpenAI investor, has found examples of data exfiltration through OpenAI developer accounts linked to DeepSeek. By extracting data from an established model, a company would be able to train a new model at a much lower cost through a process called distillation. Top White House AI advisor David Sacks has said that intellectual property theft may have indeed occurred, saying there was substantial evidence of it.\n\nJevons paradox\n\nWhile there is a lot of doubt surrounding DeepSeek's cost claims, and it appears that it could have gotten a leg up in its model's development by distilling data from OpenAI, there is also an argument to be made that even if DeepSeek was able to develop an AI model at a much cheaper price, that wouldn't necessarily hurt Nvidia. Jevons paradox is an economic theory that posits that when a resource becomes more efficient and costs are lowered, those lower costs lead to more consumption of the resource, and thus, the higher efficiency doesn't negatively impact overall demand. With AI, the belief is that lower computing costs will increase the technology's adoption.\n\nUpon initially hearing of DeepSeek's cost claims, Microsoft CEO Satya Nadella tweeted: \"Jevons paradox strikes again! As AI gets more efficient and accessible, we will see its use skyrocket, turning it into a commodity we just can't get enough of!\"\n\nA number of Wall Street analyst firms, meanwhile, have said that this could be a good thing for Nvidia. That group includes Cantor Fitzgerald, which said that this will lead to the AI industry wanting more computing power, not less.\n\nIs it time to buy the dip in Nvidia?\n\nAt this point, I think investors should be skeptical of DeepSeek's claims. There is a lot of doubt about the costs used to build its model, and apparent evidence that it piggybacked off of OpenAI's model.\n\nIn that light, the sell-off in Nvidia stock looks like a great buying opportunity. There is still a lot of AI infrastructure spending planned, and I don't think DeepSeek's claims are going to slow it down.\n\nTrading at a forward price-to-earnings (P/E) ratio of 27 based on analysts' 2025 estimates and a forward price/earnings-to-growth ratio (PEG) of under 0.85, the stock looks like a bargain. Stocks with positive PEG ratios below 1 are typically viewed as undervalued, and growth stocks often have PEGs much higher than that. For investors who have been waiting for a dip to buy Nvidia, this is their chance.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "What do Nvidia\u2019s New AI Features Mean For The Tech Industry?",
            "link": "https://technologymagazine.com/articles/what-do-nvidias-new-ai-features-mean-for-the-tech-industry",
            "snippet": "Nvidia launches GeForce RTX 5090 and 5080 GPUs with enhanced AI features, advanced video encoding capabilities and increased memory bandwidth.",
            "score": 0.917833685874939,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Watch These Nvidia Stock Price Levels Amid Continued Slump for AI Favorite",
            "link": "https://www.investopedia.com/watch-these-nvidia-stock-price-levels-amid-continued-slump-for-ai-favorite-deepseek-tariffs-8784704",
            "snippet": "Nvidia shares fell Monday after the Trump administration said it would impose tariffs, adding to steep losses for the stock that were fueled by concerns...",
            "score": 0.8500556945800781,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia shares fell Monday after the Trump administration said over the weekend it would impose tariffs, adding to steep losses for the stock that were fueled by concerns related to Chinese startup DeepSeek's low-cost AI model.\n\nNvidia shares broke down below a seven-month rising wedge pattern last week on the highest trading volume since August 2023, suggesting institutional selling participation.\n\nInvestors should watch key support levels on Nvidia's chart around $96, $76, and $50, while also monitoring important resistance levels near $130 and $150.\n\n\n\nNvidia (NVDA) shares fell Monday after the Trump administration said over the weekend it would impose tariffs on major trading partners, adding to the stock's steep losses last week that were fueled by concerns related to Chinese startup DeepSeek's low-cost AI model.\n\nThe tariffs, which impose 25% levies on Mexican and most Canadian imports, and 10% on all goods from China, have prompted concerns of a broader global trade war that could see countries introduce counter measures on items, including American AI chips.\n\nNvidia shares were down more than 2% at around $117 in late trading Monday, after falling 16% last week following the revelation of DeepSeek's success, which sparked concerns that Big Tech\u2019s investment in the chipmaker's pricey AI offerings could wane.\n\nNvidia shares have lost nearly a quarter of their value since hitting a record high in early January but are still up about 80% over the past 12 months.\n\nBelow, we zoom out on Nvidia\u2019s weekly chart to identify key longer-term price levels worth watching.\n\nRising Wedge Breakdown\n\nNvidia shares broke down below a seven-month rising wedge pattern last week on the highest trading volume since August 2023, suggesting institutional selling participation.\n\nMoreover, the relative strength index (RSI) fell below the 50 threshold to its lowest level since December 2022, confirming increasing selling momentum.\n\nAmid increasing market volatility, let\u2019s identify key support and resistance levels that investors may be watching.\n\nKey Support Levels to Watch\n\nA breakdown below the 50-period MA this week could see Nvidia shares fall to around $96, a location on the chart where they may find support near the March peak and August trough.\n\nThe next lower support level to watch sits around $76. Buyers may look for entry points in this area near the April pullback low that formed toward the end of the stock\u2019s strong trending period between October 2022 and June last year.\n\nA close below this region opens the door for a more significant correction down to the psychological $50 level. The shares may attract support in this location near the 200-period MA, which also currently aligns with the stock\u2019s August and November 2023 peaks.\n\nImportant Resistance Levels to Monitor\n\nUpon a recovery, investors should initially monitor the $130 level, an area where the stock could encounter selling pressure near the August 2024 peak and December trough.\n\nFinally, follow-through buying could see Nvidia shares revisit the $150 level. Investors who have bought during the recent selloff could look to lock in profits near a series of price points situated just below the stock\u2019s record high.\n\nThe comments, opinions, and analyses expressed on Investopedia are for informational purposes only. Read our warranty and liability disclaimer for more info.\n\nAs of the date this article was written, the author does not own any of the above securities.\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia\u2019s new RTX 5080 and 5090 graphics cards are sold out everywhere",
            "link": "https://www.polygon.com/news/518422/nvidia-rtx-geforce-5080-5090-graphics-cards-sold-out",
            "snippet": "Nvidia's new 50-series GPUs sold out within minutes at every major retailer. Some are calling the release a \u201cpaper launch\u201d that may take months to restock.",
            "score": 0.6586756110191345,
            "sentiment": null,
            "probability": null,
            "content": "In less than an hour after going on sale last week, Nvidia\u2019s latest GeForce RTX 5080 and 5090 graphics cards have all disappeared from physical and online retailers across the U.S. The inventory dried up so fast that it seems like Nvidia may not have had many to sell in the first place, which has prompted people to start calling it a \u201cpaper launch.\u201d\n\nBefore the launch last week, Nvidia\u2019s head of GeForce community Tim Adams warned everyone that the new GPUs might be hard to find: \u201cWe expect significant demand for the GeForce RTX 5090 and 5080 and believe stock-outs may happen.\u201d\n\nAnd they absolutely were, because both 50-series graphics cards sold out on Amazon, Micro Center, Best Buy, and Newegg as soon as they went live. Newegg said it \u201cexperienced an overwhelming response\u201d and sold its inventory \u201cwithin minutes,\u201d according to a news release last Friday.\n\nPC hardware YouTuber Gamers Nexus reported that one of the biggest U.S. system integrators, or companies that put together pre-built PCs, had a single RTX 5090 on launch day and only received a \u201csmall shipment\u201d of them later that same day. Another company had 20 cards that it sold via pre-builts in three minutes.\n\nA banner at the top of the Micro Center website says it\u2019s \u201cworking hard to restock more Nvidia RTX 50-series GPUs,\u201d and to check back regularly. But it might be a while before they\u2019re available again. U.K. retailer Overclockers UK doesn\u2019t expect to have the RTX 5080 again for two to six weeks, and the RTX 5090 for another three to 16 weeks. A Reddit user who managed to get an RTX 5090 at Best Buy isn\u2019t expected to get it until Feb. 21, which suggests US retailers are in the same boat.\n\nWhether or not Nvidia had any cards to sell in the first place remains unclear, but the demand is clearly high for the 50-series. According to the Steam hardware survey results for January 2025, most of its users play games on RTX 3060s, a GPU two generations behind. Modern games like Dragon Age: The Veilguard still recommend older GPUs, however upcoming games like Avowed and Monster Hunter: Wilds are starting to recommend 40-series cards to utilize ray-tracing. For all the people who felt the supply shortages and price inflation during the early years of COVID, this would\u2019ve been a great time to upgrade your graphics card\u2026 if there were any left to buy.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia stock falls as Trump's tariffs send shockwaves across the market",
            "link": "https://finance.yahoo.com/news/nvidia-stock-falls-as-trumps-tariffs-send-shockwaves-across-the-market-150548726.html",
            "snippet": "Nvidia stock dropped Monday, extending the prior week's declines as Trump's tariffs dragged down the market.",
            "score": 0.9490321278572083,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock dropped as much as 5% in early trading Monday, extending the prior week\u2019s declines as investors reacted to Trump\u2019s new tariffs.\n\nUS president Donald Trump on Saturday announced an additional tariff of 10% on imports from China and 25% on those from Mexico and Canada to the apparent surprise of investors, who had underpriced such risk. The tech-heavy Nasdaq (^IXIC) was off more than 2% Monday morning.\n\nNvidia stock pared losses and ended the day down 2.8%.\n\nRead more: The latest news and updates as Trump's tariffs are set to take effect\n\nNvidia\u2019s stock was already reeling from the news last week that the Trump administration was considering further tightening rules on exports of Nvidia chips to China. Bloomberg, citing unnamed sources, said the administration officials were in the early stages of discussions to expand restrictions on exports of Nvidia\u2019s H20 chips, a version of its Hopper AI chips designed specifically for China to comply with US export rules. Some 17% of Nvidia\u2019s 2024 sales came from China.\n\nThe report came after a new AI model released by Chinese firm DeepSeek called into question the mammoth spending from Big Tech on artificial intelligence infrastructure, sparking a massive sell-off in the tech sector. Nvidia fell 17% in a single day on the news, shaving $589 billion off the AI chipmaker's market cap \u2014 the largest single-day loss in stock market history.\n\nNvidia CEO Jensen Huang in Las Vegas, Nevada on Jan. 6, 2025, and US President Donald Trump in the Oval Office of the White House in Washington, D.C. on Jan. 31, 2025. (PATRICK T. FALLONMANDEL NGAN/AFP via Getty Images) \u00b7 PATRICK T. FALLONMANDEL NGAN via Getty Images\n\nWhile semiconductors aren\u2019t directly affected by the new tariffs, Bernstein analyst Stacy Rasgon wrote in a Monday research note that the duties would affect imports of data processing equipment, such as servers using AI chips. Higher prices of those products could reduce demand and have an indirect effect on chip sales. Rasgon pointed to the fact that the US imported $39 billion worth of data processing equipment, such as PCs and servers from China in 2023 and $28 billion from Mexico. Contract manufacturer Foxconn is building the world\u2019s largest factory for assembling servers with Nvidia's Blackwell AI chips in Mexico.\n\nThe tariff news over the weekend sent other chip stocks down, too. Nvidia rival Advanced Micro Devices (AMD) and Qualcomm (QCOM) dropped about 2%, while Micron (MU) and Broadcom (AVGO) sank nearly 3%.\n\nNvidia CEO Jensen Huang met with Trump at the White House last Friday. A source familiar with the matter told Reuters the two discussed DeepSeek.\n\nLaura Bratton is a reporter for Yahoo Finance. Follow her on Bluesky @laurabratton.bsky.social. Email her at laura.bratton@yahooinc.com.\n\nClick here for the latest stock market news and in-depth analysis, including events that move stocks\n\nRead the latest financial and business news from Yahoo Finance",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Intel Shows Why Nvidia Is Still Hard to Beat Despite DeepSeek News",
            "link": "https://www.wsj.com/tech/intel-shows-why-nvidia-is-still-hard-to-beat-92344c34",
            "snippet": "Nvidia's shares took a beating on the DeepSeek news, but Intel's struggles to break into the AI chip market keep growing.",
            "score": 0.7268812656402588,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "Using AI As A Coach In Your Career: Nvidia CEO Says It\u2019s A Must-Have",
            "link": "https://www.forbes.com/sites/chriswestfall/2025/02/03/using-ai-as-a-coach-in-your-career-nvidia-ceo-says-its-a-must-have/",
            "snippet": "Nvidia CEO Jensen Huang emphasizes the transformative potential of AI in learning and professional development. Here's why you need an AI coach and tutor,...",
            "score": 0.8514189124107361,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia\u2019s stock falls as these two questions dog investors",
            "link": "https://www.marketwatch.com/story/nvidias-stock-falls-as-these-two-questions-dog-investors-a659200a",
            "snippet": "Nvidia Corp.'s stock fell Monday as investors weighed the potential for tariffs to impact chip companies and continued to assess the fallout from DeepSeek.",
            "score": 0.9188013076782227,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Rare discounts hit NVIDIA\u2019s Shield Pro with Plex support and its TV stick counterpart starting from $130",
            "link": "https://9to5toys.com/2025/02/03/nvidia-shield-pro-and-tv-stick-from-130/",
            "snippet": "It's coming in today at $20 off, which lands it down within $10 off its all-time low, letting you step up your home theater game at the third-lowest price we...",
            "score": 0.8962898850440979,
            "sentiment": null,
            "probability": null,
            "content": "This offer has expired! Be sure to Be sure to follow us on Twitter for the latest deals and more. Sign-up for our newsletters and have our best offers delivered to your inbox daily.\n\nAmazon is offering the NVIDIA shield Pro Streaming Media Player for $179.99 shipped. Normally keeping to its full $200 price, discounts of more than a few bucks are rare to see hit this device, with the last one we saw dropping costs back in August. It\u2019s coming in today at $20 off, which lands it down within $10 off its all-time low, letting you step up your home theater game at the third-lowest price we have tracked.\n\nPacked with plenty of notable features, NVIDIA\u2019s Shield Android TV Pro comes as one of the more high-end streaming media players that we\u2019ve seen on the market, providing you with 4K content from all your favorite streaming services on top of doubling as a \u201cpowerful PC gaming rig.\u201d The Plex support here delivers hardware transcoding, letting you create a server by simply plugging in a USB hard drive or connecting to a NAS \u2013 with no additional equipment needed. There\u2019s also Alexa support, Dolby Vision HDR, as well as surround sound via Dolby Atmos and Dolby Digital Plus. Thanks to updates from last year, it also brings low-latency playback, night listening, and more to the Android 11 experience.\n\nIf you\u2019re looking to save a bit of money on a less-advanced version of the above model, Amazon is also offering the NVIDIA Shield TV Stick for $129.99 shipped, the second-lowest price that sits $5 above its all-time low from August (with only $5 discounts hitting it since then). As I mentioned, this is a less-advanced version of the Pro model above, but it still packs in much of the same features like 4K HDR playback, GeForce NOW PC gaming, voice controls, and more.\n\nNVIDIA Shield Pro Streaming Media Player features:\n\nThe Best of the Best. The world\u2019s most powerful Android TV streaming media player upgraded to Android TV version 11. Enhance HD video in real-time to 4K for clearer, crisper visuals using next-generation AI upscaling. 2x USB 3.0 ports for storage expansion, USB cameras, keyboards, controllers, and more. Plex Media Server built-in, 3 GB RAM, and 16 GB storage.Connectivity Technology : Bluetooth 5.0\n\nDolby Vision \u2013 Atmos. Bring your home theater to life with Dolby Vision HDR, and surround sound with Dolby Atmos and Dolby Digital Plus\u2014delivering ultra-vivid picture quality and immersive audio\n\n4K HDR Content. Get the most 4K content of any streaming media player. Watch Netflix, Amazon Video, Apple TV+, Disney+ and Google Play Movies & TV in crisp 4K HDR, and YouTube, Hulu, and more in 4K. Stream from your phone with built-in Chromecast 4K.\n\nGeForce NOW Cloud Gaming. GeForce NOW instantly transforms SHIELD TV into a powerful PC gaming rig. Play over 1000+ titles and nearly 100 of the biggest free to play games. The new GeForce NOW RTX 3080 membership unlocks GeForce RTX 3080 gaming servers in 4K HDR, the shortest wait times and longest session lengths, with RTX ON including ray tracing and DLSS graphics for supported games.\n\nVoice Control. The built-in Google Assistant is at your command. See photos, live camera feeds, weather, sports scores, and more on the big screen. Dim the lights and immerse yourself in your favorite show or music using only your voice. And control your SHIELD hands-free with Google Home or Alexa and Amazon Echo.\n\nFTC: 9to5Toys is reader supported, we may earn income on affiliate links\n\nSubscribe to the 9to5Toys YouTube Channel for all of the latest videos, reviews, and more!",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-02-02": {
        "0": {
            "title": "Why Nvidia Stock Plummeted This Week",
            "link": "https://www.fool.com/investing/2025/02/02/why-nvidia-stock-plummeted-this-week/",
            "snippet": "Nvidia stock saw big sell-offs to start the week due to potential risks posed by DeepSeek's R1 AI model. Nvidia's share price was also pressured by the outlook...",
            "score": 0.890588104724884,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) stock saw a big valuation pullback in this week's trading. The graphics processing unit (GPU) leader's share price fell 15.8% from its level at the previous week's market close, according to data from S&P Global Market Intelligence.\n\nNvidia was hit with sell-offs this week as information surrounding the new R1 artificial intelligence (AI) model from DeepSeek signaled a potential paradigm shift in AI training and inference. In addition to the possibility that new approaches to AI software could mean lower demand for Nvidia's GPUs, the company's stock also saw pullbacks in conjunction with geopolitical and macroeconomic pressures.\n\nThe market is weighing what DeepSeek means for Nvidia\n\nNvidia stock saw a massive pullback in Monday's trading, closing out the daily session down 17%. The sell-off worked out to a roughly $600 billion pullback in the GPU leader's market capitalization and marked the biggest-ever valuation pullback for a company on a pure-dollar basis.\n\nThe Monday sell-off for the stock was spurred by the market's reaction to a new AI model from DeepSeek -- a Chinese company. According to information from DeepSeek and other reports, its R1 model was matching or beating the performance of OpenAI's latest GPT model. At the same time, it reportedly took under $6 million to train -- far below the $100 million training figure for OpenAI's model. Strikingly, DeepSeek's model was said to be able to perform reasoning and inference applications with far lower processing and cooling requirements.\n\nNvidia's incredible valuation rise has been powered by the central role that its advanced GPUs play in training and running high-performance AI applications. If more efficient artificial intelligence models are able to deliver high levels of performance with lower processing needs, it could hurt demand for Nvidia's hardware.\n\nNvidia stock also fell due to geopolitical and macroeconomic factors\n\nIn addition to the immediate business-specific implications of DeepSeek's technology, Nvidia stock also lost ground in conjunction with broader geopolitical dynamics related to artificial intelligence. The R1 model highlighted the possibility that the U.S. is losing its lead over China in AI and the rising tensions between the two countries. Even if R1 winds up having a relatively minimal impact on Nvidia's business, the trajectory of relations between the U.S. and China could have a big impact on the company's valuation over the next five years.\n\nAdding more bearish catalysts, Nvidia stock was pushed lower by macroeconomic catalysts on two fronts. For starters, the Federal Reserve said at its meeting this week that it would be keeping the benchmark interest rate at its current level. The central banking authority also gave some cautious commentary about the outlook for rate cuts this year and indicated that it was waiting to see the impacts of new economic policies before making any moves. Investors then got another bearish development late in the week's trading when the Trump administration announced that it would be rolling out new tariffs on China, Mexico, and Canada.\n\nNvidia continues to have a strong lead in the GPU market, but it looks like the stock could continue to be volatile in the near term. The company is scheduled to release its fourth-quarter results on Feb. 26, and the report is poised to be an important performance catalyst for the AI leader's valuation and the stock market at large.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "Nvidia CEO Jensen Huang says everyone should get an AI tutor right away",
            "link": "https://fortune.com/2025/02/02/nvidia-ceo-jensen-huang-ai-tutors-future-of-work/",
            "snippet": "Nvidia CEO Jensen Huang has a personal tutor with him all the time, and he thinks AI educators are the future. With this tech-enabled intelligence,...",
            "score": 0.8802582025527954,
            "sentiment": null,
            "probability": null,
            "content": "\u00a9 2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information\n\nFORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Reminds Us That Stocks Gain Strength From Fundamentals, Not Fed",
            "link": "https://www.forbes.com/sites/johntamny/2025/02/02/nvidia-reminds-us-that-stocks-gain-strength-from-fundamentals-not-fed/",
            "snippet": "As is well known now, Nvidia shares tumbled something like 17% last Monday. Was this bad news from the Fed about its plans for interest rates?",
            "score": 0.6919026374816895,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Everyone\u2019s Rattled by the Rise of DeepSeek\u2014Except Nvidia, Which Enabled It",
            "link": "https://www.wsj.com/tech/ai/nvidia-jensen-huang-ai-china-deepseek-51217c40",
            "snippet": "Nvidia's stock swooned and regulators are restricting its chip sales, but the American AI giant sees a long game in China.",
            "score": 0.7064775228500366,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "NVIDIA Reflex 2 Only Works With RTX 50 GPUs At Launch, Older RTX Support Coming Later",
            "link": "https://wccftech.com/nvidia-reflex-2-works-with-rtx-50-gpus-at-launch-older-rtx-support-coming-later/",
            "snippet": "As per NVIDIA's confirmation, the Reflex 2 aka Reflex Frame Warp will only work with RTX 50 series GPUs for both desktop and laptops.",
            "score": 0.8980175256729126,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's latest Reflex 2 technology will only work with RTX 50 \"Blackwell\" GPUs at launch but support for older GPUs is planned.\n\nReflex 2 Frame Warp Is Limited To NVIDIA RTX 50 Series For Now, Extended Support For Older RTX Cards Later\n\nIn case you have missed the latest update on NVIDIA Reflex 2, NVIDIA announced its successor alongside the launch of GeForce RTX 50 series GPUs. The Reflex technology is known for reducing input latency and the NVIDIA Reflex 2 takes this to the next level by further adding another feature called \"Frame Warp\".\n\nWe will explain what it is all about soon, but what's crucial to know now is whether your system will support it or not.\n\nThe original video by NVIDIA didn't talk about the compatibility of the new feature, but the green team has released the information in its Game Works PDF (via @harukaze5719). As per the FAQ in the Reflex Frame Warp section, the technology will only be supported on the RTX 50 series at launch on both desktops and laptops, assuming you have an NVIDIA driver 570+.\n\nImage Source: NVIDIA FAQ\n\nNVIDIA did confirm that Reflex 2 will be coming to older RTX GPUs but through future updates, so we will have to wait for that.\n\nNVIDIA Reflex 2 is the company's latest input latency reduction tech that brings the core feature of Reflex 1 plus Frame Warp that updates the frame according to the mouse input. This is different from the original Reflex technology, which lets the CPU wait to generate the frames only when the GPU is ready to process them.\n\nImage Source: NVIDIA\n\nTraditionally, the frames get piled up for processing by the GPU in CPU-intensive titles such as CS, Valorant, Final, etc.\n\nThe Reflex improves the latency by simplifying the pipeline, but the Frame Warp in Reflex 2 works by sending the CPU-generated frame keeping the latest mouse position in consideration. The Reflex 2 then warps the frame sample from the CPU to the frame processed by the GPU for an updated camera position. This allows the frames to get updated according to the current mouse input and, with NVIDIA's Inpainting method, the final frame is predicted quickly without introducing much latency.\n\nAs per NVIDIA, the Reflex 2 is almost twice as fast as the Reflex in input and reduces the input latency by up to 75% compared to the native pipeline. Theoretically, the Reflex 2 shouldn't have any problems with previous RTX GPUs, but can only be enabled by NVIDIA.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "An Nvidia GeForce RTX 5090 with 96GB of GDDR7 memory? No, this is almost certainly the RTX 6000 Blackwell",
            "link": "https://www.techradar.com/pro/an-nvidia-geforce-rtx-5090-with-96gb-of-gddr7-memory-no-this-is-almost-certainly-the-rtx-6000-blackwell",
            "snippet": "A shipping manifest has detailed what looks like a professional workstation card; It could possibly be the successor to Nvidia's RTX 6000 Ada,...",
            "score": 0.9263405203819275,
            "sentiment": null,
            "probability": null,
            "content": "A shipping manifest has detailed what looks like a professional workstation card\n\nIt could possibly be the successor to Nvidia's RTX 6000 Ada, the most expensive graphics card in the world\n\nBased on the RTX5090, it is expected to have a whopping 96GB, twice that of it predecessor\n\nThe GeForce RTX 5090, the latest flagship graphics card for gamers and creatives in Nvidia's GeForce 50 series, was unveiled at CES 2025 and has just gone on sale - buts hortly before it did, rumors began to swirl of an RTX 5090 Ti model featuring a fully enabled GB202-200-A1 GPU and dual 12V-2\u00d76 power connectors, theoretically allowing for up to 1,200 watts of power.\n\nThis speculation began following the appearance of a prototype image on the Chinese industry forum Chiphell - reporting on the image, ComputerBase said, \u201cWith 24,576 shaders, the GB202-200-A1 GPU is said to offer 192 active streaming multiprocessors, which were previously rumored to be the full expansion of the GB202 chip. The memory is said to continue to offer 32GB capacity, but with 32Gbps instead of 28Gbps, it will exceed the 2TB/s mark.\u201d\n\nShortly after the engineering card surfaced online, ComputerBase alsospotted shipping documents on NBD Data listing a graphics card with 96GB of GDDR7 memory, marked as \u201cfor testing.\u201d It is a reasonable assumption that this unidentified model is actually a professional workstation card, potentially \u2013 let\u2019s say probably \u2013 the RTX 6000 Blackwell.\n\nUseful for AI applications\n\n(Image credit: NBD)\n\nThe GeForce RTX 5090 features 32GB of GDDR7, using sixteen 2GB modules connected through a 512-bit memory interface. 48GB would be possible if sixteen 3GB chips were used instead of 2GB chips.\n\nIf two of these 3GB chips were connected to each 32-bit controller, placing 16 chips on both the front and back of the graphics card in a \"clamshell\" configuration, the 96GB mentioned in the documents \u2013 which is twice as much as the RTX 6000 Ada, the most expensive graphics card in the world \u2013 would become a reality.\n\nThe shipping records indicate these GPUs use a 512-bit memory bus, reinforcing this theory. The internal PCB designation PG153, seen in the documents, aligns with known Nvidia Blackwell designs and has not yet appeared in any existing consumer graphics cards.\n\nNvidia is expected to introduce the RTX Blackwell series for workstations at its annual GPU Technology Conference (GTC 2025), so we should know more about them come March 2025. And yes, if you\u2019re thinking 96GB of GDDR7 memory is overkill for gaming or creative purposes I\u2019d agree with you. It is a good amount for AI tasks though, so we can expect to see Nvidia announce an AI version of the RTX 6000 Blackwell when it finally takes the wraps off its next-gen product.\n\nAre you a pro? Subscribe to our newsletter Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia Channel Chief Lists 2025 Priorities For Growing North American Partner Roster",
            "link": "https://www.crn.com/news/ai/2025/nvidia-channel-chief-lists-2025-priorities-for-growing-north-american-partner-roster",
            "snippet": "Named one of CRN's 50 Most Influential Channel Chiefs of 2025, Nvidia's Craig Weinstein asks his 500 North American partners to invest in its full stack of...",
            "score": 0.8205592632293701,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia Channel Chief Lists 2025 Priorities For Growing North American Partner Roster\n\nNamed one of CRN\u2019s 50 Most Influential Channel Chiefs of 2025, Nvidia\u2019s Craig Weinstein asks his 500 North American partners to invest in its full stack of solutions for generative AI development and says the roster will expand to support robotics and physical AI use cases.\n\nNvidia Americas Channel Chief Craig Weinstein said he expects the AI computing giant\u2019s growing roster of North American channel partners to invest in its full stack of solutions, including software, to build out generative AI and agentic AI applications.\n\nThese were among the top priorities Weinstein\u2014who was honored as one of CRN\u2019s 50 Most Influential Channel Chiefs of 2025 Monday\u2014listed for partners in his CRN Channel Chiefs profile, which said that Nvidia has 500 North American channel partners, up 42 percent from last year. Globally, Nvidia has 1,650 channel partners.\n\n[Related: Intel Cancels Falcon Shores AI Chip To Focus On \u2018Rack-Scale Solution\u2019]\n\nIn his profile, Weinstein, who is vice president of Nvidia\u2019s Americas Partner Organization, said he expects the number of channel partners Nvidia works with to increase over the next 12 months, particularly because of the growing investments the company is making in areas like robotics and physical AI.\n\n\u201cAs Nvidia expands into new markets like robotics and physical AI we'll see our ecosystem of partners evolve to engage with us in these opportunities. This is just one example of a market we'll work to develop and there will be many more and we'll build the ecosystem to support these as we invest and grow,\u201d he wrote.\n\nEighty percent of Nvidia\u2019s revenue comes from channel and alliance partners, according to Weinstein\u2019s profile. This is down from the 90 percent figure he gave last year for revenue that comes through the company\u2019s indirect channel and alliance relationships.\n\nWeinstein\u2019s 2025 Priorities For Nvidia Partners\n\nAs for the areas he would like to see Nvidia partners invest in, Weinstein first pointed to Nvidia\u2019s full stack of AI solutions, which range from GPUs, CPUs and data processing units, to servers, software and services.\n\nWith these technologies, Weinstein said partners should focus their efforts on \u201cdeveloping and implementing\u201d generative AI solutions as well as \u201cbuilding out AI infrastructure and related services.\u201d\n\nThe channel chief pointed to Nvidia\u2019s expanding software offerings, including Nvidia AI Enterprise, as another area partners should focus on, especially when it comes to creating agentic AI solutions.\n\nThe Most Innovative Initiative From Weinstein\u2019s Team Last Year\n\nWeinstein called Nvidia Blueprints, a significant driver of its agentic AI work with channel partners, the single most innovative initiative his team was responsible for last year.\n\nNvidia Blueprints consist of \u201cpretrained, customizable AI workflows designed to help enterprise developers build and deploy generative AI applications for specific use cases,\u201d according to Weinstein. The first channel partners and OEMs to support them include Accenture, Cisco, Dell Technologies, Deloitte, Hewlett Packard Enterprise, Lenovo, SoftServe and World Wide Technology.\n\n\u201cPartners are leveraging these blueprints to assist clients in various industries, including healthcare, manufacturing, telecommunications, financial services, and retail,\u201d he wrote.\n\n\u201cBy providing these blueprints, NVIDIA aims to equip partners and enterprises with the tools needed to rapidly develop and deploy customized AI applications, leveraging NVIDIA's advanced AI technologies and the expertise of our partner ecosystem,\u201d Weinstein added.\n\nWeinstein\u2019s Top Channel Goals, Nvidia\u2019s Top Partner Metrics\n\nWeinstein said his top channel goals in 2025 are to \u201cenable partners to develop an AI strategy\u201d and \u201csell AI solutions,\u201d \u201cimprove partner technical skills,\u201d and \u201cincrease the amount of professional services going through partners.\u201d\n\nThe top five metrics for which these partners are rewarded consist of their \u201cability to execute a land and expand sales strategy, their \u201cability to deliver business outcomes to customers,\u201d their ability to sell multiple products from Nvidia\u2019s portfolio, their professional services delivery capabilities and their vertical market focus, according to Weinstein.\n\nAmong the top 13 Americas partners Nvidia honored last year, there was a focus on various industries, such as financial services, health care, higher education and public sector. This focus on industry expertise with partners was among the top three accomplishments of Weinstein\u2019s organization last year, according to the channel chief.\n\n\u201cThese partners are building world class capabilities to serve the AI use cases and opportunities with the most important brands globally. We've won some of the largest enterprise AI contacts in history with our Nvidia partner ecosystem,\u201d Weinstein wrote.\n\nNvidia Plans To Increase MDF In 2025 With Industry Focus\n\nWith plans to increase market development funds this year, Weinstein said Nvidia \u201ctailors its marketing and investment strategies to specific industries like financial services, healthcare, and higher education.\u201d\n\nThe channel chief added that Nvidia\u2019s MDF strategy recognizes and rewards \u201cwho drive AI adoption and innovation across various industries.\u201d\n\nWeinstein also noted that Nvidia invests in building a comprehensive ecosystem around [its] technologies\u201d and makes corporate investments in companies that enhance its platform and expands its ecosystem\u201d with respect to its MDF profile.\n\nThirty percent of Nvidia\u2019s overall marketing budget is dedicated to channel marketing, according to Weinstein\u2019s profile. This is up from the 10 percent figure he provided to CRN last year.\n\n\u201cI love helping partners. Full stop. It's that simple. I truly believe we can help partners build capabilities that will build the foundation of their businesses for decades to come,\u201d Weinstein wrote, explaining his favorite thing about working in the channel.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Lockheed Martin\u2019s Pension Bought Nvidia, Apple, Broadcom Stock. It Sold Walmart.",
            "link": "https://www.barrons.com/articles/nvidia-stock-apple-broadcom-walmart-65198c0e",
            "snippet": "Lockheed Martin's pension materially increased stakes in Nvidia, Apple, and Broadcom stock, while slashing a position in Walmart.",
            "score": 0.9129519462585449,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "DeepSeek might not be as disruptive as claimed, firm reportedly has 50,000 Nvidia GPUs and spent $1.6 billion on buildouts",
            "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/deepseek-might-not-be-as-disruptive-as-claimed-firm-reportedly-has-50-000-nvidia-gpus-and-spent-usd1-6-billion-on-buildouts",
            "snippet": "SemiAnalysis says that DeepSeek and its venture capitalist partner spent some $1.6 billion dollar on AI hardware from Nvidia, says SemiAnalysis.",
            "score": 0.8427393436431885,
            "sentiment": null,
            "probability": null,
            "content": "Chinese startup DeepSeek recently took center stage in the tech world with its startlingly low usage of compute resources for its advanced AI model called R1, a model that is believed to be competitive with Open AI's o1 despite the company's claims that DeepSeek only cost $6 million and 2,048 GPUs to train. However, industry analyst firm SemiAnalysis reports that the company behind DeepSeek incurred $1.6 billion in hardware costs and has a fleet of 50,000 Nvidia Hopper GPUs, a finding that undermines the idea that DeepSeek reinvented AI training and inference with dramatically lower investments than the leaders of the AI industry.\n\nDeepSeek operates an extensive computing infrastructure with approximately 50,000 Hopper GPUs, the report claims. This includes 10,000 H800s and 10,000 H100s, with additional purchases of H20 units, according to SemiAnalysis. These resources are distributed across multiple locations and serve purposes such as AI training, research, and financial modeling. The company's total capital investment in servers is around $1.6 billion, with an estimated $944 million spent on operating costs, according to SemiAnalysis.\n\nDeepSeek took the attention of the AI world by storm when it disclosed the minuscule hardware requirements of its DeepSeek-V3 Mixture-of-Experts (MoE) AI model that are vastly lower when compared to those of U.S.-based models. Then DeepSeek shook the high-tech world with an Open AI-competitive R1 AI model. However, the reputable market intelligence company SemiAnalysis revealed its findings that indicate the company has some $1.6 billion worth of hardware investments.\n\nDeepSeek originates from High-Flyer, a Chinese hedge fund that adopted AI early and heavily invested in GPUs. In 2023, High-Flyer launched DeepSeek as a separate venture solely focused on AI. Unlike many competitors, DeepSeek remains self-funded, giving it flexibility and speed in decision-making. Despite claims that it is a minor offshoot, the company has invested over $500 million into its technology, according to SemiAnalysis.\n\nA major differentiator for DeepSeek is its ability to run its own data centers, unlike most other AI startups that rely on external cloud providers. This independence allows for full control over experiments and AI model optimizations. In addition, it enables rapid iteration without external bottlenecks, making DeepSeek highly efficient compared to traditional players in the industry.\n\nThen there is something that one would not expect from a Chinese company: talent acquisition from mainland China, with no poaching from Taiwan or the U.S. DeepSeek exclusively hires from within China, focusing on skills and problem-solving abilities rather than formal credentials, according to SemiAnalysis. Recruitment efforts target institutions like Peking University and Zhejiang University, offering highly competitive salaries. According to the research, some AI researchers at DeepSeek earn over $1.3 million, exceeding compensation at other leading Chinese AI firms such as Moonshot.\n\nDue to the talent inflow, DeepSeek has pioneered innovations like Multi-Head Latent Attention (MLA), which required months of development and substantial GPU usage, SemiAnalysis reports. DeepSeek emphasizes efficiency and algorithmic improvements over brute-force scaling, reshaping expectations around AI model development. This approach has, for many reasons, led some to believe that rapid advancements may reduce the demand for high-end GPUs, impacting companies like Nvidia.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nA recent claim that DeepSeek trained its latest model for just $6 million has fueled much of the hype. However, this figure refers only to a portion of the total training cost\u2014 specifically, the GPU time required for pre-training. It does not account for research, model refinement, data processing, or overall infrastructure expenses. In reality, DeepSeek has spent well over $500 million on AI development since its inception. Unlike larger firms burdened by bureaucracy, DeepSeek\u2019s lean structure enables it to push forward aggressively in AI innovation, SemiAnalysis believes.\n\nDeepSeek's rise underscores how a well-funded, independent AI company can challenge industry leaders. However, the public discourse might have been driven by hype. Reality is more complex: SemiAnalysis contends that DeepSeek\u2019s success is built on strategic investments of billions of dollars, technical breakthroughs, and a competitive workforce. What it means is that there are no wonders. As Elon Musk noted a year or so ago, if you want to be competitive in AI, you have to spend billions per year, which is reportedly in the range of what was spent.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "\u2018Time to Double Down,\u2019 Says Investor About Nvidia Stock",
            "link": "https://markets.businessinsider.com/news/stocks/time-to-double-down-says-investor-about-nvidia-stock-1034299329",
            "snippet": "Nvidia (NASDAQ:NVDA) stock has been on a rough ride over the past week, plunging 16% after China's DeepSeek unveiled its R1 AI model. This open-...",
            "score": 0.6935222744941711,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NASDAQ:NVDA) stock has been on a rough ride over the past week, plunging 16% after China\u2019s DeepSeek unveiled its R1 AI model. This open-source language model reportedly performs tasks similar to advanced models like OpenAI\u2019s but was developed at a fraction of the cost.\n\nMaximize Your Portfolio with Data Driven Insights: Leverage the power of TipRanks' Smart Score a data driven tool to help you uncover top performing stocks and make informed investment decisions.\n\nMonitor your stock picks and compare them to top Wall Street Analysts' recommendations with Your Smart Portfolio\n\nAt first glance, this could spell trouble for Nvidia. The AI chip leader relies heavily on selling high-performance GPUs to cloud providers that support AI startups. However, as these startups struggle to stay afloat \u2013 OpenAI reportedly lost $5 billion in 2024 \u2013 the demand for Nvidia\u2019s expensive chips might not remain as strong as before.\n\nEven so, one investor, known by the pseudonym Cash Flow Venue, believes this sell-off presents an opportunity. He outlines three key reasons why buying the dip could be a savvy move.\n\nFirst, CFV pushes back on the claim that DeepSeek trained its AI model for just ~$6 million, calling it \u201cstrongly misleading.\u201d The investor argues that training is an ongoing process involving repeated experiments and significant costs beyond initial GPU usage. DeepSeek benefits from its parent company, High-Flyer Quant, which made substantial investments in Nvidia GPUs before U.S. export restrictions. Reports suggest DeepSeek has access to ~50,000 Nvidia Hopper GPUs, with CAPEX for AI hardware exceeding $1.6 billion. CFV emphasizes that additional expenses \u2013 such as R&D, data collection, and operational costs \u2013 make the actual investment far greater than the reported $6 million.\n\nFurthermore, CFV believes that Nvidia\u2019s main clients, including Microsoft and Meta, remain supply-constrained and are significantly increasing CAPEX to expand AI capabilities. Microsoft has doubled its data center capacity over the past three years but still needs more infrastructure to support Azure and AI growth. Meanwhile, Meta raised its 2025 CAPEX forecast to $60-$65B to fund AI and core business investments.\n\n\u201cI believe NVIDIA is set to continue to benefit from such a high demand and investors have little to worry about in the near-term regarding the business pipeline and fundamentals. It\u2019s worth remembering that both Hopper and Blackwell systems are associated with certain supply constraints and are likely to exceed the demand in upcoming quarters,\u201d CFV opined.\n\nBeyond near-term demand, the broader AI market remains a powerful tailwind for Nvidia. According to the investor, AI adoption is projected to grow at a 29.1% CAGR, reaching over $530.6 billion by 2030. As AI spreads across industries, demand for high-performance computing is expected to remain strong, even with the rise of cost-efficient models. Geopolitical tensions surrounding AI infrastructure and data protection further cement Nvidia\u2019s position as the dominant supplier in the space.\n\nSumming up his bullish stance, CFV stated: \u201cAdd the opportunistic (by the last couple of years standards) valuation and a solid product roadmap for the upcoming years to the equation and I arrived at a \u2018strong buy\u2019 rating for NVIDIA. I am still bullish, despite recent market turmoil.\u201d\n\nThat\u2019s just one bullish take, and plenty more are floating around Wall Street. NVDA stock has a Strong Buy consensus, backed by 37 Buys and just 3 Holds. With an average price target of $178.63, analysts are eyeing a potential 48% surge in the next year. (See NVDA stock forecast)\n\nTo find good ideas for stocks trading at attractive valuations, visit TipRanks\u2019 Best Stocks to Buy, a tool that unites all of TipRanks\u2019 equity insights.\n\nDisclaimer: The opinions expressed in this article are solely those of the featured investor. The content is intended to be used for informational purposes only. It is very important to do your own analysis before making any investment.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-02-01": {
        "0": {
            "title": "Use Case : Transforming Europe\u2019s AI Landscape",
            "link": "https://www.nvidia.com/en-eu/case-studies/transforming-europe-ai-landscape/",
            "snippet": "Scaleway, a leading cloud services provider in Europe, is leveraging NVIDIA DGX to power its AI infrastructure, enabling European businesses to develop...",
            "score": 0.72906494140625,
            "sentiment": null,
            "probability": null,
            "content": "AI has ushered in a new way of building technology, pushing the boundaries of what's possible and making advanced tools available to everyone.\n\nWith the increasing demand for large-scale AI models, Scaleway has emerged as a regional alternative to global cloud competitors, offering secure, scalable, and energy-efficient AI services. The company built the Nabuchodonosor 2023 AI supercomputer\u2014known as Nabu\u2014in just two months. Powered by NVIDIA DGX, Nabu gives European companies the computing power they need to train large language models (LLMs) and other transformer models, making Scaleway the first to offer this within the European market.\n\nScaleway\u2019s commitment to data sovereignty is a key differentiator. Its AI infrastructure complies with stringent data protection laws, like the EU's General Data Protection Regulation (GDPR), ensuring that European businesses can securely manage their data without compromising privacy or control.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "What Does Chinese AI Start-Up DeepSeek Mean for Nvidia Stock?",
            "link": "https://www.fool.com/investing/2025/02/01/what-does-chinese-ai-startup-deepseek-mean-for-nvi/",
            "snippet": "DeepSeek, a Chinese artificial intelligence (AI) start-up, is sending shock waves through the U.S. tech sector by demonstrating its latest AI assistant,...",
            "score": 0.9250741004943848,
            "sentiment": null,
            "probability": null,
            "content": "DeepSeek, a Chinese artificial intelligence (AI) start-up, is sending shock waves through the U.S. tech sector by demonstrating its latest AI assistant, which achieves performance comparable to or even surpassing some of the world's best chatbots.\n\nThe kicker here is that DeepSeek reportedly accomplished this using significantly less computational power, relying on fewer and less advanced AI chips -- particularly Nvidia's (NVDA 5.27%) cutting-edge GPUs, which are typically considered essential for such tasks.\n\nThis development suggests a potentially disruptive method for developing large language models (LLMs), offering a more efficient alternative to current strategies. It raises questions about future infrastructure investments and the demand for AI chips.\n\nThe market's reaction has been swift: As of this wriging, Nvidia's stock has plunged more than 22% from its recent peak, with similar declines in other leading AI tech stocks.\n\nLet's explore what DeepSeek's emergence could mean for Nvidia and how it might affect investors' portfolios.\n\nThe innovation behind DeepSeek's success\n\nArtificial intelligence is not only a major theme in technology but also a national security issue, given its pivotal role in areas like data analysis, intelligence, and military applications. This has led the United States to impose bans or severe restrictions on exporting high-end technologies, such as specialized AI semiconductors, to China since 2023, aiming to slow the country's technological progress and safeguard crucial supply chains.\n\nDespite these constraints, DeepSeek managed to develop AI models like DeepSeek-V3 and DeepSeek-R1 with cutting-edge capabilities on a reported training budget of around $6 million. Its white paper outlines an innovative approach that combines software ingenuity with new training techniques to maximize the potential of older Nvidia GPUs, surpassing their initial capabilities.\n\nDeepSeek's models have shown impressive results next to market-leading alternatives from OpenAI, Alphabet, and Meta Platforms in benchmarks for problem-solving, mathematical reasoning, coding, and general knowledge. The DeepSeek AI Assistant rapidly ascended to become the most-downloaded free application on the Apple App Store in the United States, demonstrating significant market traction and consumer acceptance.\n\nDeepSeek's disruptive influence is further underscored by its open-source platform, which makes the code publicly accessible, allowing businesses and developers to customize AI models without incurring the high costs of proprietary systems. For those seeking to integrate DeepSeek's most advanced models into existing tech infrastructure via an application programming interface (api), the company reportedly offers commercial pricing that is significantly lower than that of its competitors.\n\nImplications for Nvidia\n\nDeepSeek's emergence has multiple implications for the broader tech sector. Given the initial sell-off, the market has interpreted this development as decisively negative, likely based on a concern that novel AI development methods could reduce the demand for cutting-edge, expensive hardware and cut into the competitive moat of established leaders.\n\nThat being said, the impact on Nvidia may be more nuanced as it looks like the next-generation AI chips, including Nvidia's Blackwell GPU architecture, are essential for pushing the boundaries of high-performance computing.\n\nIf DeepSeek managed to break AI performance benchmarks with legacy hardware, its methods toward computational efficiency implemented by other tech players could make Nvidia's high-end products incrementally even more powerful. This scenario could help expand the AI market by opening up new uses that may become cost-effective while accelerating the timetable toward future breakthroughs like artificial general intelligence (AGI) applications as a demand driver for Nvidia.\n\nOn the other hand, it is also clear that DeepSeek has introduced a new layer of complexity regarding Nvidia's growth runway. According to Wall Street analysts monitored by Yahoo! Finance, the chipmaker is expected to reach $197 billion in revenue in fiscal 2026, which ends in January 2026, driving a 51% increase in earnings per share (EPS) to $4.45. Any sign that customers are pivoting away from large orders, even at the margin, would undermine this earnings outlook and further pressure the stock.\n\nThe company's upcoming fiscal 2025 fourth-quarter and full-year earnings report, set to be released on Feb. 26, will provide Nvidia CEO Jensen Huang and hist eam a chance to address whether DeepSeek is affecting the business and reassure shareholders.\n\nMetric FY 2025 Analyst Estimate FY 2026 Analyst Estimate Revenue $129.2 billion $196.5 billion Revenue growth (YOY) 112% 52% EPS $2.95 $4.45 EPS growth (YOY) 127% 51%\n\nThe big picture for investors\n\nDeepSeek appears to have signaled a new shift toward efficiency in AI software, highlighting the rapidly changing landscape.\n\nRecognizing near-term uncertainties with an expectation for volatility to continue, I believe investors should stay the course with Nvidia as an AI leader with a technological edge in hardware. Unless there is evidence that its financials are under pressure, a positive long-term outlook should support the stock, which remains well positioned to rebound.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia CEO Jensen Huang visits White House to talk 'AI leadership' with Trump",
            "link": "https://www.datacenterdynamics.com/en/news/nvidia-ceo-jensen-huang-visits-white-house-to-talk-ai-leadership-with-trump/",
            "snippet": "The CEO of GPU designer Nvidia visited the White House to discuss AI leadership with the President of the United States. Jensen Huang met Donald Trump in a...",
            "score": 0.9009906053543091,
            "sentiment": null,
            "probability": null,
            "content": "The CEO of GPU designer Nvidia visited the White House to discuss AI leadership with the President of the United States.\n\n\"We appreciated the opportunity to meet with President Trump and discuss semiconductors and AI policy,\" Nvidia said in a statement. \"Jensen and the president discussed the importance of strengthening US technology and AI leadership.\u201d\n\nThis was the first time the two billionaires have met since Trump's second term began.\n\nHuang was notably absent from Trump's inauguration, a star-studded affair in tech circles - with Amazon founder Jeff Bezos, Google CEO Sundar Pichai, Apple CEO Tim Cook, Meta CEO Mark Zuckerberg, and OpenAI CEO Sam Altman in attendance, after donating to the event. Elon Musk, meanwhile, has been given a job in the White House.\n\nOn the same day, Huang visited Nvidia's offices in China.\n\nDiscussion points could be wide-ranging. Earlier this month, Trump reiterated long-threatened plans to impose tariffs on foreign-made semiconductors, particularly targeting Taiwan.\n\nNvidia's GPUs are made by TSMC in Taiwan, and no factories capable of developing its latest hardware exist outside of the country. Even with TSMC building fabs in the US, manufacturing GPUs at scale in the US could take years or decades. The CHIPS Act, which had hoped to spur domestic manufacturing, is now in disarray, and may or may not be revived.\n\nAt the same time as restricting what comes into the US, Trump may also impact Nvidia's exports. Biden-era diffusion rules on which countries can import high-end GPUs have yet to come into force, and Nvidia and AMD had hoped to lobby to kill the effort that could restrict where they can sell chips.\n\nBut the impact of DeepSeek on global markets - where a purported cheaper model, potentially built with smuggled GPUs caused trillions to evaporate from valuations - has led many powerful AI figures to call for even tighter export controls and an investigation into how Nvidia chips may be ending up in China.\n\nBack in 2018, prior to ever-tightening export controls under both Trump and Biden, Huang told DCD: \u201cIf you just think about Nvidia, China is one-third of Nvidia\u2019s business. However, remember, everything we sell to China goes through our distribution partners, gets integrated into computers, it gets integrated into data centers and clouds. The impact to them, to their local market of the local companies is also significant.\n\n\u201cWe have a few thousand employees in China, and so to think about today\u2019s companies as either one country or another country, I don\u2019t think really makes sense.\n\n\u201cWe sell to China, we have partners in China, China\u2019s ecosystem and economy depends on our technology. We have a lot of great employees in China, they contribute to the creation of products here.\"\n\nAccording to Nvidia's latest earnings report, since those comments were made revenue from China has steadily declined to 15 percent, although overall revenues have skyrocketed.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Edward Snowden slams Nvidia's RTX 50-series 'F-tier value,' whistleblows on lackluster VRAM capacity",
            "link": "https://www.tomshardware.com/pc-components/gpus/edward-snowden-slams-nvidias-rtx-50-series-f-tier-value-whistleblows-on-lackluster-vram-capacity",
            "snippet": "Infamous former NSA contractor and whistleblower Edward Snowden has unexpectedly shared his opinion on RTX 50 series VRAM quotas.",
            "score": 0.933854341506958,
            "sentiment": null,
            "probability": null,
            "content": "An infamous former U.S. National Security Agency (NSA) contractor and whistleblower has unexpectedly shared his opinion on the state of the graphics card market. Naturalized Russian citizen Edward Snowden has called the recently released Nvidia GeForce RTX 5080 \u201ca monopolistic crime against the consumer\u201d due to its \u201ccrippling 16GB.\u201d Imagine his ire if he was considering the upcoming RTX 50 laptop family...\n\nEndless next-quarter thinking has reduced the Nvidia brand to \"F-tier value for S-tier prices\". 5070 should have had 16GB VRAM minimum, 5080 w 24/32 SKUs, 5090 32/48/+. Releasing a $1,000+ GPU in 2025 with a crippling 16GB is a monopolistic crime against the consumer.January 31, 2025\n\nYou can see the full comments if you expand the tweet above. Snowden is brutally clear in sharing his disappointment with the RTX 50-series (Blackwell) family VRAM quotas. However, his opinion isn\u2019t dissimilar to that of Tom\u2019s Hardware GPU editor. In our review of the Nvidia GeForce RTX 5080 Founders Edition earlier in the week, we also grumbled about the VRAM gap between the RTX 5090 (32GB) and the RTX 5080 (16GB, the same amount as the RTX 4080/S).\n\nWe also noticed that some modern games like Indiana Jones and the Great Circle, running 4K are already constrained by 16GB. So, we wrote that \u201c24GB would have been far better for a $1,000 (or more) graphics card,\u201d resulting in the RTX 5080 being an underwhelming second-tier offering from the new Blackwell series.\n\nThings slide into even more desperate VRAM constraints lower the RTX 50-series GPU stack. In contrast, the RTX 5070 Ti with 16GB GDDR7 on a 256-bit bus (basically the same memory config as the RTX 5080) might find some favor among consumers; the \u201870\u2019 card this generation comes packing 12GB VRAM, just like last gen. Thus, it isn\u2019t surprising that Snowden also called out the RTX 5070, asserting that it \u201cshould have had 16GB VRAM minimum.\u201d\n\nIt is easy to agree with the criticism of the Green Team\u2019s stingy VRAM specifications for its new Blackwell series of consumer graphics cards. In February 2021, the chipmaker launched the popular RTX 3060 with 12GB as standard and lowered the RTX 3050 to 8GB. The consumer Blackwell series offering the RTX 5070 with 12GB and its expected RTX 5060 SKUs with as little as 8GB seems retrograde in 2025. As mentioned in the intro, laptop buyers are even less fortunate, particularly the RTX 5070 and 5060 laptop GPUs with 8GB VRAM, which appear to be hobbled at birth in the VRAM stakes.\n\nAMD has yet to launch the Radeon RX 9700 XT, but leaked specifications have pegged the RDNA 4 GPU and the regular Radeon RX 9700 with 16GB of VRAM. Of course, the kind of performance the pair of RDNA 4 GPUs will bring remains to be seen. AMD's next-gen GPUs will reportedly hit the market in late March.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia giving away free AI courses worth up to $90 and no, it's got nothing to do with DeepSeek's ascension",
            "link": "https://www.techradar.com/pro/nvidia-giving-away-free-ai-courses-worth-up-to-usd90-and-no-it-has-absolutely-nothing-to-do-with-deepseeks-ascension",
            "snippet": "Join Nvidia's free Developer Program and unlock 19 self-paced courses.",
            "score": 0.932944118976593,
            "sentiment": null,
            "probability": null,
            "content": "19 self-paced courses are available, lasting up to eight hours are available free of charge\n\nYou will need to join Nvidia's free developer program to access them\n\nMy personal preference would be Prompt Engineering with Llama 2\n\nNvidia has announced it will be offering free access to 19 self-paced technical courses each worth up to $90.\n\nThe initiative is part of the Nvidia Developer Program, which aims to empower developers and tech enthusiasts with cutting-edge knowledge in artificial intelligence and data science.\n\nThe courses cover a wide range of topics in five categories including Generative AI and Large Language Models (LLMs), Graphics and Simulation, Accelerated Computing, Data Science, and Deep Learning.\n\nWhat's on offer?\n\nEach course is designed to be self-paced and lasts between two to eight hours. For instance, the Generative AI and LLMs category includes the highly sought-after \"Prompt Engineering with Llama 2\" course, which teaches users how to interact with and optimize large language models effectively.\n\nMeanwhile, the Deep Learning category offers eight courses, making it the most extensive section of the program.\n\nTo take advantage of this opportunity, you\u2019ll need to join Nvidia\u2019s free Developer Program. Once registered, you\u2019ll gain access to the full catalog of courses, allowing you to learn at your own pace.\n\nThe announcement coincides with the rise of DeepSeek R1, a Chinese AI model making headlines for its impressive capabilities, low training costs, and its ability to be run locally with performance, rivalling OpenAI\u2019s own ChatGPT.\n\nAre you a pro? Subscribe to our newsletter Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nDeepSeek recently caused turmoil in the stock market with Nvidia suffering a record-breaking $600 billion share price drop, the largest single-day drop by any company in U.S. history.\n\nWhile DeepSeek R1 was reportedly trained by the company using Nvidia\u2019s H800 GPU, it relies on Huawei\u2019s Ascend 910C GPU for inference, reducing its dependence on American technology.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA GeForce RTX 5060/5060 Ti expected for March 2025 release by Colorful's main supplier",
            "link": "https://videocardz.com/newz/nvidia-geforce-rtx-5060-5060-ti-expected-for-march-2025-release-by-colorfuls-main-supplier",
            "snippet": "Chaintech has released a slide that appears to confirm the launch of the GeForce RTX 5060 Ti and RTX 5060 in March.",
            "score": 0.9487122893333435,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Win an Nvidia GeForce RTX 5080 Founders Edition graphics card",
            "link": "https://www.club386.com/win-an-nvidia-geforce-rtx-5080/",
            "snippet": "We've partnered with the team at Nvidia to give you the chance to win a brand-new, state-of-the-art GeForce RTX 5080 Founders Edition graphics card worth \u00a3979!",
            "score": 0.8339431881904602,
            "sentiment": null,
            "probability": null,
            "content": "Club386 is not only the home of definitive tech reviews and analysis, we also do our utmost to give back to the community with incredible monthly giveaways. And they don\u2019t get much more incredible than this.\n\nFor the month of February, we\u2019ve partnered with the team at Nvidia to give you the chance to win a brand-new, state-of-the-art GeForce RTX 5080 Founders Edition graphics card worth \u00a3979!\n\nWhether you need the firepower for your favourite games of today, or want to future proof with mind-boggling frame generation, this is a card that gets the job done. It also happens to look absolutely incredible inside a rig.\n\nEnter for free using the above form, and don\u2019t forget to bookmark our competitions page to see what future months have in store. Thank you for taking part and good luck!\n\nEligibility: competition open worldwide.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia does damage control, Microsoft slips, and DeepSeek's big week: Tech news roundup",
            "link": "https://qz.com/nvidia-microsoft-deepseek-stocks-markets-ai-1851752800",
            "snippet": "Nvidia does damage control, Microsoft slips, and DeepSeek's big week: Tech news roundup. Plus, DeepSeek is the 'Temu of AI,' and Big Tech's DeepSeek problem.",
            "score": 0.4979955852031708,
            "sentiment": null,
            "probability": null,
            "content": "Big Tech\u2019s multi-billion dollar spending on artificial intelligence will be under investor scrutiny this week \u2014 even more so after China\u2019s DeepSeek sent shockwaves through Wall Street and Silicon Valley with a cheap yet competitive AI model.\n\nAdvertisement\n\nRead More",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Can Meta\u2019s Massive Manhattan-Sized Data Center Take Nvidia Stock to New Record Highs?",
            "link": "https://www.barchart.com/story/news/30719239/can-metas-massive-manhattan-sized-data-center-take-nvidia-stock-to-new-record-highs",
            "snippet": "With Meta's massive AI-driven spending spree, could Nvidia's dominance in GPUs propel its stock to record highs?",
            "score": 0.7352154850959778,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NSA whistleblower Edward Snowden slams Nvidia RTX 5080 as 'a monopolistic crime against the consumer'",
            "link": "https://www.pcgamer.com/hardware/nsa-whistleblower-edward-snowden-slams-nvidia-rtx-5080-as-a-monopolistic-crime-against-the-consumer/",
            "snippet": "Whistleblower Edward Snowden, who in 2013 leaked classified documents to the press revealing the existence of an extensive US digital surveillance regime,...",
            "score": 0.9047664999961853,
            "sentiment": null,
            "probability": null,
            "content": "Whistleblower Edward Snowden, who in 2013 leaked classified documents to the press revealing the existence of an extensive US digital surveillance regime, has come out as a surprisingly scathing critic of Nvidia's new RTX 50-series of graphics cards.\n\n\"Endless next-quarter thinking has reduced the Nvidia brand to 'F-tier value for S-tier prices,'\" Snowden wrote on Twitter. \"5070 should have had 16 VRAM minimum, 5080 24/32 SKUs, 5090 32/48/+. Releasing a $1,000 GPU in 2025 with a crippling 16 GB is a monopolistic crime against the consumer.\"\n\n(Image credit: Edward Snowden via X)\n\nAfter leaking documents that revealed the extent of the US National Security Agency's regime of global digital surveillance to The Intercept, Snowden avoided prosecution in the US by fleeing to Moscow, where he lives with his family as a naturalized Russian citizen. Snowden has continued to write and speak about surveillance, cybersecurity, computing, and politics in the years since.\n\nAnd I was nodding to myself saying \"Yeah Edward Snowden, you're onto something with this one.\" For clarity, the $1,000 GPU with 16 gigs of VRAM in question is the RTX 5080, which left PCG hardware honcho Dave James nonplussed in his review, calling it a \"strangely unexciting\" card with minimal gen-on-gen performance uplift without the admittedly impressive DLSS multi frame gen technology. The RTX 5090 fared a little better, with Dave praising both its standard and AI-augmented performance despite a $400 price increase over the 4090.\n\nThe upcoming RTX 5070 is promising an impressive RTX 4090* level of performance (with multi frame gen enabled) at $550, but that 12 GB of VRAM is concerning: In a recent video, members of Digital Foundry discussed how it should be an acceptable amount for 1440p gaming right now, but what about down the line? My RTX 3070's once-voluminous 8 gigs now gets slurped up immediately by any triple-A game at 1440p, and I find myself eyeballing the $750, 16 GB 5070 Ti as a potential upgrade instead.\n\nNvidia's market dominance and AI business have left it pretty much untouchable, even as we seem to be getting another mixed bag graphics card generation after the unexciting 40-series\u2060\u2014that dominance may not be as iron-clad as it once seemed, though. The RTX 5080 and 5090 sold out immediately at UK retailers, but it's unclear how much that was driven by demand versus extremely limited stock, leading Gamers Nexus to deem it a paper launch. Meanwhile, the potential OpenAI killer out of China, DeepSeek, managed to slash more than $600 billion from Nvidia's valuation with the announcement of its new R1 AI model that demands significantly less GPU compute for a similar result to market leaders.\n\nWhat would really change things for gamers would be some consistent competition from AMD and/or Intel. At the moment, that doesn't seem super likely: The upcoming RX 9070 has so far only left us whelmed at best with its leaked benchmarks, and Intel's upcoming Battlemage cards look to be moving in the right direction, but aren't world beaters quite yet.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-01-31": {
        "0": {
            "title": "Accelerate DeepSeek Reasoning Models With NVIDIA GeForce RTX 50 Series AI PCs",
            "link": "https://blogs.nvidia.com/blog/deepseek-r1-rtx-ai-pc/",
            "snippet": "With up to 3352 TOPS of AI horsepower, NVIDIA GeForce RTX 50 Series GPUs can run the DeepSeek family of distilled models faster than anything on the PC...",
            "score": 0.8117155432701111,
            "sentiment": null,
            "probability": null,
            "content": "The recently released DeepSeek-R1 model family has brought a new wave of excitement to the AI community, allowing enthusiasts and developers to run state-of-the-art reasoning models with problem-solving, math and code capabilities, all from the privacy of local PCs.\n\nWith up to 3,352 trillion operations per second of AI horsepower, NVIDIA GeForce RTX 50 Series GPUs can run the DeepSeek family of distilled models faster than anything on the PC market.\n\nA New Class of Models That Reason\n\nReasoning models are a new class of large language models (LLMs) that spend more time on \u201cthinking\u201d and \u201creflecting\u201d to work through complex problems, while describing the steps required to solve a task.\n\nThe fundamental principle is that any problem can be solved with deep thought, reasoning and time, just like how humans tackle problems. By spending more time \u2014 and thus compute \u2014 on a problem, the LLM can yield better results. This phenomenon is known as test-time scaling, where a model dynamically allocates compute resources during inference to reason through problems.\n\nReasoning models can enhance user experiences on PCs by deeply understanding a user\u2019s needs, taking actions on their behalf and allowing them to provide feedback on the model\u2019s thought process \u2014 unlocking agentic workflows for solving complex, multi-step tasks such as analyzing market research, performing complicated math problems, debugging code and more.\n\nThe DeepSeek Difference\n\nThe DeepSeek-R1 family of distilled models is based on a large 671-billion-parameter mixture-of-experts (MoE) model. MoE models consist of multiple smaller expert models for solving complex problems. DeepSeek models further divide the work and assign subtasks to smaller sets of experts.\n\nDeepSeek employed a technique called distillation to build a family of six smaller student models \u2014 ranging from 1.5-70 billion parameters \u2014 from the large DeepSeek 671-billion-parameter model. The reasoning capabilities of the larger DeepSeek-R1 671-billion-parameter model were taught to the smaller Llama and Qwen student models, resulting in powerful, smaller reasoning models that run locally on RTX AI PCs with fast performance.\n\nPeak Performance on RTX\n\nInference speed is critical for this new class of reasoning models. GeForce RTX 50 Series GPUs, built with dedicated fifth-generation Tensor Cores, are based on the same NVIDIA Blackwell GPU architecture that fuels world-leading AI innovation in the data center. RTX fully accelerates DeepSeek, offering maximum inference performance on PCs.\n\nExperience DeepSeek on RTX in Popular Tools\n\nNVIDIA\u2019s RTX AI platform offers the broadest selection of AI tools, software development kits and models, opening access to the capabilities of DeepSeek-R1 on over 100 million NVIDIA RTX AI PCs worldwide, including those powered by GeForce RTX 50 Series GPUs.\n\nHigh-performance RTX GPUs make AI capabilities always available \u2014 even without an internet connection \u2014 and offer low latency and increased privacy because users don\u2019t have to upload sensitive materials or expose their queries to an online service.\n\nExperience the power of DeepSeek-R1 and RTX AI PCs through a vast ecosystem of software, including Llama.cpp, Ollama, LM Studio, AnythingLLM, Jan.AI, GPT4All and OpenWebUI, for inference. Plus, use Unsloth to fine-tune the models with custom data.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Trump and Nvidia CEO discuss DeepSeek, AI chip exports during meeting, source says",
            "link": "https://www.reuters.com/technology/trump-meet-with-nvidia-ceo-friday-white-house-official-says-2025-01-31/",
            "snippet": "U.S. President Donald Trump and the CEO of Nvidia Jensen Huang discussed DeepSeek - the Chinese company whose AI model's performance rocked the tech world...",
            "score": 0.9091207385063171,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia unveils preview of DeepSeek-R1 NIM microservice",
            "link": "https://www.cio.com/article/3814582/nvidia-unveils-preview-of-deepseek-r1-nim-microservice.html",
            "snippet": "The GPU-maker has released a preview of an Nvidia inference microservice (NIM) to help developers deploy the new open-weight gen AI model.",
            "score": 0.9316661953926086,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock plummeted 17% on Monday after Chinese AI developer, DeepSeek, unveiled its DeepSeek-R1 LLM. On Thursday, the chipmaker turned around and announced the DeepSeek-R1 model is now available as a preview Nvidia inference microservice (NIM) on build.nvidia.com.\n\nNIM is a set of containers and tools to help developers deploy and manage gen AI models across clouds, data centers, and workstations. For example, the company recently announced three new NIM microservices aimed at helping enterprises boost safety, security, and compliance for AI agents.\n\nDeepSeek-R1 is a new open-weight LLM based on the DeepSeek-V3 base model. Investors rushed to shed Nvidia stock on Monday because DeepSeek benchmarks rivaled those of the OpenAI o1 model but used much less powerful and advanced hardware and computing power sources. Investors feared the news might curb the demand for Nvidia\u2019s highest-end GPUs and sow chaos with the pricing strategies of commercial AI vendors.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Newegg Sells Out of NVIDIA RTX 50 Series GPUs in Record Time",
            "link": "https://www.businesswire.com/news/home/20250131110604/en/Newegg-Sells-Out-of-NVIDIA-RTX-50-Series-GPUs-in-Record-Time",
            "snippet": "Newegg Commerce, Inc. (NASDAQ: NEGG), a global leader in e-commerce for technology products, experienced an overwhelming response to the highly antici.",
            "score": 0.694963276386261,
            "sentiment": null,
            "probability": null,
            "content": "DIAMOND BAR, Calif.--(BUSINESS WIRE)--Newegg Commerce, Inc. (NASDAQ: NEGG), a global leader in e-commerce for technology products, experienced an overwhelming response to the highly anticipated launch of the NVIDIA GeForce RTX 50 Series GPUs, with inventory selling out within minutes. Gaming enthusiasts, content creators, system integrators, and PC builders rushed to secure the first batch of the latest AI-powered graphics technology, driving demand to unprecedented levels and cementing the RTX 50 Series as one of the most sought-after GPU launches in history.\n\nNewegg will release more inventory in real-time as stock becomes available. Customers are encouraged to follow Newegg on social media for the latest restock alerts: www.x.com/newegg.\n\nThe 50 Series Launch on Newegg\n\nThe RTX 50 Series introduces groundbreaking advancements in real-time ray tracing, AI upscaling with DLSS 4, and next-generation gaming performance, setting a new benchmark for high-fidelity visuals and ultra-smooth frame rates. Newegg\u2019s platform witnessed a surge in user engagement as customers eagerly sought to purchase the best gaming and content creating GPUs on the planet.\n\nKey Launch Highlights:\n\nProduct Page Domination: The RTX 50 Series accounted for 18 of Newegg\u2019s top 25 most-visited product pages, demonstrating its immense popularity.\n\nThe RTX 50 Series accounted for 18 of Newegg\u2019s top 25 most-visited product pages, demonstrating its immense popularity. Top Search Trends: 9 of the top 10 search queries on Newegg were related to the RTX 5080 and RTX 5090.\n\n9 of the top 10 search queries on Newegg were related to the RTX 5080 and RTX 5090. Surging Interest: Searches for \u201c5080\u201d and \u201c5090\u201d increased 25 times compared to the previous week.\n\nSearches for \u201c5080\u201d and \u201c5090\u201d increased 25 times compared to the previous week. Traffic Surge: Newegg experienced an 8-10x increase in site traffic during the launch window.\n\nNewegg experienced an 8-10x increase in site traffic during the launch window. Fast Sellout: The RTX 50 Series sold out entirely within 20 minutes, with most inventory claimed in just 5 minutes.\n\nThe RTX 50 Series sold out entirely within 20 minutes, with most inventory claimed in just 5 minutes. Diverse Product Selection: Newegg offered a wide range of RTX 50 Series GPUs from leading manufacturers, including ASUS, Gigabyte, AORUS, MSI, PNY, and ZOTAC. Additionally, pre-built gaming desktops featuring RTX 50 Series GPUs from brands such as ABS, iBUYPOWER, AVGPC, Cobratype, Skytech, and Yeyian were available, providing multiple high-performance options for gamers.\n\nKey Websites for the Launch:\n\n\"The response to the NVIDIA RTX 50 Series has been extraordinary,\" said Jim Tseng, VP of Product Management at Newegg. \"The overwhelming demand reaffirms the gaming and PC community\u2019s enthusiasm for the very best technology. For our customers who have not yet been able to secure a GPU, we\u2019re committed to working with NVIDIA and our AIB (Add-In Board) partners to ensure future restocks and continued availability.\" Tseng continued, \u201cGetting the latest cards into our customers is our passion, and we\u2019re proud to also offer a GPU trade-in program that makes it more affordable for customers to upgrade to the latest generation of video cards.\"\n\nNewegg GPU Trade-In Program\n\nNewegg\u2019s GPU Trade-In Program, launched in September 2023, offers customers a unique opportunity to trade in their eligible pre-owned graphics cards and CPUs. Thousands of GPUs have already been repurposed and installed into other customers\u2019 desktop PCs through this program, promoting sustainability and affordability in gaming and content creation. More information on the Newegg Trade-In Program can be found here.\n\nNewegg\u2019s ABS NVIDIA RTX 50 Series PCs\n\nNewegg\u2019s premier in-house brand, ABS (Advanced Battlestations), also unveiled its latest lineup of high-performance gaming PCs powered by the NVIDIA GeForce RTX 5080 and RTX 5090 GPUs. Designed for gamers, creators, and AI enthusiasts, ABS systems combine expert craftsmanship with state-of-the-art hardware, ensuring unparalleled speed, efficiency, and reliability. ABS RTX 50 Series PCs can be found here.\n\n\"We build each ABS PC here in California with the latest and greatest hardware Newegg has to offer,\" said Steven Chien, VP of Product Management at ABS. \"Creating the world\u2019s best gaming PCs with the latest 50 Series cards has been amazing, and we can\u2019t wait for customers to get theirs in hand.\"\n\nFor more details on future availability and product restocks, visit Newegg\u2019s NVIDIA GeForce RTX 50 Series page and follow Newegg on social media for the latest updates.\n\nAbout Newegg\n\nNewegg Commerce, Inc. (NASDAQ: NEGG), founded in 2001 and based in Diamond Bar, Calif., near Los Angeles, is a leading global online retailer for PC hardware, consumer electronics, gaming peripherals, home appliances, automotive and lifestyle technology. Newegg also serves businesses\u2019 e-commerce needs with marketing, supply chain, and technical solutions in a single platform. For more information, please visit Newegg.com. This press release contains forward-looking statements that involve risks and uncertainties. Actual results may differ materially due to factors beyond Newegg\u2019s control, including but not limited to market conditions, supply chain disruptions, and evolving customer demands. For additional information, please review Newegg\u2019s filings with the U.S. Securities and Exchange Commission.\n\nFollow Newegg on X, TikTok, Instagram, Facebook, YouTube, Twitch, Threads and Discord.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "President Trump to meet with Nvidia CEO Jensen Huang at the White House",
            "link": "https://apnews.com/article/deepseek-nvidia-trump-ai-6554b843e94f2e86c2ea7ba7c180f8bf",
            "snippet": "President Donald Trump is meeting Friday with Nvidia CEO Jensen Huang, whose company designs and supplies the advanced computer chips that play an integral...",
            "score": 0.9137871265411377,
            "sentiment": null,
            "probability": null,
            "content": "President Donald Trump is meeting Friday with Nvidia CEO Jensen Huang, whose company designs and supplies the advanced computer chips that play an integral role in developing artificial intelligence.\n\nThe meeting at the White House was confirmed by a person familiar who insisted on anonymity to discuss the conversation between Huang and Trump. The person said the meeting was set up weeks ago and would enable them to get acquainted and talk about AI policy. Nvidia, based in Santa Clara, Calif., declined to comment on the meeting.\n\nNvidia had loudly protested a last-minute move by the Biden administration in January to expand AI chip restrictions beyond adversaries like China to more than 100 other countries, including Singapore. But it remains to be seen if Trump will follow through with or drop those proposed rules.\n\nTrump signed an order on his first day in office last week that said his administration would \u201cidentify and eliminate loopholes in existing export controls,\u201d signaling that he might continue and harden Biden\u2019s approach.\n\nThe Republican president is banking on AI to foster economic growth and draw hundreds of billions of dollars in investments, but he also saw the performance of China\u2019s DeepSeek AI technology as a sign that the technology can be developed more cheaply.\n\nSpeaking Monday to House Republicans in Miami, Trump called the DeepSeek news \u201cpositive\u201d if it\u2019s accurate because \u201cyou won\u2019t be spending as much and you\u2019ll get the same result.\u201d\n\nTrump said it was a \u201cwake-up call for our industries that we need to be laser-focused on competing to win.\u201d\n\nDeepSeek has said its recent models were built with Nvidia\u2019s lower-performing H800 chips, which are not banned in China. DeepSeek began attracting more attention in the AI industry last month when it released a new AI model that it boasted was on par with similar models from U.S. companies such as ChatGPT maker OpenAI, and was more cost-effective in its use of expensive Nvidia chips to train the system on troves of data. The chatbot became more widely accessible when it appeared on Apple and Google app stores early this year.\n\nThe meeting between Trump and Huang comes as leaders of a special House committee focused on countering China have urged Trump\u2019s national security adviser, Michael Waltz, to consider the potential national security benefits of placing export controls on Nvidia semiconductor chips used by DeepSeek. They said the examination should be part of a review that Trump ordered on his first day in office that called on the secretaries of State and Commerce to review the U.S. export control system.\n\nRep. John Moolenaar, the chairman of the committee, and Rep. Raja Krishnamoorthi, the ranking Democrat on the committee, said DeepSeek made extensive use of a Nvidia chip designed specifically to fall outside U.S. export controls. The lawmakers said that the committee supports American AI innovation, and that support \u201cincludes imposing reasonable safeguards\u201d to protect those innovations from China.\n\n\u201cThis demonstrates what the Select Committee has long argued: frequently updating export controls is imperative to ensure (China) will not exploit regulatory gaps and loopholes to advance their AI ambitions,\u201d the two lawmakers wrote in letter dated Wednesday.\n\nThe pair asked that Waltz look for ways to strengthen controls on shipments through third countries \u201cthat pose a high risk of diversion.\u201d Singapore, the letter said, represented 22% of Nvidia\u2019s revenue in its most recently quarterly statement, \u201cdespite the company itself revealing most of these shipments ultimately went to users outside of Singapore.\u201d\n\nCountries such as Singapore, they added, should be subject to strict licensing requirements if they aren\u2019t willing to \u201ccrack down\u201d on China using their country as an intermediary for shipments.\n\nIn an emailed statement, Nvidia said that its revenue associated with Singapore \u201cdoes not indicate diversion to China.\u201d\n\n\u201cOur public filings report \u2018bill to\u2019 not \u2018ship to\u2019 locations of our customers,\u201d a Nvidia spokesperson said. \u201cMany of our customers have business entities in Singapore and use those entities for products destined for the U.S. and the west. We insist that our partners comply with all applicable laws, and if we receive any information to the contrary, act accordingly.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia CEO Huang Heads to White House After Wild Week for Chipmaker's Stock",
            "link": "https://www.investopedia.com/nvidia-ceo-huang-heads-to-white-house-after-wild-week-for-chipmaker-stock-8783857",
            "snippet": "Nvidia (NVDA) CEO Jensen Huang is set to meet with President Donald Trump at the White House, Bloomberg reported Friday, after a wild week for the chipmaker's...",
            "score": 0.8351997137069702,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Nvidia CEO Jensen Huang is set to meet with President Donald Trump at the White House Friday, Bloomberg reported, after a wild week for the chipmaker's stock.\n\nShares have lost over 12% this week after the emergence of a sophisticated, lower-cost AI model from Chinese startup DeepSeek spurred a reckoning on Wall Street about AI spending.\n\nAnalysts have largely remained bullish on the chipmaker's stock, suggesting the market's reaction may have been overblown.\n\nNvidia (NVDA) CEO Jensen Huang is set to meet with President Donald Trump at the White House, Bloomberg reported Friday, after a wild week for the chipmaker's stock.\n\nShares wavered between gains and losses near $125 in afternoon trading Friday, having lost more than 12% of their value this week after the rapid rise of Chinese AI startup DeepSeek rattled markets.\n\nThe surging popularity of an app from DeepSeek, which claimed it developed an AI model rivaling the performance of American competitors for a fraction of the cost, spurred a reckoning on Wall Street about the competitiveness of American AI leaders and their spending on tech, sending shares of Nvidia and other AI stocks into a tailspin earlier in the week.\n\nStill, analysts have largely remained bullish on the chipmaker's stock, with Bank of America telling clients Wednesday they \"view the recent selloff as an enhanced buy opportunity.\" Analysts at Bernstein, Citi, Wedbush, Raymond James, and elsewhere were also among those that suggested the market\u2019s reaction may have been overblown.\n\nRaymond James and Bank of America added that they expect competition from China could push Big Tech companies like Microsoft (MSFT), Amazon (AMZN), and Alphabet (GOOGL) to spend even more on AI, to the benefit of Nvidia, Broadcom (AVGO), and other AI chipmakers.\n\nIn earnings calls this week, executives from Microsoft and Meta stood by their plans to spend billions of dollars this year on AI infrastructure.\n\nHuang's meeting at the White House also comes following reports the Trump administration could be considering further tightening restrictions on sales of advanced U.S. chips to China.\n\nNvidia reported in November that sales to China accounted for more than 15% of the chipmaker's revenues in its fiscal third quarter. Nvidia is set to report its fourth-quarter results on Feb. 26.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Every Wall Street Analyst Covering Nvidia Stock Expects It to Head Higher -- Here's Why They May Be Wrong",
            "link": "https://www.fool.com/investing/2025/01/31/wall-street-analyst-nvidia-stock-head-higher-wrong/",
            "snippet": "Optimists looking for an encore performance from Wall Street were handsomely rewarded in 2024. Last year, the Dow Jones Industrial Average, S&P 500,...",
            "score": 0.8854234218597412,
            "sentiment": null,
            "probability": null,
            "content": "Is this the ultimate contrarian indicator?\n\nOptimists looking for an encore performance from Wall Street were handsomely rewarded in 2024. Last year, the Dow Jones Industrial Average, S&P 500, and Nasdaq Composite all achieved multiple record-closing highs and ended higher by 13%, 23%, and 29%, respectively.\n\nThe fuel sparking this rally has been abundant, with strong corporate earnings, a resilient U.S. economy, a decline in the prevailing inflation rate, and Donald Trump's November victory all playing a role. But at the heart of this rally is the revolution and evolution of artificial intelligence (AI).\n\nAI is the game-changing technology that's giving software and systems the capacity to reason, act, and evolve, all without the need for human intervention. With AI offering utility in most industries around the globe, PwC foresees this technology increasing global gross domestic product (GDP) by 26% come 2030.\n\nAlthough AI has been a boon for much of the tech sector, no company has more directly benefited than Nvidia (NVDA 5.27%).\n\nEvery Wall Street analyst expects Nvidia stock to rise\n\nIn a span of less than two years, we've witnessed Nvidia grow from a $360 billion business that was predominantly known for graphics processing units (GPUs) used in PC gaming and cryptocurrency mining to a $3.7 trillion company (at its peak) whose GPUs became the brains of enterprise-operated AI-accelerated data centers.\n\nDemand for the company's ultra-popular Hopper (H100) chip and successor Blackwell GPU architecture have been otherworldly. Whereas Advanced Micro Devices has been pricing its Instinct MI300X AI-accelerating chips between $10,000 and $15,000, Nvidia's Hopper has been commanding a price tag of $30,000 to $40,000. This combination of AI-GPU scarcity and exceptional demand has boosted Nvidia's pricing power and gross margin.\n\nAdditionally, no company has come particularly close to matching the computing potential of Nvidia's GPUs. The aforementioned Blackwell GPU is designed to accelerate computing in a half-dozen arenas, including quantum computing and generative AI solutions, and can do so while being considerably more energy efficient than its predecessor.\n\nIt's not just hardware, either, which is helping Nvidia hang onto its competitive advantages. The company's CUDA software platform, which is the toolkit developers use to maximize the computing potential of their Nvidia GPUs, has played a role in keeping customers loyal to its product and service ecosystem.\n\nAnd, of course, Nvidia's operating results have consistently blown away the consensus expectations of Wall Street analysts. After reporting $27 billion in full-year sales in fiscal 2023 (ended Jan. 29, 2023), Nvidia is expected to deliver $129 billion in revenue for fiscal 2025. As you can imagine, nearly quintupling sales in two years has dramatically boosted its operating cash flow and profits.\n\nPerhaps it's no surprise that every Wall Street analyst expects Nvidia stock to head higher. Out of the 40 analysts that have issued price targets, the lowest ($135 per share) implies 5% upside from where shares closed on Jan. 28. Further, out of the 67 analysts that have issued a buy, hold, or sell recommendation (not all analysts recommendations include a price target), not one rates Nvidia as the equivalent of an underperform or sell.\n\nWhile this might sound like a resounding endorsement for Nvidia, it could be the ultimate contrarian indicator.\n\nWall Street might have Nvidia all wrong\n\nIf history has proven anything, it's that Wall Street analysts tend to be reactive rather than proactive with their ratings. In other words, their rating adjusts after Nvidia delivers its quarterly operating results or its stock blows past a previous price target. Being reactive instead of proactive may be causing dozens of analysts to miss potentially serious red flags with Nvidia.\n\nOne of the biggest concerns analysts appear to be collectively overlooking is that history is undefeated when it comes to putting next-big-thing innovations in their place.\n\nApproximately three decades ago, the proliferation of the internet provided a new way for businesses to market themselves and reach their customers. Eventually, the internet lifted the growth trajectory for American and global businesses. But this wasn't without the dot-com bubble taking place and wiping out 78% of the Nasdaq Composite's value on a peak-to-trough basis.\n\nEvery game-changing innovation for three decades has needed time to mature, without exception. Yet investors (including Wall Street analysts) have consistently overestimated the adoption rate and early stage utility of every next-big-thing innovation and trend.\n\nEarlier this week, we had a perfect example of this overestimation in practice. When DeepSeek's open-source R1 large language model (LLM) rattled markets, investors briefly worried about an increase in AI competition (which is a point I'll touch on in a moment). But the bigger concern is it exposed the reality of most businesses not understanding what's needed to build and train LLMs. Put another way, most businesses lack a clear game plan with their AI investments and have no definitive idea of how to optimize AI solutions -- and that's a problem when Nvidia stock is priced for perfection.\n\nCompetition is also an inevitability that should weigh on Nvidia's gross margin. Though external competition from the likes of AMD will be a challenge, the more worrisome threat might come from Nvidia's largest customers by net sales. Microsoft, Meta Platforms, Amazon, and Alphabet are all internally developing AI chips to use in their data centers. Even if these chips pale in comparison to the computing potential of Nvidia's Hopper and Blackwell, they'll be significantly cheaper and more easily accessible than Nvidia's hardware.\n\nA strong argument can be made that AI-GPU scarcity has been more important to Nvidia than its computing speed superiority. As this scarcity wanes in the quarters to come, Nvidia's pricing power, and thus its gross margin, could take a notable hit.\n\nWall Street analysts may not be properly accounting for geopolitical risks, as well. China is one of Nvidia's largest markets by net sales, and President Donald Trump's trade policies have the potential to strain trade relations between the world's two-largest economies by GDP. Mind you, the possibility of Trump implementing tariffs on China and/or foreign chipmakers comes atop the Joe Biden administration clamping down on AI-GPU exports since 2022.\n\nLastly, both Nvidia and the stock market, as a whole, are historically pricey. Nvidia topped a price-to-sales (P/S) ratio of 40 last summer, which is a level that's consistent with bubble-bursting events for businesses on the cutting edge of next-big-thing innovations.\n\nAlthough every single analyst covering Nvidia expects its stock to head higher, there's compelling evidence to suggest they might all be wrong.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia says its new GPUs are the fastest for DeepSeek AI, which kind of misses the point",
            "link": "https://www.theverge.com/news/604412/nvidia-rtx-50-series-gpus-deepseek",
            "snippet": "Nvidia says that its RTX 50-series GPUs can \u201crun the DeepSeek family of distilled models faster than anything on the PC market.\u201d",
            "score": 0.7322096228599548,
            "sentiment": null,
            "probability": null,
            "content": "is a news editor covering technology, gaming, and more. He joined The Verge in 2019 after nearly two years at Techmeme.\n\nNvidia is touting the performance of DeepSeek\u2019s open source AI models on its just-launched RTX 50-series GPUs, claiming that they can \u201crun the DeepSeek family of distilled models faster than anything on the PC market.\u201d But this announcement from Nvidia might be somewhat missing the point.\n\nThis week, Nvidia\u2019s market cap suffered the single biggest one-day market cap loss for a US company ever, a loss widely attributed to DeepSeek. DeepSeek said that its new R1 reasoning model didn\u2019t require powerful Nvidia hardware to achieve comparable performance to OpenAI\u2019s o1 model, letting the Chinese company train it at a significantly lower cost. What DeepSeek accomplished with R1 appears to show that Nvidia\u2019s best chips may not be strictly needed to make strides in AI, which could affect the company\u2019s fortunes in the future.\n\nThat said, DeepSeek did train its models using Nvidia GPUs, merely weaker ones (H800) that the US government allows Nvidia to export to China. And today\u2019s blog post from Nvidia wants to show that its new 50-series RTX GPUs can be useful for R1 inference \u2013 or what an AI model actually generates \u2013 saying that the GPUs are built on the \u201csame NVIDIA Blackwell GPU architecture that fuels world-leading AI innovation in the data center\u201d and that \u201cRTX fully accelerates DeepSeek, offering maximum inference performance on PCs.\u201d\n\nBut how DeepSeek did its training is part of what has been such a big deal. (And it\u2019s worth noting that China is getting a less powerful version of the RTX 5090.)\n\nOther tech companies are trying to ride the DeepSeek wave, too. R1 is also now available on AWS, and Microsoft made it available on its Azure AI Foundry platform and GitHub this week. However, Microsoft and OpenAI are reportedly investigating if DeepSeek took OpenAI data, Bloomberg reports.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "I Review Graphics Cards For a Living, Let Me Help You Pick an Nvidia GPU",
            "link": "https://www.ign.com/articles/best-nvidia-graphics-cards",
            "snippet": "There are a lot of reasons to pick an Nvidia graphics card over one made by AMD or Intel. For one, whether you love it or hate it, Nvidia has plenty of...",
            "score": 0.9360763430595398,
            "sentiment": null,
            "probability": null,
            "content": "Whether you\u2019ve been playing PC games for years or you\u2019re building your first gaming PC, picking the right graphics card is probably one of the most consequential choices. It doesn\u2019t help that now there\u2019s three companies making graphics cards, which means there are more options out there than ever before.\n\nBut there are a lot of reasons to pick an Nvidia graphics card over one made by AMD or Intel. For one, whether you love it or hate it, Nvidia has plenty of software and features that are exclusive to its graphics cards, whereas things like FSR (FidelityFX Super Resolution) from AMD will work no matter what GPU is in your system. Features like DLSS, or Deep Learning Super Sampling, make an Nvidia graphics card an attractive choice \u2013 at least until the AMD Radeon RX 9070 XT comes out with FSR 4, AMD's AI upscaler.\n\nNvidia also has the most powerful graphics cards on the market right now, especially at the high-end. There simply is not another graphics card that can match the RTX 5090 in pure performance. But even when it comes to more affordable graphics cards like the RTX 4060 Ti, the AI upscaling offered by DLSS can really help stretch the performance, and can even push into higher resolutions than you would otherwise be able to.\n\nIt's still important to figure out what resolution you want to play games at, because that\u2019s going to change what type of graphics card you want to go for. If you want to play all the newest PC games at 4K with all the settings maxed, you\u2019re going to need a much more powerful card than someone that just wants to play World of Warcraft at 1080p.\n\nFeatured in this article\n\nGraphics Cards Basics\n\nWhile graphics cards are extremely complicated devices, shopping for one doesn\u2019t need to be that much of a hassle. As long as you keep an eye out for some simple specs, you should have a pretty good idea of what you\u2019re getting into.\n\nThe most obvious thing to look for is whether or not your graphics card is actually part of the current generation. You don\u2019t want to miss out on performance or features, especially if you\u2019re spending hundreds of dollars on just one part of your computer. With Nvidia graphics cards, this is actually extremely easy, just look at the two numbers following the \u2018RTX\u2019 or \u2018GTX\u2019 in the graphics card\u2019s name. Nvidia just launched its newest generation of graphics cards, which are labeled with '50.' If it\u2019s \u201840,\u2019 that\u2019s the previous generation from 2022; \u201830\u2019 and \u201820\u2019 are the two generations before that.\n\nDon\u2019t get me wrong, getting a 30-series or 20-series card will still provide an excellent gaming experience, with the RTX 3080 still able to power most games at 4K without breaking a sweat.\n\nBut what about the second two numbers? Well, take a look at the RTX 5090, the top-end graphics card on the market. Then, take a look at the RTX 4050, which is only a laptop GPU, but is way weaker. The scale typically goes from 50-90, with the higher number meaning a more powerful graphics card. Nvidia does sometimes add extra letters or words at the end of its graphics cards, with \u2018Ti\u2019 and \u2018Super\u2019 being common variants. This typically means a slightly more powerful card. For instance, the Nvidia GeForce RTX 5070 Ti is going to be slightly more powerful than the RTX 5070. The basic rule of thumb is that, after the number, any extra letters or words typically indicate a faster graphics card.\n\nWhen it comes to specs, graphics cards have a lot of numbers and lingo to swallow, which can definitely be overwhelming. But if you just want to slot a GPU in your rig and forget about it, you don\u2019t need to pay attention to every little detail.\n\nThe amount of VRAM you need is going to largely depend on what resolution you want to play your games at. If you want to play games at 4K, you\u2019re going to want as much video memory as you can afford. There are games that will eat up upwards of 20GB of VRAM at that resolution if you let them, so the sky really is the limit. For lower resolutions like 1080p, however, you can get by with 8GB of RAM, though I would recommend going with a 12GB or 16GB card if it\u2019s in your budget. An 8GB card will get you through most games, but there is a growing number of AAA PC games that need more memory.\n\nThere are some other specs you can look at if you really want to, like clock speed, CUDA core count and Streaming Multiprocessors \u2013 what Nvidia calls its compute units \u2013 but those largely just get better as you get a more high-end graphics card either way. Compute Units are essentially the same as CPU cores on a processor, and each one contains 128 CUDA cores. So, the RTX 5080 with its 84 Compute Units, contains a total of 10,752 CUDA cores. Just keep in mind that directly comparing two graphics cards of different generations by the core counts alone won\u2019t tell you how much of a performance jump it\u2019ll be.\n\nOnce you\u2019ve picked the graphics card you want, you need to make sure you have a power supply that can handle it. You can usually check the box your graphics card comes in to get an idea of how much power the card requires, and if not, Nvidia has it listed out on its website. My advice would be to check the required power for your graphics cards and then get a power supply that can provide just a bit more wattage. For instance, if your graphics card recommends a 450W power supply, I\u2019d get a 550 or a 650W unit just to be on the safe side.\n\nNvidia GeForce RTX 5090 \u2013 Photos 5 Images\n\n1. If You Just Want the Best: RTX 5090\n\n7 If you just want the best Gigabyte Nvidia GeForce RTX 5090 15 The Nvidia GeForce RTX 5090 is now the most powerful graphics card on the market, even if it claims that title with a bit less force than in previous generations. See it at Newegg Product Specifications CUDA Cores/Stream Processors 21,760 Base Clock 2.01GHz Boost Clock 2.41GHz Video Memory 32GB GDDR7 Memory Bandwidth 1.79 TB/s Memory Bus 512-bit Power Connectors 1 x 16-pin Outputs 1 x HDMI 2.1b, 3 x Display Port 1.4b Size 12 x 5.4 x 1.9 inches (L x W x H) (Dual-Slot) PROS The most powerful consumer GPU out there, period. Multi-Frame Generation is cool if you have a 4K 240Hz monitor CONS Not much more powerful than the last-gen RTX 4090 The Nvidia GeForce RTX 5090 is the most powerful graphics card on the market right now, there's no way around that. But I still wouldn't recommend most people buy it. This is a $1,999 graphics card to start and it's not exactly super available on store shelves right now. We've even started seeing scalpers selling this thing for $9000 on eBay, just a day after launch. If you do have the funds to get your hands on it, though, you can expect the best 4K gaming experience money can buy, even if it isn't light-years ahead of the RTX 4090. When I reviewed the Nvidia RTX 5090, I found that it's around 20-25% faster than the RTX 4090 at 4K, with the performance lead obviously waning at lower resolutions. That's a relatively small gen-on-gen performance uplift, but at the end of the day, faster is faster. The RTX 5090 largely gets its larger performance from an absolutely massive GPU, with 21,760 CUDA cores, which can boost up to 2.41GHz. That's a sizable jump from the RTX 4090, and it's possible primarily through a much larger power budget. This graphics card requires 575W of power to run, which is the most power a consumer graphics card has ever required \u2013 including back in the days of dual-GPU graphics cards like the AMD Radeon R9 295X2. Nvidia GeForce RTX 5090 \u2013 Benchmarks 14 Images As such, you're going to need a serious power supply to keep this GPU fed with power. Nvidia recommends at least a 1,000W unit to pair with the RTX 5090, but I'd recommend going a little beyond that, with something like a 1,200W PSU, just to make sure it stays efficient when you're really pushing it to its limits. Because this graphics card will hit that power limit, especially when DLSS 4 Multi-Frame Generation is enabled, and that's largely the reason to get this graphics card. Essentially, DLSS 4 Multi-Frame Generation, or MFG, uses the AI Tensor Cores to generate up to 3 AI frames off of each rendered frame. This can hugely improve your framerate, but can introduce serious latency if you're not already getting a solid frame rate. This is a feature you should only really enable if you're already getting 60-70 fps, but it's going to be rare to find a PC game that the 5090 is going to have any trouble hitting that frame rate on. Nvidia GeForce RTX 5080 \u2013 Photos 6 Images\n\n2. Best for 4K (for Most People): RTX 5080\n\n7 Best for 4K (for most people) Zotac Nvidia GeForce RTX 5080 3 The Nvidia GeForce RTX 5080 is the best GPU for 4K if you don't want to sell one of your kidneys. Plus, it's got the new DLSS 4 Multi-Frame Generation. See it at Newegg Product Specifications CUDA Cores 10,752 Base Clock 2.3GHz Boost Clock 2.61GHz Video Memory 16GB GDDR7 Memory Bandwidth 960GB/s Memory Bus 256-bit Power Connectors 1 x 16-pin Outputs 1 x HDMI 2.1b, 3 x DisplayPort 1.4b PROS Solid 4K perfomance Supports DLSS 4 MFG CONS Smallest gen-on-gen improvement in years Every GPU generation is a gamble of whether or not it's going to be a huge improvement on what came before, or just fall flat. The Nvidia GeForce RTX 5080 kind of leans towards the latter option, but that's not to say it's a bad graphics card. While I wouldn't recommend anyone who has an RTX 4000 graphics card to upgrade to the 5080, it's a good upgrade for anyone that's been waiting a couple of generations for a new graphics card. In my review, the RTX 5080 only ended up being around 11% faster and 8% faster than the RTX 4080 and RTX 4080 Super, respectively, at 4K. That's one of the weakest generational uplifts for a graphics card in years, and it remains slower than the last-generation flagship, the RTX 4090. But given that the RTX 4090 is still way more expensive than the RTX 5080, this is still the best Nvidia graphics card you're going to get around a thousand bucks \u2013 assuming you can find one. Nvidia GeForce RTX 5080 \u2013 Benchmarks 14 Images The main reason the RTX 5080 isn't much faster than its last-gen counterpart is because Nvidia didn't shrink the manufacturing process, remaining on a similar 4nm node, while also not drastically increasing the amount of CUDA cores on offer. Luckily, that helps avoid the massive power requirements seen on the RTX 5090, with the 5080 'only' requiring 360W of power to run. That means you don't need to fork over the cash for an expensive high-wattage power supply, which means more cash for the games you want to play. And while this graphics card might be a little disappointing for anyone who keeps a laser focus on graphics cards every generation, playing games on this thing is awesome. Throughout my entire test suite, the only game that didn't get 60 fps at 4K was Metro Exodus: Enhanced Edition, and that was because I was running it without DLSS with Ray Tracing cranked up. Turn on DLSS, and that number is going to hit the triple digits, just like pretty much any AAA game on the market right now. Add in DLSS 4 Multi-Frame Generation, and you should have no problem fully saturating a high-refresh 4K monitor. Just make sure you can hit 60-70 fps before you turn it on. Nvidia GeForce RTX 4070 Super Unboxing 5 Images\n\n3. Best for 1440p: RTX 4070 Super\n\n9 Best for 1440p Asus RTX 4070 Super Dual Evo OC 12 Performance is significantly improved over the original RTX 4070, along with more VRAM that should make the graphics card a little bit more future-proof. See it at Amazon Product Specifications CUDA Cores 7,168 Base Clock 1,980 Boost Clock 2,475 Video Memory 12GB GDDR6X Memory Bandwidth 504.2GB/s Memory Bus 192-bit Power Connectors 1 x 16-pin Outputs 1 x HDMI 2.1, 3 x DisplayPort 1.4a PROS Great for 1440p Affordable (kind of) CONS Should have had 16GB of VRAM If you ask me, 1440p is the gold standard PC gaming resolution. Not only are the monitors much cheaper than their 4K brethren, but they\u2019re much easier to power with affordable graphics cards. You can absolutely strap an RTX 4080 Super into a gaming PC to play games at 1440p, too, but you\u2019re better off dialing in the performance and saving more money for games. The Nvidia GeForce RTX 4070 Super is the perfect Nvidia graphics card for 1440p, no matter what games you\u2019re playing. In my review, I found that the RTX 4070 Super is easily able to play the most demanding games like Cyberpunk 2077 at 1440p, delivering 91 fps on the Ray Tracing Ultra preset. And in games like Forza Horizon 5, that number goes up to 158 fps, proving the RTX 4070 Super is capable of high frame rate gaming at 1440p. And at $599, it\u2019s much more affordable than the RTX 4070 Ti or RTX 4070 Ti Super, which will set you back $749 and $799, respectively. The only downside is that Nvidia didn\u2019t upgrade the VRAM when refreshing the RTX 4070. You\u2019re still getting 12GB of GDDR6X memory, which should be plenty for most games, but you might find it getting stretched in more demanding games like Black Myth Wukong. This is especially unfortunate given you can get the RTX 4060 Ti with 16GB of VRAM at a lower price \u2013 even if the RTX 4070 Super will outperform the 4060 Ti in every game regardless. RTX 4060 Ti Founders Edition 5 Images\n\n4. Best for 1080p: RTX 4060 Ti\n\n6 Best for 1080p Asus Asus TUF Gaming GeForce RTX 4060 Ti OC Edition 4 The Asus TUF Gaming GeForce RTX 4060 Ti OC Edition is the best graphics card to buy if you're on a budget. See it at Amazon Product Specifications CUDA Cores 4,352 Base Clock 2,310MHz Boost Clock 2,535MHz Video Memory 8GB GDDR6 \u2013 16GB GDDR6 Memory Bandwidth 288GB/s Memory Bus 128-bit Power Connectors 1 x 16-pin Outputs 1 x HDMI 2.1, 3 x DisplayPort 1.4a PROS Max out games at 1080p Can stretch into 1440p in most games CONS 16GB model is a waste of time According to the latest Steam Hardware Survey, 1080p is still far and away the most popular display resolution for PC gamers. While that number has been going down over time, it makes sense why so many people still play at 1080p. It\u2019s simply more affordable to play at this resolution, as you don\u2019t need an extremely powerful graphics card to play even the most impressive AAA games. Plus, because of the low power demands for this resolution, it remains extremely popular among esports players, who can get insane frame rates that are just not possible at 4K. And that\u2019s what makes the Nvidia GeForce RTX 4060 Ti such a great 1080p graphics card. The RTX 4060 Ti is affordable at $399, and is easily able to play any game at 1080p at a high frame rate \u2013 and I don\u2019t mean just 60 fps. You see, in my review, I found the RTX 4060 Ti to exceed 100 fps in most of the games I tested. In fact, the only game that it didn\u2019t top 100 fps in was Cyberpunk 2077, with 76 fps, but that number went all the way up to 122 fps once I enabled Frame Generation. In most of the games where a super high frame rate will actually matter, the RTX 4060 Ti can easily reach upwards of 200 fps. Games like Overwatch 2, Valorant and Counter-Strike 2 will love the extra frames this GPU can spit out. One thing you have to be aware of, though, is there are technically two different versions of the RTX 4060 Ti: An 8GB model for $399 and a 16GB version that costs $499. For most people who just want to play games at 1080p, the 8GB version is going to be fine. And while, yeah, 16GB is better, it\u2019s really not worth adding an extra $100 to the price tag for the two games that need more than 8GB at 1080p.\n\n5. Best on a Budget: GeForce GTX 1660 Super\n\nBest budget Yeston GeForce GTX 1660 Super Super-6G 3 The GTX 1660 Super is old, but it still packs a punch. See it at Newegg Product Specifications CUDA Cores 1,408 Base Clock 1,530MHz Boost Clock 1,785MHz Video Memory 6GB GDDR6 Memory Bandwidth 336GB/s Memory Bus 192-bit Power Connectors 1 x 8-pin Outputs 1 x DVI, 1 x HDMI 2.0, 1 x DisplayPort 1.4a PROS Still good for 1080p gaming without ray tracing Can find it for cheap on the used market CONS You might have to get it used It's FIVE years old If $399 is still too expensive, you can still pick up the Nvidia GTX 1660 Super even if it's an ancient graphics card by today\u2019s standards. The GTX 1660 Super is built on the Nvidia Turing architecture that powered the RTX 2080 back in 2018. However, while it\u2019s more than a little outdated at this point, it can still deliver solid 1080p gaming, especially in less demanding games like League of Legends. If you\u2019re going to compromise and get this older graphics card, keep in mind that you\u2019re missing out on many of Nvidia\u2019s best features. This GPU doesn\u2019t have RT cores, so it doesn\u2019t support ray tracing, and it doesn\u2019t have Tensor cores, which means no DLSS. When I reviewed the GTX 1660 Super for TechRadar , I was amazed by just how well it performed at 1080p for the price. That was five years ago at this point, but Nvidia still hasn\u2019t released a direct successor to this budget graphics card. That\u2019s a shame, because Nvidia\u2019s graphics architecture has grown a lot since then, and another budget card is long overdue. Luckily, Nvidia still hasn\u2019t discontinued the GTX 1660 Super in the years since it launched and you can find it for as little as $180. Five years later and the Nvidia GeForce GTX 1660 Super is still the best Nvidia graphics card under $200, however shameful that is.\n\nWhat Is DLSS?\n\nDeep Learning Super Sampling, or DLSS is an AI upscaling method that uses Nvidia\u2019s Tensor Cores to improve image quality. The Tensor Core will take visual data from the frame being rendered, along with motion vector data, in order to accurately upscale the image to a higher resolution. This process results in an image that looks pretty close to the native resolution, but with a much higher frame rate.\n\nIn its early days, DLSS needed to train the AI model on each game it would support, with developers needing to upload data to Nvidia. However, DLSS has gone through several iterations, and now it does not need to be trained on individual games. Instead, if a developer wants to include DLSS in their game, they can just inject Nvidia\u2019s API into the game. This means more games can support the technology, without having to wait for Nvidia\u2019s training on each game.\n\nWith DLSS 3.0, Nvidia added Frame Generation to the equation. This technology takes the visual data from two frames, along with motion vector data from the game engine and motion information from its own hardware, and creates an entirely new frame that\u2019s sandwiched between the two original frames. This would introduce a lot of latency, but Nvidia requires its Reflex technology to be enabled before Frame Generation can happen. Reflex essentially syncs the graphics card and the processor, eliminating the need for the CPU to queue up frames for the GPU to render later.\n\nTogether, Frame Generation and Reflex greatly improve your frame rate, but there\u2019s a catch. Because so much of it relies on motion data, you already need to have a decent frame rate for Frame Generation to work smoothly. So, this technology is best used for folks that can already get 60 fps or more, and just want to push to a higher frame rate.\n\nWhat Is Ray Tracing?\n\nRay tracing is just a way to render light realistically. It does this by taking a light source, then simulating each ray of light as it bounces around the scene. It\u2019s a simple concept to be sure, but it ends up requiring a ton of compute power to pull off. Any light being cast potentially has thousands of rays of light, each of which will bounce around hundreds of times, multiply that by needing a new frame 60 times a second, and you can imagine how much power you need to pull it off.\n\nThat\u2019s why ray tracing in video games needs specialized hardware to pull this lighting method off without grinding your framerate to a standstill. Luckily, Nvidia has been working this hardware into its graphics cards since the RTX 2080 in 2018.\n\nFast forward to today, and even with that dedicated ray tracing hardware being built into every mainstream graphics card \u2013 not just from Nvidia \u2013 we still need to limit ray tracing in order to maximize performance. There are only a few games that support full ray tracing, or \u2018path tracing,\u2019 with most other games limiting the ray tracing elements to a certain part of the scene, like shadows or reflections, and also limiting the amount of bounces calculated for each ray of light.\n\nIt\u2019s an expensive way to generate lighting, but it looks incredible, especially in games that rely on lighting for atmosphere. In Metro Exodus, for instance, the accurate lighting often leads to darker environments, amplifying the intense atmosphere the game already has.\n\nJackie Thomas is the Hardware and Buying Guides Editor at IGN and the PC components queen. You can follow her @Jackiecobra",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia\u2019s GeForce RTX 5080 Is a Solid High-End GPU",
            "link": "https://www.wired.com/review/nvidia-geforce-rtx-5080-fe-review/",
            "snippet": "Nvidia's newest 50 Series GPUs are slowly trickling out, and as usual, the very top-end RTX 5090 (7/10, WIRED Recommends) is the graphics card that everyone...",
            "score": 0.8657098412513733,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia\u2019s newest 50 Series GPUs are slowly trickling out, and as usual, the very top-end RTX 5090 (7/10, WIRED Recommends) is the graphics card that everyone is talking about. It boasts extreme 4K gaming, the latest in AI-powered gaming enhancements, and a power draw to match.\n\nI can\u2019t blame you for not wanting to spend $2,000 on a GPU; that\u2019s enough to build a midrange gaming PC on its own. At just $1,000 for the RTX 5080 Founders Edition, this still-expensive step-down card will be the model that more people seriously consider, even if it's still a splurge. It's a better choice from a performance perspective, meeting people where they already are in terms of monitor resolution, game choice, and existing power supplies.\n\nBut how does it fare against the more expensive card, and how does it handle some of the more popular and evergreen games? Well enough to my eyes. If you're building your next high-end gaming PC and are looking for a high-end video card to match, this might be exactly what you're looking for\u2014if you can find one for sale.\n\nSame Size, More Efficient\n\nThe form factor of the RTX 5080 is identical to its more powerful counterpart, with a true two slot design that should fit in most cases very comfortably. I really appreciate the size reduction overall, and I hope AIC cards follow suit.\n\nPhotograph: Brad Bourque\n\nWhere the RTX 5090 draws an immense 575 watts, the 5080 only asks for 360 watts with the same new power connector. Like the RTX 5090 FE, the RTX 5080 includes an adapter, and I imagine most partner cards will as well.\n\nThat means a lower overall system power requirement, with Nvidia recommending just 850 watts for the Founders Edition. I expect this will be an easier requirement for existing rigs to meet without needing to buy a new 1,000-watt or higher PSU.\n\nDLSS Performance\n\nNvidia introduced a new version of its AI-powered enhancement tools for the RTX 50 Series. These notably add support for multi-frame generation, which uses AI to generate up to three frames between. If you\u2019re interested in learning more about the effects of using this tech on image quality, make sure to check out the RTX 5090 review.\n\nThe short version is that multi-frame generation can produce minor artifacts, particularly in areas where two objects at different depths overlap, such as looking through a fence. These are hard to spot across a whole screen though, and the higher frame rate makes the gaming experience much smoother, so the frames are onscreen less time.\n\nI\u2019ll start by checking out performance in Cyberpunk 2077, one of the more demanding games that currently supports multi-frame generation.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-30": {
        "0": {
            "title": "NVIDIA App Update Adds DLSS 4 Overrides, New Broadcast Update, Improvements To RTX Video Super Resolution & More",
            "link": "https://www.nvidia.com/en-us/geforce/news/nvidia-app-update-dlss-overrides-and-more/",
            "snippet": "Also: NVIDIA Broadcast adds new AI-powered effects, NVIDIA Smooth Motion introduces a driver-based AI model that delivers smoother gameplay in many games by...",
            "score": 0.6420997381210327,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA app is the essential companion for users with NVIDIA GPUs in their PCs and laptops. Whether you're a gaming enthusiast or a content creator, NVIDIA app simplifies the process of keeping your PC updated with the latest GeForce Game Ready and NVIDIA Studio Drivers, and enables quick discovery and installation of NVIDIA applications like GeForce NOW and NVIDIA Broadcast.\n\nToday, we\u2019re releasing a major NVIDIA app update, coinciding with the release of our new GeForce RTX 5090 and GeForce 5080 graphics cards, and the new GeForce Game Ready Driver and NVIDIA Studio driver.\n\nBy installing the update and our new driver, launching at 6am PT, you can employ NVIDIA DLSS 4 to enhance over 75 games and apps with DLSS 4 with Multi Frame Generation, and new, even better AI models for DLSS Super Resolution, Frame Generation, and Ray Reconstruction.\n\nGeForce RTX 50 Series gamers can now also enable NVIDIA Smooth Motion, a new driver-based AI model that delivers smoother gameplay by inferring an additional frame between two rendered frames. For games without DLSS Frame Generation, NVIDIA Smooth Motion is a new option for enhancing your experience on GeForce RTX 50 Series GPUs.\n\nA new NVIDIA Broadcast update is now also available for download, introducing new AI-powered Studio Voice and Virtual Key Light effects to enhance your streams, and a new user interface that enables users to combine even more effects.\n\nRTX Video Super Resolution enhances the quality of video using AI. Our new NVIDIA app update reduces the GPU usage of this popular feature by 30%, adds an on-screen status indicator, and enables users to automatically adapt quality and GPU utilization when using other GPU-intensive programs.\n\nRounding out our new release is the addition of more NVIDIA Control Panel options, namely Advanced Optimus and Multiple Display management.\n\nFor further details about each of the new additions, read on; for a tour of previously released features, head here.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "DLSS 4's announcement may have convinced me to switch from AMD to Nvidia for the next generation of GPUs, and I doubt I'm the only one",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/dlss-4s-announcement-may-have-convinced-me-to-switch-from-amd-to-nvidia-for-the-next-generation-of-gpus-and-i-doubt-im-the-only-one/",
            "snippet": "You can accuse me of drinking the Kool-Aid right now, if you wish.",
            "score": 0.7568150162696838,
            "sentiment": null,
            "probability": null,
            "content": "Andy Edser, hardware writer (Image credit: Future) This month I've been testing: Handhelds and headsets. Both of which appear to be getting bigger as time goes on. It won't be long now before I strap a pair of speakers to the top of my head, stick a slab of granite in my backpack, and merrily head off to work\u2014with my spine complaining all the way.\n\nIf there's one thing I've never quite understood about the PC hardware community, it's the tribalism evident when it comes to GPU choices. I've never much cared for being team red, team green, or team blue for that matter. Give me the best bang for my buck, and a fraction of my poorly-managed finances are yours.\n\nWhich is why I currently run an RX 7800 XT in the PCIe slot of my main PC. It wasn't the chiplet-based design that won me over, nor the desire to stick one to the big green man and buy the underdog choice. Simply put, I wanted great 1440p performance on a limited budget\u2014and as a result I opted for an XFX version of AMD's mid-range card.\n\nMy other option was the RTX 4070, a card I've often used for performance comparisons and agree is also a fine choice for a reasonably-priced GPU. The RX 7800 XT was a bit cheaper though, and a bit faster in raw raster performance at 1440p, so I plumped for that with my personal cash instead.\n\nCut to just over a year and a half later, and I'm sat in my Vegas hotel room at CES 2025, watching the Nvidia RTX 50-series announcement along with many of the rest of you (my hardware overlord, Dave James, bravely dealt with the immense queues to see it in person). The lights go up, Jen-Hsun appears in an even shinier leather jacket, and the card rollouts begin. And it's time for me to make an admission: I scoffed a little when the RTX 5070 was revealed with claims of \"4090 performance\" for $549.\n\nThat'll be with a lot of upscaling help, I thought. A bit of a contentious claim, and something to potentially write up as an opinion piece later. Fake frames, man, and all that. Raw raster still counts for something, and DLSS can't really be used as a salve for all ills.\n\n(Image credit: Nvidia)\n\nAs we later found out, I wasn't entirely wrong. But then I saw the DLSS 4 demo, with all its immensely pretty ray-traced shenanigans. And the staggering frame rate boosts of Multi Frame Generation. Not to mention the claimed image quality improvements, the Neural Texturing, and all sorts of other enticing AI-based doohickeys.\n\nOf course, it was just a demo trailer. Impossible to really judge until I saw it running in front of me for myself. But if there's one thing that's bugged me during my time with the RX 7800 XT, it's that I've been locked into using lesser upscaling tech, like FSR and XeSS.\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nDLSS has been better, is better, than either of those competing solutions for some time. Miles better.\n\nI felt a twinge. It was the same twinge I've felt every time I've seen DLSS in motion for the past couple of years, the same slight regret I've felt whenever I've enabled FSR or Fluid Motion Frames on my home machine to boost the framerate in a struggling game.\n\nFSR and XeSS work fine. But DLSS works brilliantly. And now, with all these transformer-based improvements? Nothing suggests to me that has changed. In fact, Nvidia seems to be so far ahead, the competition looks to be floundering in its wake.\n\nDLSS 4 / DLSS 3 / DLSS 2 / DLSS Off Comparison | Cyberpunk 2077 - YouTube Watch On\n\nThere are still those among us who consider AI upscaling and frame generation to be cheating, to some degree. It'll always introduce some sort of artifacting or latency, they argue, and it's never quite as good as raw raster. I like my coffee black and my meat raw, those sorts of people.\n\nFor a long time, I was one of them. But now I think it's time to admit that, far from being a frame-boosting lesser option, a panacea for performance-related woes, it's now an integral part of PC gaming. Hell, gaming in general. Complaining about it at this point feels a little like King Canute shouting at the tide. Get those AI kids off my lawn, and all that.\n\nAnd in this brave new world of upscaling being more of a requirement than an option, Nvidia really does seem to be holding all the cards. Yes, FSR 3.1 isn't bad at Quality settings, but as my upscaler comparison testing shows, it's still not a patch on DLSS when it comes to image quality\u2014especially when you push it down to Balanced or even, goodness forbid, Performance levels.\n\n(Image credit: Nvidia)\n\nAMD says FSR 4 is coming, and it's now machine-learning based. That's probably why you'll need either an RX 9070 or RX 9070 XT to run it, as those cards presumably have some hardware on board equivalent to Nvidia's Tensor cores. Perhaps it can deliver similar framerate gains and image quality, but something in me doubts it. Nvidia has leant so heavily on the AI button for so long, I honestly can't see AMD catching up in a single swoop.\n\nAI-based upscaling and frame generation performance now matters more than ever. Raw raster performance? Still so, but lesser. And a $549 card that can take advantage of all the DLSS 4 benefits, to the point where it can spit out hundreds of AI-generated frames at a similar rate to the RTX 4090? Yeah, that's a darn tempting package if those claims prove out\u2014and one that AMD seems destined to struggle to beat.\n\n(Image credit: AMD)\n\nWe don't really know yet what the red team will be bringing to the table. We (and every other major outlet) received a slide presentation announcing the new cards, yet no mention was made of any of them at the briefing itself. We know they'll be mid-range equivalents, we know they'll make use of FSR 4, and we know what the cooler designs look like. Virtually everything else is still conjecture.\n\nBut unless FSR 4 pulls off the same frame-generating magic trick as DLSS 4\u2014and the pricing is ultra-competitive\u2014these new offerings look set to be a hard sell for any gamer in the face of Nvidia's claims. Even for me, sitting over here, willing AMD to provide some meaningful competition.\n\nThere's still a lot yet that might change my mind. For example, in Dave's RTX 5080 testing, he found that Multi Frame Generation struggles with latency when really pushed. Force the RTX 5080 down to 25 fps raw performance levels and the added latency becomes noticeable, which doesn't bode well for the much-less-powerful RTX 5070 being pushed into similar territory.\n\nWe'll find out just what that's like in person when our test sample arrives. The same applies to the RX 9070 and RX 9070 XT for that matter, as all is currently quiet on that front\u2014other than suggestions they'll be here in March. But again, unless the price is phenomenally cheaper than the Nvidia equivalent (or AMD pulls an upscaling rabbit out of a hat), it really does seem like the RTX 50-series stands a good chance of dominating this generation.\n\nFor what it's worth, I hope that's not the case. I hope FSR 4 is great, the RX 9070-series is impressive, the prices are low, and this article can be shoved back in my face. Competition is a good thing, and one company completely dominating all the others benefits no-one.\n\nAs things currently stand, though, when it comes to my own cash, the RTX 5070 looks like the most likely candidate for my next GPU purchase. Well, after I've convinced my partner I need a new graphics card and we don't need to be saving for a house quite as hard as we currently are.\n\nAh, damn it. I'm going to be stuck with the RX 7800 XT and old-school FSR forever, aren't I?",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Where to Preorder the Nvidia GeForce RTX 5090 and RTX 5080 Graphics Cards",
            "link": "https://nordic.ign.com/nvidia-rtx-5080/90895/deal/where-to-preorder-the-nvidia-geforce-rtx-5090-and-rtx-5080-graphics-cards",
            "snippet": "The wait is finally over. The first wave of Nvidia GeForce RTX 50-series graphics cards is up for preorder starting 6am PT on January 30.",
            "score": 0.9488471746444702,
            "sentiment": null,
            "probability": null,
            "content": "Related reads: Amazon Has The New GeForce RTX 5080 Prebuilt Gaming PCs Up for Preorder\n\nThe wait is finally over. The first wave of Nvidia GeForce RTX 50-series graphics cards is up for preorder starting 6am PT on January 30. First to be released are the two highest-end GPUs in the stack: the RTX 5090 and RTX 5080. The mid-range RTX 5070 and 5070 Ti cards will be released later in February.\n\nQuick Links: RTX 5090 and RTX 5080 Graphics Card Listings\n\nUpdate: Most retailers have already sold out.\n\nHow much will the new Nvidia RTX 50-series GPU cost?\n\nAlthough only the RTX 5090 and 5080 will launch in January, Nvidia has revealed pricing on all four known models:\n\nRTX 5090 - $1,999\n\nRTX 5080 - $999\n\nRTX 5070 Ti - $749\n\nRTX 5070 - $549\n\nWhere should I preorder a RTX 5090 or 5080 GPU?\n\nThe retailers above have listed various RTX 5090 and RTX 5080 graphics cards for sale on their sites; however, there's no way to tell exactly how many units each retailer has available for purchase. If you want to maximize your chance of picking up one of the RTX 50-series cards on launch day, your best bet is to check each and every one and buy the first card that lets you proceed all the way through checkout. Potentially thousands of other people will be clicking on the same link as you, so even if you see a GPU in stock, you might not be able to purchase it. As someone trying to pick up an RTX 5090 myself, I break down the benefits of checking different retailers below.\n\nDespite its power, the RTX 5090 FE boasts a slim 2-slot cooling solution that's smaller than the 4090.\n\nBest Buy\n\nBased on past Nvidia GPU launches, I suspect that one of your best chances of purchasing an RTX 5090 or RTX 5080 online \u2013 especially the Founder's Edition \u2013 will be at Best Buy. Although this retailer has currently listed only FE and Gigabyte graphics cards, it's a fairly safe bet that more cards from Asus, MSI, and Zotac will eventually be added.\n\nBest Buy's available inventory depends on your region; if you're fortunate enough to live in an area where the 5090 or 5080 might not be in high demand, you'll have a much better chance of scoring a GPU here than anywhere else. Also important to note is that Best Buy GPU preorders generally ship out (or are available for in-store pickup) very quickly. That means you'll likely receive your card in hand within a few days intead of weeks or even months.\n\nNewegg\n\nYou'll also definitely want to prioritize Newegg. This retailer stocks the widest selection of AIB 5090 and 5080 graphics cards, which are non-reference GPU models from third-party manufacturers. Currently, there are a total of 43 different models listed on the page including every model from Asus, MSI, Gigabyte, and Zotac (although there's no guarantee that most of these are in stock). Newegg is an authorized reseller, so as long as you are buying from Newegg direct and not a marketplace vendor, you'll be eligible for the full manufacturer's warranty.\n\nEven if the standalone cards are sold out, Newegg sells both the 5080 and 5090 as part of bundle kits that include any combination of a motherboard, CPU, RAM, or power supply. If you're working on a DIY computer build and haven't purchased all of your components, this might work to your advantage. These bundle offers do eventually sell out, but at a slower rate than the standalone cards.\n\nNvidia Store\n\nIt might make sense that the Nvidia Store should be your first destination to securing a RTX 5090 or 5080. Unfortunately, given the fact that the Founder's Edition is usually the most sought after version, you'll almost never find anything in stock here. If you're a loyal Nvidia customer, though, there's some hope. For example, in the case with the RTX 40-series cards, Nvidia sent out special invites to people who had registered Nvidia GPUs in the past, or who had signed up for notifications through the GeForce Experience or Nvidia app. That might be the case again this year.\n\nAmazon\n\nAmazon is the one-stop shop for just about everything, including Nvidia GPUs. Unfortunately, manually looking for one of these GPUs on Amazon's site is quite difficult. Amazon doesn't have a general page that lists all of the available video cards, and the search results are riddled with postings from marketplace vendors charging outrageous prices or, even worse, trying to swindle unsuspecting shoppers. To make things easier, I'll add specific GPU models when I see them available, but always make sure that the vendor is Amazon direct and not anyone else.\n\nAdorama\n\nAdorama has listed AIB RTX 5080 and 5090 GPUs up for preorder, alongside several RTX 5080 and 5090 pre-built gaming PCs. Keep in mind that Adorama is more lenient with its preorder queue than other vendors like Best Buy, Newegg, and Amazon \u2013 there have been instances where customers have waited months for their GPU to ship out. Even if you secure a preorder here, I'd suggest keeping your eye out at other retailers in case you find another one that will arrive sooner.\n\nAdorama is mostly known for their camera sales and printing services, but this is a legitimate online retailer with a retail store in NYC and is an authorized Nvidia reseller.\n\nB&H Photo\n\nB&H Photo currently lists a total of 17 RTX 5090 GPUs and 21 RTX 5080 GPUs on-site. Like Adorama, preorders might take a long time to ship, so even after you've secured a preorder, I would still recommend looking at other retailers who might be able to deliver your GPU sooner.\n\nB&H Photo is a retailer very similar to Adorama. It had its roots in camera and print and has sinced branched out from its NYC superstore to become a very large ecommerce storefront. It is also an authorized Nvidia reseller.\n\nMicro Center (In-Store)\n\nWithout a doubt, Micro Center will be your best chance at getting an RTX 5090 or 5080 on launch day if you happen to live close by. Micro Center will most assuredly reserve all their GPU sales for in-store customers. If you are local, you should still get there as early as possible. Some determined souls have already started camping out in the January cold.\n\nWe reviewed the RTX 5090 FE and 5080 FE GPUs\n\nThe Nvidia 50 series GPUs were officially announced at CES 2025. The emphasis for this round of cards has been on the new and improved AI features over hardware based raster performance. DLSS 4 technology supposedly quadruples the number of frames with minimal visual compromise. These new GPUs do offer a modest performance boost, but opinions are mixed regarding their value for PC gamers relative to the previous generation RTX 40 series cards.\n\nIn our Nvidia GeForce RTX 5090 FE review, Jackie Thomas wrote that \"the Nvidia GeForce RTX 5090 has officially taken the performance crown from the RTX 4090, but with less force than previous generations. When it comes to traditional non-AI gaming performance, the RTX 5090 provides one of the smallest generational uplifts in recent memory. However, in games that support it, DLSS 4 really does deliver huge performance gains \u2013 you just have to make your peace with the fact that 75% of the frames are generated with AI.\"\n\nIn our Nvidia GeForce RTX 5080 FE review, Jackie offers a similar sentiment. \"If you already have a high-end graphics card from the last couple of years, the Nvidia GeForce RTX 5080 doesn\u2019t make a lot of sense \u2013 it just doesn\u2019t have much of a performance lead over the RTX 4080, though the extra frames from DLSS 4 Multi-Frame Generation do make things look better in games that support it. However, for gamers with an older graphics card who want a significant performance boost, the RTX 5080 absolutely provides \u2013 doubly so if you\u2019re comfortable with Nvidia\u2019s AI goodies.\"\n\nWhy Should You Trust IGN's Deals Team? IGN's deals team has a combined 30+ years of experience finding the best discounts in gaming, tech, and just about every other category. We don't try to trick our readers into buying things they don't need at prices that aren't worth buying something at. Our ultimate goal is to surface the best possible deals from brands we trust and our editorial team has personal experience with. You can check out our deals standards here for more information on our process, or keep up with the latest deals we find on IGN's Deals account on Twitter.\n\nEric Song is the IGN commerce manager in charge of finding the best gaming and tech deals every day. When Eric isn't hunting for deals for other people at work, he's hunting for deals for himself during his free time.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "DeepSeek Creates Buying Opportunity for Nvidia Stock",
            "link": "https://io-fund.com/artificial-intelligence/semiconductors/deepseek-creates-buying-opportunity-for-nvidia-stock",
            "snippet": "DeepSeek shook the market to start the week, sending AI heavyweight Nvidia down 17% on Monday, wiping out $600 billion in market cap, while other AI...",
            "score": 0.9066503047943115,
            "sentiment": null,
            "probability": null,
            "content": "DeepSeek caused a deep rout in AI stocks earlier this week with Nvidia erasing more than $600 billion in value; the biggest one-day loss of any company in history. The R&D company out of China stated the model cost $6 million to train, which sent the market into a panic as this is pennies to the dollar compared to what Big Tech is spending. The jerk-reaction readthrough was that, in the blink of an eye, DeepSeek had fundamentally rewritten the AI capex story.\n\nThe battle between the United States and China on large language models (LLMs) following DeepSeek\u2019s challenge to OpenAI has been called AI\u2019s Sputnik moment. The most important takeaway for investors about this analogy is that Sputnik spurred massive investments. It was not the final destination, rather, it was the beginning of a multi-decade space race. The Sputnik satellite cost $15 to $20 million or $33 million with cost-adjusted inflation, yet the United States would spend an estimated $1 trillion over a sixty year period in response.\n\nThe United States takes it quite seriously to stay in the lead, and AI will spur an arms race unlike anything the world has seen before. Consider that it took sixty years for the government to spend $1 trillion (inflation adjusted) on the space race, yet in one sweeping piece of legislation, the United States will spend $500 billion in 5 years on AI infrastructure, up from $70 billion being spent in 2024 alone.\n\nRemember, it was the market\u2019s so-called \u201cefficiency\u201d that caused Nvidia\u2019s stock to drop 60% from a gaming-related miss following rumors that Ethereum\u2019s merge to Proof-of-Stake (PoS) would be the death knell for the stock. This was the very moment the powerful AI GPUs called Hopper were shipping, equipped with a Transformer Engine that would enable self-learning models, and change the world as we know it.\n\nTech is defined by disruption, by thousands of product announcements, and by leagues of competitors. It can be a noisy and costly sector when investors get whiplashed by the news of the day. My firm has an enviable track record on Nvidia \u2013 including speaking out during staggering selloffs or supply chain rumors. This includes also telling you when we are not buying, or when a stock is frothy, such as when Nvidia was trading in the $140s. We also go to great lengths to tell you when we plan to buy again. You will find dozens (perhaps hundreds) of articles on DeepSeek at this point; yet you will be hard pressed to find one other person helping investors navigate Nvidia\u2019s stock at this granular level.\n\nBelow, I provide evidence that DeepSeek is not the black swan that killed Nvidia overnight \u2013 in fact, driving down the costs of AI development has always been the plan -- and will ultimately boost Nvidia\u2019s sales in the long run as AI will leave the data center, and move on-premise for enterprises and on-device for consumers.\n\nI also touch base on what investors should keep an eye on price-wise moving forward for the GPU juggernaut.\n\nDeepSeek\u2019s DualPipe Algorithm\n\nDeepSeek\u2019s DualPipe Algorithm optimized pipeline parallelism, which essentially reduces inefficiencies in how GPU nodes communicate and how mixture of experts (MoE) is leveraged. MoE refers to distributing a computational load across \u201cmultiple experts\u201d (or neural networks) to train across thousands of GPUs using what is called model and pipeline parallelism. This enables more compute-efficient training yet the parameters still need to be loaded in VRAM, so the memory requirements remain high.\n\nTom\u2019s Hardware wrote an article about this a month ago, with an usually prescient title: \u201cChinese AI company says breakthroughs enabled creating a leading-edge AI model with 11X less compute \u2014 DeepSeek's optimizations could highlight limits of US sanctions.\u201d The article stated: \u201cThe DualPipe algorithm minimized training bottlenecks, particularly for the cross-node expert parallelism required by the MoE architecture, and this optimization allowed the cluster to process 14.8 trillion tokens during pre-training with near-zero communication overhead.\u201d\n\nBy allowing the routing of tokens to experts and the aggregation of results to be handled in parallel through code called PTX (Parallel Thread Execution), DualPipe helped to drive down costs. The software essentially optimized the hardware. The company also created a 4-node maximum to limit nodes and reduce traffic, allowing for a more efficient communication framework.\n\nMoE models like DeepSeek\u2019s can provide numerous benefits, and this is what DeepSeek is showing \u2013 an ability to train larger models at a lower cost with much faster pre-training, faster inference, and an ability to deliver decreased first-token latency. However, MoE also can require higher VRAM to store all experts simultaneously and can face challenges in fine-tuning.\n\nMixed Point Precision and Multi-Head Latent Attention Lowers Memory Usage\n\nDeepSeek\u2019s success is also found in lowering memory usage with multi-head latent attention that lowered memory usage to 5% to 13%. MLA ultimately reduces memory requirements during inference by processing long sequences of text. As pointed out by ML Engineer Zain ul Abideen, \u201cMLA achieves superior performance than MHA, as well as significantly reduces KV-cache boosting inference efficiency.\u201d\n\nIt has been estimated that HBM3e\u2019s component costs in Hopper GPUs could be as much as 25% higher than HBM3-equipped GPUs, and it\u2019s expected HBM4 will add more costs due to the complexities of delivering faster data rates.\n\nMemory is an expensive component and Hopper is known for its limited memory capacity at 80GB of HBM3e memory versus Blackwell\u2019s 192GB of HBM3e (nearly 2.5X the memory in the upcoming release). Therefore, reducing memory usage is one path to optimizing Hopper GPUs.\n\nDeepSeek\u2019s success also stemmed from its pioneering approach to model architecture. The company introduced a novel MLA (multi-head latent attention) method that lowers memory usage to just 5%\u201313% of what the more common MHA architecture consumes.\n\nNvidia\u2019s hardware excellence stands out in the Hopper generation of GPUs with the Transformer Engine. Two years ago, Hopper\u2019s transformer engine brought about Chat-GPT\u2019s big moment as the OpenAI model eliminated the need to find patterns between elements mathematically, and this opens up which datasets can be used and how quickly.\n\nThe H100s also leverage the transformer engine for mixed precision, such as FP8, FP16 or FP32, depending on the workload. Nvidia architected the ability to switch between floating precision points in order to require less memory usage. Here is what Nvidia states:\n\n\u201cThere are numerous benefits to using numerical formats with lower precision than 32-bit floating point. First, they require less memory, enabling the training and deployment of larger neural networks. Second, they require less memory bandwidth which speeds up data transfer operations. Third, math operations run much faster in reduced precision, especially on GPUs with Tensor Core support for that precision. Mixed precision training achieves all these benefits while ensuring that no task-specific accuracy is lost compared to full precision training. It does so by identifying the steps that require full precision and using 32-bit floating point for only those steps while using 16-bit floating point everywhere else.\u201d\n\nDeepSeek says that FP8 allowed it to \u201cachieve both accelerated training and reduced GPU memory usage,\u201d as it validated FP8\u2019s usage for training large scale models for a fraction of the cost. A majority of the \u201cmost compute-density operations are conducted in FP8, while a few key operations are strategically maintained in their original data formats,\u201d such as those that require higher precision due to sensitivity reasons.\n\nThough lower-precision training has often been \u201climited by the presence of outliers in activations, weights, and gradients,\u201d and tests have shown that FP8 training was prone to higher instability and more frequent loss spikes, it is now emerging as a solution for efficient training due to hardware advancements (i.e., Hopper bringing powerful FP8 support, Blackwell bringing FP4).\n\nDeepSeek also provided recommendations for future chips to accommodate low-precision training and replicate this at scale, suggesting chip designs should \u201cincrease accumulation precision in Tensor Cores to support full-precision accumulation, or select an appropriate accumulation bit-width according to the accuracy requirements of training and inference algorithms.\u201d\n\nThis is what Blackwell was designed to address, with new precisions in Tensor Cores, FP4 precision, increased SM count, and more CUDA cores versus the Hopper. Blackwell also packs 208 billion transistors to provide up to 20 petaflops of FP4, compared to the H100\u2019s 4 petaflops of FP8. The B200 features a second-generation transformer engine supporting 4-bit floating point (FP4), with the goal of doubling the performance and size of models the memory can support while maintaining accuracy.\n\nTo simply recreate DeepSeek\u2019s training efficiencies and develop large-scale models, Hopper GPUs are a requirement due to support for FP8, with Blackwell bringing FP4 to power real-time inference and supercharged training for trillion parameter models.\n\nUnderstanding the nuances of Nvidia\u2019s hardware is the reason that I first called out Nvidia\u2019s AI GPU thesis and CUDA moat in late 2018, and in 2019, Volta\u2019s AI capabilities prompted me to say on my premium stock research site: \u201cI believe Nvidia will be one of the world\u2019s most valuable companies by 2030.\u201d This has led to potential gains of over 4,000% for our free readers.\n\nBlackwell is Not Hopper\n\nThis may seem like a moment where AI software is triumphant, yet we are at the end of the Hopper generation with the H100s (and the more restricted H800) GPUs being available for two years now. Two years is eternity in the AI arms race, and the fact Hopper is reaching a point of peak optimization at the very moment that Blackwell is shipping is not a shocking new revelation --- rather, it\u2019s the point of keeping a fast-paced product road map. Per Nvidia\u2019s Computex keynote, from Pascal to Blackwell, their AI systems will deliver \u201c1,000 times increase in AI compute,\u201d while simultaneously decreasing the \u201cenergy per token by 45,000X.\n\nTherefore, the market is a bit confused to think the 11X increase in compute from software optimizations is going to catch Nvidia off guard. Below are the stated differences between the H100 and GB200 NVL72 systems on Mixture of Experts (MoE) real-time throughput and training speeds.\n\nDeepSeek acquiesced the limitations they faced in deploying the model is \u201cexpected to be naturally addressed with the development of more advanced hardware.\u201d Note, they are not saying with the development of more advanced software.\n\nI made the point nearly a year ago that Nvidia is competing with Nvidia with its one-year product release road map stating: \u201cThe product road map is the single most important thing investors should be focused on. A good chunk of the AI accelerator story is understood at this point. What is not understood is how aggressive Nvidia is becoming by speeding up to a one-year release cycle for its next generation of GPUs instead of a two-year release cycle.\"\n\nIn addition, by open sourcing the model, there will be more developers who can build new AI capabilities. As stated in a Predibase analysis, there were 500 derivative models of DeepSeek created in a few days\u2019 time.\n\nNvidia has been early to this eventual outcome with the launch of Project Digits, a $3,000 supercomputer that can run 200B-parameter models. By releasing powerful personal computers, Nvidia seeks the proliferation of its GPUs \u2013 much like Apple\u2019s iPhone -- whereas companies like OpenAI are the ones most challenged by an open source LLM that drives down input token and output token costs that are 27X less expensive than OpenAI\u2019s o1 model.\n\nBlackwell Inches United States Toward General Artificial Intelligence (AGI)\n\nThe reason that software has not officially begun to commoditize hardware, and we could be as far as 5-10 years away from this moment, is because AI development is incredibly nascent. Blackwell and future generations of GPUs are a necessity for AI development to inch closer to the start of general artificial intelligence (AGI).\n\nThere have been discussions questioning if it is possible to reach AGI with reinforcement learning: \u201cartificial general intelligence can be achieved if an agent tries to maximize a reward in a complex environment because the complexity of the environment will force the agent to learn complex abilities like; social intelligence, language, etc.\u201d\n\nReinforcement learning is an ML method where an agent or model learns to make decisions through interactions in its environment, via rewards or punishments. Agents will interact with the environment, receive a positive or negative reward, and adjust its decisions/actions based on the feedback it has received.\n\nAGI refers to the creation of a machine that is capable of performing intellectual tasks on par with humans, and have the ability to understand, learn and apply knowledge to a wide range of domains. Both RL and AGI involve learning from interactions with the environment, though RL is typically more focused on specific tasks or environments where AGI aims to be \u2018all-encompassing.\u2019\n\nIf software efficiencies from China are relatable to Sputnik, then the arrival of AGI will be the moment we land on the moon. AGI requires an order of magnitude larger models \u2013 minimum 1 trillion, up to 10 trillion or more. Reinforcement learning is certainly a step in the right direction, yet trillion+ parameter models are inevitable \u2013 and it\u2019ll require Nvidia and other AI accelerator design companies to get there.\n\nSource: DeepSeek R-1 Pictured Above: Nvidia Stock saw its market cap shed $600 billion in one day following DeepSeek\u2019s release with benchmarks surpassing OpenAI on its performance and reasoning abilities.\n\nDeepSeek is Cheap \u2026 Or is it?\n\nDeepSeek has stated V3 was trained on 14.8 trillion tokens in pre-training, with each 1 trillion tokens taking 180K H800 GPU hours (3.7 days) on its 2,048 H800 cluster. Compare this to Meta\u2019s 405 billion parameter Llama 3.1 model, which was trained in 54 days (30.8M GPU hours) on a 16,384 H100 GPU cluster, estimated to cost ~$80 million at a $2.6/hour rental price.\n\nThis accounted for a majority of the costs at $5.328 million, assuming a $2/hour rental price for the H800s (this is about in-line with long-term contract rates at Lambda for the H100, but ~20% lower than short-term costs from other startup cloud providers).\n\nThe estimated $6 million cost for DeepSeek would also be pennies on the dollar compared to estimated training costs for OpenAI\u2019s GPT-4 and Alphabet\u2019s Gemini Ultra.\n\nSource: DeepSeek\n\nWhile the training costs were estimated utilizing rental prices (and up for debate), what\u2019s important to note is that the $5.6 million cost only excluded \u201ccosts associated with prior research and ablation experiments on architectures, algorithms, or data.\u201d\n\nDemocratization of AI Helps Nvidia\n\nNvidia\u2019s goal is to not only sell GPUs to the fortresses of Big Tech. Rather, all tech companies seek large addressable markets and the democratization of AI will assist Nvidia in reaching worldwide AI device ubiquity.\n\nIn the meantime, Blackwell is sold out. Microsoft stated this week that they are still supply constrained in the cloud, and Azure needs more supply to grow. Meta doubled down on its planned capex for 2025, and signaled a willingness to spend \u201chundreds of billions\u201d towards AI infrastructure in the long run.\n\nIn the medium term, Nvidia will allocate supply to enterprises and edge AI, as lower costs will facilitate enterprise AI and edge computing. The market grew concerned that there would not be ROI on these large AI investments, and on the other hand, the market now is panicking when costs are lowered to the point where ROI can be achieved.\n\nThis has always been Nvidia\u2019s goal, with CEO Jensen Huang and other executives declaring at high profile stages such as CES and GTC that the cost of computing will go down with each generation of GPUs. At GTC 2024, Huang explained that Nvidia \u201caccelerated algorithms so quickly that the marginal cost of computing has declined so tremendously over the last decade that it enabled generative AI to emerge. He further explained that reducing the cost of computing and accelerating computing is what Nvidia \u201cdoes for a living at its core,\u201d and that the \u201cpricing that we create always starts from TCO.\u201d\n\nNvidia VP Ian Buck corroborated this at BofA GTC Conference in June 2024: \u201cThe opportunity here is to help [customers] get the maximum performance through a fixed megawatt data center and at the best possible cost and optimized for cost.\u201d\n\nSign up for I/O Fund's free newsletter with gains of up to 2600% because of Nvidia's epic run - Click here\n\nWhat to Monitor for Nvidia\u2019s Earnings Report\n\nThere seems to be no end to the amount of Blackwell supply rumors with yet another one circulated in January 2025. According to Taiwan Semiconductor\u2019s management team, \u201ccut the order, that won\u2019t happen. Actually [it will] continue to increase.\u201d\n\nHere are the additional points that cause us to believe Blackwell revenue will show up soon enough:\n\n1) Nvidia CEO Jensen Huang at CES reconfirmed that Blackwell is in full production, adding that \u201cevery single cloud service provider now has systems up and running.\"\n\n2) CFO Colette Kress in Q3\u2019s earnings: \u201cBlackwell demand is staggering and we are racing to scale supply to meet the incredible demand customers are placing on us. Customers are gearing up to deploy Blackwell at scale.\u201d\n\n3) CFO Colette Kress in Q3: \u201cWhile demand is greatly exceeding supply, we are on track to exceed our previous Blackwell revenue estimate of several billion dollars as our visibility into supply continues to increase.\u201d\n\n4) CFO Colette Kress at UBS\u2019 Global Technology Conference: \u201cWhat we see in terms of our Blackwell, which will be here this quarter is also probably a supply constraint that is going to take us well into our next fiscal year for several quarters from now. So, no, we don't see a slowdown. We continue to see tremendous demand and interest, particularly for our new architecture that's coming out.\u201d UBS Analyst Tim Arcuri added, \u201cyou are actually shipping more Blackwell than you thought you would three months ago.\u201d\n\n5) CFO Colette Kress at CES: \u201cWe are able to increase our demand and increase our revenue each quarter as well. \u2026 When we think about the demand that is in front of us, it is absolutely a growth year.\u201d\n\nBig Tech\u2019s Capex\n\nTwo weeks ago, my firm covered how Big Tech capex (AI spending) came in significantly above expectations in 2024, and is on track to repeat that in 2025. To recap, analysts had initially estimated capex of ~$200 billion for Big Tech in 2024, an increase of ~30% YoY. However, our latest checks suggest that Big Tech is on track to spend at least $236 billion in capex in 2024, 18% higher than analyst estimates and representing YoY growth of more than 52%.\n\nFor 2025, Big Tech is on track to spend $300 billion or more on capex, based on initial commitments from Microsoft and Meta totaling at least $140 billion combined. This is already tracking ~7% higher than ~$280 billion estimated by analysts, with Microsoft\u2019s $80 billion commitment 40% higher than estimated and Meta\u2019s $60-65 billion more than 18% higher than estimated.\n\nDeepSeek has highlighted one major tailwind that has been overlooked \u2013 if AI models can now be trained quicker and cheaper, it\u2019s likely to catalyze demand for GPU instances in the cloud from leading providers, as GPUs can be rented much faster than setting up new infrastructure. In order to meet elevated demand, hyperscalers will need to continue purchasing, or even accelerate purchasing, Nvidia\u2019s GPUs in order to prevent chip constraints from impeding revenue growth.\n\nAnalyst Revisions for Nvidia\u2019s Stock Remain Unchanged\n\nIn June 2024, in the analysis Here's Why Nvidia Stock Will Reach $10 Trillion Market Cap By 2030, I discussed the importance of intra-quarter analyst revisions supporting Nvidia\u2019s massive run, as data center revenue continued to blow past expectations.\n\nSeen below, Nvidia\u2019s revenue for FY25 was estimated at $120 billion in June, being revised 31.5% higher over the six months prior. In dollar terms, that represented a nearly $29 billion increase from $91.3 billion. For FY26, revenue was estimated at $157.5 billion in June, a 44.8% increase from $108.8 billion, a nearly $50 billion increase in six months.\n\nCompare that to today:\n\nFY25 revisions are up just 7.6% (or $9 billion) since July to $129.2 billion as the fiscal year comes to a close, but FY26 revenue is 20% higher to $196.5 billion. That\u2019s another $39.5 billion (or over a full quarter at the current run rate) that has been added in the past seven months.\n\nOpinions aside, intra-quarter revisions will be where you will first see any material hiccups or impacts on revenue. FY26 quarterly revisions have come down slightly over the past month, at -1.3% lower for Q4. So far, there has been no impact from DeepSeek\u2019s V3 release \u2013 in fact, estimates inched marginally higher on January 28, with Q4 FY26 revenue rising from $55.49 billion to $55.51 billion.\n\nThe bigger picture \u2013 revenue is still 14% to 24% higher than it was six months ago. At this scale, that\u2019s $6 to $10-billion-plus higher.\n\nWhere Nvidia\u2019s Stock Price Goes Next\n\nLet\u2019s talk price targets.\n\nIn the write-up \u201cWhere I Plan to Buy Nvidia Stock Next\u201d we stated that Nvidia still needs, at least, one more push higher to complete the current uptrend. This would make the volatility that started in June of 2024 a correction within a larger uptrend.\n\nWe still believe that NVDA\u2019s uptrend is not over. We stand by the two scenarios outlined in the last report; both are still in play.\n\n\u201cBlue \u2013 The final 5th wave is playing out as an ending diagonal pattern, which is common for 5th waves. This type of pattern is a 5 wave pattern in itself that is characterized with large swings in both directions. Our target zone for the bottom on this 4th wave is $126 - $116. If Nvidia can push over $140.75, then then odds favor this scenario.\n\nRed \u2013 Nvidia is in a much more complex 4th wave. If this is playing out, NVDA would see the $116 level break, which opens the door to a potential low at $101, $90, or $78.\u201d\n\nWhile the $116 support level still holds from our last analysis, the resistance worth monitoring is now $132. If Nvidia can breakout over the $132 resistance, it will shift the odds to the more immediately bullish count in blue. This would see a push into the $170 - $190 region over the coming months.\n\nOn the other hand, if Nvidia breaks below the $116 support level, it will signal that the more immediately bearish count in red is playing out. This would see us make a meaningful low in the $102 - $83 range over the coming months.\n\nWe began executing on our current buy plan, which is to layer into Nvidia at key levels. The $126 - $116 region was our first target. If the $116 region breaks, we will then target the $102 - $83 region to complete our buying. Considering the blue count is looking for a final 5th wave higher, we will likely not chase a breakout over $132.\n\nConclusion\n\nIf DeepSeek\u2019s breakthroughs are truly the key to ushering in a new paradigm of AI training and ultimately AI democratization from cost reductions, it will not be a death sentence for Nvidia; in fact, quite the opposite.\n\nThis is Jevons paradox -- where the technological advancements of Hopper and Blackwell will translate into significant efficiency gains and cost reductions for AI training, that then will drive near-ubiquity for AI services -- and thus increase demand for GPUs in the data center, on-premise for enterprises and also on edge devices.\n\nThe market\u2019s readthrough is that Big Tech has now been overspending on AI. However, The I/O Fund believes this readthrough is wrong; it\u2019s not that the United States is overspending, it\u2019s that we will accelerate spending to stay ahead. The I/O Fund recently entered five new small and mid-cap positions that we believe will be beneficiaries of this AI spending war. We discuss entries, exits and what to expect from the broad market every Thursday at 4:30 p.m. in our 1-hour webinar. Learn more here.\n\nDisclaimer: This is not financial advice. Please consult with your financial advisor in regards to any stocks you buy.\n\nRecommended Reading:",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia GeForce RTX 5080 Founders Edition Review",
            "link": "https://www.pcmag.com/reviews/nvidia-geforce-rtx-5080-founders-edition",
            "snippet": "Nvidia's GeForce RTX 5080 graphics card outruns its like-priced competitors and predecessors in many popular games. Its potent AI hardware and cutting-edge...",
            "score": 0.9413245320320129,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Not just DeepSeek: Here's why Nvidia stock hasn't recovered",
            "link": "https://mashable.com/article/why-nvidia-stock-is-down-rtx-deepseek-trump",
            "snippet": "Nvidia had a bad week. Here are some reasons why its stock hasn't recovered, from DeepSeek RTX series delays.",
            "score": 0.9092592000961304,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia had a bad week. Here are some reasons why its stock hasn't recovered.\n\nNvidia had a bad week. Here are some reasons why its stock hasn't recovered. Credit: Dominika Zarzycka/NurPhoto via Getty Images\n\nThe stock market is unpredictable. Sometimes, bad news boosts a stock, while good earnings send it tumbling. That's just how the market works.\n\nTake Nvidia, for example. The company is still reeling from the AI sector's shake-up after DeepSeek\u2019s latest model release earlier this week. Even today, as Nvidia launches its highly anticipated RTX 5090 and 5080 graphics cards, its stock continues to slide.\n\nWhy could this be happening? Let's take a look.\n\nNvidia RTX series delays\n\nThere is a strong demand online for Nvidia's new RTX 50 series graphics cards, but that doesn't necessarily translate to big sales. That's because Nvidia cannot keep store shelves stocked.\n\nMany online retailers sold out of their supply within minutes. Scalpers are now selling Nvidia's graphic cards for a premium on the aftermarkets. Some retailers are informing customers to expect months of delays and backorders of the RTX 5090 and 5080.\n\nWhile the demand is there, it's clear that's not the sole reason why Nvidia's RTX 50 series is impossible to find in stores. Many retailers received fairly low stock quantities, as Nvidia reportedly experienced manufacturing issues.\n\nOverall, these issues may not be a reason for a stock to fall. But they do point to why Nvidia couldn't depend on the RTX 50 series release to help too much in its recovery.\n\nMashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Loading... Sign Me Up By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy Thanks for signing up!\n\nDeepSeek's looming threat\n\nU.S.-based AI companies like OpenAI and Nvidia are still reeling from the China-based startup's release of the DeepSeek-R1 AI model.\n\nDeepSeek reportedly created an AI model that's at least on par with OpenAI's latest model. In addition, they say they did it with fewer resources in processing power and funding. It reportedly cost DeepSeek less than $6 million to create a model that OpenAI spent hundreds of millions on.\n\nFew companies have benefitted from the AI boom in the U.S. than Nvidia, which supplies U.S. tech companies with processing power to build their AI models.\n\nAlthough DeepSeek has done its damage, the looming threat is far from over. DeepSeek has released additional AI models for AI-generated images and is developing even more advanced models.\n\nIn addition, other China-based companies, such as Alibaba, have announced their own advanced AI models that are supposedly even more powerful than those already available.\n\nTrump's Taiwan tariff threat\n\nDeepSeek wasn't the only thing that hit Nvidia this week.\n\nIn a speech earlier this week, President Donald Trump announced a plan intended to move computer chip manufacturing from Taiwan to the U.S. What's the plan? Tariffs.\n\n\"In the very near future, we\u2019re going to be placing tariffs on foreign production of computer chips, semiconductors, and pharmaceuticals to return production of these essential goods to the United States,\" Trump said in a speech.\n\n\"They left us and went to Taiwan,\" Trump said, referring to companies like Apple, Qualcomm, and Nvidia, which manufacture their chips in Taiwan.\n\nTrump said the tariff could be as high as 100 percent. Trump's proposed tariffs could cause the price of these products to skyrocket for U.S. customers. In turn, companies like Nvidia would likely sell fewer products or make a smaller profit to compensate for the increased cost passed on to consumers.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "DeepSeek Means The End Of Big Data, Not The End Of Nvidia",
            "link": "https://www.forbes.com/sites/gilpress/2025/01/30/deepseek-means-the-end-of-big-data-not-the-end-of-nvidia/",
            "snippet": "DeepSeek spells the end of the dominance of Big Data and Big AI, not the end of Nvidia. Its focus on efficiency jump-starts the race for small AI models...",
            "score": 0.5498179793357849,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Chinese algorithm boosts Nvidia GPU performance 800-fold in science computing",
            "link": "https://www.scmp.com/news/china/science/article/3296135/chinese-algorithm-boosts-nvidia-gpu-performance-800-fold-science-computing",
            "snippet": "A breakthrough by Chinese researchers could help solve complex problems in industries ranging from aerospace to bridge design.",
            "score": 0.9480929374694824,
            "sentiment": null,
            "probability": null,
            "content": "A high-performance algorithm that could solve complicated material design problems on consumer GPUs has been developed by Chinese researchers, achieving a groundbreaking 800-fold increase in speed over traditional methods.\n\nAdvertisement\n\nDeveloped by a research team at Shenzhen MSU-BIT University, co-founded by Lomonosov Moscow State University and Beijing Institute of Technology, the new algorithm enhances the computational efficiency of peridynamics (PD), a cutting edge, non-local theory that solves difficult physical issues such as cracks, damage and fractures.\n\nIt opens up new possibilities for solving complex mechanical problems across various industries, including aerospace and military applications, on widely available chips that are low-cost and not subject to US sanctions\n\nPeridynamics has proven advantageous in modelling material damage, but its high computational complexity has traditionally made large-scale simulations inefficient, with issues such as high memory usage and slow processing speeds.\n\nTo address these challenges, Yang Yang, an associate professor, leveraged Nvidia\u2019s CUDA programming technology to create the PD-General framework. By making an in-depth analysis of the chip\u2019s unique structure, her team optimised algorithm design and memory management that led to a remarkable performance boost. Their research was published in the Chinese Journal of Computational Mechanics on January 8.\n\nAdvertisement\n\n\u201cThis efficient computational power allows researchers to reduce calculations that would typically take days to just a few hours \u2013 or even minutes \u2013 using an ordinary home-level GPU, which is a significant advancement for PD research,\u201d Yang wrote in the paper.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "GeForce RTX 5090 & 5080 GeForce Game Ready Driver Also Includes Support For DLSS 4, New NVIDIA App Features, And RTX Game Updates",
            "link": "https://www.nvidia.com/en-us/geforce/news/geforce-rtx-5090-5080-dlss-4-game-ready-driver/",
            "snippet": "The GeForce RTX 5090 and GeForce RTX 5080 are out now, and owners will need to download and install our new GeForce Game Ready Driver from NVIDIA app or...",
            "score": 0.8831009864807129,
            "sentiment": null,
            "probability": null,
            "content": "The GeForce RTX 5090 and GeForce RTX 5080 are out now, and owners will need to download and install our new GeForce Game Ready Driver from NVIDIA app or GeForce.com.\n\nOur new driver also adds support for new technologies, RTX game updates, new NVIDIA app features, and more:\n\nDLSS 4 : New AI innovations further enhance image quality, and accelerate performance. And DLSS Multi Frame Generation , in conjunction with the suite of other DLSS technologies, multiplies frame rates by up to 8X on GeForce RTX 50 Series GPUs\n\n: New AI innovations further enhance image quality, and accelerate performance. And , in conjunction with the suite of other DLSS technologies, multiplies frame rates by up to 8X on GeForce RTX 50 Series GPUs DLSS 4 Overrides : Upgrade over 75 games and apps to DLSS 4 with Multi Frame Generation using a new NVIDIA app feature\n\n: Upgrade over 75 games and apps to DLSS 4 with Multi Frame Generation using a new NVIDIA app feature NVIDIA App: RTX Video Super Resolution optimizations and enhancements, new multiple display settings, Advanced Optimus support, and more are introduced in a new NVIDIA app update\n\nRTX Video Super Resolution optimizations and enhancements, new multiple display settings, Advanced Optimus support, and more are introduced in a new NVIDIA app update NVIDIA Broadcast : Update the popular app now to add two new AI-powered effects\n\n: Update the popular app now to add two new AI-powered effects RTX Game Updates : Get Game Ready for DLSS 4 with Multi Frame Generation, Ray Reconstruction, RTX Mega Geometry, RTX Hair, and Neural Radiance Cache, in new updates for Alan Wake 2, Cyberpunk 2077, Hogwarts Legacy, Indiana Jones and the Great Circle\u2122, and Star Wars\u2122 Outlaws\n\n: Get Game Ready for DLSS 4 with Multi Frame Generation, Ray Reconstruction, RTX Mega Geometry, RTX Hair, and Neural Radiance Cache, in new updates for Alan Wake 2, Cyberpunk 2077, Hogwarts Legacy, Indiana Jones and the Great Circle\u2122, and Star Wars\u2122 Outlaws NVIDIA Smooth Motion : A new driver-based AI model delivers smoother gameplay by inferring an additional frame between two rendered frames on GeForce RTX 50 Series GPUs\n\n: A new driver-based AI model delivers smoother gameplay by inferring an additional frame between two rendered frames on GeForce RTX 50 Series GPUs New Games : Get Game Ready for the release of Kingdom Come: Deliverance II and Marvel's Spider-Man 2, two new titles featuring DLSS technologies\n\n: Get Game Ready for the release of Kingdom Come: Deliverance II and Marvel's Spider-Man 2, two new titles featuring DLSS technologies New G-SYNC Compatible Displays: Another 19 gaming displays are now supported\n\nFor details about each of these items, read on.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia RTX 5090 and 5080 Sell Out Instantly and Scalpers Are Already Cashing in With $6,000 eBay Listings",
            "link": "https://www.ign.com/articles/nvidia-rtx-5090-and-5080-sell-out-instantly-and-scalpers-are-already-cashing-in-with-6000-ebay-listings",
            "snippet": "Looking to get your hands on Nvidia's RTX 5090 or RTX 5080? Too bad, you're already too late. Both new RTX 50-series 'Blackwell' GPUs went up for sale...",
            "score": 0.5864502787590027,
            "sentiment": null,
            "probability": null,
            "content": "Looking to get your hands on Nvidia's RTX 5090 or RTX 5080? Too bad, you\u2019re already too late. Both new RTX 50-series \u201cBlackwell\u201d GPUs went up for sale earlier today, and many went in a flash.\n\nThis wasn\u2019t entirely unexpected, as Nvidia had released a warning that stock for the new generation of graphics cards was going to be low, with manufacturer MSI even delaying the launch of the RTX 5090 until February 6. Lines had grown long at U.S. distributor Micro Center, where hopeful buyers camped out for a chance at snagging one.\n\nWith stock levels low, scalpers have overtaken eBay listings for the product, with sold listings of the RTX 5090 reaching over $6,000 , a 200% increase over the $1,999 MSRP of the 5090. Third-party manufacturers are also marking up their MSRP prices of the RTX 5090, with the Asus ROG Astral reaching $2,799, a 40% increase in asking price. This isn\u2019t uncommon for higher-end cards, but the $800 increase is a whole PS5 Pro.\n\nNvidia GeForce RTX 5090 \u2013 Photos 5 Images\n\nWorldwide, Nvidia\u2019s MSRP Founder\u2019s Edition cards, which feature an advanced split PCB, have been in demand, with different regions restocking at different times. But, while demand remains high and stock levels remain low they\u2019re nearly impossible to purchase, even for the most diehard of fans.\n\nOne user commented on the Nvidia forum : \u201cI refreshed every second since 2pm and not once did it become available to buy!!!\" Other users over on Reddit are already posting memes titled \u201c Another Launch, Another Failure \u201d after failing to buy one of the new graphics cards. \u201cPaper launch is true,\u201d claimed one Redditor, echoing the sentiment around the RTX 30-series launch, which was also in short supply in 2020.\n\nBut why there are so few RTX 5090 and RTX 5080 graphics cards in circulation? In October, it was reported that the designs of Nvidia's Blackwell-based processors were flawed. CEO Jensen Huang said: \u201cWe had a design flaw in Blackwell, it was functional, but the design flaw caused the yield to be low.\u201d Fixing this flaw in its data center chips may have affected production for the RTX 5090 and RTX 5080 consumer GPUs.\n\nNvidia GeForce RTX 5090 \u2013 Benchmarks 14 Images\n\nNvidia only has so much time booked at its chip foundry, TSMC. Nvidia\u2019s Blackwell GeForce cards are made using the same TSMC 4N node, on the same architecture as the affected boards. If yields are low, then it will have to spend more time manufacturing working chips. This low yield in its initial run of B100 and B200 boards could have had a knock-on effect on other products. As Nvidia hurried to fix the issue and make yields higher toin turn make its data center customers happy, consumer products might have been hit hard.\n\nAfter all, the Blackwell B200 chip and the RTX 50-series share the same overall manufacturing process. Nvidia\u2019s revenue from its data center customers reached a staggering $30.8 billion in Q3 2025 . By comparison, its gaming revenues sat at just $3.3 billion.\n\nHowever, the stock situation for Nvidia\u2019s consumer graphics cards will likely get better in the future. For now, Nvidia may just have to face a consumer backlash from gamers who can\u2019t buy what we called the \u201cfastest graphics card on the consumer market\u201d in our RTX 5090 review .\n\nSayem is a freelancer based in the UK, covering tech & hardware. You can get in touch with him at @sayem.zone on Bluesky.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-29": {
        "0": {
            "title": "Nvidia GeForce RTX 5080 review: More like a 4080 Super Super",
            "link": "https://arstechnica.com/gadgets/2025/01/nvidia-geforce-rtx-5080-review-more-like-a-4080-super-super/",
            "snippet": "Nvidia's GeForce RTX 4080 graphics card was faster than the RTX 3080 card it replaced. But it was also faster than the RTX 3080 Ti, 3090, and 3090 Ti.",
            "score": 0.9174866676330566,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's GeForce RTX 4080 graphics card was faster than the RTX 3080 card it replaced. But it was also faster than the RTX 3080 Ti, 3090, and 3090 Ti. One of the good things about a new graphics card generation is that the new cards bring the last generation's inaccessibly expensive high-end performance down to cards that more people can actually afford.\n\nThat's not the case with the new $999 RTX 5080, which beats the previous-generation RTX 4080 Super by a little bit and the older RTX 4080 by a slightly larger bit but doesn't come close to beating or even replicating the performance of the outgoing 4090.\n\nNvidia points to its new DLSS Multi-Frame Generation technology as a mitigating factor here, leaning on its AI-generated frames to close the gap that the 5080's raw rendering performance can't close on its own. And sure, it's nice that this card can do that. On paper, the 5080 is also technically a good value compared to the flagship RTX 5090\u2014between 60 and 75 percent of the performance for half the price (though talking about the MSRP of any of these cards at launch is strictly theoretical, given allegedly short supply and the demand from both actual buyers and scalpers looking to make a buck).\n\nBut the 5080 really feels a lot more like a 4080 Super Super\u2014meaningfully better than the 4080 but still in the same general performance category. It's an upgrade, especially for anyone coming from a 30-series or older GPU, but it's a disappointing break from Nvidia's past precedent.\n\nWhat you need to know about the RTX 5080\n\nRTX 5090 RTX 4090 RTX 5080 RTX 4080 Super RTX 4080 CUDA cores 21,760 16,384 10,752 10,240 9,728 Boost clock 2,410 MHz 2,520 MHz 2,617 MHz 2,550 MHz 2,505 MHz Memory bus width 512-bit 384-bit 256-bit 256-bit 256-bit Memory bandwidth 1,792 GB/s 1,008 GB/s 960 GB/s 736 GB/s 717 GB/s Memory size 32GB GDDR7 24GB GDDR6X 16GB GDDR7 16GB GDDR6X 16GB GDDR6X TGP 575 W 450 W 360 W 320 W 320 W\n\nOf Nvidia's mid-generation Super refresh for the 40-series last year, the 4080 Super was already the mildest improvement over the original card, with just a few hundred extra CUDA cores and very mild clock speed increases. Its biggest and most important improvement was that it brought the old 4080's original $1,299 price tag down to a somewhat less offensive (and, again, strictly theoretical) price of $999.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "DeepSeek Panic Live Updates: Nvidia Stock Drops 4%\u2014As Trump Reportedly Mulls China Chip Sale Restrictions",
            "link": "https://www.forbes.com/sites/dereksaul/2025/01/29/deepseek-panic-live-updates-nvidia-stock-drops-4-as-trump-reportedly-mulls-china-chip-sale-restrictions/",
            "snippet": "Nvidia shares' 9% recovery Tuesday was the second-best day in terms of market cap added for any company ever\u2014but the company faced another selloff...",
            "score": 0.9672684073448181,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia GeForce RTX 5080 review: the new 4K graphics card to go for",
            "link": "https://www.rockpapershotgun.com/nvidia-geforce-rtx-5080-review",
            "snippet": "Nvidia's RTX 5080 gets put through its paces with our 4K and 1440p games benchmarks.",
            "score": 0.8457658290863037,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia GeForce RTX 5080 Founders Edition specs: CUDA Cores: 10,752\n\n10,752 Base Clock Speed: 2.3GHz\n\n2.3GHz Boost Clock Speed: 2.62GHz\n\n2.62GHz VRAM: 16GB GDDR7\n\n16GB GDDR7 Power: 360W\n\n360W Recommended System Power: 850W\n\n850W Price: \u00a3979 / $999\n\nHas any genus of graphics card been as dramatically storied as the GeForce XX80s? The RTX 3080 was a thing of beauty, only to be tarnished by the worst wheeler-dealing spree (and crypto mining misappropriation) in PC component history. Then the RTX 4080 rocked up with its laughable \u00a31269 / $1199 price tag, a miscalculation so severe that the RTX 4080 Super looked good \u2013 despite hardly being any faster \u2013 simply for not repeating it. For the new RTX 5080\u2019s sake, you almost want it to be boring.\n\nIt isn\u2019t. But then, neither is it a blood-boiler like the RTX 4080, nor a largely aspirational show-off piece like the RTX 5090. By maintaining the 4080 Super\u2019s course correction on price while tooling up on compelling DLSS 4 improvements, the RTX 5080 is an agreeable GPU from the off. Particularly, if you\u2019ve got the 4K monitor to take full advantage of it.\n\nIt\u2019s certainly a more sensible buy than the RTX 5090 for high-rez gamesplaying. A Founders Edition, like the one I\u2019m testing here, will set you back \u00a3979 / $999, or roughly half the price of the next (and final) RTX 50 model in the pile. Plus, while it has the same 16GB of 256-bit VRAM as the 4080 and 4080 Super, that\u2019s still enough for all the AAA industry\u2019s biggest resource-eaters, and it\u2019s been upgraded to GDDR7 for a stretch of extra bandwidth. Get the Founders Edition and you\u2019ll also bag yourself the same lovely metallic design and effective dual-fan cooler that the RTX 5090 FE has, running even colder here thanks to the RTX 4080\u2019s lower power needs.\n\nImage credit: Rock Paper Shotgun\n\nIt's still a thirsty creature, to be sure: Nvidia reckon you\u2019ll need at least an 850W PSU to run it, and max usage is rated at a toasty 360W. Unlike on the RTX 5090 FE, however, I didn\u2019t record the RTX 5080 FE actually hitting its limit, with it peaking at 322W while running games. Its GPU temperature high of 64\u00b0c was a good nine degrees cooler the 5090 as well. See? Sensible. Ish.\n\nNvidia GeForce RTX 5080 review: 4K benchmarks\n\nThe biggest and scariest of mainstream monitor resolutions doesn\u2019t faze the RTX 5080 a jot, even in the more recent tough runners. The RTX 5090 is faster still, though the RTX 5080 is always able to match a good majority of its sibling\u2019s frames-per-second output for half the cash. It\u2019s also a juicy upgrade over the RTX 30 series\u2019 elite \u2013 encouraging, if you\u2019re upgrading from an older card \u2013 and it even comes somewhat close to the RTX 4090, which at the time of writing is still going for upwards of \u00a31300 in the secondhand market.\n\nImage credit: Rock Paper Shotgun\n\nKeen bar chart fans may recall that the RTX 4080 did beat the RTX 3090 and RTX 3090 Ti outright, so it\u2019s a bit of a backwards shuffle for the RTX 5080 to not completely outpace the previous generation\u2019s top dog in turn. I\u2019m not sure that\u2019s a particularly huge deal, outside of maintaining conventions, though I do wish the RTX 5080 gave more of a speed boost to rasterised, native-rez games over the RTX 4080 Super.\n\nTo look at this, we must \u2013 temporarily but with apologies \u2013 return to the old RPS test rig and its Core i5-11600K, which is the only one I have RTX 4080 Super results for. Cyberpunk 2077 sees a decent bump, rising from 51fps (with Ultra settings) on the 4080 Super to 71fps on the 5080, though the newest GPU\u2019s 81fps in F1 22 (on Ultra High) is an uptick of just 12fps. Metro Exodus barely improved either, averaging 99fps on the 4080 Super and 106fps on the 5080.\n\nWhat none of these RTX 40 series cards have, however, is a full suite of DLSS 4 framerate-massaging tools. While one of my personal favourites among these \u2013 the new Transformer model option for upscaling, which enhances overall image quality in exchange for a smaller performance boost \u2013 is available to older RTX models, the headlining multi Frame Generation (MFG) is exclusive to the RTX 50 series. And this actually can nudge the RTX 5080 past the RTX 4090, at least in terms of purely visible smoothness.\n\nImage credit: Rock Paper Shotgun\n\nIf you\u2019re unfamiliar, MFG takes the AI frame generation of DLSS 3 and can throw up anywhere between one and three generated frames for every one of the frames that your PC actually renders. This adds some extra input lag, and it won\u2019t necessarily feel smoother (as the generated frames don\u2019t take into account your mouse movements or keyboard presses), but the visual impact can look very fine indeed. On the RTX 5080, MFG also opens up the possibility of above-60fps framerates at while combining 4K with some of the most demanding ray tracing and path tracing effects in the business. In all three games I tried, applying the 4x frame gen option let it saunter past the RTX 4090, whose DLSS 3 version maxes out at the equivalent of MFG\u2019s 2x setting.\n\nImage credit: Rock Paper Shotgun\n\nOutside of benchmark nerdery, everything looks and feels playable, though Alan Wake II did start pushing 100ms of latency with 4x MFG on \u2013 enough for aiming to get a little sluggish. I\u2019d probably drop DLSS upscaling from Quality to Balanced, in that case, to bump up that base of 37 non-AI frames per second. That would improve visual smoothness even more while touching up responsivity. Otherwise, though, DLSS 4 makes a persuasive argument in the RTX 5080\u2019s favour, and with 75 compatible games confirmed, it\u2019s getting a far more widely-supported start to life than rival frame gen tech like FSR 3 and XeSS 2 did.\n\nNvidia GeForce RTX 5080 review: 1440p benchmarks\n\nAs ever, anything 4K can do, 1440p can do faster, and there are good reasons to choose the RTX 5080 for this resolution if you\u2019ve redirected all your monitor upgrade budget into a GPU fund. CPU bottlenecking snips the tall-poppyness of the RTX 5090, allowing this cheaper card to keep pace more effectively, while also narrowing the gap against RTX 4090 and downright styling on the RTX 3090.\n\nImage credit: Rock Paper Shotgun\n\nThat said, it\u2019s worth waiting for the RTX 5070 and RTX 5070 Ti at this stage. The RTX 5080 is undoubtedly a barnstormer at 1440p but unlike with 4K, there is sometimes a feeling that you\u2019re paying for more power and features than you can use. 4x MFG, for instance, isn\u2019t as useful because the framerates you can get without it are already sky-high: take Cyberpunk 2077 with Psycho ray tracing, which the RTX 5080 could run at 106fps with just Quality-level DLSS upscaling. Most monitors wouldn\u2019t even be able to display all the extra AI frames, even if your eyes could perceive the difference between such speeds \u2013 and diminishing returns very much do come into effect.\n\nThen again, there is a case for 2x frame gen in some games. F1 24 on Ultra High rose from 116fps with only Quality DLSS to 148fps with frame gen, and that actually did make for a noticeable (if minor) smoothness improvement. The RTX 5080 clearly isn\u2019t a bad GPU for Quad HD \u2013 it\u2019s just not the best resolution for spreading its DLSS 4 wings, hence why it\u2019ll be intriguing to see whether the more affordable RTX 5070 duo can make for more suitable matches.\n\nOr AMD\u2019s Radeon RX 9070, maybe. That will have its own souped-up DLSS rival in FSR 4, though it sounds like this will be a simpler 2x frame generator in the DLSS 3 vein. With the faster and more flexible MFG, DLSS 4\u2019s pin-sharp Transformer model option, and the GeForce line\u2019s historic advantages with ray and path tracing, it\u2019s hard to see how RDNA 4 GPUs like this will be able to stand up to the RTX 50 series on features.\n\nImage credit: Rock Paper Shotgun\n\nFor 4K, anyway, the RTX 5080 is the one to beat. It\u2019s cooler, more efficient, and (not, uh, counting any potential stock shortages) drastically more attainable than the RTX 5090, while still having enough muscle for slick, max-quality performance. Is it as lovable as that poor, reseller-abused RTX 3080? Not so much \u2013 a bigger jump up from the RTX 4080 Super would have been appreciated. But it\u2019s better deal overall than both the Super and the original RTX 4080, especially once you start exploiting its more advanced tech. Tech that can help it leave the RTX 4090 behind, let alone those 4080s.\n\nThis review is based on a retail unit provided by the manufacturer.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Trump officials discussing tightening curbs on Nvidia's China sales, sources say",
            "link": "https://www.reuters.com/technology/artificial-intelligence/trump-officials-discussing-tightening-curbs-nvidias-china-sales-bloomberg-2025-01-29/",
            "snippet": "U.S. President Donald Trump's administration is considering tightening restrictions on artificial intelligence leader Nvidia's sales of its H20 chips...",
            "score": 0.8797195553779602,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "NVIDIA GeForce RTX 5080 Founders Edition Review With Copious Benchmarks",
            "link": "https://hothardware.com/reviews/nvidia-geforce-rtx-5080-blackwell-review",
            "snippet": "The NVIDIA GeForce RTX 5080 Founders Edition looks just like the flagship RTX 5090, and offers the same features, but packs a smaller GPU at only.",
            "score": 0.8013281226158142,
            "sentiment": null,
            "probability": null,
            "content": "\n\nNVIDIA GeForce RTX 5080 Founders Edition: MSRP $999\n\nThe NVIDIA GeForce RTX 5080 Founders Edition looks just like the flagship RTX 5090, and offers the same features, but packs a smaller GPU at only half the price.\n\n\n\n\n\nLatest Architecture And Feature Set\n\nEasy Overclocker\n\nRelatively Power Friendly\n\nMore Compact Form Factor\n\nRuns Cool And Mostly Quiet\n\nDLSS 4 + RTX Neural Rendering\n\nUpgraded Media Engine\n\nPrice Premium\n\nMild Upgrade Over The RTX 4080\n\n\n\n\n\nNVIDIA GeForce RTX 5080 Specifications\n\nCheck Out The NVIDIA GeForce RTX 5080 Founders Edition\n\nIn terms of its industrial design and overall aesthetics, the GeForce RTX 5080 Founders Edition LOOKS just like the RTX 5090 FE. There are some key differences we should note, however.\n\n\n\nNVIDIA is ready with the second member of the GeForce RTX 50 series, the GeForce RTX 5080. Last week we were able to show you NVIDIA\u2019s own flagship GeForce RTX 5090 and an awesome partner board from MSI, the cool and quiet GeForce RTX 5090 Suprim Liquid SOC . Both of those cards are crazy powerful, but they also sit at the very, tippy-top of NVIDIA\u2019s product stack, and as such, they command a hefty premium. The RTX 5090\u2019s MSRP is two grand, and we\u2019d be surprised if anyone other than the brave few already camping out for cards get one for the suggested retail price. The new GeForce RTX 5080 is a different animal though, and its suggested price is half of the RTX 5090\u2019s, at just under a grand.Although the RTX 5080 Founders Edition card we\u2019ll be showing you here looks essentially identical to the RTX 5090, there are plenty of differences under the hood. In fact, the cards aren\u2019t even built using the same GPU. More on that in a bit.The NVIDIA GeForce RTX 5080 Founders Edition\u2019s packaging and bundle is exactly the same as the RTX 5090\u2019s. In stead of writing about all of the environmentally friendly and sustainable goodness in NVIDIA\u2019s latest packaging again, we\u2019ll let Davo tell you directly. Check out the video above to see for yourselves \u2013 just imagine that 9 is an 8...As you can see in the comparison table above, in terms of its specification, the GeForce RTX 5080 is an upgrade over the previous-gen GeForce RTX 4080 in virtually every way. The GeForce RTX 5080 has more \u2013 and newer -- cores, more memory bandwidth, more L1 cache, a larger register file, higher clocks, an upgraded media engine, a native PCIe Gen 5 interface, and a higher pixel fill rate. Surprisingly though, the GPU powering the RTX 5080 is comprised of fewer transistors than the RTX 4080\u2019s AD103, though the chips have similar memory interfaces, ROP counts, and L2 cache configurations.The GeForce RTX 5080 is built around the 45.6 billion transistor GB203 GPU, but unlike the RTX 5090, which has a scaled back GB202 at its heart, the GeForce RTX 5080 features a full-fat chip. The GB203 is equipped with 11 GPCs, 42 TPCs, 84 SMs, and eight 32-bit memory controllers, for an aggregate 256-bit bus width. Each SM contains 128 FP32 CUDA Cores, for a grand total of 10,752 CUDA Cores, with 84 4th Gen RT Cores, 336 5th Gen Tensor Cores, 336 Texture Units, and 112 ROPS. NVIDIA's GB203 also includes 10,752 KB L1 cache, a 21,504 KB Register File, and 65,536 KB L2 cache. The GPU is linked to 16GB of GDDR7 memory operating at an effective 30Gbps, for up to 960GB/s of bandwidth.While much of the IP in the GB203 powering the GeForce RTX 5080 is scaled back versus the bigger GB202 on the GeForce RTX 5090, their features and capabilities remain the same. RTX Neural Rendering, RTX Mega Geometry, DLSS 4 with Multi-Frame Gen, the new AI Management Processor, and updated media engine with hardware acceleration for 4:2:2 video, are all part of the GeForce RTX 5080's feature set. You can learn about all of those features in our previous NVIDIA Blackwell GPU architecture coverage, if you\u2019re not already aware.Like the GeForce RTX 5090, the GeForce RTX 5080 is much thinner than its direct predecessor. The card measures 304mm in length, 137mm in height, and it is truly two-slots (40mm) wide \u2013 it\u2019s not just a 2-slot case bracket, with a protruding cooler assembly. It's a dense package that uses premium materials through-and-through. You can feel the build quality as soon as you pick up the GeForce RTX 5080.NVIDIA was able to shrink its top-end GeForce RTX 50 series cards this generation by developing what it calls a \u201cDouble Flow Through\u201d cooler design, which allows both cooling fans to blow air straight through the heatsink, for optimal performance with reduced noise.To achieve this, the GeForce RTX 5080 has a multi-part PCB setup, similar to the 5090's. The central PCB is home to the GPU, memory, and power circuitry. It is situated in the center of the card, with only small portions protruding underneath each fan. A separate daughterboard attaches to the central PCB to the PCIe x16 connector, and a third, flexible board runs perpendicular along the bottom edge with high-speed signaling connections from the central PCB to the display outputs.A ton of engineering went into the design of the GeForce RTX 5090 and RTX 5080. The cooling solution on this card is more capable than previous-gen offerings, and NVIDIA had to solve a number of problems to ensure proper signal integrity over the multiple PCBs and their connectors. NVIDIA discusses much of the design process in a video available here , if you\u2019re interested in some of the finer details.At the top of the card, you'll find the familiar 1 2VHPWR 16-pin connector , also used on the RTX 40 series . On the RTX 5080 though, the connector is angled off the back on the PCB, and recessed slightly in the shroud. This configuration should allow for easier cable management and minimize the need to bend the power feed in thinner PC cases. The included power adapter requires four PCIe 8-pin feeds, but the cabling is longer and far more flexible than the adapters included in older RTX 40-series cards. The connector on the adapter is also beefier and emits a solid \"click\" when pushed fully into place.The GeForce RTX 5090 has a newly-designed 3D Vapor Chamber affixed to its GPU, memory and power circuitry for cooling purposes, with a liquid metal TIM to aid in heat transfer. The GeForce RTX 5080, however, has a more traditional cold plate, with a dense array of heatsink thin-fins, linked via 5 heat pipes -- it is not outfitted with the same vapor chamber as the 5090. NVIDIA also uses a more traditional phase-change TIM on the RTX 5080, so none of the complexities associated with liquid metal are at play here. Externally though, you can't really tell. The same dual axial fans are present, along with the same directional outlets on the heatsink and top and bottom vents on the shroud.Outputs in the GeForce RTX 5080 (and other RTX 50 series cards) mirror the RTX 5090 as well, and include a trio of DisplayPorts (2.1b) and a single HDMI port (2.1b). Their orientations have been reversed versus previous-gen cards though, and the case bracket features a solid front bezel and an anti-fingerprint coating.Now that you're intimately familiar with the GeForce RTX 5080's design, let's get to the benchmarks...",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Reports: Illinois Shows Off Quantum Park to NVIDIA",
            "link": "https://thequantuminsider.com/2025/01/29/reports-illinois-shows-off-quantum-park-to-nvidia/",
            "snippet": "NVIDIA representatives reportedly visited Illinois' planned Quantum and Microelectronics Park, a signal of potential interest.",
            "score": 0.8951579332351685,
            "sentiment": null,
            "probability": null,
            "content": "Insider Brief\n\nNVIDIA representatives reportedly visited Illinois\u2019 planned Quantum and Microelectronics Park, a signal of potential interest in the state\u2019s quantum initiative.\n\nThe Illinois park has already secured commitments from IBM, PsiQuantum, and DARPA, with a goal of attracting $20 billion in investment over the next decade.\n\nIllinois joins several states, including Colorado and Maryland, that are investing heavily in quantum research and development.\n\nA team from NVIDIA reportedly visited Illinois\u2019 quantum research park and, while it\u2019s very early, it could signal another success for the state\u2019s push to be a leader in next-generation computing, Crain\u2019s Chicago Business.\n\nRepresentatives from the semiconductor giant recently toured the planned Illinois Quantum and Microelectronics Park, a 128-acre research hub on Chicago\u2019s Far South Side, according to multiple sources cited by Crain\u2019s Chicago Business. The visit included a dinner with state officials and site visits, underscoring the park\u2019s growing appeal to major tech firms.\n\nNVIDIA, the dominant maker of graphics-processing chips that power artificial intelligence and supercomputers, declined to comment on its interest in Illinois. State officials also did not provide details. However, attracting a company of Nvidia\u2019s stature would be a major validation of Governor J.B. Pritzker\u2019s efforts to position Illinois as a hub for quantum technology.\n\nThe state has invested heavily in quantum, leveraging its early foothold in the field. The research park, announced last year, aims to draw $20 billion in public and private investment over the next decade. Several prominent organizations have already signed on, including PsiQuantum, IBM and the Defense Advanced Research Projects Agency (DARPA).\n\nIBM recently announced plans to open a quantum-algorithm center in Chicago and deploy a quantum computer at the park. PsiQuantum, a Silicon Valley startup, committed $1 billion to develop one of the first commercially viable quantum computers. DARPA, known as a global leader for funding and nurturing cutting-edge technologies, is establishing a $280 million quantum proving ground at the site, with equal contributions from the federal government and the state.\n\nIllinois is not the only state aggressively engaged in quantum ecosystem development. States such as Colorado and Maryland are aggressively pursuing their own quantum initiatives. Last year, Colorado secured a $40 million federal grant for a national quantum hub and is building a 70-acre research park near Denver. Maryland recently unveiled its \u201ccapital of quantum\u201d plan, targeting $1 billion in investments over five years.\n\nAdding a semiconductor leader like NVIDIA to the Illinois project would strengthen its ecosystem. Quantum computing relies on principles of quantum mechanics to process information in ways that far exceed conventional computers. While the technology is still in early development, many experts believe that quantum and classical computing will work together and that semiconductor firms will be critical players in the industry\u2019s evolution.\n\nNVIDIA has already developed quantum-classical computing platforms and collaborates with quantum-computer makers. With no single approach to quantum computing yet proving dominant, companies and governments are hedging their bets, investing in multiple technologies to see which will scale first.\n\nThe business magazines cautions readers not to get the ribbon and comedically big scissors our just yet. State officials will need to continue to engage with NVIDIA and other tech companies to expand Illinois\u2019 quantum park. According to Crain\u2019s Chicago Business, negotiations for such projects can take years. Whether Nvidia formally joins the initiative remains uncertain, but its visit signals that Illinois\u2019 quantum ambitions are attracting serious interest from industry leaders.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia falls 4% after report of potential additional curbs on China sales",
            "link": "https://finance.yahoo.com/news/nvidia-falls-4-after-report-of-potential-additional-curbs-on-china-sales-185442684.html",
            "snippet": "Nvidia shares hit their lows of the day on Wednesday after Bloomberg reported Trump administration officials are \"exploring additional curbs\" on the...",
            "score": 0.945440948009491,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock slid 4% Wednesday after Bloomberg reported that Trump administration officials are \"exploring additional curbs\" on Nvidia's chip sales to China.\n\nBloomberg's report added that the talks are in the \"very early stages.\" The new restrictions would likely cover Nvidia's H20 chips, which is a scaled-down product the chipmaker offers to meet existing US restrictions on shipments to China.\n\nNvidia said in a statement the company is \"ready to work with the Administration as it pursues its own approach to AI. The thresholds set by the Biden Administration are based on performance levels reached five years ago and achieved by leading gaming and workstation products.\u201d\n\nThe news comes amid a volatile week for Nvidia shares, which saw their worst one-day market cap loss in history on Monday. The stock slid nearly 17% as investors digested the growing popularity of a new cost-effective artificial intelligence model from the Chinese startup DeepSeek.\n\nThe team behind DeepSeek, the artificial intelligence model maker, claimed that its new AI model uses cheaper chips and less data. Investors grew concerned this could hurt future AI chip sales as well as bring into question the dominance of US hyperscalers in the market. They also worried the developments could challenge a consistent trend in the bull market of Nvidia and other Big Tech companies seeing their earnings estimates revised higher.\n\nNvidia shares rebounded 9% on Tuesday as many on Wall Street largely argued the sell-off was \"overblown.\" But Wednesday's quick move lower following the Bloomberg report shows that the stock doesn't appear to be out of the woods just yet.\n\nInvestors will be closely listening for updates from large Nvidia customers Tesla (TSLA), Microsoft (MSFT), and Meta (META) after the bell on Wednesday for further details on the current demand for AI chips.\n\nStockStory aims to help individual investors beat the market.\n\nJosh Schafer is a reporter for Yahoo Finance. Follow him on X @_joshschafer.\n\nClick here for in-depth analysis of the latest stock market news and events moving stock prices\n\nRead the latest financial and business news from Yahoo Finance",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Does DeepSeek's Massive AI News Make Nvidia a Sell -- or a Buy?",
            "link": "https://www.fool.com/investing/2025/01/29/does-deepseeks-massive-ai-news-make-nvidia-a-sell/",
            "snippet": "Nvidia (NVDA 2.63%) has soared over the last two years, thanks to its dominance in artificial intelligence (AI) -- a market set to reach $1 trillion by the...",
            "score": 0.8572943210601807,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) has soared over the last two years, thanks to its dominance in artificial intelligence (AI) -- a market set to reach $1 trillion by the end of the decade from about $200 billion today. Investors have piled into this leader, betting on the company's ability to win in this fast-growing market and deliver both earnings growth and share performance over time.\n\nBut news this week prompted investors to doubt Nvidia's prospects in the high-growth market, and Nvidia stock tumbled nearly 17% in one trading session. Chinese start-up DeepSeek announced the release of a large language model (LLM) that it trained for only two months and at a cost of less than $6 million. The idea here is that customers may not need Nvidia's most expensive and top-performing chips to train their LLMs -- and that could result in a drop in revenue for the tech giant.\n\nBefore jumping to conclusions, though, it's important to take a closer look at this story and consider the possible outcomes for Nvidia. Let's do that -- and figure out whether Nvidia is a sell or a buy on the DeepSeek news.\n\nThe most sought-after AI chips\n\nFirst, a bit of background on Nvidia. The company sells the world's most sought-after -- and expensive -- AI chips and a variety of other AI products and services that have generated double-digit and triple-digit revenue growth in recent quarters. Revenue has reached records, into the billions of dollars, and profitability on sales is high, too, with Nvidia maintaining a gross margin of more than 70%. And the pace hasn't seemed ready to let up.\n\nNvidia has spoken of \"staggering\" demand for its new Blackwell architecture, and in recent quarters, big-tech customers have talked about increasing their spending in AI. For example, Meta Platforms, one of Nvidia's customers, has spoken about the need to lift its AI infrastructure spending this year. And last fall, Oracle co-founder Larry Ellison even said he and Tesla chief Elon Musk had \"begged\" Nvidia for more of its chips -- otherwise known as graphics processing units (GPUs).\n\nAll of this demonstrates Nvidia's strength in the AI market. Now it's time to look at the DeepSeek news.\n\nThe company, as mentioned, quickly trained its LLM and for a cost that's much lower than the billions of dollars U.S. players have poured into their AI platforms. DeepSeek, on its website, says its R1 model rivals OpenAI's model o1 -- both involve reasoning, spending time to think out a problem.\n\nDeepSeek, as a Chinese company, didn't have access to Nvidia's latest chips, due to export controls. The U.S. government has restricted the export of the highest-performance chips to China for security concerns. As a result, DeepSeek says it used chips Nvidia designed specifically to adhere to the export rules.\n\nDid DeepSeek only spend $6 million?\n\nWhat this means for Nvidia?\n\nAt first, it may seem U.S. companies have overspent and now may turn to cheaper chips -- from Nvidia or others -- for the training of models. But there are a few things to remember, and these companies probably will consider them before making a move.\n\nThough DeepSeek says it's spent less than $6 million, investors don't know if that's the reality. Development of these models requires many experiments and steps, which are all costly and may not be included in the announced figure. We also can't be 100% sure that DeepSeek hasn't used more powerful AI chips -- in spite of the export ban -- or more AI chips at some point in the process.\n\nSo it's difficult to apply the DeepSeek strategy to every LLM training project out there. Also, each LLM and AI project is different, meaning companies can't use a \"one-size-fits-all\" technique.\n\nThe training of models are just one part of the AI story. There's also inferencing, or the ability of that model to reason and make decisions, and the application of AI to the real world through the creation of AI agents, for example. All of this involves GPUs and other products and services sold by Nvidia.\n\nNvidia's optimism\n\nNvidia doesn't seem overly worried about the DeepSeek news. In a statement, the company called DeepSeek's work \"an excellent AI advancement\" and said it showed how new models could be developed by \"leveraging widely available models and compute that is fully export control compliant.\" This suggests these Nvidia chips have been successful, and there should be need for more of them as DeepSeek or others advance new projects.\n\nOn top of this, Nvidia emphasized that inferencing \"requires significant numbers of Nvidia GPUs and high-performance networking.\" This also suggests the need for Nvidia's products.\n\nThe worst, best, and most likely outcomes\n\nWhat does all of this mean for Nvidia? The very worst outcome would be a sharp drop in demand for Nvidia chips, but that seems extremely unlikely, considering the points I've made above.\n\nThe more logical worst outcome would be the following: Some customers may turn to lower-priced chips to follow DeepSeek's success story. But that actually could result in higher volume for Nvidia as more companies -- which hesitated earlier due to the cost -- will launch AI projects. Nvidia's dominance in the field means these customers surely will come to the company for at least some of their needs.\n\nIn the best situation, Nvidia will gain volume from smaller players, as I mentioned above -- and from business in China -- as these customers get in on Nvidia's lower-priced chips. At the same time, big tech companies will carry on with current plans.\n\nFinally, I think the most likely scenario is business will continue as usual for Nvidia. I don't see the DeepSeek news as something that will prompt every major customer to overhaul spending and change strategy.\n\nNvidia's top GPUs still have proven their efficiency, and everyone wants to get to the finish line first in this highly competitive market. That's why it's a great idea to hold onto your shares of Nvidia -- and if you don't yet own the stock, consider buying it on this recent dip.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia GeForce RTX 5080 Founders Edition Review",
            "link": "https://www.club386.com/nvidia-geforce-rtx-5080-review/",
            "snippet": "Following in the footsteps of RTX 5090, Nvidia's high-end Blackwell GPU comes to play at just $999. Here are the GeForce RTX 5080 benchmarks.",
            "score": 0.9413245320320129,
            "sentiment": null,
            "probability": null,
            "content": "Rather than racing out the door to capitalise on the breakneck momentum set by Blackwell\u2019s flagship, Nvidia GeForce RTX 5080 strolls out leisurely, aware it\u2019s uncontested. There\u2019s a calm air about it, knowing nothing out there hits the same beat and won\u2019t for quite some time.\n\nAll eyes here at Club386 continue to wander over to the mighty GeForce RTX 5090 with dreams of sky-high performance, but there\u2019s enough here to keep your attention at half the price. It doesn\u2019t break the bank in the same way at $999, matching the cost of RTX 4080 Super at launch. Whether that keeps is another question entirely, but it\u2019s off to a good start.\n\nNvidia GeForce RTX 5080 \u00a3979 / $999 Pros DLSS 4 is fantastic\n\nPrice is fitting\n\nOutstanding build quality\n\nDual-slot form factor\n\nFastest GDDR7 memory clocks Cons 16GB of memory sets a lower ceiling\n\nIncremental rasterised generational uplift Buy at Amazon Buy at Nvidia Club386 may earn an affiliate commission when you purchase products through links on our site.\n\nHow we test and review products.\n\nBlackwell architecture brings with it solutions to concerns of stagnating rasterised performance, leveraging Nvidia\u2019s AI expertise to trade brute force for much-improved neural rendering techniques. GeForce RTX 5080 is our first indicator of just how well this sorcery scales down to the high-end and indeed gives us a glimpse at what we can expect from future entries to the series.\n\nLet\u2019s consult with the trusty Club386 Table of Doom\u2122 to see where it sits compared to predecessors.\n\nSpecifications\n\nGeForce RTX 5080 4080 Super 3080 Launch date Jan 2025 Jan 2024 Sep 2020 Codename GB203 AD103 GA102 Architecture Blackwell Ada Lovelace Ampere Process TSMC 4N (5nm) TSMC 4N (5nm) Samsung 8N Transistors (bn) 45.6 45.9 28.3 Die size (mm2) 378 378.6 628.4 SMs 84 of 84 80 of 80 68 of 84 CUDA cores 10,752 10,240 8,704 Boost clock (MHz) 2,617 2,550 1,710 Peak FP32 TFLOPS 56.3 52.2 29.77 RT cores 84 (4th Gen) 80 (3rd Gen) 68 (2nd Gen) RT TFLOPS 170.6 121 58 Tensor cores 336 (5th Gen) 320 (4th Gen) 272 (3rd Gen) FP16 Acc TFLOPS 450.2 418 238.2 Peak FP4 TFLOPS 1,801 836 \u2013 ROPs 112 112 96 Texture units 336 320 272 Memory size (GB) 16 16 10 Memory type GDDR7 GDDR6X GDDR6X Memory bus (bits) 256 256 320 Memory clock (Gb/s) 30 23 19 Bandwidth (GB/s) 960 737 760 L2 cache (KB) 65,536 65,536 5,120 PCIe interface Gen 5 Gen 4 Gen 4 Video engines 2 x NVENC (9th Gen)\n\n2 x NVDEC (6th Gen) 2 x NVENC (8th Gen)\n\n1 x NVDEC (5th Gen) 1 x NVENC (7th Gen)\n\n1 x NVDEC (5th Gen) Power (watts) 360W 320W 320W MSRP $999 $999 $699\n\nGeForce RTX 5080 uses all the GB203 chip has to offer, packing it to the brim. All seven graphics processing clusters (GPCs) are in effect, giving it a modest 5% more CUDA cores than its predecessor at 10,752. That\u2019s half of what you get inside the top-of-the-range GeForce RTX 5090, but still a decent leap from where we were just four years ago.\n\nSome might question Nvidia steering the course with an identical memory configuration as GeForce RTX 4080. HD texture packs and ray tracing raise the ceiling ever higher, potentially leaving 16GB across a 256-bit bus something of a stickler in some games. As the old adage goes, it\u2019s not how big it is, but how you use it.\n\nNvidia has moved to high-speed GDDR7, amping up speeds courtesy of 30Gb/s modules \u2013 the fastest we\u2019ve seen yet. There is a desire for more as the total bandwidth is once again half its bigger brother at 960GB/s, but that\u2019s still a 30% leap over other 80-class cards.\n\nOf course, none of this paints the full picture of Blackwell. Nvidia kicks off its AI-led revolution with some impressive architectural tinkering and AI witchcraft in an attempt to create a card that\u2019s great for both work and play.\n\nArchitecture\n\nGeForce RTX 5080 elicits some deja vu with hardware, featuring the same TSMC 4N process as its predecessor. With an ever-so-slightly smaller 378mm2 die, Nvidia carves out room for its expanded 84 streaming multiprocessors (SMs) by dialling back transistors a touch. A definitively worthwhile trade, owing to its new shader layout packing a huge punch.\n\nEach SM carries the same general configuration of 128 shaders, four Tensor Cores, four Texture Units, and a single RT Core, but with a couple of key differences. Firstly, Tensor Cores enter their fifth generation, now supporting new quantisation formats and precisions to speed up training and utilising machine learning. It\u2019s Nvidia\u2019s AI bedrock decades in the making.\n\nSecondly, and more importantly for a gaming card such as this, cores can now process either floating point or integer workloads. We go into more detail in our GeForce RTX 5090 review, but this essentially leaves no wasted potential as games make use of INT32. Being even handed, Nvidia doesn\u2019t increase the ROP count between generations, meaning raster performance won\u2019t see the jump it could have done.\n\nMulti-frame Generation\n\nNvidia came to play with its RTX 40 Series, boosting performance with Frame Generation on top of AI upscalers. The premise is simple enough, using Tensor Cores to sandwich a single AI-generated frame in between each rasterised frame for a sweet 55% fps jump using DLSS Quality in our GeForce RTX 4090 review.\n\nTaking things a step further, Blackwell amplifies this threefold with its new Multi-frame Generation (MFG) slipping in a total of three synthetic frames for every rendered one. You can see the results for yourself by heading into the performance section, but in many cases, it can net up to double the frame rate in some games.\n\nIt\u2019s an idea Nvidia has long ruminated on but only managed to implement successfully into RTX 50 Series after a few necessary tweaks under the hood. Not quite the full-on Google approach to software, instead creating harmony with the hardware underneath.\n\nImproved Flip Metering nails the timing of each AI-generated frame, keeping them in line with rasterised ones to prevent stumbling and stuttering that would otherwise make all improvements null and void. Also gone is the previous hardware-based Optical Flow Accelerator, traded in for a more reliable deploy-once AI model that\u2019s both 40% faster and consumes 30% less VRAM.\n\n16GB of GDDR7 on top of RTX 5080 starts to look a little more understandable as a result. Finally, a new AI Management Processor (AMP) keeps all of this in check like a good project manager should.\n\nTransformer Model\n\nPart and parcel with fresh rendering technology is a new AI model it can use to generate pixels. Convolutional Neural Networks (CNN) is still alive and kicking in games that support super resolution, but Nvidia has already refined it to within an inch of its life. Whatever pitfalls you\u2019ve noticed when using it, such as ghosting and blooming, are there to stay.\n\nThe only way to improve visual fidelity and combat those pesky artefacts is to switch things up, which is where Google\u2019s Transformer Model comes in. Dating back to 2017, it\u2019s currently the most popular model seen within ChatGPT and Gemini, and for good reason. It carries a larger overhead, as Nvidia claims it requires twice the parameters and four times the compute compared to CNN, but with it comes improved temporal stability that\u2019s the closest we\u2019ve ever been to native without directly rendering each individual frame.\n\nRTX 5080 and its Blackwell cohorts benefit most with the enhancements it brings to Multi Frame Generation, but it\u2019s not confined to the latest lineup. It also improves standard Frame Generation for RTX 40 Series cards, and extends its reach to uplift Ray Reconstruction, Super Resolution, and Deep Learning Anti-Aliasing (DLAA) across the entire RTX ecosystem. You can run an RTX 2060 and you\u2019ll still enjoy the augmentation.\n\nThe only caveat is one it shares with MFG in that it requires developer implementation. DLSS Override from within the Nvidia App only takes you so far without the codebase to work from. I have my highest hopes that devs will be quick to set it all up after seeing the wonderous things it can do in Alan Wake 2 and Cyberpunk 2077, but that\u2019s the test of time as we know it.\n\nFounders Edition\n\nNvidia has the craft of its Founders Edition models down to a fine art at this point, producing some of the sexiest graphics cards on the market. If you ask me, RTX 40 Series lost the magic a little as its three-slot bulk didn\u2019t suit the professional aesthetic Team Green strives for, but it was a necessary evil of sorts to tame the beast beneath. Fortunately, elegance is well and truly back in RTX 50 Series thanks to major cooling innovations.\n\nReturning to a slimmer two-slot form factor, GeForce RTX 5080 has no trouble fitting in most rigs at 304mm x 137mm. These dimensions qualify it for Nvidia\u2019s SFF-ready badge of honour, although you\u2019re always best whipping out your trusty tape measure before sticking it inside a mini PC.\n\nSharing the exact same chassis as GeForce RTX 5090 FE, Nvidia toes a familiar line with its colour scheme. As much as I\u2019d love to see a bleached Founders Edition primed for my white build, there\u2019s no arguing that grey and black fit in most themes. Gone is large, front-facing branding, replaced with a subtle etching to let everyone know which model you sport, while dinky LED strips accentuate the eye-catching X pattern in the centre.\n\nBoth fans now sit side by side in a more traditional arrangement, blowing air past the new vapour chamber and out through two concaved fin stacks. The combination of curved and straight fins helps reduce pressure, with the longest optimising airflow where it\u2019s greatest. Meanwhile, angled exhausts on the side do their darndest to direct heat away from the card rather than recycling it back through the fans again.\n\nAll in all, it\u2019s a lot of engineering to keep the uniquely centralised 378.6mm2 die cool on top of Nvidia\u2019s smallest PCB to date, but the changes don\u2019t stop there. Relocating everything to the middle of the card increases the distance between the board and both the PCIe slot and I/O, prompting boffins to spin a web of proprietary PCBs and ribbon cables to reach all the necessary ports.\n\nNone of this technical wizardry slows RTX 5080 down in the slightest. In fact, Nvidia outfits it with the latest cables out there, and its single HDMI 2.1b and three DisplayPort 2.1b slots have enough bandwidth to push up to 4K 12-bit HDR at 480Hz or 8K 12-bit HDR at 165Hz \u2013 alongside display stream compression (DSC) that is.\n\nThe return of the 12VHPWR power connector is no surprise despite its exceptionally rocky start in life. After all, its pins deliver an ample 600W current through a single cable, which is more than enough to saturate RTX 5080\u2019s 360W hunger. You\u2019ll be pleased to know Nvidia has made some tweaks to the interface by following the much-improved 12V-2\u00d76 variant, reinforcing the contact block and securing the pins. Still, if you\u2019re a worry wart like me, MSI adapters (sold separately) offer peace of mind with yellow housing to ensure nothing\u2019s loose.\n\nI highly suggest pairing RTX 5080 with a modern power supply outfitted with a 12VHPWR header, as it offers a streamlined look with one thin cable from end to end. However, you don\u2019t need to invest in a new PSU just for a simple wire. You\u2019ll spot a three-way, eight-pin splitter in the box that\u2019ll hook up to most units out there. Just keep in mind Nvidia recommends 850W to account for all your other components.\n\nBetter safe than sorry, but 850W seems like a conservative estimate to me. RTX 5080 is a rather polite guest inside our beefy Club386 test bench, as the whole system runs at just 475W when gaming and 101W while idling.\n\nThis is less than Radeon RX 7900 XTX, which itself carries a slimmer 800W suggestion from AMD. Fill out four memory lanes, perhaps add in an RGB strip or two and power expectations will creep up, but the ceiling seems lower than initially anticipated, which is a welcome bonus.\n\nSlimming power demands within 4% of its predecessor pairs well with Nvidia\u2019s Double Flow Through cooler, keeping RTX 5080 as cool as a cucumber \u2013 you must like hot cucumbers (ed). Peak gaming temperatures reach no higher than 64\u00b0C, which is impeccable for a dual-slot solution eschewing liquid metal in favour of a traditional thermal compound solution, all while fans spin at just 39.5dB. We\u2019ve yet to hear anything quieter, which is quite the oxymoron.\n\nA quick reminder of its $999 price tag, and GeForce RTX 5080 appears to be one of the most compelling cards in the Blackwell line-up. Sadly, you might struggle to get your hands on one if rumblings of stock shortages are anything to go by, particularly in a sleek Founders Edition dress. Just keep an eye on Nvidia\u2019s website and your finger hovering over F5 any time from tomorrow, January 30.\n\nPerformance\n\nTo capture where GeForce RTX 5080 sits in the current market, we\u2019ve tested a total of nine graphics cards from scratch. Interestingly, while RTX 5090 benchmarked without a hitch, RTX 5080 proved more problematic. We experienced unexpected and intermittent black screens with the new card in situ and are putting this down to pre-release drivers while we investigate further.\n\nApplication & AI\n\nLeaked 3DMark Time Spy results already spoiled this one for us ahead of launch as other tests in the Futuremark suite seem to follow suit. RTX 5080 shows its ray tracing chops in 3DMark Speed Way by slotting in between two predecessors, offering a 20% uptick over RTX 4080 Super while just 14% shy of RTX 4090.\n\nIt\u2019s no match for the mighty RTX 5090, but 37% less performance doesn\u2019t seem so bad when you save half the cost. Meanwhile, AMD Radeon RX 7900 XTX just about claws into the top-five graphics cards we\u2019ve tested by default. One has to wonder if that\u2019ll stay the case when more Blackwell GPUs arrive.\n\n3DMark Steel Nomad strips all the fancy rendering techniques to focus purely on rasterisation, yielding fairly similar results to its ray-traced counterpart. RTX 5080 once again falls behind the flagship\u2019s staggering lead by 40%, but it still over performs relative to its price \u2013 and this is before we get into the magic of AI.\n\nIt\u2019s a far closer fight in the high end, as RTX 5080 tickles the heels of RTX 4090 with just 11% in it between the two. Otherwise, you can expect anything from a 23% lead over its rivals and predecessors.\n\nRTX 5080 might be more of a gaming card, but content creation isn\u2019t lost on the pusher of pixels. It\u2019s capable of 9,189 samples per minute in Blender, placing it 39% short of its bigger brother and 18% behind the Ada Lovelace champion. Again, it\u2019s hard not to highlight the disparity in value here, given it also saves you 50% and 33% from their respective MSRPs.\n\nComparatively, RTX 5080 is just 8% more productive in 3D rendering than the likes of RTX 4080 Super, but for the same cost. One for one, it\u2019s a net positive. This is sure to perk up the ears of RTX 30 Series or AMD Radeon owners, who net anything from 64% to double the performance.\n\nGiven Nvidia\u2019s focus on artificial intelligence, it\u2019s no surprise to see RTX 5080 standing tall as the only graphics card able to go toe-to-toe with the king. It\u2019s still 16% short of RTX 5090 in Geekbench AI, but a comfortable 15% ahead of the closest competition. As far as cost-conscious AI performance goes, there is simply nothing else in this price range.\n\nLeaving the flagship to play in a league of its own, RTX 5080 is almost neck and neck with RTX 4090 in Procyon\u2019s AI Text Generation benchmark. This subjects our graphics cards to several large language models (LLMs) using Llama 3.1 as a framework.\n\nOnce again expect to see double the performance of AMD Radeon\u2019s best and brightest, a minimum 31% leap from Nvidia\u2019s Ampere Series, and a 19% jump from the previous generation. It\u2019s safe to say it\u2019ll make short work of generative AI.\n\nGame Rasterisation\n\nSynthetic tests tell but half the story, so allow good old games to fill in the real-world blanks. Our baseball team of graphics cards is batting against a roster of triple A titles, each pitching with the three most popular resolutions \u2013 1920\u00d71080 (FHD), 2560\u00d71440 (QHD) and 3840\u00d72160 (UHD).\n\nCredit where it\u2019s due, AMD Radeon RX 7900 XTX shows up when it comes to crunch time, but it\u2019s still not enough to topple RTX 5080 in Assassin\u2019s Creed Mirage. Granted, Team Red might clinch the value proposition with just a few frames in it between the two, but don\u2019t count out Blackwell\u2019s high-end just yet as we\u2019ll soon see its secret weapon in action.\n\nSpeaking of price to performance, RTX 5080 sits within just 12% of the flagship at FHD. This grows to 20% at QHD and 27% at UHD, but still half the price you\u2019ll pay for the pleasure. It\u2019s starting to look like the card gamers dream about.\n\nIt\u2019s a similar story with Final Fantasy XIV: Dawntrail. RTX 5080 proves its mettle in FHD compared to 90-class alternatives but still keeps a respectable margin at higher resolutions. It outpaces everything AMD has to offer, while falling just 35% short of the top dog.\n\nYou might get more out of RTX 4090 if you hunt around for a good deal, but it\u2019ll need to be lower than $700 to offer anything of substance. All in all, it just further underscores how much of a bargain RTX 5080 is at MSRP.\n\nForza Motorsport is a particularly demanding game where even the best graphics card in the world can\u2019t achieve triple-digit frame rates at 4K. Fortunately, RTX 5080 keeps a smooth frame rate at all resolutions compared to most of its peers, proving a worthwhile upgrade from Ampere and Radeon 7000 Series alike. It even steals some thunder from RTX 4090, sitting withing just 7fps.\n\nMount & Blade II: Bannerlord is the first time we see RTX 5080 wobble, slipping halfway down the pack. It\u2019s hard to whine about any graphics card capable of 350fps, but there\u2019s no denying that the strategy game responds to other models better at lower resolutions.\n\nCrank things higher and the Blackwell GPU starts to come into its own again, leaping 17% past its predecessor and 6% over its rival at UHD.\n\nSlight fluctuations in Tom Clancy\u2019s Rainbow Six Extraction don\u2019t detract from the overall theme that RTX 5080 belongs in between RTX 4080 Super and RTX 4090 in terms of rasterised performance, regardless of resolution. Simply take a look at this averaged table to see for yourself using UHD as a reference point:\n\nAverage increase or decrease at 4K RTX 5090 to RTX 5080 -32% RTX 4090 to RTX 5080 -15% RTX 4080 Super to RTX 5080 +14% RX 7900 XTX to RTX 5080 +15% RX 7900 XT to RTX 5080 +37% RTX 3090 to RTX 5080 +52% RTX 3080 to RTX 5080 +77% RX 7800 XT to RTX 5080 +84%\n\nMoving to Blackwell might prove a tough sell for previous generation owners going off rasterised performance alone, but RTX 5080 is a far more meaningful option for anyone coming from the midrange or something a little older. Still, let\u2019s not draw any conclusions just yet. There\u2019s still the matter of all those AI accelerators under the hood.\n\nFrame Generation\n\nFirst, a quick history lesson about frame generation (FG) to lay the groundwork. Call of Duty: Black Ops 6 benefits from both FSR and DLSS, but our RTX 5080 barely sees a bump moving from native performance to Quality upscaling. Instead, the largest uplift comes from added AI-generated frames, seeing 30% more than other rendering techniques.\n\nThis is true of all models, with DLSS Frame Generation coming out slightly ahead of FSR3 Frame Generation for supported RTX 40 and 50 Series GPUs. To paraphrase Happy Gilmore, \u201cit ain\u2019t over yet. The way Nvidia sees it, we\u2019ve only just begun.\u201d\n\nPutting all of Nvidia\u2019s upscalers against the wall, 3DMark\u2019s new DLSS test shows a decent uplift simply moving from one generation of FG to the next. Alongside reflecting on just how far we\u2019ve come from the small beginnings of DLSS 2, there\u2019s proof in the pudding that even RTX 40 Series benefits from Team Green\u2019s tweaking.\n\nIt doesn\u2019t stop there, though. GeForce RTX 5080 uses Multi Frame Generation, which slips a second (3x) and third (4x) AI-generated frame in for good measure. While our high-end card already performs admirably, it eclipses RTX 4090\u2019s capabilities by up to 55% when the feature is dialled up to 11, cementing its place as the second-best graphics card money can buy right now.\n\nPutting this into practice, an early build of Alan Wake 2\u2019s MFG update shows just how much the survival horror game benefits from the new feature. Settings such as path tracing make even the strongest graphics cards buckle without a helping hand, but RTX 5080 goes from barely playable at UHD using just DLSS to maxing out 144Hz gaming monitors with room to spare. It feels like magic, considering it\u2019s all nestled in a single option.\n\nMuch like Nvidia showcased during its reveal, it has noticeably polished its neural rendering suite, as there are far fewer shimmers, glimmers, ghosting, and blooming than previous upscalers. However, that doesn\u2019t mean MFG is perfect. Keen eyes will eventually spot the odd thing that doesn\u2019t belong, but to my peepers, neon signs boast more clarity than ever, and finer details like wires and hair behave as they should.\n\nWhile pure rasterisation debatably feels better as you pan the camera around the titular character, 153fps with MFG is undoubtedly smoother than sub-60fps without the artificial leg up. Synthetic is simply the way forward as brute force hardware yields diminishing returns. Nvidia is just refining it quicker than anyone else.\n\nCyberpunk 2077 is another one renowned, or perhaps infamous, for its incredibly demanding ray-tracing presets. A graphics card even remotely glancing at Overdrive starts to work up a sweat. You can see for yourself in the tables above, with Radeon struggling to muster up enough gumption to break triple digits at FHD and falling far short of the 60fps ideal at UHD.\n\nMulti Frame Generation changes the game entirely, delivering comfortable frame rates across the board. It works best with a base of at least 60fps to work with, but that doesn\u2019t stop 4x frame generation from boosting RTX 5080\u2019s lowly 19fps native showing at 4K to a staggering 137fps \u2013 a 621% increase.\n\nOf course, it\u2019s early days for MFG and I\u2019ve yet to see how it plays out across the 75 announced launch titles, but it makes a solid first impression. Its scalability opens plenty of doors, from higher resolutions such as 8K when displays inevitably land on our doorstep to competitive frame rates without giving up that all important detail, and improved battery life should it ever grace a gaming handheld.\n\nAs much as my mind can wander musing the endless possibilities of its bright future, I can\u2019t argue with its impact in the here and now, too. Just make sure you check DLSS MFG support in your favourite titles before deciding to cop an RTX 50 Series card for yourself, as while there\u2019s possibility of using it via override in the control panel, optimised implementation remains in the hands of developers.\n\nConclusion\n\nWith GeForce RTX 5080, Nvidia cements Blackwell graphics cards as the go-to for gamers, content creators, and AI enthusiasts. Much like GeForce RTX 5090, it arrives on the scene with no equal yet carries a slimmer $999 price tag than RTX 4080 to lower the barrier to entry. No small chunk of change, but those with deep enough pockets will find value, even compared to the flagship. It\u2019s half the price, but always more than half its performance.\n\nLeaner generational gains in rasterised performance alongside sticking with 16GB of memory make it less enticing for anyone jumping ship from high-end RTX 40 Series solutions, but Nvidia has built on its frame-generation foundations well. Combined with a smarter neural network, RTX 5080 pushes supported games to the nth degree, often doubling anything RTX 4080 Super can handle while sporting fewer artefacts in the process.\n\nOptimised Multi Frame Generation compatibility rests in the hands of developers, making additional performance something of a bonus rather than a given. Still, 75 games and apps right off the bat shows commitment to the cause, instilling faith that there are many more to come. Even without the promise the feature will appear in your favourite title, RTX 5080 is a fine card.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia GeForce RTX 5090 Founders Edition Review",
            "link": "https://www.pcmag.com/reviews/nvidia-geforce-rtx-5090-founders-edition",
            "snippet": "Nvidia's GeForce RTX 5090 graphics card is exceptionally powerful but forbiddingly expensive. (You could build a complete RTX 5080-based gaming PC for the...",
            "score": 0.9417252540588379,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-28": {
        "0": {
            "title": "Nvidia faces a reckoning as Chinese upstart raises questions about Wall Street\u2019s darling",
            "link": "https://apnews.com/article/nvidia-stock-ai-deepseek-wall-street-5ef5329d2a864f7f733f7e8984366238",
            "snippet": "The superstar run for Nvidia's stock the last few years has been astonishing. So was its tumble Monday, which caused $595 billion in wealth to vanish.",
            "score": 0.7762444019317627,
            "sentiment": null,
            "probability": null,
            "content": "NEW YORK (AP) \u2014 The superstar run for Nvidia\u2019s stock the last few years has been astonishing. So was its tumble Monday, which caused $595 billion in wealth to vanish. That\u2019s about as much as PepsiCo, McDonalds, Starbucks and Target are worth, combined.\n\nMostly known only in gaming and crypto circles a few years ago, Nvidia burst into the zeitgeist after seeing its sales surge because customers wanted its chips to train their chatbots and other artificial intelligence products.\n\nNvidia became a household name as its stock more than tripled in 2023 and then more than doubled in 2024. Investors and analysts lauded CEO Jensen Huang as the \u201cGodfather of AI.\u201d Nvidia grew into a $3 trillion-plus behemoth and traded places with titans like Apple to become the most valuable company on Wall Street.\n\nBut that all came to a screeching halt Monday, at least for a moment, after a Chinese upstart called DeepSeek said it had developed a large-language model that could perform like ChatGPT and other U.S. rivals, but by using far less computing power.\n\nHere\u2019s a look at how it all got to this point:\n\nHow did it become a market darling?\n\nNvidia\u2019s roots began in gaming.\n\nThe Santa Clara, California-based tech company\u2019s invention of the graphics processor unit, or GPU, in 1999 helped spark the growth of the PC gaming market and redefined computer graphics. Now Nvidia\u2019s specialized chips are key components that help power different forms of artificial intelligence, including the latest generative AI chatbots such as ChatGPT and Google\u2019s Gemini.\n\nHuang has dubbed AI \u201cthe next industrial revolution,\u201d and Nvidia\u2019s GPUs are designed to perform artificial intelligence tasks faster and more efficiently than general-purpose chips like CPUs. Tech giants are snapping up Nvidia chips as they wade deeper into AI \u2014 a movement that\u2019s enabling cars to drive by themselves, and generating stories, art and music.\n\nThe demand helped Nvidia\u2019s revenue grow by eye-popping levels, quarter after quarter. On Feb. 23, 2023, after Nvidia breezed past analysts\u2019 expectations for quarterly profit, Huang said that \u201cAI is at an inflection point, setting up for broad adoption reaching into every industry.\u201d The company\u2019s quarterly revenue at the time was $6.05 billion.\n\nThat ramped up to $7.19 billion just three months later and then nearly doubled to $13.51 billion three months after that. Revenue has since vaulted to $35.08 billion in the three months through October 2024.\n\nThe company\u2019s stock price has similarly soared, and its total market value quickly passed rivals like Intel, Microsoft and others. Nvidia alone accounted for more than a fifth of all of the S&P 500 index\u2019s total return last year. No other stock came close, and it had more than triple Apple\u2019s impact.\n\nBuy an S&P 500 index fund today, and nearly 6 cents out of every $1 will go only into Nvidia. That leaves 94 cents for all the other 499 companies.\n\nIs it still a darling?\n\nUnlike the dot-com boom, real money was behind Nvidia\u2019s surge, and its stock price rose on expectations of ever more to come. Those expectations came into question on Monday.\n\nDeepSeek and its seemingly lower-cost operations raised worries about whether companies would need to spend as many dollars on Nvidia chips as previously thought. The concerns dragged down stocks across the AI industry, including suppliers to the chip industry and the power companies hoping to electrify the vast data centers that were expected to get built to run those chips. But Nvidia was in the spotlight because its stock has become the brightest symbol of the AI bonanza.\n\nSome on Wall Street saw Monday\u2019s nearly 17% plunge for Nvidia\u2019s stock as an opportunity rather than a signal of pending doom, saying the stock became more affordable. If AI does become cheaper to run, it could open the door to new kinds of customers and software innovations that could ultimately help the industry in the long term.\n\n\u201cAs for Nvidia itself, this isn\u2019t the first time a major tech stock has faced existential questions,\u201d said John Belton, portfolio manager at Gabelli Funds. \u201cWe\u2019ve seen similar situations with Microsoft, Apple, Meta, Google, Amazon, and Netflix \u2014 companies that were once doubted but ultimately rebounded.\u201d\n\nDeepSeek\u2019s entrance certainly adds uncertainty to the entire AI ecosystem, but it doesn\u2019t change the overwhelming momentum behind the movement, according to Brian Colello, strategist at Morningstar.\n\n\u201cWe believe that AI GPU demand still exceeds supply,\u201d he wrote in a report. \u201cSo, while slimmer models may enable greater development for the same number of chips, we still think tech firms will continue to buy all the GPUs they can as part of this AI \u2018gold rush.\u2019\u201d\n\nFor its part, Nvidia\u2019s stock wobbled between gains and losses early Tuesday following its worst plunge since the 2020 COVID crash, and then rallied a bit and ended the day nearly 9% higher.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "DeepSeek sparks AI stock selloff; Nvidia posts record market-cap loss",
            "link": "https://www.reuters.com/technology/chinas-deepseek-sets-off-ai-market-rout-2025-01-27/",
            "snippet": "Global investors are worried the emergence of a low-cost Chinese AI model will threaten the dominance of AI leaders.",
            "score": 0.8759281635284424,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia stock begins recovery after DeepSeek AI frenzy prompted near $600 billion loss",
            "link": "https://finance.yahoo.com/news/nvidia-stock-begins-recovery-after-deepseek-ai-frenzy-prompted-near-600-billion-loss-134240328.html",
            "snippet": "Nvidia stock surged as the AI chipmaker began to recover from a massive decline the prior day that shaved nearly $600 billion off its market cap.",
            "score": 0.8399544358253479,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock rose nearly 9% Tuesday as the AI chipmaker began to recover from a massive decline the prior day that shaved nearly $600 billion off its market cap.\n\nNvidia\u2019s 17% freefall Monday was prompted by investor anxieties related to a new, cost-effective artificial intelligence model from the Chinese startup DeepSeek. Some Wall Street analysts worried that the cheaper costs DeepSeek claimed to have spent training its latest AI models, due in part to using fewer AI chips, meant US firms were overspending on artificial intelligence infrastructure.\n\nThat created a concern among the investment community that Nvidia\u2019s high GPU (graphics processing unit, or AI chip) prices could come under pressure and that demand for semiconductors could wane.\n\nNvidia\u2019s $589 billion market cap decline was the largest single-day loss in stock market history.\n\nThe DeepSeek announcements drove down not only Nvidia but the market at large, with the tech-heavy Nasdaq (^IXIC) dropping 3%. Chip stocks dropped across the board Monday, but some names began to recover. After dropping more than 17% to start the week, Broadcom (AVGO) rose 2.6% Tuesday.\n\nNvidia itself didn\u2019t express much anxiety over the DeepSeek buzz, calling R1 \"an excellent AI advancement\" in a statement Monday.\n\nJensen Huang speaking at NVIDIA Keynote at Michelob Ultra Arena in Las Vegas, NV, on Jan. 6, 2025. Credit: DeeCee Carter/MediaPunch /IPX \u00b7 DeeCee Carter/MediaPunch/MediaPunch/IPx\n\nWall Street analysts continued to reflect on the DeepSeek-fueled market rout Tuesday, expressing skepticism over DeepSeek\u2019s reportedly low costs to train its AI models and the implications for AI stocks.\n\nJPMorgan analyst Harlan Sur and Citi analyst Christopher Danley said in separate notes to investors that because DeepSeek used a process called \u201cdistillation\u201d \u2014 in other words, it relied on Meta\u2019s (META) open-source Llama AI model to develop its model \u2014 the low spending cited by the Chinese startup (under $6 billion to train its recent V3 model) did not fully encompass its costs.\n\n\u201cWe believe it is crucial to validate these costs before drawing conclusions,\u201d Sur wrote.\n\nDanley added: \u201cGiven Deepseek is based on leveraging cloud service providers [Meta] and AI is still in its infancy, we lean towards the argument of continued strong growth in AI spending.\u201d\n\nEven so, DeepSeek \u201cclearly doesn\u2019t have access to as much compute as US hyperscalers and somehow managed to develop a model that appears highly competitive,\u201d Raymond James analyst Srini Pajjuri wrote in a note to investors Monday.\n\nStockStory aims to help individual investors beat the market.\n\nLaura Bratton is a reporter for Yahoo Finance. Follow her on Bluesky @laurabratton.bsky.social. Email her at laura.bratton@yahooinc.com.\n\nClick here for the latest stock market news and in-depth analysis, including events that move stocks\n\nRead the latest financial and business news from Yahoo Finance",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "What is DeepSeek, and why is it causing Nvidia and other stocks to slump?",
            "link": "https://www.cbsnews.com/news/what-is-deepseek-ai-china-stock-nvidia-nvda-asml/",
            "snippet": "Chinese startup DeepSeek has debuted an AI app that challenges OpenAI's ChatGPT and other U.S. rivals, sending a shock through Wall Street.",
            "score": 0.8770735859870911,
            "sentiment": null,
            "probability": null,
            "content": "A Chinese artificial intelligence company called DeepSeek is grabbing America's attention \u2014 and sending a shock wave through Wall Street \u2014 due to its new tech, which some experts say rivals that of OpenAI's ChatGPT.\n\nDeepSeek is also catching investors off guard because of the low development costs for its AI app, which Wedbush Securities analyst Dan Ives pegged at only $6 million. By comparison, OpenAI, Google and other major U.S. companies are on track to invest a total of roughly $1 trillion in AI over the coming years, according to Goldman Sachs.\n\nOn Monday, DeepSeek's rollout roiled shares of AI stalwarts such as Nvidia, the high-flying manufacturer of advanced chips engineered for AI development, and Dutch company ASML, another chipmaker. The Chinese company's tech is raising questions about whether demand for Nvidia's chips could take a hit, as well as whether investors are overvaluing tech stocks that have been buoyed by the promise of AI, from Meta to Microsoft, experts said.\n\n\"DeepSeek has taken the market by storm by doing more with less,\" said Giuseppe Sette, president at AI market research firm Reflexivity, in an email. \"This shows that with AI the surprises will keep on coming in the next few years.\"\n\nDeepSeek's latest app comes just days after President Trump announced a new $500 billion venture with ChatGPT maker OpenAI, Softbank and Oracle, dubbed Stargate, which he touted as ensuring \"the future of technology\" in the U.S.\n\nAI-related stocks took a hit on Monday, with Nvidia shares tumbling 17%, shedding $600 billion in value and marking the single-biggest one-day loss for a company in stock market history, according to CNBC. ASML sank 6%, while Broadcom, another semiconductor stock, also slumped 17%.\n\nSome energy-related stocks also plunged on Monday on investor worries that the new tech could require less energy to run, translating into lower demand from the tech sector. GE Vernova, which makes wind and gas turbines, plunged 21%, while electricity generator Vistra slumped 28%.\n\nThe tech-heavy Nasdaq index slumped 3%, or 612 points, while the S&P 500 declined 1.5%. The blue-chip Dow Jones Industrial Average added 0.7%.\n\nDespite the sharp drop on the Nasdaq, it's far from the worst day for the index during the past five years. The worst one-day decline since Jan. 27, 2020, came on March 16, 2020, when the index plunged more than 12% as COVID-19 pandemic hit the economy.\n\nWhat is DeepSeek?\n\nDeepSeek is a private Chinese company founded in July 2023 by Liang Wenfeng, a graduate of Zhejiang University, one of China's top universities, who funded the startup via his hedge fund, according to the MIT Technology Review. Liang has about $8 billion in assets, Ives wrote in a Jan. 27 research note.\n\nLiang, who had previously focused on applying AI to investing, had bought a \"stockpile of Nvidia A100 chips,\" a type of tech that is now banned from export to China. Those chips became the basis of DeepSeek, the MIT publication reported.\n\nBen Reitzes, head of technology research at Melius, told investors in a note that DeepSeek makes legitimate breakthroughs as an AI tool, including better learning and more efficient use of memory, although he expressed skepticism about the \"amount of chips used.\"\n\nIs DeepSeek available in the U.S.?\n\nThe company's AI app is available in Apple's App store, as well as online at its website. The service is free and as of Monday morning was the top download on Apple's store, although some people were having trouble signing up for the app.\n\nOn its Chinese site, DeepSeek blamed \"large-scale malicious attacks\" on its service, requiring it to temporarily limit new registrations. \"Existing users can log in as usual,\" the company said in the post, which was dated shortly after midnight Jan. 28 in China's local time.\n\nThe company released its latest AI model on Jan. 20, which is causing Wall Street to reappraise the AI sector.\n\n\"Last week DeepSeek launched a model that rivals OpenAI's ChatGPT and Meta's Llama 3.1 and was #1 on Apple's App Store over the weekend,\" Wedbush's Ives wrote. \"DeepSeek built the model using reduced capability chips from Nvidia. which is impressive and thus has caused major agita for U.S. tech stocks with massive pressure on Nasdaq this morning.\"\n\nHow is DeepSeek different than other AI apps?\n\nDeepSeek is an open-source large language model that relies on what is known as \"inference-time computing,\" which Sette said in layman's terms means \"they activate only the most relevant portions of their model for each query, and that saves money and computation power.\"\n\nSome experts praised DeepSeek's performance, with noted tech investor Marc Andreessen writing on X on Jan. 24, \"DeepSeek R1 is one of the most amazing and impressive breakthroughs I've ever seen \u2014 and as open source, a profound gift to the world.\"\n\nDavid Sacks, a venture capitalist named by Mr. Trump to help oversee AI and cryptocurrency policy, said on social media Monday that DeepSee's app \"shows that the AI race will be very competitive.\"\n\nHowever, Ives said he's skeptical the service will gain ground with major U.S. businesses.\n\n\"No U.S. Global 2000 is going to use a Chinese startup DeepSeek to launch their AI infrastructure and use cases,\" Ives wrote. \"At the end of the day there is only one chip company in the world launching autonomous, robotics, and broader AI use cases and that is Nvidia.\"\n\nAre there privacy issues with DeepSeek?\n\nSome experts are raising concerns about the personal data that DeepSeek is collecting, given that the company stores data from users \u2014 including their date of birth, keystrokes, text or audio inputs, uploaded files, chat history and other data \u2014 on servers located in China, according to its privacy policy.\n\nThat echoes some of the issues raised with TikTok, another company with Chinese ownership that sparked worries about the supposed risks its ties to China posed to national security. Last year, Congress passed a law banning TikTok in the U.S. as long as it is under Chinese ownership, although that is now in flux after President Trump signed an executive order directing the Justice Department to not enforce the ban for 75 days.\n\nWhat does DeepSeek mean for Nvidia and other tech companies?\n\nWall Street is trying to assess the long-term impact of a low-cost AI tool from China that rivals ChatGPT and other so-called generative AI apps. It also raises questions about whether Silicon Valley is overspending on tech advancements in the AI sector, noted Angelo Zino, senior equity analyst at CFRA Research, in an email.\n\n\"The fact that this technology is supposed to take less energy and is more cost-effective than U.S.-based models has U.S. technology investors very concerned,\" Jay Woods, chief global strategist at Freedom Capital Markets, said.\n\nIt's also unclear what type of pushback or reaction could come from the White House, given that Mr. Trump has raised the possibility of placing new tariffs on Chinese imports, although he also gave the Chinese-owned TikTok a reprieve by ordering the Justice Department not to enforce a looming ban.\n\nSome Wall Street analysts think Monday's stock selloff is an overreaction, noting that the enormous demand for AI will continue lifting key players in the sector.\n\n\"It's one thing to train a [large language] model for less money, but accommodating the huge demand for the consumption of all this AI technology is still going to require massive amounts of infrastructure,\" Adam Crisafulli of VitalKnowledge said in a report.\n\nAnalysts at Bernstein Research also noted that while DeepSeek's technology looks promising, it likely isn't revolutionary, suggesting that Monday's investment rout is overblown.\n\nWhat is Nvidia saying about DeepSeek?\n\nIn a statement to CBS News, Nvidia offered praise for DeepSeek.\n\n\"DeepSeek is an excellent AI advancement and a perfect example of test-time scaling,\" the company said in an email. \"DeepSeek's work illustrates how new models can be created using that technique, leveraging widely available models and compute that is fully export-control compliant.\"\n\nBut, Nvidia added, AI inference, or using AI models to make decisions or predictions, \"requires significant numbers of NVIDIA GPUs and high-performance networking. We now have three scaling laws: pre-training and post-training, which continue, and new test-time scaling.\"",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia is in danger of losing its monopoly-like margins",
            "link": "https://www.economist.com/business/2025/01/28/nvidia-is-in-danger-of-losing-its-monopoly-like-margins",
            "snippet": "IT IS Common lore in Silicon Valley that no company has a bigger moat than Nvidia, the world's dominant supplier of chips for artificial intelligence (AI).",
            "score": 0.9560917019844055,
            "sentiment": null,
            "probability": null,
            "content": "I T IS Common lore in Silicon Valley that no company has a bigger moat than Nvidia, the world\u2019s dominant supplier of chips for artificial intelligence ( AI ). That was true until January 27th, when a Chinese firm called DeepSeek, bearing an AI model that it said cost less than $6m to train, blew a nearly $600bn hole in the value of the semiconductor giant, marking the biggest one-day loss in the history of America\u2019s stockmarket. Think of it as the 21st-century equivalent of a shot from a trebuchet, the medieval contraption capable of reducing castle walls to rubble. Have Nvidia\u2019s defences finally been breached?",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Opinion | I Study Financial Markets. The Nvidia Rout Is Only the Start.",
            "link": "https://www.nytimes.com/2025/01/28/opinion/nvidia-deepseek-ai-valuation-ouroboros.html",
            "snippet": "Nvidia's sharp turn of fortunes illustrates much deeper problems that investors are ignoring as they send the valuations of Big Tech to the heavens.",
            "score": 0.9239941835403442,
            "sentiment": null,
            "probability": null,
            "content": "During Monday\u2019s stock market swoon, Nvidia, the artificial intelligence giant, lost nearly $600 billion in value, the biggest single-day loss for a public company on record. How could the fortunes of one of our leading companies fall so far so suddenly? While some will seek answers in the promising A.I. start-up coming out of China or the vicissitudes of trade policy, these movements speak to deeper changes in our financial markets that can best be explained, oddly enough, by revisiting ancient mythology.\n\nThe image of the ouroboros, a serpent eating its own tail, is a remarkably durable and pervasive motif. Ancient Chinese, Egyptian, European and Latin American civilizations seemed captivated by the image or ones like it, variously symbolizing the cyclic nature of life, the totality of the universe or fertility. Today, the more resonant lesson comes from the self-cannibalism of the ouroboros, which helps us understand the most significant financial puzzle of our day.\n\nLike the ouroboros, I believe Big Tech is eating itself alive with its component companies throwing more and more cash at investments in one another that are most likely to generate less and less of a return. Monday\u2019s correction shows that our financial markets \u2014 and possibly your retirement portfolio \u2014 may be starting to reflect an understanding of this dynamic.\n\nEven after Monday\u2019s dip, the disjunction in valuations between Big Tech \u2014 sometimes referred to as the Magnificent 7 of Microsoft, Apple, Amazon, Nvidia, Tesla, Meta and Alphabet \u2014 and the rest of the stock market remains staggering. The Magnificent 7 still constitute more than 30 percent of the market capitalization of the S&P 500 (up from just under 10 percent a decade ago). When you compare their stock prices with their earnings or sales, the traditional way to measure the valuation of a share, our tech Goliaths trade at ratios that are two to three times those of the Unmagnificent 493.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Why NVIDIA's share price dropped 17% after DeepSeek news",
            "link": "https://www.ig.com/en/news-and-trade-ideas/why-nvidia-s-share-price-dropped-17--after-deepseek-news-250128",
            "snippet": "Explore why NVIDIA's stock fell 17%, wiping nearly $600 billion off its value, following DeepSeek's AI breakthrough. Learn about the market impact and...",
            "score": 0.9612458348274231,
            "sentiment": null,
            "probability": null,
            "content": "What caused NVIDIA's stock drop?\n\n\u200bNVIDIA's share price experienced an unprecedented decline on Monday, January 27, 2025, falling 17% and erasing nearly $600 billion in market value.\n\n\u200bThe dramatic sell-off was triggered by Chinese startup DeepSeek's announcement of their R1 artificial intelligence (AI) model, which reportedly achieves similar performance to Western models at significantly lower cost.\n\n\u200bDeepSeek's breakthrough has raised questions about future demand for high-performance AI chips, NVIDIA's core business. The model was developed using stockpiled NVIDIA graphics processing units (GPUs).\n\n\u200bThis development sent shockwaves through the semiconductor sector, affecting other major players like Advanced Micro Devices (AMD).\n\nMarket impact and broader implications\n\n\u200bThe semiconductor sector saw widespread declines as investors reassessed valuations. Companies including Marvell, Broadcom, and Taiwan Semiconductor Manufacturing Company (TSMC) all experienced significant drops. The US technology sector as a whole dropped by 5.6% on Monday.\n\n\u200bThe market reaction reflects growing concerns about competitive pressures in the AI chip industry, particularly from Chinese companies developing more cost-effective solutions.\n\n\u200bThese developments could impact how tech companies approach AI infrastructure investments, potentially focusing more on optimising existing resources rather than continuous hardware upgrades.\n\n\u200bHowever, some analysts suggest the market reaction may be overdone, citing potential benefits from increased AI adoption and efficiency.\n\nLong-term outlook for NVIDIA\n\n\u200bDespite the immediate market reaction, NVIDIA maintains a strong position in the AI sector. The company remains a crucial partner in major US AI infrastructure projects.\n\n\u200bSome industry experts, including Microsoft CEO Satya Nadella, argue that more efficient AI models could actually expand the market, potentially benefiting established players like NVIDIA.\n\n\u200bThis concept, known as the Jevons Paradox, suggests that increased efficiency often leads to higher overall demand rather than reduced consumption.\n\n\u200bNVIDIA's future success will likely depend on its ability to adapt to evolving AI technologies while maintaining its technological leadership.\n\nTrading considerations\n\n\u200bInvestors should consider both short-term volatility and long-term growth potential when evaluating semiconductor stocks. The sector may see continued turbulence as the market adjusts.\n\n\u200bRisk management becomes particularly important during periods of heightened uncertainty. Using tools like trading alerts can help monitor market movements.\n\nTechnical analysis on the NVIDIA share price\n\n\u200bTechnical analysis suggests that at least a medium-term top has been formed in NVIDIA's share price performance since a fall through its one-year uptrend line at $125.31 has taken place.\n\n\u200bNVIDIA weekly candlestick chart",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Why Nvidia Stock Rallied on Tuesday",
            "link": "https://www.fool.com/investing/2025/01/28/why-nvidia-stock-rallied-on-tuesday/",
            "snippet": "Nvidia stock rebounded after a rout sparked by reported artificial intelligence (AI) advances in China. Some investors are concerned that demand for...",
            "score": 0.9123468995094299,
            "sentiment": null,
            "probability": null,
            "content": "Fears regarding the rise of artificial intelligence (AI) chatbot DeepSeek may be overblown.\n\nShares of Nvidia (NVDA 5.27%) surged higher on Tuesday, after taking it on the chin Monday. The maker of graphics processing units (GPUs) used for artificial intelligence (AI) applications gained as much as 7.2%. As of 1:30 p.m. ET, the stock was still up 7.1%.\n\nInvestors have had a day to digest the news about reported AI developments in China -- and cooler heads are beginning to prevail.\n\nOn the heels of a rout\n\nNvidia stock was crushed yesterday following reports that Chinese AI start-up DeepSeek developed an AI model that delivered comparable results to existing chatbots at a fraction of the cost. The model, dubbed R1, reportedly used far fewer computing resources and was developed using older, less advanced AI chips.\n\nSome investors were concerned this could tank demand for Nvidia's top-shelf processors, and the stock plunged 17%, wiping nearly $600 billion from its market cap.\n\nMany on Wall Street believe the selling has simply gone too far. After digging into the data, Baird analysts are \"skeptical\" about reports that less advanced chips were used in R1's development. They further believe strong demand for Nvidia's flagship Hopper and Blackwell processors will continue.\n\nTigress Financial upgraded Nvidia to a strong buy and increased its price target to $220, which represents potential upside for investors of 86% compared to Monday's closing price. The analysts posit that demand from data centers will remain strong. Nvidia is the leading provider of data center GPUs and stands to benefit from this ongoing trend.\n\nWedbush analyst Dan Ives called the sell-off a \"golden buying opportunity.\" Even if DeepSeek's claims are true, the analyst suggests that \"no U.S. Global 2000 company\" will pin their AI future on a Chinese start-up.\n\nNvidia chimes in\n\nNvidia CEO Jensen Huang waded into the debate, calling DeepSeek \"an excellent AI advancement,\" saying these developments will fuel further AI adoption, increasing demand for its processors.\n\nAstute investors will note that after yesterday's shellacking, Nvidia is much cheaper. The stock is selling for 47 times earnings, and while that's a premium, that's far below its three-year average of 82. Furthermore, analysts' consensus estimates predict Nvidia's earnings per share will hit $4.45 in fiscal 2026 (which begins later this month). That works out to just 27 times next year's earnings, a reasonable price considering the magnitude of the opportunity.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "DeepSeek\u2019s Rise Exposes Nvidia\u2019s Weakness",
            "link": "https://www.wsj.com/tech/ai/deepseeks-rise-exposes-nvidias-weakness-3bfb300f",
            "snippet": "A new Chinese AI model threatens to diminish the need for Nvidia's most-expensive chips, but some say the concerns are overblown.",
            "score": 0.7379083037376404,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia and the stock market come roaring back after historic sell-off",
            "link": "https://markets.businessinsider.com/news/stocks/us-stock-futures-nvidia-recover-deepseek-tech-wipeout-2025-1",
            "snippet": "After plunging 17% on Monday, Nvidia climbed almost 9% as investors rushed in to buy this historic dip in shares of the top chip maker.",
            "score": 0.8006539344787598,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia rallied to gain almost 9% on Tuesday, a day after notching the biggest single-day market-cap loss ever.\n\nMajor indexes also reversed course after a tech-led crash hit the S&P 500 and Nasdaq on Monday.\n\nInvestors are closely watching for more details on DeepSeek and bracing for mega-cap tech earnings on Wednesday.\n\nAfter Monday's massive exodus out of US tech stocks, investors proved eager to buy the dip in Tuesday's session.\n\nNvidia climbed to end more than 8% higher at $128.86 a share, bouncing back from a drastic double-digit plunge that wiped out $589 billion of its market cap.\n\nThe S&P 500 and tech-heavy Nasdaq Composite also regained ground, jumping about 1% and 2%, respectively.\n\nHere's where US indexes stood at the 4 p.m. closing bell on Tuesday:\n\nNvidia and the broader market's sell-off was sparked by the reveal of DeepSeek, an AI tool from a startup in China on par with US technology, but at a fraction of the cost. Other tech mega-cap stocks weren't spared, tanking benchmark indexes.\n\nInvestors fled over fear that DeepSeek's chatbot marked the end of Wall Street's AI dominance, but, by Tuesday, this began to seem like an overreaction.\n\nAmerican AI hyperscalers are still \"fantastic companies,\" said Robert Teeter, chief investment strategist at Silvercrest Asset Management. They're not necessarily set to lose out from DeepSeek's discounted model.\n\n\"Anytime you get this intense competition, the likely outcome is costs come down, and when costs come down, you get broader use of AI and broader opportunity to deploy that AI in a way that improves productivity and margins,\" he told CNBC.\n\nGene Munster, a managing partner at Deepwater Asset Management, questioned whether DeepSeek's tool will still be able to keep spending down when it comes to developing higher-level AI.\n\nMeanwhile, stocks are likely rebounding ahead of this week's mega-cap earnings, with Tesla Microsoft, and Meta scheduled to report on Wednesday.\n\n\"Consensus is that they're going to have some sort of reassuring comments, that the AI trade, the hardware trade, the capex spending is intact,\" the tech investors said during an interview with CNBC.\n\nSemiconductor firms were Monday's biggest losers, but the sector broadly recovered alongside Nvidia. Broadcom and Oracle climbed almost 3% and 4%, respectively.\n\nBond yields edged up, reversing a downtrend that took over on Monday as investors piled into safe-haven assets amid the spike in tech-related risks. The 10-year Treasury yield rose to 4.538%.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-27": {
        "0": {
            "title": "Nvidia says DeepSeek advances prove need for more of its chips",
            "link": "https://www.reuters.com/technology/nvidia-says-deepseek-advances-prove-need-more-its-chips-2025-01-27/",
            "snippet": "Nvidia on Monday said Chinese AI firm DeepSeek's advances show the usefulness of its chips for the Chinese market and that more of its chips will be needed...",
            "score": 0.7546066045761108,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "Nvidia sheds almost $600 billion in market cap, biggest one-day loss in U.S. history",
            "link": "https://www.cnbc.com/2025/01/27/nvidia-sheds-almost-600-billion-in-market-cap-biggest-drop-ever.html",
            "snippet": "Nvidia shares plunged 17% on Monday, resulting in a market cap loss of close to $600 billion, the biggest drop ever for a U.S. company.",
            "score": 0.9645382165908813,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang holds a Blackwell GeForce RTX 50 Series GPU (L) and a RTX 5000 laptop as he delivers a keynote address at the Consumer Electronics Show (CES) in Las Vegas, Nevada on January 6, 2025.\n\nNvidia lost close to $600 billion in market cap on Monday, the biggest drop for any company on a single day in U.S. history.\n\nThe chipmaker's stock price plummeted 17% to close at $118.58. It was Nvidia's worst day on the market since March 16, 2020, which was early in the Covid pandemic. After Nvidia surpassed Apple last week to become the most valuable publicly traded company, the stock's drop Monday led a 3.1% slide in the tech-heavy Nasdaq.\n\nThe sell-off was sparked by concerns that Chinese artificial intelligence lab DeepSeek is presenting increased competition in the global AI battle. In late December, DeepSeek unveiled a free, open-source large language model that it said took only two months and less than $6 million to build, using reduced-capability chips from Nvidia called H800s.\n\nNvidia's graphics processing units, or GPUs, dominate the market for AI data center chips in the U.S., with tech giants such as Alphabet , Meta and Amazon spending billions of dollars on the processors to train and run their AI models.\n\nAnalysts at Cantor wrote in a report Monday that the release of DeepSeek's latest technology has caused \"great angst as to the impact for compute demand, and therefore, fears of peak spending on GPUs.\"",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia loses nearly $600 billion in market value after Chinese AI startup bursts onto scene",
            "link": "https://www.nbcnews.com/business/business-news/nvidia-loses-market-value-chinese-ai-startup-deepseek-debut-rcna189431",
            "snippet": "Shares of chipmaker Nvidia plunged Monday, for its worst day since the global market sell-off in March 2020 triggered by the coronavirus pandemic.",
            "score": 0.9611655473709106,
            "sentiment": null,
            "probability": null,
            "content": "Shares of chipmaker Nvidia plunged Monday, for its worst day since the global market sell-off in March 2020 triggered by the coronavirus pandemic.\n\nThe plunge came amid a global tech stock sell-off over fears about America's leadership in the AI sector. Those fears were largely sparked by advances claimed by a Chinese artificial intelligence startup.\n\nShares of the chipmaker, one of the primary beneficiaries of the artificial intelligence boom in tech stocks, plummeted as much as 18%. That pushed Nvidia's market value below $3 trillion. Still, shares of the firm are up more than 480% over the last two years.\n\nThe drop accounted for nearly $600 billion in lost market value though. It is the biggest market value drop in U.S. stock market history, according to Bloomberg. And nearly double the second worst drop in history, also seen by Nvidia shareholders in September 2024, when the company shed $279 billion in value.\n\nFor some perspective, the amount of market value lost by Nvidia on Monday is more than the entire market value of Exxon Mobil, Costco, Home Depot or Bank of America.\n\nNvidia said in a statement Monday that advances by DeepSeek, a Chinese AI company whose latest large language model has taken Silicon Valley by storm, were an example of what is possible, \u201cleveraging widely available models and compute that is fully export control compliant.\u201d\n\nThe company added that the surge in interest in DeepSeek, which topped the Apple app store on Monday, could create more demand for its graphics processing units, or GPUs.\n\n\u201cInference requires significant numbers of NVIDIA GPUs and high-performance networking,\u201d it said.\n\nDue to the AI-fueled surge in mega-cap tech stocks, Nvidia catapulted into the top five most valuable companies in the world in 2023. The surge didn't stop there, with the company soaring past Alphabet, Microsoft and the most valuable company in the world: Apple. At its most recent peak, Nvidia reached a towering $3.7 trillion.\n\nWith Monday's losses, Apple has retaken the title of world's most valuable company and Nvidia's value sank to around $2.9 trillion.\n\nNvidia's drop was also a drag on the Dow Jones Industrial Average, which finished the day higher but began the day in the red. Nvidia joined the prestigious 30-stock index in November, replacing rival chipmaker Intel. The Nasdaq Composite, which more closely tracks publicly traded tech companies, slid around 3%.\n\nThe global sell-off in tech stocks also meant the S&P Technology sector fell into the red for the year so far, the only sector lower over that time.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia stock plummets, loses record $589 billion as DeepSeek prompts questions over AI spending",
            "link": "https://finance.yahoo.com/news/nvidia-stock-plummets-loses-record-589-billion-as-deepseek-prompts-questions-over-ai-spending-135105824.html",
            "snippet": "Nvidia stock dropped more than 11% as a social media panic over the potential rise of a Chinese AI model upended chip stocks and the broader AI investment...",
            "score": 0.9636831283569336,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock dropped nearly 17% Monday, leading a sell-off across chip stocks and the broader market after a new AI model from China's DeepSeek raised questions about AI investment and the rise of more cost-efficient artificial intelligence agents.\n\nNvidia's decline shaved $589 billion off the AI chipmaker's market cap, the largest single-day loss in stock market history.\n\nChinese startup DeepSeek released a new AI model on Jan. 20 viewed as a threat to OpenAI. American venture capitalist Marc Andreessen called the model \u201cone of the most amazing and impressive breakthroughs I\u2019ve ever seen.\u201d\n\nThe news came just a month after DeepSeek said one of its latest AI models cost just $5.6 million to train. OpenAI\u2019s GPT-4 model cost more than $100 million to train.\n\nThe announcements spurred fears that AI models may begin to require fewer chips and energy than they currently use. Nvidia has become the world's largest company on the back of exploding demand for its high-end chips that help train and use AI models.\n\n\u201cIf DeepSeek\u2019s innovations are adopted broadly, an argument can be made that model training costs could come down significantly even at U.S. hyperscalers, potentially raising questions about the need for 1-million XPU/GPU clusters as projected by some,\u201d wrote Raymond James semiconductor analyst Srini Pajjuri in a note to investors Sunday evening.\n\nChip stocks also dropped across the board Monday, with Broadcom (AVGO) down over 17%, Micron (MU) off almost 12%, and Advanced Micro Devices (AMD) down more than 6%.\n\nHowever, Pajjuri continued, \u201cA more logical implication is that DeepSeek will drive even more urgency among U.S. hyperscalers to leverage their key advantage (access to GPUs) to distance themselves from cheaper alternatives.\u201d\n\nBernstein analyst Stacy Rasgon also called into question the $5.6 million training cost for DeepSeek\u2019s model, which \u201cdoes not include all the other costs associated with prior research and experiments on architectures, algorithms, or data.\u201d\n\nRasgon believes DeepSeek's announcement was \"not really worthy of the hysteria that has taken over the Twitterverse over the last several days.\"\n\nDeepSeek and Nvidia logos. (Photo by Costfoto/NurPhoto via Getty Images) \u00b7 NurPhoto via Getty Images\n\nNvidia itself didn't seem too worried about the DeepSeek buzz, calling R1 \"an excellent AI advancement\" in a statement.\n\nTightened US export restrictions announced in former President Joe Biden\u2019s final days in office could also add a threat to DeepSeek\u2019s ability to continue training new models.\n\nThe new rules limit China\u2019s ability to buy Nvidia\u2019s AI chips through resellers and access chips in remote data centers. And China is restricted from importing the most advanced chipmaking machines required to make artificial intelligence semiconductors from the Dutch firm ASML (ASML).",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Nvidia Loses $589 Billion as DeepSeek Batters Stock: Evening Briefing Americas",
            "link": "https://www.bloomberg.com/news/newsletters/2025-01-27/nvidia-loses-589-billion-as-deepseek-batters-stock-evening-briefing-americas",
            "snippet": "Wall Street had a shock to the system on concerns that a cheap artificial intelligence-model from Chinese startup DeepSeek could make valuations that have...",
            "score": 0.703168511390686,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Biggest Market Loss In History: Nvidia Stock Sheds Nearly $600 Billion As DeepSeek Shakes AI Darling",
            "link": "https://www.forbes.com/sites/dereksaul/2025/01/27/biggest-market-loss-in-history-nvidia-stock-sheds-nearly-600-billion-as-deepseek-shakes-ai-darling/",
            "snippet": "Nvidia lost more market value Monday than the total valuations of American stalwarts UnitedHealth, Costco and Bank of America.",
            "score": 0.967208743095398,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Nvidia Loses More Than $590 Billion in Market Cap, Biggest One-Day Fall",
            "link": "https://www.wsj.com/livecoverage/stock-market-today-dow-sp500-nasdaq-live-01-27-2025/card/nvidia-stock-is-down-more-than-10-here-s-why--sZmsM8tvQFTS3iUBASHa",
            "snippet": "The sharp pullback in Nvidia shares Monday cost investors $592.7 billion in paper losses as investors fled the stock amid rapid advancements at [China's...",
            "score": 0.940848171710968,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Why Nvidia Stock Just Had Its Worst Day Since 2020",
            "link": "https://www.investopedia.com/nvidia-stock-worst-day-since-2020-deepseek-ai-wall-street-8780814",
            "snippet": "Shares of Nvidia tumbled Monday after the release of a sophisticated artificial intelligence model from a Chinese startup prompted a reckoning regarding AI...",
            "score": 0.9261120557785034,
            "sentiment": null,
            "probability": null,
            "content": "Shares of Nvidia (NVDA) tumbled Monday after the release of a sophisticated artificial intelligence model from a Chinese startup prompted a reckoning on Wall Street regarding AI spending.\n\nChinese company DeepSeek last week released R1, an AI model that appears to rival the capabilities of models from U.S. tech giants like OpenAI and Google despite running on fewer, less powerful chips. U.S. investors began to take notice of the model late last week, and over the weekend started to worry about what it means for richly valued U.S. tech stocks.\n\nNvidia\u2019s earnings and stock price have soared in the past two years as tech giants like Microsoft (MSFT), Alphabet (GOOG)(GOOGL), and Amazon (AMZN) have spent massively on the company's AI systems. DeepSeek has led investors to question whether all that spending was necessary and, if not, why it would continue.\n\nNvidia stock was hammered by Monday\u2019s sell-off. Shares tumbled nearly 17%, its largest one-day drop since the Covid-19 crash in March 2020. Monday\u2019s sell-off wiped about $589 billion from Nvidia\u2019s market cap, the largest loss in history.\n\n\n\nWhat Analysts Were Saying\n\nWall Street analysts were mostly skeptical about DeepSeek and the sell-off it prompted, though their commentary often reflected uncertainty about the effects DeepSeek could have on the market.\n\nCiti analysts expressed doubt that DeepSeek had achieved its results without the most advanced chips. They maintained their \"buy\" rating on Nvidia stock and said they don't expect major U.S. AI companies to move away from using its advanced GPUs.\n\nJefferies analysts, however, noted DeepSeek's success could press Silicon Valley management to \"refocus on efficiency and ROI, meaning lower demand for computing power as of 2026.\"\n\nConcern about future AI models requiring less computing power weighed on other high-flying AI stocks, including nuclear power providers Vistra (VST) and Constellation Energy Corp. (CEG), which slumped 28% and 21%, respectively. Nvidia competitor Broadcom (AVGO) also dropped about 17%.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "China's DeepSeek AI Model Shocks the World: Should You Sell Your Nvidia Stock?",
            "link": "https://www.fool.com/investing/2025/01/27/chinas-deepseek-ai-model-shocks-world-sell-nvidia/",
            "snippet": "Could Nvidia's (NVDA 2.63%) magical two-year run be coming to an end? Up until now, there has been insatiable demand for Nvidia's latest and greatest...",
            "score": 0.7987436056137085,
            "sentiment": null,
            "probability": null,
            "content": "Could Nvidia's (NVDA 5.27%) magical two-year run be coming to an end? Up until now, there has been insatiable demand for Nvidia's latest and greatest graphics processing units (GPUs). As the artificial intelligence races heated up, big tech companies and start-ups alike rushed to buy or rent as many of Nvidia's high-performance GPUs as they could in a bid to create better and better models.\n\nBut last week, Chinese AI start-up DeepSeek released its R1 model that stunned the technology world. R1 is a \"reasoning\" model that has matched or exceeded OpenAI's o1 reasoning model, which was just released at the beginning of December, for a fraction of the cost.\n\nBeing able to generate leading-edge large language models (LLMs) with limited computing resources could mean that AI companies might not need to buy or rent as much high-cost compute resources in the future. The consequences could be devastating for Nvidia and last year's AI winners alike.\n\nBut as always, the truth is more complicated.\n\nWhat is DeepSeek?\n\nDeepSeek is an AI lab spun out of a quantitative hedge fund called High-Flyer. CEO Liang Wenfeng founded High-Flyer in 2015 and began the DeepSeek venture in 2023 after the earth-shaking debut of ChatGPT.\n\nDeepSeek has been building AI models ever since, reportedly purchasing 10,000 Nvidia A100s before they were restricted, which are two generations prior to the current Blackwell chip. DeepSeek also reportedly has a cluster of Nvidia H800s, which is a capped, or slowed, version of the Nvidia H100 designed for the Chinese market. Of note, the H100 is the latest generation of Nvidia GPUs prior to the recent launch of Blackwell.\n\nR1 shocks the world\n\nOn Jan. 20, DeepSeek released R1, its first \"reasoning\" model based on its V3 LLM. Reasoning models are relatively new, and use a technique called reinforcement learning, which essentially pushes an LLM to go down a chain of thought, then reverse if it runs into a \"wall,\" before exploring various alternative approaches before getting to a final answer. Reasoning models can therefore answer complex questions with more precision than straight question-and-answer models can't.\n\nIncredibly, R1 has been able to meet or even exceed OpenAI's o1 on several benchmarks, while reportedly trained at a small fraction of the cost.\n\nJust how cheap are we talking about? The R1 paper claims the model was trained on the equivalent of just $5.6 million rented GPU hours, which is a small fraction of the hundreds of millions reportedly spent by OpenAI and other U.S.-based leaders. DeepSeek is also charging about one-thirtieth of the price it costs OpenAI's o1 to run, while Wenfeng maintains DeepSeek charges for a \"small profit\" above costs. Experts have estimated that Meta Platforms' (META 2.96%) Llama 3.1 405B model cost about $60 million of rented GPU hours to run, compared with the $6 million or so for V3, even as V3 outperformed Llama's latest model on a variety of benchmarks.\n\nHow DeepSeek Pulled It Off\n\nAccording to an informative blog post by Kevin Xu, DeepSeek was able to pull this minor miracle off with three unique advantages.\n\nFirst, Wenfang built DeepSeek as sort of an idealistic AI research lab without a clear business model. Currently, DeepSeek charges a small fee for others seeing to build products on top of it, but otherwise makes its open-source model available for free. Wenfang also recruited largely young people who have just graduated from school or who were in Ph.D. programs at China's top universities. This led to a culture of free experimentation and trial-and-error without big expectations, and set DeepSeek apart from China's tech giants.\n\nSecond, DeepSeek uses its own data center, which allowed it to optimize the hardware racks for its own purposes.\n\nFinally, DeepSeek was then able to optimize its learning algorithms in a number of ways that, taken together, allowed DeepSeek to maximize the performance of its hardware.\n\nFor instance, DeepSeek built its own parallel processing algorithm from the ground up called the HAI-LLM framework, which optimized computing workloads across its limited number of chips. DeepSeek also uses F8, or 8-bit, data input framework, a less-precise framework than F32. While F8 is \"less precise,\" it also saves a ton in memory utilization, and R1's other processes were also able to then make up for the lack of precision with a greater number of efficient calculations. DeepSeek also optimized its load-balancing networking kernel, maximizing the work done by each H800 cluster, so that no hardware was ever left \"waiting\" for data.\n\nThese are just a few of the innovations that allowed DeepSeek to do more with less. But when cobbling all of these \"hacks\" together, it led to a remarkable increase in performance.\n\nThe negative implication for Nvidia is that by innovating at the software level as DeepSeek has done, AI companies may become less dependent on hardware, which could affect Nvidia's sales growth and margins.\n\nCounterpoints to the doom thesis\n\nAs dire as R1 may seem for Nvidia, there are several counterpoints to the thesis that Nvidia is \"doomed.\"\n\nFirst, some are skeptical that the Chinese startup is being totally forthright in its cost estimates. According to machine learning researcher Nathan Lampbert, the $5.6 million figure of rented GPU hours probably doesn't account for a number of extra costs. These extra costs include significant pre-training hours prior to training the large model, the capital expenditures to buy GPUs and construct data centers (if DeepSeek truly built its own data center and didn't rent from a cloud), and high energy costs. There is also the matter of DeepSeek's engineering salaries, as R1 had 139 technical authors. Since DeepSeek is open-source, not all of these authors are likely to work at the company, but many probably do, and make a sufficient salary.\n\nLampert estimates DeepSeek's annual costs for operations are probably closer to between $500 million and $1 billion. That's still far below the costs at its U.S. rivals, but obviously much more than the $6 million put forth by the R1 paper.\n\nThere are also some who simply doubt DeepSeek is being forthright in its access to chips. In a recent interview, Scale AI CEO Alexandr Wang told CNBC he believes DeepSeek has access to a 50,000 H100 cluster that it isn't disclosing, because those chips are illegal in China following 2022 export restrictions.\n\nHowever, given that DeepSeek has openly published its techniques for the R1 model, researchers should be able to emulate its success with limited resources. As of now, it appears the R1 efficiency breakthrough is more real than not.\n\nEven if true, it may not be over for Nvidia\n\nWhile DeepSeek is no doubt impressive, ex-OpenAI executive Miles Brundage also cautioned against reading too much into R1's debut. Brundage notes that OpenAI is already out with its o3 model and soon its o5 model. While DeepSeek has been able to hack its way to R1 with novel techniques, its limited computing power is likely to slow down the pace at which it can scale up and advance from its first reasoning model.\n\nBrundage also notes that limited computing resources will affect how these models can perform simultaneously in the real world:\n\nEven if that's the smallest possible version while maintaining its intelligence -- the already-distilled version -- you'll still want to use it in multiple real-world applications simultaneously. You wouldn't want to choose between using it for improving cyber capabilities, helping with homework, or solving cancer. You'd want to do all of these things. This requires running many copies in parallel, generating hundreds or thousands of attempts at solving difficult problems before selecting the best solution. ... To make a human-AI analogy, consider Einstein or John von Neumann as the smartest possible person you could fit in a human brain. You would still want more of them. You'd want more copies. That's basically what inference compute or test-time compute is -- copying the smart thing. It's better to have an hour of Einstein's time than a minute, and I don't see why that wouldn't be true for AI.\n\nThe Jevons paradox\n\nFinally, investors should keep in mind the Jevons paradox. Coined by English economist William Stanley Jevons in 1865 regarding coal usage, this is the phenomenon that occurs when a technological process is made more efficient. According to Jevon's paradox, if a resource is used more efficiently, rather than seeing a decrease in the use of that resource, consumption increases exponentially. The increased demand then usually more than fully offsets the efficiency gained, leading to an overall increase in demand for that resource.\n\nFor AI, if the cost of training advanced models falls, look for AI to be used more and more in our daily lives. That should, according to the paradox, actually increase demand for computing power -- although probably more for inference rather than training. So that could actually benefit Nvidia, strangely. On the other hand, it is thought that AI inferencing may be more competitive relative to training for Nvidia, so that may be a negative. But that negative would arise from more competition, not decreased computing demand.\n\nThe bottom line is that demand for AI computing should continue to grow a lot for years to come. After all, on Jan. 24, Meta Platforms CEO Mark Zuckerberg announced that Meta would be building an AI data center almost as big as Manhattan and will ramp up its capital spending to a range of $60 billion to $65 billion this year, up from a range of $38 billion to $40 billion in 2024.\n\nThis announcement came four days after DeepSeek's release, so there was no way Zuckerberg wasn't aware of it. Yet he still thinks a huge 50%-plus increase in AI infrastructure spending is warranted.\n\nNo doubt, the advent of DeepSeek will have an effect on the AI races. But rather than being \"game over\" for Nvidia and other \"Magnificent Seven\" companies, the reality will be more nuanced.\n\nAs the AI races progress, investors will have to assess which companies have a true AI \"moat,\" as AI business models evolve at rapid speed and in surprising ways, as DeepSeek R1 just showed.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia plunges 18% and tech stocks slide as China's DeepSeek spooks investors",
            "link": "https://markets.businessinsider.com/news/stocks/nvidia-tech-stocks-deepseek-ai-race-nasdaq-2025-1",
            "snippet": "Shares of chip titan Nvidia plunged on Monday amid the markets' fears over a new artificial intelligence tool from a startup in China.",
            "score": 0.9603323936462402,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock plunged as much as 18% Monday as investors reacted to a new AI model from China.\n\nDeepSeek has challenged market narratives around AI, valuations, and high spending.\n\nOther stocks tied to the AI trade dropped, including Broadcom, Microsoft, and Alphabet.\n\nShares of chip titan Nvidia plunged on Monday amid the markets' fears over a new artificial intelligence tool from a startup in China.\n\nNvidia stock was down as much as 17.7% Monday morning, trading as low as $117.26. The stock had recovered slightly to trade at $120.56 just before noon ET.\n\nTech stocks were hammered after an unexpected launch by the Chinese artificial intelligence startup DeepSeek spooked investors.\n\nThe Nasdaq 100 fell about 3% and the S&P 500 was down nearly 2%.\n\nNvidia, led by CEO Jensen Huang, stands to have hundreds of billions wiped off its value if those losses hold until the end of the day. It closed Friday as the world's most valuable company worth almost $3.5 trillion after Nvidia stock gained almost 130% over the prior 12 months.\n\nMicrosoft was nearly 4% shortly before midday, while Palantir fell 6%, and Alphabet dipped 3%.\n\nDutch chip equipment maker ASML also fell 7.7% in Amsterdam.\n\nIn Asia, the OpenAI investor SoftBank fell more than 8%, while Tokyo Electron slipped 4.9%. The Japanese chip companies Disco Corp and Advantest, a key supplier to Nvidia, were also down 1.8% and 8.6% respectively.\n\nThe declines come after DeepSeek unveiled a new flagship AI model called R1 that showcases a new level of \"reasoning.\" The Chinese AI lab's solution, released in a paper last Monday, was close in capability to OpenAI's model while being far cheaper.\n\nOpenAI fully released o1 \u2014 \"models designed to spend more time thinking before they respond\" \u2014 to a positive reception in December.\n\nDeepSeek has shown how quickly it can close the gap, sparking fears among investors, politicians, and developers about how long the US can maintain supremacy in the AI race.\n\n\"While they don't offer the cutting-edge tech of Nvidia's graphics processing units, the efficacy of the budget version and the willingness of DeepSeek to share its know-how may start to chip away at Nvidia's dominance,\" said Hargreaves Lansdown's Susannah Streeter.\n\nBuying opportunity\n\nCompeting US and Chinese spheres of AI influence look set to emerge, Streeter added.\n\nDan Ives of Wedbush Securities wrote in a morning note that the AI-related slide represented a rare buying opportunity, particularly for Nvidia.\n\n\"No US Global 2000 is going to use a Chinese start-up DeepSeek to launch their AI infrastructure and use cases,\" he wrote. \"At the end of the day there is only one chip company in the world launching autonomous, robotics, and broader AI use cases and that is Nvidia. Launching a competitive LLM model for consumer use cases is one thing ... launching broader AI infrastructure is a whole other ballgame and nothing with DeepSeek makes us believe anything different.\"\n\nSince last Monday, the DeepSeek app has hit No. 1 on the Apple app store's Top Free Apps chart. The Silicon Valley investor Marc Andreessen wrote on X that DeepSeek's R1 was one of \"the most amazing and impressive breakthroughs\" he'd ever seen.\n\nPresident Donald Trump is pushing for the US's AI development to accelerate. He signed an executive order on Thursday calling for the US \"to sustain and enhance America's global AI dominance\" and announced an up-to $500 billion private-sector AI infrastructure investment project called Stargate.\n\n\"The AI super-race is seeing new challengers emerge and not everyone is going to win. The companies that enjoyed first-mover advantage will now be under pressure to launch something even better or be left behind,\" said Russ Mould, an AJ Bell investment director, in a morning note.\n\n\"These market movements suggest investors are worried about disruption to what has so far been an easy ride for most stocks linked to the AI theme.\"\n\nThe high-performance budget offering from DeepSeek will also \"put into question the necessity of spending hundreds of billions of dollars on Nvidia chips and development going forward,\" said Joshua Mahony of Scope Markets.\n\nPotential upside\n\nDeepSeek's advances show that the huge investment undertaken by Big Tech hasn't made them impenetrable to Chinese competition, he said.\n\nYet there could be positives for non-tech US businesses.\n\n\"The potential for businesses to obtain the benefits of AI at a fraction of the cost could be good news for the wider market,\" said Mahony.\n\nA slew of Big Tech earnings are due this week, with updates from Meta, Microsoft, Apple, and Tesla. Many have spent huge sums on AI and investors will be assessing when, or if, returns on the investment are likely.\n\nCorrection: January 27, 2025 \u2014 An earlier version of this story misstated the source of the funding behind the Stargate AI project. A private-sector consortium, not the Trump administration, is funding the initiative.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-01-26": {
        "0": {
            "title": "Buying Nvidia Stock Looks Like a No-Brainer After This Key Event",
            "link": "https://www.fool.com/investing/2025/01/26/buying-nvidia-stock-looks-like-a-no-brainer-after/",
            "snippet": "TSMC reported Q4 2024 results this month, and its guidance for 2025 points to another solid year for AI chip sales. Nvidia is one of TSMC's largest...",
            "score": 0.9217358231544495,
            "sentiment": null,
            "probability": null,
            "content": "Taiwan Semiconductor Manufacturing (TSM 1.46%) released its fourth-quarter 2024 results on Jan. 16, and the semiconductor giant's results offered insight into the state of the semiconductor industry.\n\nPopularly known as TSMC, the Taiwan-based company is a semiconductor foundry that makes chips for fabless chipmakers that do not own any manufacturing facilities of their own, such as Nvidia (NVDA 5.27%), along with consumer electronics companies. TSMC points out that it served 522 customers last year, manufacturing almost 12,000 products across multiple categories such as smartphones, personal computers (PCs), data centers, automotive applications, and the Internet of Things (IoT).\n\nLast quarter, its revenue increased 37% year over year to $26.9 billion, and management served up terrific guidance for Q1 2025 that calls for top-line growth of 34% to $25.4 billion (at the midpoint). For the full year, TSMC expects its 2025 revenue to increase in the mid-20% range.\n\nAll of that suggests demand for the chips the company manufactures is set to remain healthy, which is encouraging news for Nvidia shareholders too.\n\nTSMC's results point to better times for Nvidia\n\nNvidia is among TSMC's top three customers, and the artificial intelligence (AI) pioneer has faced various challenges recently, including a potential slowdown in spending on AI infrastructure, stiffer competition, and the recent restrictions imposed by the U.S. government on sales of chips to foreign countries. However, TSMC's outlook is a bullish signal that Nvidia could have another solid year in 2025.\n\nAfter all, TSMC is anticipating sales of AI chips to double in 2025, owing to \"the strong surge in AI-related demand.\" That explains why the company's capital expenditures (capex) this year will range from $38 billion to $42 billion, a substantial jump over last year's outlay of $29.8 billion. Management noted that around 70% of its 2025 capex will be dedicated to the manufacture of advanced chips.\n\nThese advanced chips are based on process nodes that are 7-nanometer (nm) or smaller. TSMC generated 74% of its Q4 chip revenue from sales of these advanced technologies, up from 67% in the year-ago quarter. More specifically, 60% of the company's revenue came from 3nm and 5nm chips, which is not surprising as these process nodes are being used for manufacturing AI graphics processing units (GPUs) for the likes of Nvidia.\n\nTSMC's decision to invest significantly in these advanced process nodes is a clear signal the demand for AI chips remains healthy. This also means Nvidia should be able to manufacture more AI GPUs in 2025 thanks to TSMC's higher capex. Additionally, Nvidia has reportedly been allocated 60% of TSMC's advanced chip packaging capacity this year.\n\nAll of this puts Nvidia on course to fulfill more orders for its latest Blackwell chips, and the company is already seeing demand exceeding its supply. As a result, there is a strong possibility of Nvidia beating Wall Street's expectations this year. This is probably why Nvidia's revenue estimate for fiscal 2026 (which begins later this month) has moved higher once again following a dip caused by the announcement of the proposed export restrictions.\n\nWall Street seems to have regained confidence in Nvidia's ability to support its growth streak in 2025. TSMC also remarked on the latest earnings call that any potential export restrictions should be \"manageable.\"\n\nThe valuation remains attractive\n\nDespite its 171% gain last year, investors can still get their hands on Nvidia stock at a reasonable valuation -- about 30 times fiscal 2026 earnings estimates. Analysts are projecting a 51% increase in Nvidia's earnings next year to $4.45 per share, but the company may be able to beat that number based on TSMC's sunny outlook and capex spending.\n\nNvidia also sports a price/earnings-to-growth ratio (PEG ratio) of 0.99 based on its projected earnings growth for the next five years, according to Yahoo! Finance. A PEG ratio of less than 1 typically indicates a stock is undervalued with respect to the earnings growth it can deliver. Combine that with the strong outlook from its key foundry partner, and Nvidia remains an attractive buy in 2025.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Stock May Fall As DeepSeek\u2019s \u2018Amazing\u2019 AI Model Disrupts OpenAI",
            "link": "https://www.forbes.com/sites/petercohan/2025/01/26/nvidia-stock-may-fall-as-deepseeks-amazing-ai-model-disrupts-openai/",
            "snippet": "America's policy of restricting Chinese access to Nvidia's most advanced AI chips has unintentionally helped a Chinese AI developer leapfrog U.S. rivals who...",
            "score": 0.9685441255569458,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia RTX 5090 vs RTX 4090: we put Nvidia's best graphics cards head-to-head",
            "link": "https://www.techradar.com/computing/gpu/nvidia-rtx-5090-vs-rtx-4090",
            "snippet": "How does Nvidia's latest flagship compare against the last-gen powerhouse?",
            "score": 0.657569944858551,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia GeForce RTX 5090 $3,999.99 at Amazon $4,599.99 at Amazon $5,299.99 at Walmart Compute Units: 170\n\nShaders: 21,760\n\nRay processors: 170\n\nAI/Tensor processors: 680\n\nBoost clock: 2,407 MHz\n\nMemory type: GDDR7\n\nMemory pool: 32 GB\n\nMemory speed (effective): 28 Gbps\n\nMemory bandwidth: 1,790 GB/s\n\nBus interface: 512-bit\n\nTGP: 575W\n\nPower connector: 1 x 16-pin\n\nSlot width: Dual For Best-in-market performance\n\nGaming in 8K is actually possible, even without DLSS 4\n\nDLSS 4 is remarkable\n\nExcellent dual-slot design\n\nFinally, a PCIe 5.0 GPU Against Ludicrously expensive\n\nAn absolute glutton for wattage\n\nOverkill?\n\nYou'll need to have a premium gaming PC setup to really take advantage of this card Nvidia GeForce RTX 4090 $3,250 at Amazon $3,398 at Amazon Check Walmart Compute Units: 128\n\nShaders: 16,384\n\nRay processors: 128\n\nAI/Tensor processors: 512\n\nBoost clock: 2,520 MHz\n\nMemory type: GDDR6X\n\nMemory pool: 24 GB\n\nMemory speed (effective): 21 Gbps\n\nMemory bandwidth: 1,010 GB/s\n\nBus interface: 384-bit\n\nTGP: 450W\n\nPower connector: 1 x 16-pin\n\nSlot width: Triple For Jaw-dropping performance\n\nDLSS 3 is game changing\n\nCreatives will absolutely love it\n\nOutstanding value for a luxury card Against Still very expensive\n\n16-pin connector will test cable management skills\n\nRTX 4080 Super and RX 7900 XTX a better bet for gamers\n\nWith the launch of the Nvidia GeForce RTX 5090 on January 30, 2025, gamers, creatives, and all-around computing enthusiasts have a new flagship Nvidia card to put on their wishlist, and it's easy to see why.\n\nThe Nvidia RTX 5090 is easily the best graphics card on the market right now, offering fantastic performance for gaming, creating, and AI workloads that few other cards can compete with. There is one card that can hold its own against Nvidia's latest high-end card, and that's Nvidia's last-gen flagship, the Nvidia GeForce RTX 4090.\n\nWhile stock for the RTX 4090 is expected to become more scarce now that the RTX 5090 is on the scene, that doesn't mean that it's disappeared entirely, and for a lot of folks, the RTX 4090 might be the better option, especially if demand for the RTX 5090 forces the price of the RTX 4090 down from the ridiculously inflated heights we've seen pretty much since it launched.\n\nSo which of the two Nvidia graphics cards are the best bet for your needs and budget? Fortunately, I've extensively tested both cards and I'm here to guide you through your decision backed by data and my own experience with the two cards so you can buy the graphics card that's right for you.\n\nNvidia RTX 5090 vs RTX 4090: Price\n\n(Image credit: Future)\n\nSo there are a couple of caveats when discussing the price of the RTX 5090 vs RTX 4090, and that's how prices for the best Nvidia graphics cards have been all over the place in recent years.\n\nFor the record, the MSRP of the RTX 5090 is $1,999 / \u00a31,939 / AU$4,039 for the Founders Edition cards from Nvidia, and $1,599 \u00a31,679 / AU$2,959 for the Founders Edition RTX 4090 cards released back in October 2022.\n\nAlmost immediately the RTX 4090 started selling for a 50% premium or more, depending on the card and seller, and that's not even factoring in the MSRPs of the various third-party cards from Nvidia partners like PNY, MSI, Asus, Gigabyte, and others. Prices for these cards have shot above the $2,000 / \u00a32,000 / AU$4,000 mark and have stayed there pretty much since the RTX 4090's release.\n\nBut while it might look like the RTX 5090 and RTX 4090 are selling for the same price, the RTX 5090 is also going to face the exact same kind of upward price pressure as the RTX 4090, and so you definitely shouldn't expect to find many Nvidia RTX 5090 cards to sell at its MSRP.\n\nThat means that, ultimately, the RTX 5090 will still be more expensive than the RTX 4090, and by a fairly substantial amount, making the RTX 4090 an attractive option if you're not willing to pay the exorbinant markups the RTX 5090 will be experiencing for the forseeable future.\n\nWinner: Nvidia RTX 4090\n\nNvidia RTX 5090 vs RTX 4090: Specs\n\n(Image credit: Future / John Loeffler)\n\nSwipe to scroll horizontally Nvidia GeForce RTX 5090 specs vs RTX 4090 specs Header Cell - Column 0 Nvidia RTX 5090 Nvidia RTX 4090 % Diff Process Technology TSMC 4nm TSMC 5nm N/A Transistors (Billion) 92.2 76.3 20.83% Compute units 170 128 32.81% Shaders 21760 16384 32.81% AI/Matrix cores 680 512 32.81% Ray Tracing Cores 170 128 32.81% Render Output Units 192 176 9.09% Texture Mapping Units 680 512 32.81% Boost Clock (MHz) 2407 2520 -4.48% Memory type GDDR7 GDDR6X N/A VRAM (GB) 32 24 33.33% VRAM Bus Width 512 384 33.33% VRAM Speed (Gbps) 28 21 33.33% Bandwidth (GB/s) 1790 1010 77.22% TDP (watts) 575 450 27.77% PCIe Interface PCIe 5.0 x16 PCIe 4.0 x16 N/A\n\nGiven that it's the latest flagship graphics card from Nvidia, it goes without saying that the RTX 5090 has a spec advantage over the older RTX 4090, but it's advantage goes beyond just an increase in core counts or more memory.\n\nThe two biggest things to know about the RTX 5090 vis a vis its specs is that it is the first consumer GPU to use cutting-edge GDDR7 VRAM, as well as the new PCIe 5.0 GPU interface, both of which mean faster data throughput for some pretty critical funtions.\n\nOf the two, GDDR7 is easily the most important, as this substantially increases the memory bandwidth of the RTX 5090 over the RTX 4090, meaning faster texture processing while gaming at higher resolutions (which directly translates into faster framerates), as well as opening up the possibility of serious 8K gaming down the road once 8K monitors go mainstream.\n\nThe RTX 4090, meanwhile, still uses the slower PCIe 4.0 interface and GDDR6X VRAM, so while these are sufficient for 4K gaming, you may hit a ceiling when it comes to memory long before the RTX 5090 does. If you want a futureproofed card, the RTX 5090 is the way to go.\n\nAlso worth noting is that the RTX 5090 has introduced a new thermal design for the Founders Edition card that utilizes dual flow-through, meaning that there are two fans pushing cooler air over the GPU's heatsink fins, giving it better cooling than the RTX 4090.\n\nThat extra cooling is pretty much eaten up entirely by the new 575W TDP for the RTX 5090, compared to the still excessive, but less so, 450W TDP for the RTX 4090. You're also going to need to do something with all that heat being blown off the card, so you'll need to consider the airflow situation in your case when using the RTX 5090.\n\nStill, those caveats can't take away from the next-gen specs in the RTX 5090, making it a card you can feel comfortable will go the distance should you decide to buy it.\n\nWinner: Nvidia RTX 5090\n\nNvidia RTX 5090 vs RTX 4090: Performance\n\n(Image credit: Future)\n\nUltimately, if you're investing in either of these cards, you're definitely looking for the best performance possible for your money. Fortunately, both cards are seriously powerful GPUs, and there really aren't any others in their class.\n\nBut ont card consistently comes out ahead in my benchmark testing, and the results shouldn't be very surprising.\n\nStarting with synthetic benchmarks, the RTX 5090 nearly sweeps the entire synthetic benchmark battery, losing out to the RTX 4090 in only one test, 3DMark Night Raid, and even that might be more of a fluke than anything. Overall though, the RTX 5090 comes in about 21% faster than the RTX 4090 during synthetic testing, so it's off to a good start.\n\nWhen it comes to creative workloads, the RTX 5090 also comes out the overall winner, though that is almost entirely down to its dominating performance in 3D model rendering, as measured by Blender Benchmark 4.30.\n\nOn the video editing side, there's not much difference between the two cards, but that may change as better processors are released, as both cards are outpacing what the CPU can manage at this point.\n\nAs for photo editing and graphic design work, the tool I use to measure this, the PugetBench for Creators Adobe Photoshop benchmark, kept crashing during testing, so I wasn't able to get any definitive data on this front, but once the software is updated, I'll try again and update these charts with that data once I have it.\n\nOn the gaming side of things, as you'd expect, the Nvidia RTX 5090 is an absolute beast, chewing through 1440p gaming without much trouble, as does the RTX 4090. But the RTX 5090 manages a roughly 20% faster average 1440p frame rate while gaming for both average fps and minimum/1% fps. Turn on ray tracing (without upscaling), and you can expect 23-26% better gaming performance from the RTX 5090. Basic DLSS 3 upscaling on balanced (so no frame generation) will get you between 14-23% faster performance on the RTX 5090 than the RTX 4090, depending on whether you use ray tracing as well.\n\nBut where the RTX 5090 really shines is its 4K gaming performance. Thanks to that faster GDDR7 memory and wider memory bus, 4K textures fly through the pipeline, boosting framerates considerably over the RTX 4090 by about 26% for average fps, and nearly 29% on average for minimum/1% fps. The RTX 5090's ray tracing performance at 4K is also exceptional, getting about 34% faster average framerates overall.\n\nAnd none of this even factors in DLSS 4 with Multi-Frame Generation, which you can expect to substantially boost the numbers for the RTX 5090 (you can check out Matt Hanson's DLSS 4 deep dive that explains how much of a difference you can expect to see in your favorite games).\n\nOverall, then, there really is no contest on the performance front, with the RTX 5090 scoring decisive win after decisive win. The RTX 4090 certainly isn't a slough compared to the rest of the graphics card market, but the RTX 5090 is simply out ahead on its own, performance-wise.\n\nWinner: Nvidia RTX 5090\n\nNvidia RTX 5090 vs RTX 4090: Final Verdict\n\n(Image credit: Future)\n\nSo, which one should you buy?\n\nConsidering that you're going to be spending a fortune on either one of these cards, I'd recommend going for the RTX 5090. Yes, it's more expensive, but the advanced features and specs make for better future proofing, and the performance gains gen-on-gen more than justify the investment.\n\nThat doesn't mean you should ignore the RTX 4090, however, especially if its price comes down in the coming weeks and months now that the RTX 5090 is on the scene. If you can get it for a bargain, you should definitely consider the RTX 4090, but barring that, there's no beating the RTX 5090.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia, Broadcom Among Tech Stocks to Sink on DeepSeek Threat",
            "link": "https://www.investopedia.com/tech-stocks-sinking-on-deepseek-threat-nvidia-update-8780706",
            "snippet": "A slew of stocks, including chip maker Nvidia, fell Monday after Chinese startup DeepSeek released an AI model that runs on less-advanced chips and at a...",
            "score": 0.9505847096443176,
            "sentiment": null,
            "probability": null,
            "content": "KEY TAKEAWAYS A slew of stocks, including chip maker Nvidia, tumbled Monday after Chinese startup DeepSeek released an AI model that runs on less-advanced chips and at a lower cost than those of U.S. rivals like OpenAI.\n\nAmong the shares falling were those of nuclear power providers like Constellation and Vistra.\n\nBroadcom's shares slumped around 17%, while Micron Technology was down around 11%.\n\nA slew of stocks, including chip maker Nvidia (NVDA), fell Monday after Chinese startup DeepSeek released a cutting-edge artificial intelligence model that runs on less-advanced chips and at a lower cost than those of U.S. rivals like OpenAI.\n\nThe Chinese company's launch is raising questions about tech shares being overvalued and that Big Tech is overspending on their AI buildout. DeepSeek\u2019s app was a top downloaded app on the Apple store in the U.S. Monday afternoon. (Here's more on what you need to know about DeepSeek.)\n\nDeepSeek earlier this month said its performance was \u201con par with ChatGPT.\u201d DeepSeek\u2019s success has come despite U.S. export controls on cutting-edge chips that have hampered Chinese AI companies.\n\nNvidia fell more than 16% Monday, while U.S.-listed shares of Taiwan Semiconductor Manufacturing Co. (TSM) were down 13% and ASML Holding (ASML), the Dutch company that makes semiconductor-making equipment, fell about 6%.\n\nOther chipmakers were also hit: Broadcom's (AVGO) shares slumped around 17%, while Micron Technology (MU) was down around 11%.\n\nEven the nuclear power providers that had been seen as essential to powering AI data centers slumped: Constellation Energy (CEG) and Vistra (VST) were down more than 20% apiece.\n\nCiti Sticks With Nvidia Buy Call\n\nIn a note published Sunday Jan. 26, Citi analysts said they were sticking with a buy rating on Nvidia, and expected AI companies to continue buying advanced chips.\n\n\u201cWhile DeepSeek\u2019s achievement could be groundbreaking, we question the notion that its feats were done without the use of advanced GPUs to fine tune it and/or build the underlying [large language model] the final model is based on through the Distillation technique,\u201d the analysts wrote.\n\n\"While the dominance of the US companies on the most advanced AI models could be potentially challenged... we estimate that in an inevitably more restrictive environment, US' access to more advanced chips is an advantage,\" they added.\n\nUpdate: This story has been updated to reflect fresh share-price information.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "NVIDIA's tight margins on GeForce RTX 50 series: pressure on AIBs as 'MSRP feels like charity'",
            "link": "https://www.tweaktown.com/news/102804/nvidias-tight-margins-on-geforce-rtx-50-series-pressure-aibs-as-msrp-feels-like-charity/index.html",
            "snippet": "NVIDIA's new GeForce RTX 50 series GPUs are nowhere near their $1999 MSRP, apart from the RTX 5090 Founders Edition, with custom RTX 5090s priced as high as $...",
            "score": 0.9560631513595581,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "MSI Confirms Shortage Of NVIDIA RTX 50 Series GPUs, Initial Batch To Have Limited SKUs",
            "link": "https://wccftech.com/msi-confirms-shortage-of-nvidia-rtx-50-series-gpus/",
            "snippet": "NVIDIA's major board partner has also confirmed the reports of the RTX 50 series shortage, which is said to affect availability on launch day.",
            "score": 0.7333714365959167,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's major board partner has also confirmed the reports of the RTX 50 series shortage, which is said to affect availability on launch day.\n\nMSI Says it Won't Have Enough RTX 50 Series GPUs on 30th January Due to Insufficient Supply from NVIDIA\n\nThis time, we have an official confirmation about the RTX 50 series shortage from NVIDIA's board partner, MSI. We have reported recently that several sources have confirmed that there won't be enough RTX 50 series GPUs at the launch time, and this will be particularly serious for the RTX 5090 and RTX 5080 launches.\n\nThese cards are about to hit the retail shelves officially on 30th January, which is just five days from now. However, it will be extremely hard for customers to buy these GPUs at MSRP or even at higher prices since they won't be available in a good number. MSI's official handle (via ITHome) has confirmed that the RTX 50 series GPUs will have a tight supply on 30th January.\n\nImage Credit: ithome.com\n\nAs we reported previously, this was reportedly due to poor communication between NVIDIA and its board partners. Apparently, most AIBs didn't get enough chips to make their custom editions, and we have heard reports that some retailers have gotten the RTX 5090 in just single digits. In some regions, the RTX 5090 and RTX 5080 prices have soared to the point that they are now at twice the official MSRP.\n\nImage Credit: ithome.com\n\nHowever, it is reported that the availability of these cards will improve in February, but it will be gradual. Most users aiming to secure one of these GPUs at the launch time won't be able to buy them easily as NVIDIA may nitpick retailer distribution of these GPUs. The RTX 5090 and RTX 5080 are already quite expensive and will incur premium taxes in the EU region this time. The shortage will result in even higher prices, drastically affecting the price-to-performance ratio of these cards.\n\nIt's not yet known whether the RTX 5070 cards will also face a similar situation, but it's possible because the RTX 5070 Ti is reportedly going to hit the shelves on 20th February. However, currently, no such reports have emerged.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Is Nvidia\u2019s reign over AI chips at risk with focus on high-end GPUs?",
            "link": "https://www.thenationalnews.com/business/2025/01/26/is-nvidias-reign-over-ai-chips-at-risk-with-focus-on-high-end-gpus/",
            "snippet": "Rivals such as Amazon and Microsoft are pouring billions into more affordable AI models to challenge Nvidia's market dominance.",
            "score": 0.6281725168228149,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NVIDIA's tight GeForce RTX 50 margins put pressure on board partners: 'MSRP feels like charity'",
            "link": "https://videocardz.com/newz/nvidias-tight-geforce-rtx-50-margins-put-pressure-on-board-partners-msrp-feels-like-charity",
            "snippet": "Where are RTX 50 cards at MSRP? What is this? Report Ad. GeForce reviews are typically split between MSRP and non-MSRP models. This was NVIDIA's way of...",
            "score": 0.9569154381752014,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia shares spooked as China\u2019s DeepSeek raises questions over AI capex",
            "link": "https://www.investing.com/news/stock-market-news/nvidia-shares-spooked-as-chinas-deepseek-raises-questions-over-ai-capex-3831006",
            "snippet": "Investing.com-- Artificial intelligence darling Nvidia's shares fell on Friday following the launch of Chinese generative AI program DeepSeek last week,...",
            "score": 0.8281360268592834,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia is winding down developer support for 9 and 10-series graphics cards, but they'll likely keep getting driver updates for a while yet",
            "link": "https://www.pcgamer.com/hardware/nvidia-is-winding-down-developer-support-for-9-and-10-series-graphics-cards-but-theyll-likely-keep-getting-driver-updates-for-a-while-yet/",
            "snippet": "Don't panic buy a new GPU: Two years passed between 7-series cards' CUDA deprecation and the end of their driver support.",
            "score": 0.9444921612739563,
            "sentiment": null,
            "probability": null,
            "content": "First reported by Tom's Hardware, the patch notes for the latest update to Nvidia's CUDA Toolkit state that support for the Maxwell and Pascall architectures\u2060\u2014GTX 9 and 10-series cards\u2060\u2014will be deprecated in an upcoming update. Those cards will still be getting GeForce driver updates, and while Nvidia has not yet announced for how long, we can look back at the mothballing of a previous Nvidia architecture to get an idea.\n\nThe news comes from the update 12.8 release notes for the CUDA Toolkit, Nvidia's collection of tools and libraries for programming GPU-powered applications. Under section 1.5.1, \"Deprecated Architectures,\" the patch notes read: \"Architecture support for Maxwell, Pascal, and Volta is considered feature-complete and will be frozen in an upcoming release.\"\n\nVolta was almost exclusively used in enterprise hardware, but Maxwell and Pascal are of dear importance to PC gamers: The 9 and 10-series cards represent a price vs. performance golden age in hindsight, with the GTX 970, 980 Ti, 1060, and 1080 Ti in particular being fondly remembered cards that could reasonably support a gaming hobby to this day, depending on what graphical and resolution compromises you're willing to make. The 1060 was still the Steam Hardware Survey's most popular GPU in March 2022, six years after its launch, while the GTX 970 was the minimum-spec GPU for Dragon Age: The Veilguard, a triple-A RPG released 10 years after the 970's launch. In December's Steam Hardware Survey, Maxwell and Pascal cards together accounted for 10.75% of respondents.\n\nNvidia has not revealed when driver updates will end for these cards, but I think we can look at a previous generation for an idea of the timeline we can expect. CUDA support for Kepler, the architecture behind GTX 7-series GPUs like the 780 Ti, started to be deprecated in CUDA v10.2 in November 2019\u2060\u2014the archive page was updated in 2020, but the Wayback Machine shows Kepler's deprecation was present from when the page first went live in 2019.\n\nThe final driver update for Kepler came two years later in August 2021, so there's good reason to expect continued driver support for 9 and 10-series cards for a similar window after the CUDA deprecation. Maxwell has already outlived Kepler by more than two years in terms of CUDA support, and the large install base Maxwell and Kepler still command inclines me to believe that Nvidia will prolong their already-amazing run into 2026, if not longer.\n\nEven with souped-up AI-powered cards like the RTX 5090 crashing on us like a great wave, there's still plenty of mileage to be gotten out of old hardware. The GTX 980 Ti remains the most powerful consumer graphics card ever released to still boast an analogue DVI port, making it a prize item for those in possession of holy grail CRT monitors like the blisteringly high-refresh Iiyama Vision Master Pro 512. That's not exactly a common use case though, and even your average CRT gaming sicko will be well-served by a good adapter like the ones made by StarTech.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2025-01-25": {
        "0": {
            "title": "Nvidia Shares Tumbles as Japanese Rivals Gain Ground in AI Chips",
            "link": "https://finance.yahoo.com/news/nvidia-shares-tumbles-japanese-rivals-060105606.html",
            "snippet": "Shares of Nvidia Corp. (NVDA, Financial) fell more than 3% on Friday as investors worried about growing competition from Japan's semiconductor companies;...",
            "score": 0.962047278881073,
            "sentiment": null,
            "probability": null,
            "content": "Shares of Nvidia Corp. (NVDA, Financial) fell more than 3% on Friday as investors worried about growing competition from Japan's semiconductor companies; Japan's Advantest renewed its leadership in chip testing equipment. With advancements in generative AI pushing demand, the global market is now dominated by Advantest , where the U.S. rival Teradyne (TER, Financial) commands about 50%, according to sources.\n\nSoC testing tool revenue is expected to climb 32 percent to JPY 324 billion ($2.1 billion) by March 2025, says the company. For such advanced AI chip processes as chiplets and 3D packaging, highly accurate and efficient tools are essential to yield improvement.\n\nAdvantest's fiscal year 2024 R&D investments came in at JPY 65.5 billion ($424.5 million), illustrating the importance that is placed on innovation by Advantest. Much greater profitability has been made possible through its R&D efficiency.\n\nAdvantest CEO Douglas Lefever, seeing continued strong growth in AI chip demand, views the technology as positive for the industry. While Nvidia falls, that speaks to the changing market dynamic around evolving semiconductor technologies.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "Nvidia Stock Got Incredible News From This Hot AI Start-Up",
            "link": "https://www.fool.com/investing/2025/01/25/nvidia-stock-got-incredible-news-from-this-hot-ai/",
            "snippet": "Heading into 2025, the biggest question for Nvidia (NVDA 2.63%) is whether the artificial intelligence (AI) chip superstar can continue to grow at a...",
            "score": 0.8587955832481384,
            "sentiment": null,
            "probability": null,
            "content": "If there's an AI bubble, it's nowhere in sight.\n\nHeading into 2025, the biggest question for Nvidia (NVDA 5.27%) is whether the artificial intelligence (AI) chip superstar can continue to grow at a breakneck pace.\n\nSome investors have argued that an AI bubble is forming, and there is evidence that the new technology is following in the footsteps of earlier bubbles due in part to the dramatic growth in some AI stocks. Nvidia has been the flag bearer for the AI boom thus far after posting several quarters of triple-digit revenue growth, and there were news reports earlier this month that should reassure investors that the company and the broader AI sector still have a long runway of growth in front of them.\n\nAI start-ups are on fire\n\nChatGPT creator OpenAI might be a household name by now, but there's a lesser-known AI start-up that is starting to make waves.\n\nThat's Anthropic, the creator of the Claude AI chatbot, which counts Amazon and Alphabet among its backers, having raised billions of dollars from the tech giants. Anthropic is now in talks for a massive new funding round, and it's a bullish signal for Nvidia and the broader sector as well.\n\nAccording to multiple media outlets, Anthropic was in advanced talks earlier in January to raise $2 billion at a price that values the company at $60 billion, up from a valuation of $16 billion less than a year ago.\n\nThe news is the latest sign of skyrocketing valuations for privately held AI start-ups, which show growing investor enthusiasm for AI and confidence that companies like Anthropic will justify that valuation over the long term, eventually generating billions in profits.\n\nUltimately, the frenzy over AI start-ups will benefit Nvidia as a significant chunk of that $2 billion is likely to be spent on Nvidia chips to power Anthropic's AI models.\n\nAnthropic has a history of working with Nvidia and buying its chips, though the company said it would use Amazon's Trainium and Inferentia chips to train future foundation models when it took $4 billion from Amazon last November.\n\nThe details of the current funding round aren't clear as it hasn't closed, but the deal is likely to yield a windfall for Nvidia in some capacity as it is widely recognized as the leader in AI chip technology, and fully tying itself to Amazon could make Anthropic less competitive. Even Amazon Web Services CEO Matt Garman has said that the company views its AI processors as a \"supplement\" to Nvidia's GPUs rather than a replacement.\n\nThe funding round also adds fuel to the AI arms race among start-ups and should encourage investors in rival companies to shell out billions more. OpenAI, for example, completed a $6.6 billion funding round valuing the company at $157 billion in October.\n\nOpenAI has been a major customer of Nvidia, and Nvidia CEO Jensen Huang hand-delivered the first H200 AI supercomputer to OpenAI. The Sam Altman-led start-up has visions of developing its own AI chips, but that's likely years away. For the foreseeable future, it will be reliant on Nvidia's chips, and its recent funding round could mean billions more flowing into Nvidia's coffers. Nvidia was also an investor in the round, which could strengthen its relationship with the AI start-up.\n\nWhat it means for Nvidia\n\nWhile big tech companies like Amazon and others are working on their own AI chips, it's going to be difficult to dethrone Nvidia, whose market share in data center GPUs is estimated to be around 95%. Nvidia continues to rapidly innovate, preparing its Rubin platform as a more advanced version of the recently released Blackwell.\n\nNvidia also benefits from a flat management structure that makes decision-making faster and easier and avoids the kind of siloing that has plagued competitors like Intel. Finally, it has an advantage over potential rivals like Amazon as it's a dedicated pure-play semiconductor company, while businesses like Amazon have other priorities.\n\nNvidia's AI foundation arguably began in 2006 when it launched its CUDA parallel computing model that now contains hundreds of software libraries and AI models, showing it has a significant technological advantage. CEO Jensen Huang has long been regarded as a visionary in AI as well.\n\nOverall, what's good for AI is good for Nvidia, and the billions flowing into start-ups at soaring valuations show the AI boom and Nvidia's AI-driven growth still have a long way to run. Expect Nvidia to deliver another strong year for investors in 2025.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Chinese AI Lab DeepSeek Has 50,000 NVIDIA H100 AI GPUs, Says AI CEO",
            "link": "https://wccftech.com/chinese-ai-lab-deepseek-has-50000-nvidia-h100-ai-gpus-says-ai-ceo/",
            "snippet": "According to Scale AI CEO and founder Alexandr Wang, Chinese AI lab DeepSeek behind the popular R1 model has access to 50000 NVIDIA H100 GPUs.",
            "score": 0.8908005356788635,
            "sentiment": null,
            "probability": null,
            "content": "This is not investment advice. The author has no position in any of the stocks mentioned. Wccftech.com has a disclosure and ethics policy.\n\nChinese artificial intelligence lab DeepSeek, whose artificial intelligence model R1 took the industry by storm this month, has access to tens of thousands of NVIDIA's GPUs for training, believes the CEO of an AI company. DeepSeek R1 is one of the most advanced AI models in the world, and it stands shoulder to shoulder with leading American platforms such as OpenAI's o1 and Meta's Llama.\n\nNVIDIA's Hopper chips are its current generation GPUs, which are the most widely used in the world as the firm ramps up shipments of the next-generation Blackwell chips. The latest details were shared by Scale AI's founder and CEO Alexandr Wang in an interview given to CNBC earlier this week, with Wang adding that R1 has met or beat all top-performing AI models in his firm's most challenging AI test.\n\nDeepSeek R1 Is As Good Or Better Than OpenAI's o1, Says AI CEO\n\nWang started his interview, given to Andrew Ross Sorkin of CNBC, by sharing details of his firm\u2019s latest AI test. Called \u201cHumanity\u2019s Last Exam,\u201d the test uses the \u201chardest questions\u201d provided by \u201cmath, physics, biology, chemistry professors\u201d that are relevant to the latest research. After testing all the latest AI models with this test, Wang\u2019s team discovered that DeepSeek\u2019s latest model was \u201cactually the top performing, or roughly on par with the best American models, which are o1. . ..\u201d\n\nWhen asked about the AI competition between the US and China, Wang commented that \"it has been true for a long time that the United States has been ahead.\" However, he added that DeepSeek's latest models do attempt to change the calculus. According to Wang, he thinks it \"is symbolic that the Chinese lab releases, you know, an Earth-shattering model on Christmas Day when you know the rest of us are sort of celebrating a holiday.\"\n\nThe conversation then turned to the amount of GPUs that DeepSeek and Chinese AI players might have for their AI models. NVIDIA's GPUs are among the most highly sought products in the world, and due to their potential, their exports are also regulated by the US government. The firm's current-generation Hopper GPUs lineup consists of the H100 and H200 GPUs.\n\nThe primary difference between the two GPU models is their memory clock speed and capacity. The Biden administration had restricted the sale of these GPUs to China in 2022 through a rule that prevented NVIDIA from selling chips vastly superior to the older A100 GPUs. These restrictions forced NVIDIA to develop alternative chips called H800 and A800, and these were also banned a year later in October 2023.\n\nAccording to Wang, when it comes to the Chinese accessing NVIDIA's advanced GPUs, \"the reality is yes and no. You know the Chinese labs, they have more H100s than, than people think.\" He added and shared that his \"understanding is that DeepSeek has about fifty thousand H100s.\" Wang outlined, \"they can't talk about obviously because it is against the export controls that United States has put in place.\" He also thinks that \"they have more chips than other people expect.\"\n\nHowever, the Chinese might find it difficult to procure additional chips. According to Wang, \"But also on a go-forward basis they are going to be limited by the chip controls and the export controls that we have in place.\"\n\nUpdate at January 27th, 3:23:36 pm ET: We reached out to NVIDIA for a comment. You can read the full statement here.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Sorry, AMD, Nvidia's price tags for its RTX 5000 GPUs could win me over",
            "link": "https://www.techradar.com/computing/gpu/sorry-amd-nvidias-price-tags-for-its-rtx-5000-gpus-could-win-me-over",
            "snippet": "Nvidia RTX 5000-series prices are starting on a much better foot.",
            "score": 0.9043511748313904,
            "sentiment": null,
            "probability": null,
            "content": "One of my main complaints concerning Nvidia's RTX 4000-series of graphics isn\u2019t, surprisingly enough, the massive wattage (though that is a legitimate concern) but the pricing. For the past couple of generations, Nvidia has been massively hiking up the cost of its cards, especially its best graphics cards like the RTX 4080. But it's mid-range offerings haven't been spared either. It\u2019s become particularly noticeable with the absence of budget-minded cards to offset this phenomenon.\n\nWhen rumors for the RTX 50-series began to accelerate, buyers rightfully worried that prices would face a far steeper climb while offering a performance return that didn\u2019t reflect the increase. However, the 50-series received its full announcement at CES 2025 and we now know that the MSRP is shockingly affordable .\n\nThe flagship Nvidia GeForce RTX 5090, widely thought to be a Titan due to its incredible specs, is priced at $1,999 / \u00a31,999 / AU$4,039 \u2014 while steep, it\u2019s not too far off from the original $1,599 price tag of the RTX 4090. This is especially promising since the rumors all pointed to a price nearly double what it ended up being.\n\nThen there\u2019s the RTX 5080, which is $999 / \u00a3939 / AU$2,019, and the RTX 5070 and 5070 Ti with respective retail prices of $549 / \u00a3549 / AU$1,509, and $749 / \u00a3749 / AU$1,109. The 5080 is $200 cheaper than the RTX 4080 was at launch, and will hopefully have a solid increase in performance to boot.\n\nMost impressively, Nvidia claims that the RTX 5070 will offer comparable performance to the RTX 4090's native performance with some help from DLSS 4, but at around a third of the initial MSRP, while the RTX 5070 Ti will come in at just $150 more with even better performance.\n\nThis is also incredibly promising since it is a generational decrease in price, as the RTX 4070 Ti initially retailed for $799, and the RTX 4070 launched at $599.\n\nAMD has been fumbling hard lately\n\n(Image credit: AMD / TechPowerup)\n\nI\u2019ve long complained about how graphics cards need to be made more affordable, and how Intel has been stepping in the right direction with its Intel Arc Battlemage series . Now it feels like Nvidia is finally doing the same, capping overall price increases while lowering the cost of its mid-range offerings somewhat to remain accessible.\n\nThis, unfortunately, is in direct opposite of AMD\u2019s current approach. AMD has been known for years as being the more affordable option for cards, balancing solid performance with more competitive price tags. However, some troublesome new reports suggest that the upcoming RDNA 4 cards may miss the mark in terms of pricing.\n\nThe RX 9070 XT and RX 9070 don\u2019t currently have official prices (though apparently will be \u2018competitive\u2019 with Nvidia ), and it\u2019s been officially confirmed that it won\u2019t launch until at least March 2025 . This puts Team Red on the back foot as Team Green\u2019s RTX 5070 Ti and RTX 5070 will be launching in February 2025 with a healthy MSRP. By the time AMD releases their GPUs, everyone might already have bought new Nvidia cards, or may just opt for the more premium Nvidia offerings if AMD's prices are too close to its competitors.\n\nEven worse, new reports indicate that the RX 7400 and 7300, which would have been new aggressively budget-friendly RDNA 3 options, have been axed in favor of fully supporting RDNA 4. If this turns out to be true, then Team Red is giving up a serious advantage in the market.\n\nRight now, we\u2019re looking at a potentially major upset in terms of better budget options for graphics cards. Nvidia has been at least responsive to complaints about price inflation, while AMD seems rather lost on the GPU front.\n\nIt\u2019s a shame too, as it would be more beneficial to consumers to have two giants on an even playing field in order to keep prices low.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Meta to build 2GW data center with over 1.3 million Nvidia AI GPUs \u2014 invest $65B in AI in 2025",
            "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/meta-to-build-2gw-data-center-with-over-1-3-million-nvidia-ai-gpus-invest-usd65b-in-ai-in-2025",
            "snippet": "Reuters reports that Meta plans to invest between $60 billion and $65 billion in 2025 to enhance its artificial intelligence (AI) and other infrastructure.",
            "score": 0.5790802240371704,
            "sentiment": null,
            "probability": null,
            "content": "Reuters reports that Meta plans to invest between $60 billion and $65 billion in 2025 to enhance its artificial intelligence (AI) and other infrastructure. This marks a significant leap from its estimated $38 billion\u2014$40 billion spending in 2024, citing Mark Zuckerberg's post on Facebook. The company aims to solidify its competitive position in the AI industry against rivals like Google, Microsoft, and Open AI. However, $65 billion is slightly smaller than Microsoft's $80 billion.\n\nThe report says that Meta will construct a 2GW data center using some 1.3 million Nvidia [presumably H100] GPUs as part of the plan. The spending probably includes the company's custom data center-grade processors, too. Yet, we are speculating here.\n\n\"We are planning to invest $60 billion \u2014 $65 billion in CapEx this year while also growing our AI teams significantly, and we have the capital to continue investing in the years ahead,\" Zuckerberg said in a Facebook post. \"This is a massive effort, and over the coming years it will drive our core products and business, unlock historic innovation, and extend American technology leadership.\"\n\nThe announcement comes amidst escalating competition in AI investments. Microsoft is allocating $80 billion for data centers in fiscal 2025, Amazon expects to exceed $75 billion in spending for the same year, and OpenAI, SoftBank, and Oracle have committed $500 billion to their Stargate initiative (yet it remains to be seen where precisely those Stargate money will come from). According to Reuters, this spending far exceeds analyst estimates of $50.25 billion for Meta\u2019s 2025 capital expenditures.\n\n\"This will be a defining year for AI. In 2025, I expect Meta AI will be the leading assistant serving more than 1 billion people, Llama 4 will become the leading state of the art model, and we'll build an AI engineer that will start contributing increasing amounts of code to our R&D efforts,\" Zuckerberg wrote.\n\nUnlike its rivals, Meta offers open access to Llama AI models to end users in the U.S. The company allegedly monetizes this with its primary business, Facebook. Meta has also advanced its AI offerings with products like Ray-Ban smart glasses. By 2025, Meta expects its AI assistant to reach over 1 billion users, up from 600 million monthly active users last year.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Will Chinese DeepSeek Disrupt NVIDIA?",
            "link": "https://www.forexlive.com/news/will-chinese-deepseek-disrupt-nvidia-20250125/",
            "snippet": "The global AI race has taken an intriguing turn with the emergence of China's DeepSeek, touted by some as a potential disruptor to NVIDIA's dominance in the...",
            "score": 0.7998733520507812,
            "sentiment": null,
            "probability": null,
            "content": "Will Chinese DeepSeek Disrupt NVIDIA and the American AI Industry?\n\nThe global AI race has taken an intriguing turn with the emergence of China's DeepSeek, touted by some as a potential disruptor to NVIDIA's dominance in the AI hardware space. As speculation swirls, stock investors, analysts, and thought leaders are left grappling with the critical question: Is DeepSeek a genuine threat to NVIDIA and the broader American AI industry, or is this a case of overhyped news amplified by interested parties?\n\nThe news that DeepSeek's model surpassed OpenAI's o1 in specific reasoning tests has sparked intense debate within the AI community. This development has significant implications for the AI market, potentially impacting the stock prices of companies like NVIDIA and other AI-related companies.\n\nI\u2019ll preface this by saying that I am not a renowned expert in deep technology or AI hardware. I don\u2019t claim to have a definitive answer. Instead, I propose a method to find the answer\u2014not by theorizing endlessly but by observing how the stock market reacts. The stock market, with its collective wisdom and ability to process information, often acts as the closest thing we have to a truth detector.\n\nLet\u2019s explore the context, evaluate the key factors at play, and develop a logical framework to assess the significance of DeepSeek\u2019s emergence.\n\nThe Landscape: NVIDIA\u2019s Moat vs. DeepSeek\u2019s Promise\n\nNVIDIA has long been the undisputed leader in AI hardware, leveraging its GPUs and CUDA software ecosystem to dominate the market. Its ecosystem is sticky\u2014developers, hyperscalers, and enterprise clients are deeply entrenched in its platform, creating significant switching costs. This is not unlike Apple\u2019s ecosystem, where loyal users and interconnected services create barriers to exit.\n\nDeepSeek, on the other hand, reportedly offers a more cost-efficient AI solution. The analogy some have drawn is that DeepSeek could be the \u201ccheaper smartphone\u201d to NVIDIA\u2019s \u201ciPhone.\u201d In the smartphone industry, the availability of cheaper alternatives has undeniably shifted market share, particularly in emerging markets. But did this disrupt Apple\u2019s stock price or its dominance among its loyal user base? Not significantly. The same could apply to NVIDIA, depending on whether DeepSeek can replicate NVIDIA\u2019s ecosystem or merely compete on price and performance.\n\nThe Role of Market Reactions: Finding the Answer in the Price\n\nHere\u2019s where the stock market comes in. Instead of relying solely on speculative opinions, I suggest that the best way to gauge the impact of DeepSeek is to observe NVIDIA\u2019s stock price. The stock market is often a better truth-teller than any single analysis. It reflects collective sentiment, informed by analysts, insiders, and countless external factors.\n\nWhat to Watch:\n\nThe Reaction to the News\n\nSince the DeepSeek news broke, NVIDIA\u2019s stock has not shown any drastic reaction. It actually rose apx 2.5% after the news came out, if we look at NVDA stock price from the next day it traded, to the close of the week. This lack of significant movement, not to mention lack of stock price decline (crash...?) -- suggests that the market does not yet see DeepSeek as a significant disruptor. If it were, we would expect a sharp and sustained price decline. Who remembers other cases, such as Facebook\u2019s Cambridge Analytica scandal a few years ago and the immediate FB stock decline? Earnings as a Catalyst\n\nNVIDIA\u2019s upcoming earnings report will be pivotal. Pay close attention to management\u2019s commentary, particularly regarding competitive risks in the AI space. If DeepSeek is a genuine threat, it could show up in NVIDIA\u2019s guidance or performance, especially in its data center segment. I propose observing NVIDIA\u2019s stock price from the time the news broke until two weeks after earnings . This period should provide enough data to assess whether DeepSeek is having a tangible impact, even if will not be talked about. The price knows, and it almost always knows better than you, or anyone else .\n\nInterpreting the Price Action Post Upcoming Nvidia Earnings (Scheduled for 26 Feb 2025): What It Might Tell Us\n\nNVDA stock price is your best guide to the \"truth\"\n\nScenario 1 for the Upcoming Nvidia Earnings Report: Stock Rises or Remains Stable\n\nIf NVIDIA\u2019s stock rises or stays within its current range, the market is likely signaling that DeepSeek is not a major disruptor. Perhaps NVIDIA\u2019s ecosystem is too entrenched, or DeepSeek\u2019s solution doesn\u2019t offer enough differentiation. It\u2019s also possible that other factors, such as AI demand growth or NVIDIA\u2019s new Blackwell chips, outweigh any competitive threats.\n\nScenario 2 for the Upcoming Nvidia Earnings Report: Stock Declines\n\nIf NVIDIA\u2019s stock declines, the reasons warrant closer examination. Is the drop due to bottlenecks in Blackwell chip deliveries, macroeconomic pressures, or broader market trends? Or does it reflect legitimate concerns about DeepSeek? Analysts will need to scrutinize earnings commentary, data center revenue trends, and any shifts in market share. It\u2019s critical to avoid jumping to conclusions without understanding the underlying reasons for the decline.\n\nThe Unknowns: Factors and Their Weights\n\nOne of the challenges in analyzing stock movements is that we can never know all the factors affecting a stock price, nor the relative weights of those factors . We might think we know, we may hear great explanations, but we don't exacly know. Earnings reports often illustrate this: a company might beat revenue and EPS estimates, yet the stock declines. Analysts scramble to explain why\u2014was it slower sales growth, weaker guidance, or some other reason? The reality is that the stock market processes countless variables simultaneously, and not all of them are visible to us. The granular variables and their weights will never be known to us. But price knows.\n\nThe same principle applies here. Even if NVIDIA\u2019s stock remains stable, it\u2019s possible that DeepSeek is a growing threat but not yet significant enough to outweigh other factors. Conversely, if the stock declines, DeepSeek may not be the primary cause. This complexity is why I emphasize looking at the stock price holistically and over time, rather than rushing to judgment based on short-term moves.\n\nSeeking the Deep Behind DeepSeek: Trust the Price, Not the News\n\nDeepSeek\u2019s potential to disrupt NVIDIA and the American AI industry is an open question, but the best way to find an answer is to let the stock market speak. Watch NVIDIA\u2019s price action from the time the news broke until two weeks after its upcoming earnings. If the stock remains stable or rises, it\u2019s likely that DeepSeek is not a significant threat. If it declines, dig deeper into the reasons\u2014whether they\u2019re related to DeepSeek or other factors.\n\nWhile price action can act as a \u201ctruth detector,\u201d it\u2019s not infallible. The market may not immediately price in all information, and short-term movements can be influenced by noise, speculation, or interested parties (e.g., short sellers amplifying fears about DeepSeek). As always, exercise caution and remain open to multiple interpretations. But do follow the price more than anyone's opinion, especially yours .\n\nUltimately, the price knows. Follow the money. Follow the price and you shall be closer to finding the truth.\n\nSo far, here is the most simple price chart of NVDA stock within the past 100 days. I don't see any panic yet from the \"truth machine\", do you? You can decide for yourself is you see any DeepSeek news related panic. But let's wait till 2 weeks after the next earnings to unveal further \"truth\".\n\nNVDA trading range: $127-$152 (100 days). Weekly chart.\n\nDisclaimer: This article is for informational purposes only and should not be construed as investment advice. Always do your own research and consult with a professional before making investment decisions.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Chinese AI firm DeepSeek has 50,000 NVIDIA H100 AI GPUs says CEO, even with US restrictions",
            "link": "https://www.tweaktown.com/news/102798/chinese-ai-firm-deepseek-has-50-000-nvidia-h100-gpus-says-ceo-even-with-us-restrictions/index.html",
            "snippet": "Chinese AI company DeepSeek says its DeepSeek R1 model is as good, or better than OpenAI's new o1 says CEO: powered by 50000 NVIDIA H100 AI GPUs.",
            "score": 0.7963063716888428,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Everything We Know About Nvidia's Personal AI Supercomputer",
            "link": "https://hackernoon.com/everything-we-know-about-nvidias-personal-ai-supercomputer",
            "snippet": "Today AI is rapidly becoming part of everyday life, and NVIDIA has once again taken center stage with a new development: Project DIGITS.",
            "score": 0.9476125240325928,
            "sentiment": null,
            "probability": null,
            "content": "In a world where artificial intelligence is rapidly becoming part of everyday life, NVIDIA has once again taken center stage with an exciting new development: Project DIGITS. Announced at CES 2025 by CEO Jensen Huang, DIGITS aims to give creators, developers, and enthusiasts a compact AI supercomputer that\u2019s as user-friendly as it is powerful. Here\u2019s everything we know from the official sources and how it could revolutionize personal AI computing.\n\n\n\n\n\nDisclaimer: This story was written with the assistance of an AI writing program called ChatGPT.\n\nWhat Is DIGITS?\n\nAccording to NVIDIA CEO Jensen Huang\u2019s Keynote at CES 2025 (see transcript below), DIGITS is the culmination of over a decade of research that began as \u201cProject DIGITS\u201d but later evolved into the \u201cDGX\u201d series for enterprise AI computing. Now, NVIDIA is returning to its roots by unveiling a personal AI supercomputer that runs the entire NVIDIA AI stack, offering:\n\n\n\n\n\nCompact Design: Smaller and more accessible than ever before. Cutting-Edge Hardware: Powered by the GB110 chip\u2014NVIDIA\u2019s smallest \u201cGrace Blackwell\u201d GPU\u2014paired with a custom CPU built in collaboration with Mediatek. Plug-and-Play AI: Designed to be used as a \u201ccloud supercomputer\u201d on your desk or as a local Linux workstation for AI tasks. Seamless Integration: Supporting NVIDIA\u2019s full suite of AI frameworks and capable of linking multiple units for larger workloads.\n\nKey Takeaways from Jensen Huang\u2019s Keynote\n\nIn Huang\u2019s words, \u201cEverybody who uses computers today as a tool will need an AI supercomputer.\u201d This underscores the vision behind DIGITS: AI should be as commonplace in the professional landscape as a laptop or smartphone. Previously, AI supercomputers like DGX-1 were large and out of reach for smaller teams and individual creators. With DIGITS, you can:\n\n\n\n\n\nUnbox and Deploy : No need to build a specialized facility or tinker with complicated infrastructure.\n\nScale Up : Connect multiple DIGITS units using high-speed networking technology such as ConnectX and GPUDirect.\n\nStay Versatile: Whether you\u2019re using a PC, Mac, or any other platform, DIGITS can be accessed wirelessly or through a workstation configuration.\n\n\n\n\u201cThis is the new way of doing software,\u201d Huang emphasized, reminding the audience how AI and deep learning are reshaping software development across industries.\n\nWhy It Matters\n\nDemocratizing AI: DIGITS could empower entrepreneurs, small research labs, and even hobbyists with supercomputer-level horsepower in a user-friendly package. Ecosystem Compatibility: Because it integrates seamlessly with NVIDIA\u2019s AI tools and cloud services, you can leverage existing deep learning frameworks like TensorFlow, PyTorch, and more. Future-Proof: As AI technologies and workloads expand, the modular nature of DIGITS (plus the ability to combine multiple units) means you won\u2019t be left behind.\n\nOfficial Images and Sneak Peeks\n\nBelow are some official visuals provided directly by NVIDIA\u2019s YouTube channel and website:\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese images showcase DIGITS\u2019 compact design, modular internal architecture, and how it can be conveniently placed alongside a typical workstation setup.\n\nThe Road Ahead\n\nHuang announced that DIGITS is \u201cin full production\u201d and is expected to be available around May. Given NVIDIA\u2019s track record for delivering powerful hardware (as seen with the original DGX-1 that catalyzed AI breakthroughs), the anticipation is high for this more accessible version. If you\u2019re a developer, data scientist, or creative professional who wants a personal AI workhorse, keeping tabs on DIGITS\u2019 release timeline is a must.\n\nFull Transcript Reference\n\nFor those who want the complete context, here\u2019s a snippet from NVIDIA CEO Jensen Huang\u2019s Keynote at CES 2025 (official NVIDIA YouTube channel):\n\n\n\n\n\n\u201c...none of this would be possible if not for this incredible project that we started a decade ago inside the company called Project DIGITS... And now artificial intelligence is everywhere... every software engineer, every creative artist, everybody who uses computers today as a tool, will need an AI supercomputer...\u201d\n\n\u201c...if you use PC, Mac, you know anything... it\u2019s a cloud computing platform that sits on your desk... you could also use it as a Linux workstation if you like...\u201d\n\n(For the full transcript, see the original video at this link.)\n\nFinal Thoughts\n\nDIGITS might just be the device that turns AI from a specialized tool into a universal commodity. Whether you\u2019re a software engineer experimenting with generative AI models or a content creator seeking new ways to generate media (like myself), this personal AI supercomputer promises to harness the power of data and accelerate your workflows.\n\n\n\n\n\nIf you\u2019ve been waiting for a more compact, ready-to-go AI solution, Project DIGITS could be the breakthrough. Stay tuned for further updates as NVIDIA moves closer to the official release. With any luck, we\u2019ll see more \u201cdigits\u201d in the hands of developers, researchers, and creatives worldwide\u2014truly bringing AI supercomputing to the masses.\n\nSources Cited\n\nTranscript: NVIDIA CEO Jensen Huang Keynote at CES 2025, Official NVIDIA YouTube Channel (Watch here) Images & Teaser: Screenshots and photos of Project DIGITS from NVIDIA\u2019s official teaser video and website\n\nWho Am I?\n\nI\u2019m Aswin Barath, a Software Engineering Nerd who loves building Web Applications and sharing my knowledge through Blogging\n\nConnect with me Socials (Linktree)\n\n\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "All the news about Nvidia\u2019s RTX 50-series GPUs",
            "link": "https://www.theverge.com/2025/1/25/24351798/nvidia-rtx-5090-5080-5070-gpu-news-rumors",
            "snippet": "Nvidia's new RTX 5090, RTX 5080, RTX 5070 Ti, and RTX 5070 promise big improvements over their predecessors, including through DLSS 4's AI-powered frame...",
            "score": 0.9411766529083252,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia\u2019s RTX 50-series GPUs are just around the corner, with the first releases \u2014 the RTX 5090 and RTX 5080 \u2014 dropping on January 30th. The RTX 5070 Ti and RTX 5070 will follow that with their own releases in February, but some are already getting a sneak peek at the software benefits of the GPUs through DLSS 4.\n\nTom Warren\u2019s Verge review of the $1,999 RTX 5090 indicates it\u2019s expectedly a powerhouse but not quite the generational leap that the RTX 4090 was over its own predecessor. That didn\u2019t stop The Verge\u2019s Sean Hollister from being impressed with the two-slot RTX 5090 Founders Edition GPU when he stuffed it into his aging small form factor PC.\n\nAlong with the 50-series GPUs comes DLSS 4 with Multi Frame Generation, a software trick that may be just as big of a story as the hardware itself. This latest version of DLSS uses AI to predictively generate frames, making it possible to run games at higher resolutions without taking the same frame rate hit they would without DLSS 4 turned on. Gamers who are already trying DLSS 4 out in Cyberpunk 2077 using RTX 40-series GPUs report seeing huge improvements already.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia prepares to move Maxwell, Pascal, and Volta GPUs to legacy driver status",
            "link": "https://www.techspot.com/news/106498-nvidia-prepares-move-maxwell-pascal-volta-gpus-legacy.html",
            "snippet": "Nvidia's CUDA 12.8 release notes indicate that support for the older architectures is now considered \"feature-complete\" and will be frozen in an upcoming...",
            "score": 0.6986548900604248,
            "sentiment": null,
            "probability": null,
            "content": "In context: Nvidia is preparing to transition its Maxwell, Pascal, and Volta GPU architectures to a legacy driver branch, signaling the end of an era for these iconic products. This transition reflects the company's focus on supporting more recent hardware capabilities, particularly in areas such as AI and ray tracing.\n\nNvidia's CUDA 12.8 release notes indicate that support for the older architectures is now considered \"feature-complete\" and will be frozen in an upcoming release.\n\nThis move marks a significant shift for Nvidia as it begins to phase out support for the remaining GTX-era architectures. While CUDA support will continue for Maxwell, Pascal, and Volta GPUs, they will no longer receive new features in future updates. It's important to note that this change does not immediately affect GeForce gaming driver support, as Maxwell and Pascal GPUs are still included in the support list for the GeForce RTX series driver.\n\nNvidia has not provided a specific date for the end of full support for these three GPU architectures, but the transition is expected to occur soon. Once this change takes effect, the GTX 16-series, based on the Turing architecture of the RTX 20-series, will be the only remaining GTX-series GPUs with full support.\n\nThe Maxwell architecture, introduced 11 years ago, represents the oldest of the outgoing GPU architectures still supported by Nvidia on the consumer side. It debuted with the GeForce GTX 750 series and was followed by the GTX 900 series. Maxwell brought significant performance-per-watt improvements over its predecessor, Kepler, and was particularly notable for its efficiency in mobile GPUs.\n\nPascal, introduced in 2016 with the GeForce GTX 1000 series, marked one of Nvidia's most significant architectural advancements in the 2010s. It utilized TSMC's 16nm finFET plus technology, doubling the density of Maxwell's 28nm node and delivering substantial performance gains. The GTX 1080, for instance, offered 60-65 percent higher performance than its predecessor, the GTX 980.\n\nVolta, released in 2017, was primarily focused on AI applications and enterprise use. It introduced Tensor cores, specialized units designed for AI workloads, which provided nine times the performance of Pascal in AI-specific tasks. Volta was largely confined to the enterprise sector, with the Titan V being the only desktop GPU to feature this architecture.\n\nFor Linux users, most distributions will continue to support legacy versions of the Nvidia driver, ensuring that affected cards will remain functional for the foreseeable future. However, users should be aware that they will not receive new features or optimizations moving forward.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-24": {
        "0": {
            "title": "Nvidia GeForce RTX 5090 costs as much as a whole gaming PC\u2014but it sure is fast",
            "link": "https://arstechnica.com/gadgets/2025/01/review-nvidias-geforce-rtx-5090-is-the-first-gpu-that-can-beat-the-rtx-4090/",
            "snippet": "Nvidia's GeForce RTX 5090 starts at $1,999 before you factor in upsells from the company's partners or price increases driven by scalpers and/or genuine...",
            "score": 0.8828427791595459,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's GeForce RTX 5090 starts at $1,999 before you factor in upsells from the company's partners or price increases driven by scalpers and/or genuine demand. It costs more than my entire gaming PC.\n\nThe new GPU is so expensive that you could build an entire well-specced gaming PC with Nvidia's next-fastest GPU in it\u2014the $999 RTX 5080, which we don't have in hand yet\u2014for the same money, or maybe even a little less with judicious component selection. It's not the most expensive GPU that Nvidia has ever launched\u20142018's $2,499 Titan RTX has it beat, and 2022's RTX 3090 Ti also cost $2,000\u2014but it's safe to say it's not really a GPU intended for the masses.\n\nAt least as far as gaming is concerned, the 5090 is the very definition of a halo product; it's for people who demand the best and newest thing regardless of what it costs (the calculus is probably different for deep-pocketed people and companies who want to use them as some kind of generative AI accelerator). And on this front, at least, the 5090 is successful. It's the newest and fastest GPU you can buy, and the competition is not particularly close. It's also a showcase for DLSS Multi-Frame Generation, a new feature unique to the 50-series cards that Nvidia is leaning on heavily to make its new GPUs look better than they already are.\n\nFounders Edition cards: Design and cooling\n\nRTX 5090 RTX 4090 RTX 5080 RTX 4080 Super CUDA cores 21,760 16,384 10,752 10,240 Boost clock 2,410 MHz 2,520 MHz 2,617 MHz 2,550 MHz Memory bus width 512-bit 384-bit 256-bit 256-bit Memory bandwidth 1,792 GB/s 1,008 GB/s 960 GB/s 736 GB/s Memory size 32GB GDDR7 24GB GDDR6X 16GB GDDR7 16GB GDDR6X TGP 575 W 450 W 360 W 320 W\n\nWe won\u2019t spend too long talking about the specific designs of Nvidia\u2019s Founders Edition cards since many buyers will experience the Blackwell GPUs with cards from Nvidia\u2019s partners instead (the cards we\u2019ve seen so far mostly look like the expected fare: gargantuan triple-slot triple-fan coolers, with varying degrees of RGB). But it's worth noting that Nvidia has addressed a couple of my functional gripes with the 4090/4080-series design.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "The Nvidia RTX 5090 Generates So Many Frames, It Scares Me",
            "link": "https://gizmodo.com/the-nvidia-rtx-5090-generates-so-many-frames-it-scares-me-2000554311",
            "snippet": "Nvidia's multi frame gen on the GeForce RTX 5090 GPU can produce framerates above 300 FPS, though that doesn't mean you need a $2000 card.",
            "score": 0.498190313577652,
            "sentiment": null,
            "probability": null,
            "content": "The $2,000 Nvidia RTX 5090 is so powerful that it made me wonder if there were such a thing as too many frames when it came to PC gaming. The thought didn\u2019t occur to me until I played Dragon Age: The Veilguard with Nvidia\u2019s touted Multi Frame Generation as part of its DLSS 4 update. During a hectic enemy encounter, it went above 360 FPS on the highest graphical settings and even higher when the action died down. It\u2019s partly a factor of the new Blackwell architecture, but the graphics card is only a vehicle. The rest is AI and \u201cfake frames.\u201d\n\nSee Nvidia RTX 5090 at Best Buy\n\nMore to the point, it\u2019s a process called Multi Frame Generation. With the launch of the latest update to Nvidia\u2019s AI upscaler DLSS 4, the new Nvidia cards can essentially generate up to three frames in between two rendered frames. If the card is already powerful, then multi-frame gen is like putting a cherry on top of the most expensive, decadent gold leaf sundae. What\u2019s the difference between playing a single-player roleplaying game at 120 FPS versus 360 FPS? After playing multiple big-name titles with the new DLSS 4 capabilities, I can\u2019t tell whether there\u2019s any reason you need those framerates other than the primordial joy of watching a number go up.\n\nThis isn\u2019t a full review of Nvidia\u2019s new GPU. I can\u2019t compare the RTX 5090 Founders Edition to the $1,600 RTX 4090 because I don\u2019t have access to one. I can compare it to the RTX 4080 Super and RTX 4070 Ti Super, solid cards with asking prices well below the 5090. With any high-end card, you always need to rely on software. That\u2019s true for the 5090 as well. If you want to play Cyberpunk 2077 at 4K with max settings and ray tracing enabled without DLSS, you won\u2019t hit 60 FPS. With DLSS on balanced settings, you\u2019ll see over 100 FPS. When you apply frame gen, you get a kick out of the high number, but as with anything generated with the help of AI, there will inevitably be some unintended side effects.\n\nThe Nvidia GeForce RTX 5090 Founders Edition is set to launch on Jan. 30.\n\nHow Do Games Run with All the RTX 5090\u2019s Bells and Whistles?\n\nThe Nvidia GeForce RTX 5090 specs are only slightly less ludicrous than its price tag. The card is based on the new Blackwell architecture, and the two-fan Founders Edition config packs 32 GB of VRAM, 21,760 CUDA cores with a 2.01 GHz base clock, and a 2.41 GHz boost clock. The card comes with 4th-gen ray tracing cores with a claimed 318 TFLOPS performance. All that\u2019s well and good, but Nvidia has focused most of its attention on the 5th-gen Tensor cores and the promised 3,352 TOPS of AI performance.\n\nThe AI is the name of the game here. It would be difficult to justify the price increase without the Blackwell architecture and multi frame gen capabilities. Like previous versions of frame gen first released with DLSS 3, this inputs a generated frame between two rendered frames. Multi Frame Gen is better than before, according to Nvidia. It should be more efficient and 40% faster. What helps is it uses an AI Management Processor on the GPU itself to assign these various AI tasks.\n\nThe Blackwell cards use so-called \u201cflip metering\u201d when generating frames. This slots them in such a way to reduce latency. Even then, Nvidia wants you to rely on RTX Reflex 2 to reduce latency further. If this is all starting to sound like a lot to add on top of the traditional horsepower of a new graphics processor, that\u2019s because it is.\n\nSome games won\u2019t be quick on the draw to update their UI for Multi Frame Gen or any new DLSS 4 features. The Nvidia app includes a DLSS override feature that lets you force it to upgrade to the new upscaler and frame gen. Either way, you can choose 4x, 3x, or 2x frame gen.\n\nOlder RTX cards will receive some upgrades to DLSS 4. Still, while the RTX 40-series will have access to the transformer model DLSS and some upgrades to 2x frame generation, Nvidia said the Blackwell architecture is necessary for Multi Frame Generation.\n\nHow Do Games Play When Most of Your Frames are Generated?\n\nI tested Nvidia\u2019s new flagship on the prebuilt Origin PC Neuron 3500X, which included an Intel Core Ultra 285K CPU and 32 GB of LDDR5X RAM. The PC packs a 1000 W 80 Plus Gold PSU. The power supply is the minimum Nvidia recommends for the 5090. This is a near-$3,400 desktop tower MSRP. A 1000 W PSU is nominal for most high-end PCs, though before, it was for the sake of having headroom and for future-proofing. Considering the RTX 5090 is $2,000, every other component you need to make the most of the GPU capabilities makes an excessive PC feel more excessive.\n\nThere\u2019s an unnerving feeling when you jump into a game and watch that frame counter skyrocket. There\u2019s a sense that this can\u2019t be real, and you immediately look for problems. The thing is, I couldn\u2019t spot many or practically any visual discrepancies. I analyzed the foliage as closely as possible in Dragon Age: The Veilguard but couldn\u2019t find any noticeable issues with sharpness or textures.\n\nI took it into other demanding games, like Hogwarts Legacy and Cyberpunk 2077. I never thought I\u2019d ever witness Cyberpunk push 200 FPS, but life tends to throw its curveballs. Multi Frame Gen also meant I could push path tracing to ultra and still maintain such high FPS anybody looking on would assume I\u2019m faking it (and to a point, I am).\n\nWhile cruising the streets of Night City on a bike, I didn\u2019t encounter any hints of hitching. However, I noticed some of the lights on Jackie\u2019s bike oddly flickering while driving around at high speed. The flickering occurred at 2x, 3x, and 4x frame gen, though it was worse when relying on more generated frames. You can see the issue for yourself in the video above.\n\nI won\u2019t say I wasn\u2019t distracted by the odd UI flickering, though the game was perfectly playable anyway. I also didn\u2019t notice any floatiness in the controls or artifacts in the visuals, but that doesn\u2019t mean there weren\u2019t any other issues I couldn\u2019t make out myself. With Reflex 2, I didn\u2019t notice any responsiveness issues either.\n\nWithout Frame Gen turned on, Cyberpunk was running at about 100 FPS with DLSS upscaling on balanced mode. Without DLSS, it would run at sub-60 FPS. The game is perfectly playable on an RTX 5090 without frame gen. The excess frames don\u2019t necessarily add to the experience. Those \u201cfake frames\u201d could distract some players more than it aids.\n\nI was struck by how normal games looked while running all these generated frames. In Dragon Age: The Veilguard, I noticed one cutscene seemed to pass too fast, but the issue didn\u2019t repeat afterward. Playing Alan Wake II, I can\u2019t possibly get beyond 60 FPS with path tracing on Ultra and every other setting turned up to 11, even with the 5090 and DLSS on balance. When enabling 4x multi frame gen, it jumps to past 190 FPS. There were odd issues with pop-in, but that\u2019s a known issue for the game running on the maximum settings on PC. I tried it and found that the difference between 70 FPS without frame gen and 250 FPS with 4x frame gen didn\u2019t disrupt gameplay.\n\nIs the RTX 5090\u2019s Frame Gen Even Necessary?\n\nBut does it enhance gameplay? I normally test games at 4K on an AOC U27G3X monitor, which only goes up to a 120 Hz refresh rate. That\u2019s perfectly fine for most GPUs, and it would be fine for the 5090 if it weren\u2019t for multi-frame generation. Without the 2x, 3x, or 4x frame generation, I could only hope to hit around 100 FPS with medium DLSS upscaling and no path tracing for the most demanding titles.\n\nIf I wanted to make the most of 360 FPS, I would need a really, really expensive monitor like the LG Ultragear monitor that can go up to 480 Hz refresh rates, but then you may sacrifice the sacred 4K resolution. This last CES was stuffed with 240 Hz 4K OLED monitors from every OEM under the sun. You could grab a beautiful, high-end curved monitor that does 4K and 240 Hz for well over $1,000, but you\u2019ll struggle to find one to make sense of all those frames. LG\u2019s lauded bendable 5K2K monitor will only stick at 165 Hz if you want a higher resolution.\n\nSo, plenty of monitors can do 240 Hz, but fewer do more. That\u2019s because there\u2019s a point when framerate doesn\u2019t make a material difference. Even a professional FPS esports competitor will compete on a 240 Hz monitor. You won\u2019t notice a major difference between 120 FPS and 240 FPS if you game casually. I took the 5090 for a spin with Marvel Rivals. It\u2019s the kind of game that doesn\u2019t demand too much from a PC. It will have over 300 FPS with multi frame gen. I\u2019m not a capable enough gamer to know whether the generated frames help or hinder me. It\u2019s not going to make my aim better, in either case.\n\nNvidia CEO Jensen Huang made it clear in a recent Q&A with press and analysts that the company priced RTX 5090 at $2,000, knowing your average gamer won\u2019t afford it. In the CEO\u2019s words, this is the GPU for the people who want \u201cthe best,\u201d and they don\u2019t care what they spend to get \u201cthe best.\u201d Having the best also means you would need the best CPU, the highest-rated power supply, and one of the most expensive monitors you can buy. It\u2019s a card made for people where money is no object. At that point, what is the point of a review? You already know the 5090 is a step above the 4090. It had better be because it cost $400 more than Nvidia\u2019s previous flagship.\n\nYou still need a game to run at a stable 60 FPS to make frame generation useful, which is why I\u2019m more curious about the RTX 5070 and 5070 Ti for the sake of gamers who can\u2019t spend the equivalent of their monthly rent or mortgage payment on a single graphics card. Multi Frame Gen will be a better bargain for cheaper cards. That\u2019s why I\u2019m more excited for the still unannounced RTX 5060. That will depend on how good the base card offers a standard frame rate with fully rendered frames. Still, budget gamers are used to making compromises. Those who pay $2,000 for a graphics card demand none.\n\nSee Nvidia RTX 5090 at Best Buy",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "NVIDIA GeForce RTX 5090 Founders Edition Review & Benchmarks: Gaming, Thermals, & Power",
            "link": "https://gamersnexus.net/gpus/nvidia-geforce-rtx-5090-founders-edition-review-benchmarks-gaming-thermals-power",
            "snippet": "GPUs NVIDIA GeForce RTX 5090 Founders Edition Review & Benchmarks: Gaming, Thermals, & Power January 24, 2025 Last Updated: 2025-01-24 We take a deep dive...",
            "score": 0.939106285572052,
            "sentiment": null,
            "probability": null,
            "content": "First off, we have a huge amount of content related to this card coming up since the Founders Edition model is so unique. Make sure you check back regularly over the next few days to catch our benchmarks in common mini-ITX cases, the impact of the GPU on CPU and CPU cooler thermals, and some other tests. We also have a tear-down coming up.\n\nNormally, these Founders Edition models don\u2019t warrant a ton of discussion. This one does, but we\u2019ll keep it short.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe RTX 5090 Founders Edition moves to a 2-slot design and uses a dual flow-through configuration, so they\u2019ve sandwiched the PCB centrally and offset the PCIe slot to the side and down as a result. That also means that NVIDIA needs a separate PCB for the I/O that feeds monitors, connected via a flex cable to the main PCB. To get a 2-slot cooler capable of handling 575W or more, NVIDIA is using liquid metal with a triple-walled gasket to both contain the liquid metal and prevent exposure that could change its consistency and efficacy.\n\nThe FE model does a lot of small things to improve performance, like exhausting the air out the top of the card and away from the GPU inlet. You\u2019ll see that in our Schlieren imaging below.\n\nWe have a full video with Malcolm Gutenberg, Lead Thermal Engineer on the FE card, breaking down the changes.\n\nRTX 5090 Pricing\n\nThe NVIDIA RTX 5090 is supposed to be $2,000 and will have official availability on January 30th, joined by the $1,000 MSRP RTX 5080 on the same date. NVIDIA also has the 5070 Ti and 5070 launching in presumably February for $750 and $550.\n\nThe NVIDIA RTX 4090 had an MSRP of $1,600, then was regularly priced around $2,000-$2,500 due to shortages and demand, and is basically out of stock except at terrible third-party seller prices now. The RX 7900 XTX (watch our review) is AMD\u2019s closest competitor. Pricing is around $870 to $900. The company has also bowed out of the high-end race. The 9070 and 9070 XT, AMD\u2019s next cards, should be coming around March or so.\n\nIntel is currently only fighting at the low-end and mid-range.\n\nWhich makes all of this somewhat weird, because there are no head-to-head competitors right now. The closest comparison is the RTX 4090, then maybe the RX 7900 XTX from AMD\u2019s side.\n\nRTX 5090 Overview\n\nThe RTX 5090 has 32GB of GDDR7 memory, which is a big change, and runs on the Blackwell architecture, which follows Ada Lovelace. It\u2019s introduced alongside multi-frame generation (MFG) and DLSS4, which we\u2019ll talk about later.\n\nThe 5090 is also a true PCIe Gen5 device, but that\u2019ll be another separate piece soon to check back for the differences.\n\nIf you want to see our testing methodology, we\u2019ve published the test bench and the list of games and their settings here, which will let you get quick answers to what we\u2019re doing. It doesn\u2019t have every answer, but we\u2019re slowly adding to it with each review cycle.\n\nRTX 5090 Thermals\n\nHere\u2019s a thermal chart running the RTX 5090 under its auto VBIOS fan curve with a Port Royal RT stress test at 4K.\n\nGPU temperature plots at about 72 degrees Celsius once it hits steady state for overall temperature when tested in a controlled room ambient of 21 to 22 degrees Celsius. This GPU temperature is genuinely impressive considering the size of the card, and that\u2019s important to remember. At 2 slots versus that 4-slot monster we\u2019ve seen for the last few years, this is an excellent result given the size. Our prototype testing already told us what NVIDIA can do with a fully committed, fatter design if you\u2019re curious what that\u2019d look like.\n\nMemory temperature ran warm, unfortunately, at 89 to 90 degrees Celsius. This is higher than we\u2019d like to see, especially considering it could be warmer in certain case configurations with a higher internal ambient temperature. This is technically still within the TjMax of these memory modules as far as we could find, so there isn\u2019t an imminent threat to the card, but this would be an area for NVIDIA to improve; our primary concern is in hotbox cases or small form factor solutions, which we\u2019re looking into as a follow up that you should check back regularly for. While these results are higher than what we\u2019d like to see, in most high airflow ATX cases, it is okay.\n\nAdding GPU fan speed to this chart, the fans both hit around 1570 RPM. We should get to acoustic testing for more on this.\n\nRTX 5090 Acoustics\n\nWe took the RTX 5090 to our hemi-anechoic sound chamber to evaluate it. A good GPU temperature is an achievement at this size, but that can almost always be done by just blasting the fan speeds and compromising on noise levels.\n\nWe ran the card at the default fan RPM that the card set itself to at steady state under our standard thermal workload.\n\nHere\u2019s the frequency spectrum plot. In our acoustic chamber with a noise floor of about 14-14.5 dBA on the day of testing, the RTX 5090 was measured in our passive test bench at about 32.5 dBA total. That\u2019s at a distance of 1 meter.\n\nThe RTX 5090 had some spikes during testing, including above our frequency cutoff, but overall has a very gradual curve for the plot. The limited presence of peaks and spikes in this plot help illustrate the relative uniformity of the whirring noise, which we subjectively think helps it blend into the background more. Noise is subjective, so although this plot objectively tells us that there\u2019s a ping at 350 Hz and a bump in the plot around 515 Hz and again around 2,000 Hz, what matters is how it sounds.\n\nThis is a sound sample for you to judge on your own. Note that this is not identical to what we\u2019re presenting as we have boosted it for purposes of being level with our video audio. Listen for the type of noise, not the volume.\n\nSchlieren Imaging of RTX 5090\n\n\n\n\n\nIn our interview with Malcolm Gutenberg, he explained that the 5090\u2019s thermal solution was designed to reduce recirculation using angled covers, which direct airflow.\n\nIn this image, we're looking at the GPU straight-on, with it perpendicular to the camera frustum. This is when the fans are off but the heat load has already started. You can see the density change as the hot air leaves the card passively.\n\n\n\n\n\n\n\nAs the fans turn on, we see a sudden flare-up and movement of air to the right through the flow-through area out the back and toward the CPU tower. What's super cool here is that we can see the air kick up and out to the right at a 20-30 degree angle or so. We also see a really high flow area of air exiting from the fins at the outer edges of the heatsink design. This hyper focuses the flow and reduces recirculation around the front area of the card, which just means the whole design is incredibly efficient at getting air away from the board and into case exhaust fans.\n\nHere's the table shifted to see more exhaust. The flow-through area has super high speed exhaust, illustrating why flow-through is so much more effective than shoving air straight into a PCB wall.\n\n\n\n\n\nLooking at the fans spinning down at the end of a load and returning to passive cooling. Everything drifts up and away.\n\nThis next angle shows the card on the left and centered. The most interesting thing we see is this straight line of air shot out from those fins at the outer edges of the fan diameter. This is what Gutenberg was talking about in our interview, where they're capitalizing on the area of most efficacy for the fan blade.\n\nFinally, here's the card straight-on, where we can see the amount of air shot up and out. You'll want to choose cases with some spacing between the glass panel and the card to help get the warmed air away faster.\n\nRTX 5090 Frequency\n\nThis next line plot is to ensure the GPU is functioning properly and meets the spec NVIDIA publicly claims. NVIDIA claims the 5090 runs at 2.01 GHz base and 2.41 GHz boost, with room for that to change based on the load. Assuming the software monitoring is correct for this new architecture, we measured about 2600-2700 MHz during the test, commonly 2600-2650 MHz.\n\nThe RTX 4090 ran at about 2745 MHz in the same benchmark back when it launched and we tested it. Frequency clearly isn\u2019t everything though, and it\u2019s common that higher end configurations bring frequency down in some capacity. It\u2019s also true that architectural differences also make frequency indirectly comparable.\n\nUltimately: The card is exceeding the specification advertised by NVIDIA, so it\u2019s running as expected, which is good.\n\nRTX 5090 Game Benchmarks\n\nLet\u2019s get into gaming benchmarks.\n\nFFXIV 4K\n\nIn Final Fantasy 14 at 4K, the RTX 5090 ran at a comically high 182 FPS AVG, with 1% lows that were nearly identical to the average framerate of the RTX 4090. That makes it 31% higher average framerate than the RTX 4090.\n\nFor a quick value discussion: The RTX 5090 is supposed to be $2,000, with the RTX 4090\u2019s MSRP previously being $1,600. The 4090 is not commonly available anymore for a reasonable price, though. MSRP-to-MSRP, the 5090 is 25% more expensive and 31% higher framerate in this test. The memory capacity increase benefit isn\u2019t seen in this game either, as that\u2019d be more of an impact in professional applications like Premiere, 3D work, or ML workloads.\n\nThe RX 7900 XTX ran at 104 FPS AVG, the same as when we tested it in December (so there\u2019s been no change), which gives the RTX 5090 a lead in this rasterized benchmark of 74%. The 0.1% lows are about the same between all of these devices at the top-end, which mostly comes down to pacing within the game.\n\nPrior NVIDIA flagships include the RTX 3090 Ti (watch our review) at 88 FPS AVG, meaning that the 5090 has doubled that performance. The frametime pacing was excellent on the 3090 Ti as it closely follows the average. The 3090 (watch our review) was more or less a flagship as well and at 77 FPS AVG. The 6950 XT (watch our review) was also once a flagship, closer to the RTX 3080 (watch our review) for performance.\n\nThe RTX 2080 Ti (watch our review) held a 54 FPS AVG, meaning 5090 owners would see an increase of 237% over the 2080 Ti.\n\nFFXIV 1440p\n\nAt 1440p, the 5090 again continues the comically high framerate by running at 317 FPS AVG. This has it about 17% ahead of the RTX 4090\u2019s 272 FPS AVG. The advantage has been trimmed here, which could be because of an encroaching CPU bottleneck and/or because of architectural changes -- 1080p will help answer that below.\n\nFor games like this, you\u2019d need a high-end CPU and ideally more intensive resolution to really get full use of the 5090.\n\nSince we\u2019re bottlenecked, we\u2019ll move along but quickly stop to look at 1080p -- just for fun.\n\nFFXIV 1080p\n\nIf you thought the previous framerate was funny, cast your sights upon 407 FPS AVG at 1080p. Sorry -- that\u2019s 407.1 FPS AVG.\n\nWhew. Close one. As we all know, 407 FPS AVG is below the threshold of acceptability for the modern gamer. That 0.1 FPS is critical and is what finally pushes NVIDIA into playable territory for this game.\n\nIn serious news: The RTX 4090 at 376 FPS AVG means the 5090 is still about 8% ahead. This test really is just for fun though, but is a good reminder of the limitations of even a 9800X3D to boost the ceiling.\n\nBlack Myth: Wukong - 4K\n\nBlack Myth: Wukong is relatively new to our test suite and is tested using the built-in benchmark. We benchmarked it at 4K for this. Currently, we consider this test in our suite to be \u201cexperimental,\u201d meaning our confidence in it is present, but lower than other tests as we evaluate its reliability. We have been moving toward removing experimental status from it with each review.\n\nAt 4K and where we\u2019ve only tested a handful of cards due to the intensive load, the RTX 5090 ran at 86 FPS AVG with lows at 74 and 70. This has it 28% higher in average framerate than the 67 FPS AVG of the RTX 4090. So far, we\u2019re seeing a few titles around this 30% number at 4K. Comically, the 1% lows, which for us is an average of the slowest 1% of frames, are higher than the average framerate for any other card on this chart; in fact, even the RTX 5090\u2019s slowest 0.1% of frames are faster than the average framerate of the RTX 4090. That\u2019s crazy.\n\nThe 7900 XTX\u2019s 49 FPS AVG gives the 5090 a 74% lead, with the 3090 Ti giving it an 89% lead. Improvement over the 2080 Ti is enough to feel irrelevant as a percentage, as it takes it from totally unplayable to relatively fluid.\n\nBlack Myth: Wukong - 1440p\n\nAt 1440p, Black Myth has the RTX 5090 at 130 FPS AVG, 23% improved over the RTX 4090\u2019s 106 FPS AVG. The lows also improve. The rapid rundown against other flagships is as follows:\n\nThe 5090 has a 51% higher average framerate than the 7900 XTX, 75% higher than the 3090 Ti FTW3 (RIP EVGA), 99% higher than the RTX 3090 Master, and 189% higher than the RTX 2080 Ti former flagship.\n\nBlack Myth: Wukong - 1080p\n\nBlack Myth Wukong is heavy enough that 1080p still has some meaningful spacing, even without ray tracing. The RTX 5090 ran at 160 FPS AVG, with good frametime pacing establishing 127 FPS and 116 FPS lows. The 160 FPS result has it 20% ahead of the RTX 4090, diminishing the earlier lead (which was 28% at 4K, 23% at 1440p, and now 20%). This isn\u2019t just a CPU limit, as we also saw in Final Fantasy, but speaks to other advantages on the 5090 especially at higher resolutions. We think the memory bandwidth is likely a large part of that additional scaling.\n\nThe RX 7900 XTX ended up 113 FPS AVG, with the 3090 Ti former flagship at 94 FPS and the 2080 Ti at 62 FPS.\n\nGrab a GN15 Large Anti-Static Modmat to celebrate our 15th Anniversary and for a high-quality PC building work surface. The Modmat features useful PC building diagrams and is anti-static conductive. Purchases directly fund our work! (or consider a direct donation or a Patreon contribution!)\n\nStarfield - 4K\n\nStarfield is up next. We haven\u2019t run that many cards for this at 4K, but have a lot of 1440p data. We\u2019ll start with the more limited 4K data set.\n\nAt 4K, the RTX 5090 held 108 FPS AVG with lows that were within expectations for this game. The RTX 4090 ran 92 FPS AVG, giving the 5090 a lead of just 17%, lower than we\u2019ve seen in some other tests.\n\nThe lead over the 7900 XTX\u2019s 77 FPS AVG is 40%, with the lead over the 58.3 result for the 3090 Ti at 85%. The 3080 (watch our review) was down at 48 FPS AVG, with the 2080 Ti at 33 FPS AVG. AMD\u2019s 7900 XTX and 7900 XT (read our revisit) are its highest-end cards available for the company right now, but the 6950 XT was a good deal in the back half of its life.\n\nStarfield - 1440p\n\nAt 1440p, the RTX 5090 ran at 147 FPS AVG against the 132 FPS AVG of the RTX 4090. This is down to a 12% uplift. The 7900 XTX ran at 112 FPS AVG, a big improvement from its 4K result as you would expect, with the 4080 FE (watch our review) at 108 FPS AVG. The 4080 Super (read our review) would be around 1-3% better here if we had retested it.\n\nStarfield - 1080p\n\nThere aren\u2019t many reasons you\u2019d play this game at 1080p with an RTX 5090, but just for sake of data: The 5090 ran at 165 FPS AVG here, with the 4090 at 155 FPS AVG. Although technically better for the 5090, we\u2019re effectively at the CPU limit here.\n\nDragon\u2019s Dogma 2 - 4K\n\nDragon\u2019s Dogma 2 is up next. This is another new one that we added in 2024 and has been heavy on GPUs and CPUs alike depending on the test area.\n\nIn this limited suite of cards, we have the RTX 5090 at 133 FPS AVG, leading the RTX 4090 by 35%. This is one of the largest gains we saw in our test suite. The lows and 0.1% lows also scaled up, showing that frametime pacing wasn\u2019t at the expense of higher FPS.\n\nThe RX 7900 XTX ran at 77 FPS AVG, with the 4080 FE at 72 FPS. Again, the 4080 Super would be about 1-3% above that.\n\nThe 2080 Ti from 2018 ran at 36 FPS AVG, and that\u2019s without RT. The improvement to the 5090 is 267%. Climbing the flagships, the 3090 Ti\u2019s 64 FPS AVG ends up giving the 5090 a 108% lead.\n\nDragon\u2019s Dogma 2 - 1440p\n\nAt 1440p, the RTX 5090 FE climbs in framerate to 189 FPS AVG, with extremely well-paced frametime consistency shown in the high 0.1% and 1% low values.\n\nThe 5090 ends up leading the 4090\u2019s 156 FPS AVG by 21% and the 7900 XTX by 50%. The lead against these cards has fallen from the 4K results.\n\nDragon\u2019s Dogma 2 - 1080p\n\nAlthough we\u2019re in territory where it\u2019s not meaningful for the experience, it\u2019d help us to understand the behavior by looking at 1080p. The framerate still increases, so we weren\u2019t totally bound by the CPU. The 5090 hits 214 FPS AVG, leading the 4090 by 13%. What\u2019s interesting is that the 4090 is now at the same framerate that the 5090 had when the 5090 was at 1440p.\n\nCyberpunk 2077: Phantom Liberty - 4K\n\nCyberpunk is up now. We\u2019re testing the Phantom Liberty expansion in-game in the expansion area.\n\nThe RTX 5090 ran at 95 FPS AVG, with lows at an impressive 81 FPS 1% and 77 FPS 0.1%. These lows are excellent numbers and similar to what we saw in Black Myth: Wukong, where the 5090\u2019s lows are outperforming the 4090\u2019s average. The improvement in average FPS was large at 50%, moving from 64 FPS AVG on the RTX 4090. This is the biggest gain we\u2019ve come across so far. Cyberpunk is very particular though and sensitive to areas of the game. Checking with Wendell, his Level1 Techs team saw similarly huge uplift.\n\nThe RTX 4090 had a large 32% lead over the RTX 4080 already. As for the older cards, the 5090 and 3090 Ti are in entirely different classes. The 2080 Ti is down at 27 FPS AVG and struggling to run, although to its credit, its frametime pacing in relation to the average is excellent -- it\u2019s just that the framerate is low.\n\nCyberpunk 2077: Phantom Liberty - 1440p\n\nAt 1440p, the RTX 5090 ran at 181 FPS AVG, with lows at 126 and 108. The RTX 4090 held a 137 FPS AVG, with the advantage of the 5090 being reduced to a still respectable but lower 33%. The 7900 XTX ran at 120 FPS AVG here, which has remained a good result considering the price of the 7900 XTX as compared to its neighbors. That story is totally different with RT, though.\n\nThe RTX 3090 Ti ran at 91 FPS AVG, with the 2080 Ti at 57 FPS AVG.\n\nWe were fully CPU bound at 1080p, so we\u2019ll skip it.\n\nDying Light 2 - 4K\n\nDying Light 2 at 4K is another heavy load for these GPUs. The RTX 5090 shows a familiar scenario of the 1% lows and 0.1% lows, which represent the slowest frames in our test passes, outperforming the average framerate of the RTX 4090. NVIDIA has managed to move the needle for at least the flagships, which we think is partly thanks to cache and memory configuration changes.\n\nThe 5090 leads the 4090 by 38%, another impressive jaunt not distant from what we saw with Phantom Liberty. The 7900 XTX did OK in this test as compared to the 4080. The 5090 runs 74% higher average framerate than the 7900 XTX and also costs about 127% more, depending on what price the XTX is. For professional users though, the memory benefit isn\u2019t accounted for in almost any gaming scenarios we test and would be in other applications.\n\nDying Light 2 - 1440p\n\nAt 1440p, the RTX 5090 holds a 216 FPS AVG against the 4090\u2019s 173. This has the 5090 25% ahead of the RTX 4090, down from its lead of 38% at 4K. We won\u2019t burn chart time on it, but 1080p is only about 15 FPS higher, so part of this reduction in scaling is because we\u2019re starting to approach the CPU limit.\n\nResident Evil 4 - 4K\n\nResident Evil 4 is up next, first rasterized and at 4K.\n\nThe RTX 5090 landed at 207 FPS AVG here, with lows running higher as a result of consistent frame pacing. The end result is a lead over the 151 FPS AVG of the 4090 by 37%, a lead over the 7900 XTX of 64%, and lead over the 4080 of 101%. Against prior flagships, the 3090 Ti landed at 89 FPS AVG, giving the 5090 an uplift of 133%.\n\nResident Evil 4 - 1440p\n\nAt 1440p, the RTX 5090 continued scaling and hit almost 350 FPS AVG, with lows that are at ridiculous levels with 281 FPS 1%. This puts the 5090\u2019s average framerate 25% ahead of the 4090\u2019s average framerate, so we\u2019re seeing a reduction from the 37% at 4K, consistent with what we\u2019ve seen elsewhere.\n\nThe 7900 XTX held on at 232 FPS AVG here, followed by cards like the 3090 Ti at 162 FPS and 2080 Ti at 92 FPS AVG.\n\nResident Evil 4 - 1080p\n\nAt 1080p, we see there was still scaling all the way up to almost 400 FPS AVG, which is crazy. This has reset our expectations of where the CPU ceiling is. If anything, this is showing just how good the 9800X3D is for keeping up so well.\n\nThe gap between the 5090 and 4090 is around 9% here, so we are actually hitting external limits.\n\nRTX 5090 Ray Tracing Benchmarks\n\nVisit our Patreon page to contribute a few dollars toward this website's operation (or consider a direct donation or buying something from our GN Store!) Additionally, when you purchase through links to retailers on our site, we may earn a small affiliate commission.\n\nAnd now we\u2019re moving to ray tracing benchmarking. This contains games like Black Myth and Cyberpunk, which tend to favor NVIDIA, and games like Resident Evil, Dying Light, and Dragon\u2019s Dogma, which give some more variety.\n\nRay Tracing - Black Myth: Wukong 4K\n\nBlack Myth is first. This is an experimental chart, so once again, our disclosure is that experimental charts have a greater risk of unexpected results as we are still researching its behaviors. This particular title is considered experimental in our test suite because its performance leans so heavily in one direction that we want to slowly accumulate results to explore it further.\n\nThe 5090 ran at 88 FPS AVG at 4K, outperforming the RTX 4090\u2019s 65 FPS AVG result by 36%. That\u2019s a big jump. This is with upscaling, so it\u2019s not like-for-like with the 4K raster results.\n\nAMD\u2019s 7900 XTX ran at 20 FPS in this title, which is why we say it\u2019s NVIDIA-favored. The 3090 Ti ran at 34 FPS AVG here.\n\nRay Tracing - Black Myth: Wukong 1080p\n\nSkipping 1440p and going to 1080p with FSR to get more cards on the chart, here\u2019s where we land. The 5090 is at 158 FPS AVG here, leading the 4090\u2019s 120 FPS AVG result by 31%. Against the 3090 Ti, the 5090 leads by 103%, and against the 2080 Ti\u2019s 49 FPS AVG, it\u2019s about a tripling.\n\nThe 4070 (watch our review) outperforms the 3090 Ti in this test when using FSR, with the entire top half of the cards outperforming the 7900 XTX. This test, again, is heavily favored for NVIDIA with the heavy ray tracing use.\n\nRay Tracing - Dragon\u2019s Dogma 2 4K\n\nDragon\u2019s Dogma 2 is up next. Again, we haven\u2019t done a ton of 4K Ray Tracing tests here because it\u2019s such a heavy workload normally, but the RTX 5090 ran at 113 FPS AVG with lows at 97 FPS and 94 FPS. The 4090 landed at around 85 FPS AVG, giving the 5090 an uplift of 33%. The RX 7900 XTX does better in this game compared to Black Myth, instead outperforming the RTX 4080 and 3090 Ti, the latter of which is at 55 FPS AVG.\n\nRay Tracing - Dragon\u2019s Dogma 2 1440p\n\nAt 1440p, the 5090 jumped to 165 FPS AVG and the 4090 held 136 FPS AVG, still keeping about a 30 FPS gap between them, or an improvement generationally of 22%. The uplift has fallen as compared to 4K, keeping with prior trends. The 7900 XTX does similarly here to last time, landing just ahead of the RTX 4080 (watch our review).\n\nRay Tracing - Dragon\u2019s Dogma 2 1080p\n\nAt 1080p, the 5090 continues to climb to 194 FPS AVG, reducing the generational uplift to 15% over the 4090. Let\u2019s move on to something more interesting.\n\nRay Tracing - Dying Light 2 4K\n\nHere\u2019s Dying Light 2 ray-traced. Again, we haven\u2019t historically run 4K here because only the 4090 and 4080 could be argued as capable. It looks like this next generation of hardware -- and hopefully that also includes AMD\u2019s next card -- is changing that. The RTX 5090 ran at 109 FPS AVG, leading the 80 FPS result of the 4090 by 37%. The 7900 XTX is led by 137%. AMD has publicly claimed that its next generation will significantly improve upon this, so we\u2019ll see where they land probably closer to March.\n\nRay Tracing - Dying Light 2 1440p\n\nAt 1440p, the RTX 5090 ran at 176 FPS AVG and held lows of 152 and 126. The 176 result has it about 40 FPS, or 29%, ahead of the RTX 4090. The 4080 hit 104 FPS AVG with the 3090 Ti at 88 FPS. Our 2080 Ti was approaching a decent framerate, but still falling short at 46 FPS AVG.\n\nRay Tracing - Dying Light 2 1080p\n\nAt 1080p, the 5090 held 224 FPS AVG, mostly establishing that we weren\u2019t bound previously by the CPU. So when it was at 4K, the scaling was a 37% generational improvement, then 29% at 1440p, and now is 24.5% at 1080p. The reduction from 1440p to 1080p isn\u2019t as big as we might expect from other tests, probably because there remains enough GPU load to where the CPU isn\u2019t heavily taxed.\n\nRay Tracing - Resident Evil 4 4K\n\nResident Evil 4 with Ray Tracing is up now, tested at 4K first. The 5090 ran at 210 FPS AVG using FSR as defined in the chart title. The 160 FPS RTX 4090 result establishes a 31% generational improvement favoring the 5090.\n\nThe lead over the 7900 XTX is 56%, with the improvement on the 3090 Ti at 113%.\n\nRay Tracing - Resident Evil 4 1440p\n\nWe\u2019ll keep this short: At 1440p, the RTX 5090\u2019s lead falls to 23% over the 4090. This trend is consistent.\n\nRay Tracing - Cyberpunk 4K RT Ultra\n\nCyberpunk with RT Ultra at 4K is heavy even for the RTX 5090 when not using some form of upscaling, which we toggle off in testing specifically because of how unreliable Cyberpunk\u2019s sticky settings are. The 53 FPS AVG puts the 5090 35% ahead of the 4090\u2019s 39 FPS AVG result, remaining consistent with prior tests. The poor, old 2080 Ti nearly burst into flames trying to run this, holding an 8.8 FPS AVG as it crawled across the finish line.\n\nRay Tracing - Cyberpunk 4K RT Medium\n\n4K with RT Medium is interesting. Dropping from Ultra to Medium predictably increased performance, but grew the gap between the cards with a 59 FPS AVG and 40 FPS AVG result.\n\nRTX 5090 Efficiency Benchmarks\n\nNow we\u2019re getting into efficiency benchmarking and idle power consumption. For this, although we tested a lot of games, we\u2019re going to simplify the charts and just look at a couple of game tests plus idle. These convey the whole story pretty well.\n\nTesting is done by measuring the GPU power consumption at the PCIe cables and the PCIe slot with an interposer. Although we initially had trouble getting the card to work on the riser due to PCIe generation differences, in the final hours before going live, we found a solution to measure through the riser. This testing eliminates the remainder of system power consumption, so we\u2019re isolating for just the GPU.\n\nRTX 5090 Power Consumption: Idle\n\nTesting idle power consumption, the RTX 5090 FE landed at 46W on the desktop with our benchmarking approach. The RTX 5090 FE measured lower in idle power draw than the Arc A580 (read our review) and about the same as the A750 (read our revisit). Even just sitting there, it\u2019s drawing a good amount of power. Our testing uses Windows High Performance power plan for benchmarking performance, so switching to Balanced may help reduce this; however, we use that plan for all tests, so these are like-for-like comparable. We measured the RTX 4090 at 28-29W. The 5090 has relatively high idle power consumption with our test approach and this is an area where there\u2019s clearly some room for improvement if only judging by the 4090, although the power consumption of the TDP is higher on this card.\n\nEfficiency: FFXIV 4K\n\nFinal Fantasy 14 at 4K is low on results since we just started using this for efficiency for this launch. The RTX 4090 was the most efficient here, at 391.7 W to produce 138-139 FPS AVG. That puts it at 0.35 FPS/W. The RTX 5090 FE was efficient as compared to the other cards we\u2019ve tested here, but technically worse off than the 4090. Realistically, they\u2019re about the same. Despite framerate improving by 31%, the power consumption also increased by 37%. The end result is reduced or equal efficiency versus the last generation. This might be why NVIDIA is pushing the narrative so hard that MFG improves efficiency, except that\u2019s like saying \u201cwhy compare apples to apples when you can compare apples to oranges?\u201d\n\nIn the very least, against the 3090 Ti in a like-for-like comparison, we can see clear and massive iterative improvements.\n\nEfficiency: FFXIV 1440p\n\nWe\u2019re showing 1440p to get a wider selection of cards, though the lighter load won\u2019t look better for the 5090. The RTX 5090 ended up around 520W average for this work, landing it at 0.61 FPS/W. Efficiency is down comparatively overall since we saw the performance advantage also go down when at 1440p. The card should show the best gains in heavy 4K/RT workloads like F1.\n\nEfficiency: F1 24 4K\n\nHere\u2019s F1 24 at 4K and with ray tracing.\n\nOn a technicality, the RTX 5090 is the most efficient in this test. It pulled 569W on average during testing and had spikes up to 580-590W, and because of the framerate advantage over the RTX 4090 with its 428W draw, it ends up at 0.21 FPS/W instead of 0.20. This isn\u2019t particularly exciting and we have to highlight that NVIDIA\u2019s claims of efficiency improvements largely centered around artificially generating frames, which isn\u2019t like-for-like because the frame itself may not be the same or comparable.\n\nRTX 5090 Conclusion\n\nGrab a GN Soldering & Project Mat for a high-quality work surface with extreme heat resistance. These purchases directly fund our operation, including our build-out of the hemi-anechoic chamber for our acoustic testing! (or consider a direct donation or a Patreon contribution!)\n\nFirst of all, we need to start with NVIDIA\u2019s complete bulls*** marketing. Unfortunately, NVIDIA just couldn\u2019t help itself except to unfairly misrepresent its RTX 5090\u2019s performance in the following slide on its site.\n\nThis image shows the RTX 5090 as being 2x faster than the RTX 4090 regularly, but as you all know from the review you just read, that\u2019s not true. This image doesn\u2019t say \u201cdifferent DLSS versions where we needlessly compare apples to oranges even though we have nothing to be shy of if we tested properly, it says \u201cPerformance.\u201d And the accompanying caption isn\u2019t even part of the image. Just saying \u201cPerformance\u201d while making big 2x bars makes the 5090 look 2x better than the 4090. NVIDIA technically lists the DLSS version in the bottom, but most people don\u2019t know what that means. Most people don\u2019t know that writing \u201cDLSS 4\u201d under BOTH bars of the RTX 4090 and RTX 5090 isn\u2019t actually the same setting. DLSS 4 does not do the same thing on both of these devices. NVIDIA\u2019s own line of gray text that blends into the background at nearly the same color states the test configuration. This states that Frame Gen was used on the 40 series and 4X multi-frame gen was used on the 50 series, which isn\u2019t like-for-like. NVIDIA is generating more artificial frames per real frame on the 5090 than the 4090, but they just list \u201cDLSS 4\u201d under the bars instead of making it clear.\n\nNVIDIA didn\u2019t have to do any of this, but between this insane reach of marketing and the claim CEO Jensen Huang made about an RTX 5070 performing the same as an RTX 4090, it comes across like NVIDIA feels like it isn\u2019t good enough on its own. It has to put a bunch of makeup on the charts to be good enough.\n\nAnyway, enough of the marketing bulls***. The recap is this:\n\nKnow what kind of user you are. Users of VRAM-intensive professional applications should consider the 5090 heavily, but we don\u2019t have data for you today. Maybe in the future. For gaming, anyone who isn\u2019t playing on 4K or higher resolutions won\u2019t get as much relative gain out of this card\n\nGenerally, we saw 27-35% uplift in 4K gaming over the RTX 4090.\n\nIn one instance, we saw 50%\n\nIn one RT instance, we saw 44%\n\nMost of the time, it hung around 30%\n\nAt 1440p, we saw those drop down into the 20s typically, even when not CPU bound\n\nThe card is one of the most power hungry idle power consumers we\u2019ve tested\n\nThe card is the most power hungry for maximum power consumption and we worry that NVIDIA has elected to continue using 12VHPWR while pushing so close to the limit of the cable spec. We have a full video on the dumpster fire of 12VHPWR and 12v2x6 if you want to learn more\n\nEfficiency is about the same as the 4090\n\nThermal design is excellent, with the FE card impressing us in big ways. That NVIDIA managed to make this 2 slots and still in the 70s while not being ridiculously loud is impressive, and they deserve every bit of credit for that\n\nAs far as value, we\u2019ll be able to more easily talk about that next week when we can review the RTX 5080. For now, NVIDIA is launching the 5090 first, which makes it hard for us to know how close the 5080 gets to the 5090. If the 5080 accomplishes most of the performance but reduces VRAM, that\u2019d be important information for us to have before making a value statement on the 5090, so we\u2019ll wait for now on that\n\nWe\u2019ll have a lot more coming up.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "3 things all Nvidia GeForce RTX 5090 reviews are saying about the graphics card",
            "link": "https://mashable.com/article/nvidia-geforce-rtx-5090-reviews",
            "snippet": "The most powerful of Nvidia's newest Blackwell graphics cards is a \"beast.\" Review after review has brought up benchmarks showcasing that when it comes to...",
            "score": 0.92802494764328,
            "sentiment": null,
            "probability": null,
            "content": "It appears that the Nvidia GeForce RTX 5090 is living up to the hype.\n\nIt appears that the Nvidia GeForce RTX 5090 is living up to the hype. Credit: Bridget Bennett/Bloomberg via Getty Images\n\nThe Nvidia GeForce RTX 5090 is finally here. Everyone in the tech space has at least heard something about Nvidia's most powerful graphics card yet. The company stole the show earlier this year at CES , the biggest technology conference in the world, when it announced the GeForce RTX 5090 GPU.\n\nNow, the reviews are coming out from outlets that have had a chance to give the Nvidia GeForce RTX 5090 a spin. While Mashable hasn't tested the GPU out ourselves, our sibling publication PC Mag did! And we poured through reviews from multiple other outlets as well.\n\nMashable found that each outlet had plenty to say about the Nvidia GeForce RTX 5090. To make things easier, here are some findings we've found constant in all the reviews.\n\nRTX 5090 is a 'beast'\n\nGamers who are looking for the best of the best, look no further than the Nvidia GeForce RTX 5090.\n\nThe most powerful of Nvidia's newest Blackwell graphics cards is a \" beast .\" Review after review has brought up benchmarks showcasing that when it comes to gaming, the RTX 5090 blows its predecessors and other graphics cards away.\n\nHowever, it appears that for many consumers, the RTX 5090 is overkill.\n\nMost consumers will just not notice a difference unless they have a really high-end 4K gaming monitor with a 240Hz refresh rate. And even then, the current generation of AAA games on the market aren't utilizing the RTX 5090 to its full potential.\n\nMashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Loading... Sign Me Up By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy Thanks for signing up!\n\n\"The games just aren\u2019t there yet,\" IGN said about users looking to upgrade their gaming PCs.\n\nIf you're looking to future proof your gaming PC though, the slate of next generation games will likely take advantage of at least some of what the Nvidia GeForce RTX 5090 has to offer.\n\nDLSS 4 with multi-frame generation makes a big difference\n\nThe RTX 5090's AI-generated multi-frame feature has all the reviewers talking about it.\n\nBasically, Nvidia has a suite of tools called deep learning super sampling or DLSS which basically uses AI to improve image quality and boost frame rate. This isn't entirely new from Nvidia. However, the latest version in the RTX 5090, DLSS 4 with multi-frame generation, apparently takes it to a whole new level.\n\nSEE ALSO: Everything Nvidia announced at CES 2025\n\nBasically, Nvidia is now able to insert 3 AI-generated frames for every one \"real\" frame in order to increase the FPS on a game and in turn making it much smoother and more realistic looking.\n\nAccording to the reviews, it works extremely well. The one drawback here is, again, a user would need a high-end 4K monitor to really see the difference.\n\nExpect GeForce RTX 5090 to be hard to find\n\nOf course, these are two relevant pieces of every Nvidia GeForce RTX 5090 review.\n\nThe Nvidia GeForce RTX 5090 is expensive at $1,999 for the Founders Edition. That's hundreds of dollars more than its predecessor. In fact, consumers can easily buy an entire gaming computer right now for around half the price of the graphics card alone. Plus, that also doesn't include any markups from third-party retailers and sellers.\n\nAnd that last bit is very relevant because regardless of that nearly $2,000 pricepoint, this thing is going to be hard to find. Between gamers and its AI processing use cases, the Nvidia GeForce RTX 5090 is going to be in high demand. So, if you really want one, get ready to shell out a premium for it.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia\u2019s toughest competition might just be Alphabet \u2014 but there\u2019s a catch",
            "link": "https://www.marketwatch.com/story/nvidias-toughest-competition-might-just-be-alphabet-but-theres-a-catch-a93b2b64",
            "snippet": "Wall Street is always musing about alternatives to Nvidia Corp. One analyst just suggested a big name that's not often in the conversation.",
            "score": 0.8389707803726196,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia RTX 5090: 3 reasons to buy and 2 reasons to skip",
            "link": "https://www.tomsguide.com/computing/gpus/nvidia-rtx-5090-3-reasons-to-buy-and-2-reasons-to-skip",
            "snippet": "The Nvidia GeForce RTX 5090 graphics card is about to land on store shelves with a whopping $1999 price tag. It's a lot of graphics for your gaming PC,...",
            "score": 0.9258373975753784,
            "sentiment": null,
            "probability": null,
            "content": "The Nvidia GeForce RTX 50-series cards have arrived, and the Nvidia GeForce RTX 5090 is the biggest and baddest of the lot.\n\nThese cards were the subject of leaks and rumors for months ahead of their debut at CES 2025, and now that we've been formally introduced it's clear that these cards will continue the company's tradition of launching big, powerful and power-hungry GPUs.\n\nOne other tradition that continues is sky-high pricing on the high-end cards. The top-of-the-line RTX 5090 is slated to retail for $1,999 when it hits store shelves January 30, which is $400 more than the MSRP of the RTX 4090 and the highest launch price of any GeForce GPU to date.\n\nIs it worth the money? That's a question lots of us in the tech press are trying to answer, and luckily here at Tom's Guide we've acquired our own RTX 5090 and have had a chance to play games and run some tests on it. So if you're on the fence about whether to drop $2k on the new Nvidia GeForce RTX 5090, good news: I've got some data and details that will help you make the call.\n\nTo show you what I mean, here a few good reasons to buy and skip the Nvidia GeForce RTX 5090.\n\nReasons to buy the Nvidia RTX 5090\n\n(Image credit: Tom's Guide)\n\nIf you're not sure whether the juice is worth the squeeze, let me walk you through the big reasons why the Nvidia RTX 5090 has so many PC enthusiasts counting their pennies to try and afford one.\n\nThe most power you can get in a GPU\n\nIt's probably obvious, but the number one reason most of us are thinking about spending $2,000 on an Nvidia GeForce RTX 5090 is because it's the most powerful consumer-grade GPU on the market right now.\n\nThat means if you're building a gaming PC, this is the cream of the crop when it comes to graphics cards. We tested Nvidia\u2019s RTX 5090 desktop GPU earlier this month, and the results are clear: this beast chews through the best PC games with ease.\n\nSwipe to scroll horizontally RTX 5090 game benchmarks Game benchmark RTX 5090 (FPS) RTX 4090 (FPS) % difference Cyberpunk 2077 (4K Ray Tracing: Ultra) 57.32 41.11 28.28% Borderlands 3 (4K Ultra) 176.28 130.76 25.82% DiRT 5 (4K Ultra) 227.6 185.8 18.37% Far Cry 6 (4K Ultra) 161 109 32.30% Shadow of the Tomb Raider (4K Ultra) 166 129 22.29%\n\nAs you can see from our results, the Nvidia GeForce RTX 5090 delivers serious performance improvements in all of the games we use for our benchmarking tests. When we compared the results of the Nvidia GeForce RTX 5090 vs Nvidia GeForce RTX 4090 in those games, we saw average frames per second (FPS) jump 20-30% in basically every test.\n\nSo if you value raw performance in games, the Nvidia GeForce RTX 5090 is an expensive way to guarantee you're on the cutting edge of PC gaming for at least a year or two. And honestly, you could probably use this GPU for the next decade without ever having to worry about upgrading.\n\nWhere to buy RTX 5090 (U.S.)\n\nDLSS 4 and Multi-Frame Generation\n\nNvidia's Deep Learning Super Sampling (DLSS) tech helps your PC run games that support it (most of the best Steam games do) better by rendering frames at a lower resolution, then using machine learning technology to upscale it to your target resolution.\n\nThis effectively allows you to play games at 1440p or 4K with demands on your PC that are closer to if you were playing the game at 1080p, so you usually get better framerates without losing much in terms of graphical fidelity.I know because I've been using DLSS 3 for years now to better enjoy games like Cyberpunk 2077 and Star Wars Outlaws, and it makes a noticeable difference. Now Nvidia is launching DLSS 4, and while some of its features will be supported on older Nvidia GPUS, it will also have features that are only available on the Nvidia GeForce RTX 50-series cards.\n\nThe upshot is that your GPU is effectively using AI to generate more frames per second than the game is actually giving you, leading to a higher framerate.\n\nMost notably you get a significant upgrade with DLSS 4 to multi-frame generation, which improves upon DLSS 3's existing frame generation feature. It's pretty technical, but the simple explanation is that if you turn frame generation on in games that support it your Nvidia graphics card will try to give you more FPS by intelligently looking at two frames while playing and generating a third that can go between them, then inserting it seamlessly while you play.\n\nThe upshot is that your GPU is effectively using AI to generate more frames per second than the game is actually giving you, leading to a higher framerate. And with DLSS 4's multi-frame generation feature it will be capable of generating even more frames in supported games \u2014 you can see it in action in Nvidia's sizzle reel above.\n\nAccording to Nvidia, \"DLSS Multi Frame Generation generates up to three additional frames per traditionally rendered frame\", which could mean that games which support it will run even faster on your gaming PC. However, one caveat with frame generation: it can sometimes make games feel a little more sluggish or slow to respond, especially fast-paced action games, because you can experience more input lag due to the artificially-generated frames. I've never personally noticed the feeling, but I'm not particularly sensitive to input lag or stutter.\n\nLocal AI powerhouse\n\nAI has been the buzzword du jour in the tech industry for a few years now, and in the laptop and PC space there's been a lot of hay made about the value of being able to run AI applications locally on your PC.\n\nThat's why we've seen Microsoft put oodles of money and effort into launching its new tier of Copilot+ PCs, which have unique features that are only available if your PC has an NPU (Neural Processing Unit) capable of 40+ TOPS (trillion operations per second). There's a lot of marketing being done to sell you on these PCs that ties into the ability to run AI apps like ChatGPT locally, and while you can run your own AI chatbot locally on Windows (or Mac), the best AI powerhouse isn't your NPU\u2014it's your graphics card.\n\nThe Nvidia GeForce RTX 4090 was a powerhouse for running AI locally, so the RTX 5090 should be even more monstrous when it comes to generating images and video. And Nvidia is going even harder on AI in 2025 and beyond, so I expect the company will continue to improve its existing local AI features like Chat with RTX, which lets you train a GPT large language model (LLM) locally on your own files and data, so you can ask it questions about your files and PC without worrying about it needing to connect to the Internet.\n\nReasons to skip the Nvidia RTX 5090\n\nRTX 50 series GPU at CES 2025. (Image credit: Future)\n\nEven though it's clearly the new top dog in the GPU market, there are still good reasons to skip the Nvidia GeForce RTX 5090 for the moment. Here are two very good ones I think you should consider before pulling the trigger on a purchase.\n\nSticker shock\n\nNvidia has priced the GeForce RTX 5090 at a cool $1,999, which is enough to buy you one of the best gaming PCs on the market and still have money left over for accessories, software and lunch.\n\nNow, I'm not going to say the company is doing this just to make money on performance-obsessed PC gamers, but you should know that the 5090 is $400 more at launch than the GeForce RTX 4090, which was priced around $1,599. And frankly, I think you could buy yourself an RTX 4090 (or even an RTX 4080) tomorrow, put it in your PC and still have a great gaming rig that would run all the best Steam games at 1080p/60FPS or better for years to come.\n\nSo as your finger hovers over the buy button on that 5090, take a second to think about whether you really need that feeling of owning the best of the best. Is it worth the extra $400-$1,000 you're spending over an older 40-series card, or even one of the less powerful 50-series GPUs? The RTX 5080 is a thousand bucks cheaper, for example, while the Nvidia RTX 5070 actually feels semi-affordable at $549.\n\nSwipe to scroll horizontally Nvidia GeForce RTX 50-series desktop GPUs GPU name Starting price RTX 5090 $1,999 RTX 5080 $999 RTX 5070 Ti $749 RTX 5070 $549\n\nAt CES 2025 Nvidia chief Jensen Huang famously claimed that an RTX 5070 would deliver the same performance as an RTX 4090, and while there are major caveats to that claim (chief among them being that multi-frame generation is required to achieve that performance), it should give you some comfort that you don't need to spend two grand to get an Nvidia 50-series card that delivers excellent gaming performance.\n\nThis beast is power-hungry\n\nA few years ago the Nvidia GeForce RTX 4090 was criticized for being a massive card that ate up power, to the point that it had a high (for the time) 450 Watt power draw rating.\n\nWorse, some users reported seeing power spike higher than the 450W during extended or heavy usage, which could cause problems if the PC power supply wasn't big enough to handle the drain. This led Nvidia to recommend that 4090 owners use at least a 850W power supply in their PC.\n\nUnsurprisingly, the RTX 5090 is even hungrier for power. Nvidia has rated it for 575W, and recommends you have a PSU (power supply unit) capable of kicking out at least 1000W in your PC if you want to use the 5090 safely.\n\nIf you haven't built a PC in a few years, trust me: that's a lotta watts. Most of us don't have that kind of power supply in our PC, so chances are you'll have to build a whole new rig to support your new RTX 5090.\n\nOutlook\n\nMSI Nvidia GeForce RTX 50-series cards at CES 2025. (Image credit: Future)\n\nIf you've made it this far, you should have a pretty good sense of the ups and downs of investing $2,000 in Nvidia's new top-of-the-line GPU.\n\nFrankly, it's too rich for my blood. Back in the day I was excited to see what the latest and greatest PC components could do for my rig, and I loved booting up Crysis to see how well my new GPU made it look.\n\nBut those days are behind me, and I can't pretend I think it's a good idea to spend a month's rent (Bay Area, bay-bee) on a new graphics card, or more if you need to build a whole new PC to handle it. And frankly, after seeing such great results gaming on an Nvidia GeForce RTX 4070 Super, which had no trouble running 2024's latest PC games at 4K/60 FPS with DLSS enabled, I don't see any reason to pay through the nose for a few dozen more frames.\n\nNvidia knows that, and it's not targeting me with these cards. The company is going hard on marketing the 5090 to AI enthusiasts and hardcore PC gamers with thick wallets, letting the rest of us content ourselves with looking forward to a 5070 upgrade some day.\n\nAnd honestly, I'm psyched about it. Because as Nvidia pushes the cutting edge of its consumer-grade GPUs forward, I can hang back a generation or so and try to pick up a nice discounted 40-series card later this year.\n\nI expect those cards will still be viable for years to come, so don't feel pressured to race out and buy a new 50-series RTX GPU as soon as they hit store shelves later this month \u2014 as our testing shows, the 5090 is an incredible graphics card, but there are lots of other options out there that are way easier on your wallet.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia starts phasing out Maxwell, Pascal, and Volta GPUs \u2014 GeForce driver support status unclear",
            "link": "https://www.tomshardware.com/pc-components/gpu-drivers/nvidia-starts-phasing-out-maxwell-pascal-and-volta-gpus-geforce-driver-support-status-unclear",
            "snippet": "Nvidia has revealed Maxwell, Pascal, and Volta architectures are considered feature-complete and will be frozen in an upcoming CUDA release.",
            "score": 0.9284436106681824,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's release notes for CUDA 12.8 revealed that Maxwell, Pascal, and Volta GPUs will likely transition to the legacy driver branch. The document states that \"architecture support for Maxwell, Pascal, and Volta is considered feature-complete and will be frozen in an upcoming release.\"\n\nThis move represents the beginning of the end for all remaining GTX-era Nvidia architectures. While CUDA support for Maxwell, Pascal, and Volta remains, the legacy GPUs will not receive any new features Nvidia might provide in the future. It's crucial to highlight that this has nothing to do with GeForce gaming driver support. In fact, Maxwell and Pascal continue to be on the support list for the GeForce RTX series driver, unlike Kepler. Nvidia didn't detail whether or when it'll drop support for Maxwell, Pascal, and Volta GPUs for the gaming driver.\n\nNvidia has not issued an exact date for the end of full support for these three GPU architectures, but it will soon. The current CUDA toolkit still supports the three affected architectures, but they won't receive future updates. Once the move goes through, the only remaining GTX-series GPUs with full support will be the GTX 16-series, based on the RTX 20-series' Turing architecture.\n\nThe Maxwell architecture is the oldest outgoing GPU architecture that is still supported by Nvidia (at least on the consumer side). It brought massive performance-per-watt improvements over Kepler, its predecessor. Maxwell was first unleashed in the GTX 700 series as the GTX 750 Ti and GTX 750. Both 750-series GPUs went down as some of the most power-efficient entry-level GPUs for their time, featuring the performance of competitor GPUs with triple-digit power consumption numbers at a TDP of just 60 watts.\n\nMaxwell was the first Nvidia architecture designed around mobile GPUs, using TSMC's existing 28nm process (at the time) but an all-new architecture that made significantly more efficient use of that node. The full-blown iteration of Maxwell in the GTX 900 series carried forward Maxwell's incredible efficency, with the GTX 980 and 970 featuring the best power efficency on the market for the time.\n\nPascal would set the stage as one of Nvidia's most significant architectural advancements in the 2010s, providing further power efficiency improvements combined with giant performance leaps. Pascal was Nvidia's first architecture to use TSMC's 16nm finFET plus technology, providing twice the density of Maxwell's 28nm node. As a result, The GTX 1080 was, on average, 60-65% faster than the GTX 980 and 30-35% faster than the GTX 980 Ti. The GTX 1080 Ti would go on to show the full potential of the Pascal architecture, featuring 60% higher performance than the GTX 980 Ti at just $700.\n\nVolta was the foundation for all of Nvidia's architecture moving forward. It was the first architecture geared primarily toward AI. Volta was also the first Nvidia architecture to sport AI-specific Tensor cores, which provided substantially higher compute capabilities than Nvidia's shader/CUDA cores for AI-specific workloads. These first-gen Tensor Cores only provided 120 Tensor TFLOPs of performance but were nine times faster than Pascal in the same type of workload.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nVolta was focused almost exclusively on the enterprise world and was never put inside any GeForce-branded GPUs. The flagship GV100 was more than 30% larger than the previous gen GP100 (Pascal-based) GPU, operating on TSMC's 12nm FFN process. The only Volta GPU Nvidia made for desktop PCs was the Titan V (yes, you could even game on it if you wanted to).",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Stock Drops as Meta Unveils Bold AI Ambitions",
            "link": "https://finance.yahoo.com/news/nvidia-stock-drops-meta-unveils-170648244.html",
            "snippet": "Nvidia Corp. (NVDA, Financials) shares fell 1.78% on Friday, closing at $144.60 as of 11:55 am GMT-5. The fall followed a statement by Meta Platforms (META,...",
            "score": 0.836745023727417,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia Corp. (NVDA, Financials) shares fell 1.78% on Friday, closing at $144.60 as of 11:55 am GMT-5.\n\nThe fall followed a statement by Meta Platforms (META, Financials) CEO Mark Zuckerberg on the major AI investment plan of the business.\n\nZuckerberg said in a public post 2025 as a \"defining year for AI\" and underlined Meta's intentions to create sophisticated AI model Llama 4 and assign an AI engineer to support its initiatives on research and development. With 1GW of computational power slated to come online in 2025, the announcement also included Meta's commitment to building a 2GW+ data center. By the end of that year, the facility will host over 1.3 million GPUs.\n\nEmphasizing its emphasis on artificial intelligence and infrastructure development, Meta aims to spend $60 to 65 billion in capital expenditures in 2025. According to Zuckerberg, the money is meant to propel Meta's main offerings as well as expand American technological supremacy.\n\nKey GPU provider Nvidia has been crucial in the development of artificial intelligence infrastructure all around. Meta's disclosure emphasizes even more the growing need for GPUs as businesses raise their capacity for artificial intelligence.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "AI Maps Titan\u2019s Methane Clouds in Record Time",
            "link": "https://blogs.nvidia.com/blog/ai-maps-titan-clouds/",
            "snippet": "Using NVIDIA GPUs, the researchers trained a deep learning model to analyze years of Cassini data in seconds.",
            "score": 0.852532684803009,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA GPUs powered deep learning to decode years of Cassini data in seconds\u2014helping researchers pioneer a smarter way to explore alien worlds.\n\nMethane clouds on Titan, Saturn\u2019s largest moon, are more than just a celestial oddity \u2014 they\u2019re a window into one of the solar system\u2019s most complex climates.\n\nUntil now, mapping them has been slow and grueling work. Enter AI: a team from NASA, UC Berkeley and France\u2019s Observatoire des Sciences de l\u2019Univers just changed the game.\n\nUsing NVIDIA GPUs, the researchers trained a deep learning model to analyze years of Cassini data in seconds. Their approach could reshape planetary science, turning what took days into moments.\n\n\u201cWe were able to use AI to greatly speed up the work of scientists, increasing productivity and enabling questions to be answered that would otherwise be impractical,\u201d said Zach Yahn, Georgia Tech PhD student and lead author of the study.\n\nRead the full paper, \u201cRapid Automated Mapping of Clouds on Titan With Instance Segmentation.\u201d\n\nHow It Works\n\nAt the project\u2019s core is Mask R-CNN \u2014 a deep learning model that doesn\u2019t just detect objects. It outlines them pixel by pixel. Trained on hand-labeled images of Titan, it mapped the moon\u2019s elusive clouds: patchy, streaky and barely visible through a smoggy atmosphere.\n\nThe team used transfer learning, starting with a model trained on COCO (a dataset of everyday images), and fine-tuned it for Titan\u2019s unique challenges. This saved time and demonstrated how \u201cplanetary scientists, who may not always have access to the vast computing resources necessary to train large models from scratch, can still use technologies like transfer learning to apply AI to their data and projects,\u201d Yahn explained.\n\nThe model\u2019s potential goes far beyond Titan. \u201cMany other Solar System worlds have cloud formations of interest to planetary science researchers, including Mars and Venus. Similar technology might also be applied to volcanic flows on Io, plumes on Enceladus, linea on Europa and craters on solid planets and moons,\u201d he added.\n\nFast Science, Powered by NVIDIA\n\nNVIDIA GPUs made this speed possible, processing high-resolution images and generating cloud masks with minimal latency \u2014 work that traditional hardware would struggle to handle.\n\nNVIDIA GPUs have become a mainstay for space scientists. They\u2019ve helped analyze Webb Telescope data, model Mars landings and scan for extraterrestrial signals. Now, they\u2019re helping researchers decode Titan.\n\nWhat\u2019s Next\n\nThis AI leap is just the start. Missions like NASA\u2019s Europa Clipper and Dragonfly will flood researchers with data. AI can help handle it, processing it onboard, mid-mission, and even prioritizing findings in real time. Challenges remain, like creating hardware fit for space\u2019s harsh conditions, but the potential is undeniable.\n\nMethane clouds on Titan hold mysteries. Researchers are now unraveling them faster than ever with help from new AI tools accelerated by NVIDIA GPUs.\n\nRead the full paper, \u201cRapid Automated Mapping of Clouds on Titan With Instance Segmentation.\u201d\n\nImage Credit: NASA Jet Propulsion Laboratory",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Should You Forget Nvidia and Buy This Tech Stock Instead?",
            "link": "https://www.fool.com/investing/2025/01/24/should-you-forget-nvidia-and-buy-this-tech-stock/",
            "snippet": "Looking for the next big thing in AI and tech stocks? Check out why one supporting player might be a better pick than the usual household name.",
            "score": 0.905400812625885,
            "sentiment": null,
            "probability": null,
            "content": "Looking for the next big thing in AI and tech stocks? Check out why one supporting player might be a better pick than the usual household name.\n\nChip designer Nvidia (NVDA 5.27%) is a hot topic. Its 147% total return in 52 weeks is among the 10 best performances on the S&P 500 (^GSPC 2.13%) index. The company benefits greatly from the artificial intelligence (AI) boom that started in late 2022. Leading AI experts rely on Nvidia's chips to train their systems.\n\nAs a result, Nvidia sports a $3.6 trillion market value. It is the largest company in the world by some measures, and many investment pros still think it can keep climbing.\n\nThe examples are both numerous and illustrious. A Motley Fool research report from the spring of 2024 found Nvidia in the top 10 holdings of many billionaires' hedge funds. Philippe Laffont's Coatue Management and Ken Griffin's Citadel fund held more Nvidia stock than anything else. Six months later, Ray Dalio's Bridgewater Associates and David Tepper's Appaloosa Management were buying more Nvidia stock.\n\nShould you buy Nvidia stock, like most billionaire fund managers?\n\nSo Nvidia has its fans. You might want to follow their example and buy Nvidia stock in January 2025. After all, the generative AI craze is only getting started, and the big money will probably be made in the future. As the early king of the AI hardware hill, Nvidia is the company to beat.\n\nThen again, the pros don't nail every call. Hitting the ball on every third swing easily qualifies you for the baseball Hall of Fame. In investing, the percentages may be higher but every genius strikes out fairly often.\n\nAnd I'm not so sure about Nvidia being a buy right now. Overly enthusiastic investors seem to be counting on many years of perfect business results, baking unrealistic assumptions into the lofty stock price. Then there's the law of large numbers, which explains that it's more difficult to generate strong returns from a very large market cap.\n\nIf you want to invest in the generative AI market today, I would suggest leaving Nvidia alone. Another semiconductor company is tapping into the same resource, and its stock looks downright cheap next to Nvidia's valuation.\n\nMicron Technology: An under-the-radar AI champion\n\nI'm talking about memory-chip expert Micron Technology (MU 6.23%). The company is actually a prominent Nvidia partner, providing large amounts of high-speed memory for its AI accelerator cards. In fact, Nvidia CEO Jensen Huang recently named Micron as a key supplier of important components. That statement was enough to drive Micron's stock more than 6% higher in a single day.\n\nBut that's not an exclusive deal. The company serves a wide variety of customers, from consumers to massive enterprises, around the world. One Micron customer stood for more than 10% of the company's revenues in fiscal year 2024. Management didn't name it in business filings, but this giant client ordered memory chips in the mobile, embedded, and computer systems markets. These signs won't quite work for Nvidia, which doesn't do a lot of business in the mobile segment. Instead, they point to iPhone maker Apple (AAPL 1.82%) or Android owner Alphabet (GOOG 1.75%) (GOOGL 1.68%).\n\nIn other words, Micron runs a diversified business where no single customer is of game-changing importance to the company's results. At the same time, Micron records a significant sale when Nvidia finds a buyer for its high-end AI products. Why pick a winner in the AI hardware wars when you can buy stock in Micron, which partners up with pretty much everybody?\n\nWhat's not to love about Micron and its stock?\n\nI haven't even talked about Micron's modest valuation yet. The stock is changing hands at 4.2 times trailing earnings and 9.8 times next year's estimated earnings. The company recently came back from a couple of years with negative bottom-line results, due to the cyclical nature of the computer memory market and a deep downturn ultimately springing from the COVID-19 pandemic.\n\nMicron has emerged from that crisis with a hard-to-beat portfolio of innovative memory products. It is the only memory chip maker of note that isn't based in South Korea, and the company's deeply American business model should serve it well in an era of intense international trade conflicts.\n\nThis stock holds many advantages over Nvidia's right now. It is far less expensive. It could soar on positive trends in non-AI markets such as smartphones or chip-laden modern cars. And the memory chip industry, in which Micron plays a leading role, is due for a cyclical upswing.\n\nFour of the 16 billionaire-led funds in that research report also held plenty of Micron stock. Only time will tell which fund group made the right move, but I have high hopes for the Micron team.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-23": {
        "0": {
            "title": "\u2018Baldur\u2019s Gate 3\u2019 Mod Support Launches in the Cloud",
            "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-baldurs-gate-3-mods/",
            "snippet": "GeForce NOW is expanding mod support for hit game Baldur's Gate 3 in collaboration with Larian Studios and mod.io for Ultimate and Performance members.",
            "score": 0.593144953250885,
            "sentiment": null,
            "probability": null,
            "content": "Plus, stream seven new games on GeForce NOW.\n\nGeForce NOW is expanding mod support for hit game Baldur\u2019s Gate 3 in collaboration with Larian Studios and mod.io for Ultimate and Performance members.\n\nThis expanded mod support arrives alongside seven new games joining the cloud this week.\n\nLevel Up Gaming\n\nTime to roll for initiative \u2014 adventurers in the Forgotten Realms can now enjoy a range of curated mods uploaded to mod.io for Baldur\u2019s Gate 3. Ultimate and Performance members can enhance their Baldur\u2019s Gate 3 journeys across realms and devices with a wide array of customization options. Stay tuned to GFN Thursday for more information on expanding mod support for more of the game\u2019s PC mods at a later time.\n\nDownloading mods is easy \u2014 choose the desired mods from the Baldur\u2019s Gate 3 in-game mod menu, and they\u2019ll stay enabled across sessions. Or subscribe to the mods via mod.io to load them automatically when launching the game from GeForce NOW. Read more details in the knowledge article.\n\nLearn more about how curated mods are made available for Baldur\u2019s Gate 3 players and read the curation guidelines.\n\nGeForce NOW members can bring their unique adventures across devices, including NVIDIA SHIELD TVs, underpowered laptops, Macs, Chromebooks and handheld devices like the Steam Deck. Whether battling mind flayers in the living room or crafting spells on the go, GeForce NOW delivers experiences that are seamless, immersive and portable as a Bag of Holding.\n\nNOW Playing\n\nJ\u00f6tunnslayer: Hordes of Hel is a gripping roguelike horde-survivor game set in the dark realms of Norse Mythology. Fight waves of enemies to earn divine blessings of ancient Viking Deities, explore hostile worlds and face powerful bosses. Become a god-like warrior in this ultimate showdown.\n\nJ\u00f6tunnslayer: Hordes of Hel (New release on Jan 21, Steam)\n\n(New release on Jan 21, Steam) Among Us (Xbox, available on PC Game Pass)\n\n(Xbox, available on PC Game Pass) Amnesia: Collection (Xbox, available on the Microsoft Store)\n\n(Xbox, available on the Microsoft Store) Lawn Mowing Simulator (Xbox, available on the Microsoft Store)\n\n(Xbox, available on the Microsoft Store) Sins of a Solar Empire: Rebellion (Xbox, available on the Microsoft Store)\n\n(Xbox, available on the Microsoft Store) STORY OF SEASONS: Friends of Mineral Town (Xbox, available on the Microsoft Store)\n\n(Xbox, available on the Microsoft Store) Townscaper (Xbox, available on the Microsoft Store)\n\nWhat are you planning to play this weekend? Let us know on X or in the comments below.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "NVIDIA GeForce RTX 5090 Founders Edition Review - The New Flagship",
            "link": "https://hardforum.com/threads/nvidia-geforce-rtx-5090-founders-edition-review-the-new-flagship.2039332/",
            "snippet": "The consensus is that it's typically 25-35% faster than the 4090, Nvidia padded their presentation with shaky comparisons, it's power hungry, and it's...",
            "score": 0.8815004825592041,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "NVIDIA GeForce RTX 5090 Founders Edition Review: Still King Of PC Gaming",
            "link": "https://hothardware.com/reviews/nvidia-geforce-rtx-5090-review",
            "snippet": "NVIDIA's latest flagship GPU, the powerful GeForce RTX 5090, has arrived and we've tested it thoroughly with AI, gaming, and creator workloads to.",
            "score": 0.8406173586845398,
            "sentiment": null,
            "probability": null,
            "content": "\n\nNVIDIA GeForce RTX 5090: MSRP $1,999\n\nNVIDIA's powerful but pricey GeForce RTX 5090 is the company\u2019s latest flagship GPU and our tests show it crushing everything in AI, gaming, and content creator workloads.\n\n\n\n\n\nFastest GPU Ever\n\nBleeding Edge Features\n\nDLSS 4\n\nRTX Nerual Rendering\n\nGreat For AI, Creators, And Gaming\n\nSmaller Form Factor Than Previous-Gen\n\n32B Of VRAM\n\nSteep Price Premium\n\nHigh Power Consumption\n\nNot A Massive Leap Over 4090\n\n\n\n\n\nNVIDIA GeForce RTX 5090 Specifications\n\n\n\n\n\nThe NVIDIA GeForce RTX 5090 Founders Edition\n\nIf you watched our unboxing video above, you\u2019ve already seen the new, more environmentally friendly packaging NVIDIA is using with the GeForce RTX 50 series. The new packaging is not only much smaller than the previous-gen, but the inner box is 100% plastic free, inkless and made of paper fibers. The carboard outer box also has a few cool design elements that hint at what\u2019s inside. Watch the vid if you haven\u2019t already.\n\n\n\n\n\nSince it was announced at CES a few weeks back, the GeForce RTX 50 series, the Blackwell GPU architecture, and the flagship GeForce RTX 5090 in particular have been the topics of much discussion. Over two years out from the launch of the previous-gen GeForce RTX 4090, enthusiasts, gamers and creators were itching to see what NVIDIA had in store. As such, company CEO Jensen Huang revealed entire desktop and mobile GPU line-ups, including the GeForce RTX 5070 all the way on up the top-end GeForce RTX 5090 we\u2019ll be showing you here.Before we continue though, we\u2019ve provided a ton of Blackwell and GeForce RTX 50 series coverage, that are must-reads to better understand what the GeForce RTX 5090 is all about. At the very least, please check out our Blackwell GPU architecture piece , which details the many advancements coming with the architecture, including RTX Neural Rendering, RTX Mega Geometry, DLSS 4 with Multi-Frame Gen, the new AI Management Processor, and the RTX 50 series\u2019 updated media engine. Details on the rest of the line-up are also available here . We not going to re-hash many of the deeper details available in those previous articles here, so please give them a read if you want the full scoop before proceeding...The GeForce RTX 5090 is built around the GB202 GPU, which is manufactured on the same TSMC 4nm process as the previous gen. The GB202 features 92 billion transistors, and as configured on the RTX 5090, it has 21,760 CUDA Cores, 170 RT Cores, and 680 Tensor cores. The GB202 is based on the NVIDIA Blackwell architecture and is outfitted with new 5th gen Tensor Cores, 4th gen Ray Tracing Cores, and updated CUDA cores.Connected to the GPU via a wide 512-bit interface is 32GB of GDDR7 memory offering up to 1.8 TB/s of total memory bandwidth, which is a 78% increase over the previous gen. The GB202 also features a native PCIe Gen5 interface, and its updated display engine supports DisplayPort 2.1b with UHBR20 and HDMI 2.1b.In total, a full GB202 GPU includes 12 Graphics Processing Clusters (GPCs), 96 Texture Processing Clusters (TPCs), 192 Streaming Multiprocessors (SMs) each with 128 CUDA Cores, and a 512-bit memory interface with sixteen 32-bit memory controllers. As such, a full GB202 GPU includes 24,576 CUDA Cores, 192 RT Cores, 768 Tensor Cores, and 768 Texture Units. A full GB202 GPU also features 128 MB of total L2 cache, along with a 256 KB Register File, and 128 KB of L1/Shared cache.The GB202 is not fully enabled on the GeForce RTX 5090, however. While the full GB202 GPU has a total of 12 GPCs, 96 TPCs, and 192 SMs, \u201conly\u201d 11 GPCs, 85 TPCs, and 170 SMs are enabled on the RTX 5090. As such, although the GB202 is outfitted with 128 MB of total L2 cache, the RTX 5090 has 96 MB of L2 enabled.The GB202 may not be fully enabled on the GeForce RTX 5090, but it\u2019s still a step forward over the AD102 powering the RTX 4090 . The number of texture units has increased from 512 on the RTX 4090 to 680 on the RTX 5090, which results in higher bilinear-filtered texel fill rates. The RTX 5090 delivers 1636.76 Gigatexels/sec, compared to 1290.2 Gigatexels per second on RTX 4090, though the RTX 5090\u2019s pixel fill rate is actually a bit lower than the 4090\u2019s.What\u2019s inside that box is a totally redesigned GPU, which features a number of innovative firsts. Although it is much more powerful than its predecessor, the GeForce RTX 5090 is much thinner. The RTX 5090 measures 304mm in length, 137mm in height, and it is truly two-slots (40mm) wide \u2013 it\u2019s not just a 2-slot wide case bracket. It's a dense package for sure, and feels premium through-and-through.NVIDIA was able to shrink the GeForce RTX 5090 by developing what it calls a \u201cDouble Flow Through\u201d cooler design, which allows both cooling fans to blow air straight through the heatsink, for optimal performance.To achieve this, the GeForce RTX 5090 has a multi-part PCB setup. The central PCB houses the GPU, memory, and power circuitry. It rests in the center of the card, with only small portions protruding underneath each fan. A separate daughterboard attaches to the central PCB for the PCIe x16 connector, and a third, flexible board runs perpendicular along the bottom with high-speed signaling connections from the central PCB to the display outputs.The amount of engineering that went into the GeForce RTX 5090\u2019s design is nothing short of impressive. Not only is the cooler far more capable, but NVIDIA had to solve a number of problems to ensure proper signaling over the adjacent PCBs and their connectors. NVIDIA discusses much of the design process in a video available here if you\u2019re interested in some of the nitty-gritty details.At the top of the card, you'll find the familiar 12VHPWR 16-pin connector, also used on the RTX 40 series. On the RTX 5090 though, the connector is angled off the back on the PCB, and recessed slightly in the shroud. This configuration should allow for easier cable management and minimize the need to bend the power feed in shallower PC cases. The included power adapter requires four PCIe 8-pin feeds, like the RTX 4090 did, but the cabling is longer and far more flexible. The connector on the adapter is also beefier and emits a sold 'click' when pushed fully into place. Hopefully, these tweaks and updates to the power connector and adapter help prevent some of the issues that affected the RTX 4090.Sitting atop the GPU, memory and power circuitry is a new, unified 3D Vapor Chamber that allows vapor to travel directly to heat pipes to increasing thermal performance, which is connected to a combination of straight and curved fins in the heatsink assembly. designed to reduce back pressure and optimizes airflow through the fins. Directional outlets on the side direct heat up-and-out to combat recirculation of warm air as well.To ensure optimal heat transfer from the GPU to the vapor chamber, NVIDIA is also using a very high performance liquid metal thermal interface material, similar to what's in the PS5. Liquid metal TIM has much lower thermal resistance than even today's best thermal pastes, which allows more heat to transfer more quickly into the vapor chamber, ultimately lowering GPU temperatures, but there is a catch. Liquid metal is electrically conductive, and if not secured in place, it can run like any liquid and potentially cause shorts. NVIDIA sealed the GPU to sure the liquid metal stays in place though, with the card installed in any orientation. Just be aware that \"re-pasting\" the RTX 5090 down the road will require extra care to prevent potential problems.Outputs in the GeForce RTX 5090 include a trio of DisplayPorts (2.1b) and a single HDMI port (2.1b). Their orientations have been reversed versus previous-gen cards though, and the new case bracket -- which doesn't require venting due to the dual flow-though cooler design -- also features an anti-fingerprint coating.And with all of that out of the way, let's get to some benchmarks...",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia GeForce RTX 5090 review: the ultimate DLSS 4 billboard",
            "link": "https://www.rockpapershotgun.com/nvidia-geforce-rtx-5090-review",
            "snippet": "Nvidia's newest top-spec GPU, the RTX 5090, gets a thorough benchmarking - including its DLSS 4 features, like Multi Frame Generation.",
            "score": 0.9164840579032898,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia GeForce RTX 5090 Founders Edition specs: CUDA Cores: 21,760\n\n21,760 Base Clock Speed: 2.01GHz\n\n2.01GHz Boost Clock Speed: 2.14GHz\n\n2.14GHz VRAM: 32GB GDDR7\n\n32GB GDDR7 Power: 575W\n\n575W Recommended System Power: 1000W\n\n1000W Price: \u00a31939 / $1999\n\nReaders, I have spent two full days in the benchmark pits to tell you what you've already guessed: the GeForce RTX 5090 is very fast, too expensive, and laden with more AI tech than Philip K. Dick\u2019s cheese dreams. At least two of those points will, I\u2019m sure, send the average graphics card shopper running, especially at a time when even game developers are growing suspicious of generative AI and its many-thumbed, robot-voiced nostrums.\n\nYet while there\u2019s not much to be done about the RTX 5090 costing at minimum \u00a31939 / $1999, hundreds more than the infamously spenny RTX 4090, its suite of more purely performance-focused artificial intelligence tools is \u2013 dare I say it \u2013 quite neat. These range from Multi Frame Generation (MFG), which is basically DLSS 3 frame gen but up to twice as fast, to DLSS 4\u2019s general upscaling enhancements and even the ability to apply newer DLSS versions to older games. All this will come to the rest of the RTX 50 series as well, with some trickling down to the enter RTX range, so maybe the RTX 5090 is best understood not as a practical GPU purchase in itself but as a Picadilly Square-filling advert for its more affordable siblings can do.\n\nThere\u2019s definitely a sense of Nvidia going \"Sod it\" with this one, cutting the sensibility brakes to produce something that even hyperenthusiasts might find a bit too mad. Besides the price, this is by far the biggest PSU-muncher to ever wear a GeForce badge: the Founders Edition I tested demands a thundering 1000W power supply and is rated to hoover up to 575W at once, a threat that it made good on by pulling 578W in Cyberpunk 2077. In comparison, the 450W-rated RTX 4090 looks like it can be powered by potatoes. Credit to the redesigned dual-fan cooler for keeping a lid on temperatures, which I nonetheless measured peaking at a very manageable 74\u00b0c.\n\nImage credit: Rock Paper Shotgun\n\nOn that note, if there\u2019s something the RTX 5090 does brilliantly without any AI caveats, it\u2019s the industrial design. This might actually be \u2013 granted that it\u2019s not a high bar \u2013 the best-looking graphics card I\u2019ve seen in my life, a modern yet mature slice of precision-cut aluminium. Also, unlike the RTX 4090 FE, it isn\u2019t so big that it could feasibly have come from the age when computers needed their own cupboards. It\u2019s a solid 2cm thinner, to be exact, finally arresting and reversing the trend of ever-fattening premium GPUs.\n\nNvidia GeForce RTX 5090 review: 4K benchmarks\n\nUltra HD is the RTX 5090\u2019s spiritual home, just as with the RTX 4090 and RTX 3090 before it. Since I had all three to hand, I introduced them to the new test rig I'd piloted with the Intel Arc B580, which combines 16GB of DDR5 RAM with an Intel Core i9-13900K.\n\nIn terms of unaltered, native-rez games performance, let alone any frame gen gubbins, the RTX 5090 is inarguably the new quickest card of them all. It\u2019s the first ever to smash through 100fps in Cyberpunk 2077 with these settings, and provided you\u2019ve got at least a 144Hz monitor (comparatively not a big ask, if you can afford \u00a32K\u2019s worth of graphics card) then you could see a visible improvement over the RTX 4090 in other tough games like Total War: Warhammer III and F1 24.\n\nImage credit: Rock Paper Shotgun\n\nNevertheless, it\u2019s hard to ignore how the jump from the RTX 4090 to the RTX 5090 isn\u2019t nearly as big or as consistent as the former\u2019s improvement over the RTX 3090. Nvidia\u2019s top two GPUs are almost level in Horizon Forbidden West and Assassin\u2019s Creed Mirage, and the differences remain minor once you start adding upscaling. Quality-level DLSS in Forbidden West, for example, got it up to 137fps on the RTX 5090 and 125fps on the RTX 4090: hardly a grand leap forward.\n\nThe RPS test PC: CPU: Intel Core i9-13900K\n\nIntel Core i9-13900K RAM: 16GB Crucial DDR5 Pro\n\n16GB Crucial DDR5 Pro Motherboard: Asus ROG Maximus Z790 Dark Hero\n\nAsus ROG Maximus Z790 Dark Hero PSU: NZXT C1000 Gold\n\nOn the other hand, the RTX 5090\u2019s Blackwell architecture does appear a little better at handling the performance hit from ray tracing \u2013 and this was already one of Nvidia\u2019s strong suits. After enabling Ultra-quality RT effects, Metro Exodus dropped from 163fps to 117fps on the RTX 5090 and from 135fps to 87fps on the RTX 4090: that makes for reductions of 28% and 36% respectively.\n\nThen there\u2019s DLSS 4, and its most numbermaking component, Multi Frame Generation. This works similarly to the frame generation that DLSS 3 introduced for the 40 series: the card takes data from the frames that the GPU is rendering and uses AI to generate lookalike frames that can be slipped in-between the rendered ones, boosting overall framerates in exchange for a touch of added input lag. However, whereas the DLSS 3 version creates up to one AI frame for every rendered frame, MSG can generate three or four, resulting in scenes like this:\n\nImage credit: Rock Paper Shotgun\n\nThat is indeed the RTX 5090 running fully pathed-traced Cyberpunk 2077 and Alan Wake II, at 4K, at \u2013 as far as your eyes are concerned \u2013 200fps. And crucially, 4x MFG doesn\u2019t significantly increase latency over what classic 2x frame generation already did, nor does it look any worse in terms of image quality.\n\nThat\u2019s not to say it\u2019s immaculate sorcery. Input lag is input lag, and while MFG doesn\u2019t make this tradeoff noticeably worse, it also doesn\u2019t make it any better. Cyberpunk 2077 could also get quite blurry when making rapid camera movements \u2013 an issue I didn\u2019t really have with AW2 and Dragon Age: The Veilguard, to be fair \u2013 and it\u2019s important to remember that no matter how many imitation frames an AI generator can add, it\u2019s not going to make these games feel faster in your hands. That 218fps will only ever have the mouse snappiness of a 64fps game, because those frames \u2013 the ones delivered via the usual PC pipeline \u2013 are the only ones that can accurately reflect your inputs.\n\nAll that in mind, the visual smoothness is a stark upgrade, and if you\u2019re not playing something that needs hyper-accurate twitch aiming\u2026 why not? Especially if the underlying GPU is strong enough to get comfortable 'real' framerates out of max-quality settings like path tracing, in which case frame generation becomes the luxury it really should be. Not a crutch that games without adequate low-end settings can use to pump their numbers up.\n\nMFG is also but one part of DLSS 4, which covers a range of tricks that both are and aren\u2019t exclusive to this new Nvidia generation. The RTX 5090 thus also has the duty of showcasing some tech that will benefit GPUs going right back to the RTX 20 series, such as DLSS 4\u2019s new 'Transformer' model and support for 'DLSS overrides' in the Nvidia app.\n\nImage credit: Rock Paper Shotgun\n\nFirstly, the Transformer model is a rejigged technique for how DLSS applies its best-in-class anti-aliasing. It runs slightly slower than the preexisting \u2018Convolutional Neural Network\u2019 model, averaging 120fps in Cyberpunk 2077 at 4K/Ultra versus the 132fps of the older version. But it does look sharper and more detailed: a lot of Nvidia\u2019s promotional comparison images have focused on textures, but what I found most noticeable was the improvement to fine-edged objects like grass and wire fences. DLSS already looked better than its rivals FSR and XeSS on these, but the Transformer model cleans and sharpens them up even further, bringing the overall picture closer than ever to true native-rez quality. Good stuff.\n\nSecondly, DLSS overrides could be a very big deal in bringing newer, superior versions of DLSS to games that either never updated their support or lacked it in the first place. It still requires a degree of support in the Nvidia app \u2013 you can\u2019t just go slapping DLSS 4 on anything ever made \u2013 but there will be a batch of 75 compatible games at launch, and the options are impressively flexible. You could force a game to use DLAA anti-aliasing if it was never officially implemented, for example, or take a game that has Convolutional Neural Network DLSS support and upgrade it to the new Transformer model. And, if you\u2019ve got a 50 series GPU like this one, you can have a game with DLSS 3 frame gen capability also work with full 4x MFG, as I did with Dragon Age: The Veilguard to get those benchmark results above.\n\nI\u2019m still not seeing the value in a lot of what Nvidia does with AI, up to and including their attempts at creating creepy, artless NPCs. But it\u2019s much, much easier to get on board with these DLSS advancements, which devalue nothing, steal from nowhere, and could well breathe life into older hardware as much as they enrich the new.\n\nNvidia GeForce RTX 5090 review: 1440p benchmarks\n\nBefore we get too lovey-dovey, though, time for another reminder of the RTX 5090\u2019s limitations. Buying a top-shelf GPU and hooking it up to less demanding 1440p screen might seem like galaxy brain setup-building to some \u2013 the PC equivalent of those Fiats with massive engines poking out the back because they won\u2019t fit inside. In practice, your mismatched rigmobile will just send you crashing into a CPU bottleneck:\n\nImage credit: Rock Paper Shotgun\n\nAgain, the RTX 3090 is left well behind, but at this resolution the RTX 5090 is effectively only as powerful as the GPU it replaces. You could potentially open up the gap again by laying on thick with the ray tracing, but still, the results underwhelm: Cyberpunk 2077, with Psycho RT effects and Quality DLSS, averaged 124fps on the RTX 5090 and 111fps on the RTX 4090. All that cash for an extra 11fps? Not a great deal. MFG isn\u2019t really useful here either, as you\u2019re already getting so many frames shoved down your eyes that regular 2x frame generation will push supporting games beyond the point where you\u2019ll seldom be able to perceive a load more.\n\nNope, 4K is where the RTX 5090 is at. But then, trying to be practical with a card like this kind of feels like missing its point. Yes, we once had the GTX 1080 Ti and the RTX 2080 Ti, GPUs that could top a range while still being somewhat attainable high-enders. I miss those days too. But is the RTX 5090 not more of an aspirational device anyway? A concept model that\u2019s just happened to escape the design wing and sneak into the production factory?\n\nImage credit: Rock Paper Shotgun\n\nLet\u2019s be real: you were never going to buy this. I was never going to buy this. Even the people who might buy this, presumably when they\u2019re not busy executing their generals, probably have at least one eye on the RTX 5080 as well. It\u2019s just so far beyond us normies that there doesn\u2019t seem to be much sense in hating it either. The GPU itself won\u2019t care, and Nvidia certainly won\u2019t. They\u2019re probably already planning the RTX 6090 for \u00a32,500.\n\nInstead, let\u2019s just see this for what it really is, an eye-grabbing exercise for DLSS features that are far more likely to make waves than any one single graphics card. You\u2019ve got me there, RTX 5090, even if you aren\u2019t getting my money.\n\nThis review is based on a retail unit provided by the manufacturer.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia Geforce RTX 5090 Founders Edition review",
            "link": "https://www.club386.com/nvidia-geforce-rtx-5090-review/",
            "snippet": "The fastest graphics card Nvidia has ever produced is also the most innovative. Head this way for analysis of the \u00a31939 GeForce RTX 5090.",
            "score": 0.9417252540588379,
            "sentiment": null,
            "probability": null,
            "content": "There\u2019s a feeling of excitement at Club386 HQ that only new graphics launches can provide. Sam\u2019s smitten with Intel Arc B580, Fahd can\u2019t wait to see if Radeon RX 9070 XT is a worthwhile upgrade from his current card, and Damien already has plans for a SFF build featuring GeForce RTX 5080.\n\nTruth is that appreciation of all those cards comes alongside a common caveat: GeForce RTX 5090 is the one they really want.\n\nNvidia GeForce RTX 5090 Founders Edition \u00a31,939 / $1,999 Pros DLSS gets better and better\n\nThe fastest graphics card ever\n\nOutstanding build quality\n\nDual-slot form factor\n\n32GB GDDR7 memory Cons Prohibitive pricing\n\nPower hungry Check Amazon price Buy at Nvidia Club386 may earn an affiliate commission when you purchase products through links on our site.\n\nHow we test and review products.\n\nNvidia\u2019s latest flagship has elicited its fair share of ooohs and aaahs in our labs, and only a lofty \u00a31,939 price tag prevents our collective credit cards from coming out. Such a figure represents an insurmountable hurdle for the majority of gamers.\n\nNevertheless, we PC enthusiasts love a halo product, and specifically a GPU that paints a picture of what\u2019s to come in subsequent years. It\u2019s a deep-rooted feeling that has persisted for me since my first graphics card acquisition as a youth \u2013 a Diamond Monster 3D complete with 4MB of memory \u2013 from the local computer market at Wolverhampton racecourse.\n\nThree decades later, I get the privilege and thrill of powering up a GeForce RTX 5090 for the first time. Only, this is a launch that feels different. The landscape has shifted, and goalposts moved as we transition from brute-force muscle to a new generation of innovative neural rendering.\n\nThe ramifications for the future of computer graphics are significant, especially so with an industry leader laser-focussed on all things AI. Nvidia\u2019s supremacy in the high-end space is such that I\u2019m confident in stating you\u2019ll see no faster solution than RTX 5090 in 2025. Rivals Intel and AMD have unofficially ceded defeat to focus on the mid-range, and with nobody to challenge Nvidia\u2019s might at the top of the stack, there\u2019s no reason to assume an RTX 5090 Ti is waiting in the wings. This is as good as it gets.\n\nSo without further ado, let\u2019s break out the Club386 Table of Doom as we embark on this journey of discovery together.\n\nSpecifications\n\nGeForce RTX 5090 4090 3090 Launch date Jan 2025 Oct 2022 Sep 2020 Codename GB202 AD102 GA102 Architecture Blackwell Ada Lovelace Ampere Process TSMC 4N TSMC 4N Samsung 8N Transistors (bn) 92.2 76.3 28.3 Die size (mm2) 750 608.5 628.4 SMs 170 of 192 128 of 144 82 of 84 CUDA cores 21,760 16,384 10,496 Boost clock (MHz) 2,407 2,520 1,695 Peak FP32 TFLOPS 104.8 82.6 35.6 RT cores 170 (4th Gen) 128 (3rd Gen) 82 (2nd Gen) RT TFLOPS 317.5 191 69.5 Tensor cores 680 (5th Gen) 512 (4th Gen) 328 (3rd Gen) Peak FP16 TFLOPS 419 330.3 142.3 Peak FP4 TFLOPS 3,352 \u2013 \u2013 ROPs 176 176 112 Texture units 680 512 328 Memory size (GB) 32 24 24 Memory type GDDR7 GDDR6X GDDR6X Memory bus (bits) 512 384 384 Memory clock (Gb/s) 28 21 19.5 Bandwidth (GB/s) 1,792 1,008 936 L2 cache (KB) 98,304 73,728 6,144 PCIe interface Gen 5 Gen 4 Gen 4 Video engines 3 x NVENC (9th Gen)\n\n2 x NVDEC (6th Gen) 2 x NVENC (8th Gen)\n\n1 x NVDEC (5th Gen) 1 x NVENC (7th Gen)\n\n1 x NVDEC (5th Gen) Power (watts) 575 450 350 MSRP ($) 1,999 1,599 1,499\n\nGoing by high-level specifications, RTX 5090 represents a scaled-up version of RTX 4090. What we have in retail form is a potent flagship that enables 11 of 12 maximum graphics processing clusters (GPCs). This translates to a mighty 21,760 CUDA cores, which itself represents a 33% uptick over RTX 4090. Look back even further to RTX 3090 and note the number of cores more than doubles.\n\nThat firepower is allied to gains at the back end, too, where memory is bolstered significantly. The available pool climbs 33%, from 24GB to 32GB, memory bus widens from 384 bits to 512, and more importantly, the introduction of high-speed GDDR7 at a swift 28Gb/s sees total bandwidth balloon to 1,792GB/s. Yes, you\u2019re reading that right, memory bandwidth improves by almost 80% over RTX 4090.\n\nWelcome gains, no doubt, and the underlying Blackwell architecture carries a few tricks up its sleeve not illustrated above.\n\nArchitecture\n\nBuilt on a custom TSMC 4N process also used in the previous generation, the new champ packs in more of everything that matters. Nevertheless, even mighty RTX 5090 is not a full implementation of the GB202 die that has over 11x the transistor count as there are humans on this planet.\n\nLike RTX 4090 before it, there\u2019s room for a Titan-esque RTX 50 Series card packing all 192 SMs and 24,576 CUDA cores, instead of 170 SMs and 21,760 cores available here. I very much doubt you\u2019ll witness the Titanic beast because Nvidia has no reason to allocate precious full-die GPUs to the consumer gaming space.\n\nOne of 170 resident on RTX 5090 zoomed in, the Blackwell SM continues tradition by running with 128 FP32 cores, one RT Core, four Tensor Cores, and four Texture Units, as found on RTX 40 Series.\n\nApproaching shaders first, you may remember that RTX 40 Series, aka Ada, split each of the SMs into 64 FP32 and a further 64 capable of FP32 or INT32. Now, RTX 50 Series still carries the same number per SM, but all 128 can process floating point or integer, though not at the same time. The distinction is important insofar as games do use INT32, so having core flexibility is useful in maximising performance by keeping the engines full to the brim with work to do.\n\nBigger changes are afoot for the other SM constituents. Let me explain how by way of the table below.\n\nRTX 4090 RTX 5090 Increase Gain per SM ISO CUDA FP32 TFLOPs 81.92 108.8 32.8% Linear RT TFLOPS 191 317 66% 2x Tensor FP16 (FP32 Acc) 330.3 419 26.9% Linear* Tensor FP8 1,321 1,676 26.9% Linear* Tensor FP4 1,321** 3,352 153.7% 4.7x * RTX 50 Series must run Tensor Cores at lower speeds for gain to be 26.9% rather than 32.8%\n\n** RTX 40 Series can run FP4, though it uses FP8 Tensor Units to do so\n\nAssuming an identical frequency, RTX 5090 provides a linear increase in CUDA FP32 throughput most closely associated with rasterised rendering. Actual gain is a little less as the new GPU is clocked in a smidge lower.\n\nYou\u2019d expect the same to be true of the RT cores \u2013 a 33% increase when run at the same frequency \u2013 but this is not the case. Nvidia claims double the ray-triangle intersection testing ability, thus making RTX 50 Series more efficient at ray tracing. From an engineering point of view, this is Nvidia saying ray tracing is very much here to stay.\n\nThose with a keen eye will realise the Triangle Cluster Intersection Engine and Linear Swept Spheres support are new RT Core additions. Their purpose is to hardware accelerate a new RTX feature called Mega Geometry \u2013 available on all RTX 20 Series and above \u2013 and efficiently run the math behind calculating complicated structures such as hair. Put simply, baked into hardware, it\u2019s an invitation for developers to speed-up certain processing for more realistic-looking imagery.\n\nA key callout is RTX 50 Series\u2019 ability to chomp through AI calculations. Part and parcel of DLSS technology underpinning performance, they\u2019re a big deal. Blackwell also introduces specific hardware support for FP4 (AI TOPS) operations, and this is how Nvidia claims startling increases over previous generations. Aimed toward generative AI, running FP4 typically halves VRAM requirement and offers easily twice the speed as FP16, with little loss in fidelity.\n\n\u201cNeural shaders are set to become the predominant form of shaders in games, and in the future, all gaming will use AI technology for rendering.\u201d Nvidia, January 2025\n\nBeing even handed, if your Tensor Core (AI) workload runs precision at INT8, FP8 or FP16, the gain from RTX 4090 to RTX 5090 is a far more modest 26.9%, which is less than the increase in card-wide cores. A probable explanation is that RTX 5090 runs its more numerous cores at a lower frequency to save power.\n\nEagled-eyed purveyors of GPU architecture will doubtless appreciate the only meaningful place where RTX 5090 cedes ground to RTX 4090 is with respect to pixel fill-rate, determined by multiplying ROPs by clock frequency. Doing so gives the 40 Series card a slight advantage of 443.5 vs. 423.6 Gigapixels per second.\n\nMulti-frame Generation\n\nNvidia\u2019s Frame Generation technology is a key RTX 40 Series asset for boosting performance. Requiring developer support, it works by inserting an AI-created frame between traditionally rendered frames. Tensor Cores do most of the heavy lifting, working quickly enough to ensure seamless transition from raster to AI and back.\n\nKnowing this area is ripe for further exploration, RTX 50 Series duly builds on the technology with Multi Frame Generation (MFG). This time around, though, up to three AI-generated frames are inserted after a regular raster frame, up from a single frame on the previous gen.\n\nPart of DLSS 4, MFG is possible because of changes in the name of simplification. The hardware-based Optical Flow Accelerator used by RTX 40 Series Frame Generation is replaced by a deploy-once AI model capable of generating multiple frames. It\u2019s 40% faster and requires 30% less VRAM, too. Going from dedicated hardware to Tensor Core-run processing opens up the tantalising opportunity of broadening MFG\u2019s appeal in the future.\n\nIn most performant form, MFG is run alongside DLSS (upscaling) and Ray Reconstruction (AI-generated pixels in complex ray-tracing environments) to output 15 out of every 16 pixels you see on the screen. Think about that for a second. One raster, 15 AI.\n\nChatting to engineers at a press event, Nvidia wanted to implement MFG on RTX 40 Series cards but couldn\u2019t do so for other technical reasons, the most pressing of which was the inability to pace MFG frames properly between rastered ones. This makes sense if I think about it, as the hardware needs to keep all this AI-generated traffic running smoothly and evenly. Any poorly timed frame delivery is immediately apparent to the gamer. Getting around this, RTX 50 Series has improved Flip Metering which entails shifting frame-pacing duties from the CPU to dedicated hardware in the display engine.\n\nThere\u2019s also a new AI Management Processor (AMP) whose job it is to schedule disparate RT, CUDA, and Tensor Core workloads smoothly. Think about the following context. You\u2019re playing a game with multiple AI models running concurrently, tasked with generating multiple frames through to running large-language models (LLMs) powering unique dialogue from non-playing characters. Then add in a sprinkling of ray tracing on top whilst taxing traditional CUDA cores to the limit.\n\nJuggling these facets is no mean feat. The spinner of gaming plates, AMP is intended to keep everything in check, ensuring your NPC\u2019s dialogue doesn\u2019t interfere with Frame Generation or Ray Reconstruction.\n\nTransformer Model\n\nUp until today, DLSS used a pixel-generating methodology based on Convolutional Neural Networks (CNNs). Optimised over the previous six years, it\u2019s as refined as the technology allows. Now, however, helping the DLSS cause is a more modern technique known as the Transformer Model. Developed by Google in 2017, this deep learning model is the gold standard for AI, adopted in popular applications such as ChatGPT and Gemini.\n\nDLSS 4 Transformer requires more innate horsepower to run because it\u2019s more complex. Nvidia cites twice the parameters and four times the compute when compared to CNN. Set against the backdrop of computer graphics, the new model carries noticeable image-quality benefits for DLSS Ray Reconstruction, Super Resolution, and DLAA.\n\nHere\u2019s an illustration of how Ray Reconstruction improves temporal stability and increases detail. I saw this demonstration firsthand at the same event. The effect is subtle enough that you don\u2019t really notice it until it\u2019s pointed out. But once it is, you immediately see the flaws in even the well-optimised CNN model.\n\nThe absolute kicker is that Nvidia isn\u2019t tying Transformer Model\u2019s IQ gains to RTX 50 Series alone. The company certainly could have, incentivising upgrades to the latest and greatest hardware, yet it\u2019ll be available to all RTX cards. I know it\u2019ll run well on recent GeForces such as the RTX 4080 Super, but I\u2019m genuinely curious to see how it fares on older cards.\n\nOf course, DLSS refinements like MFG and Transformer need developer implementation on a game-by-game basis. One simply cannot run them on every title. Key to success is how quickly Nvidia manages to get its host of technologies into all the leading games.\n\nOther than a larger and more transistor-rich slab of silicon than has come before, my key takeaway from GeForce RTX 5090 is increased reliance on AI for smart rendering. The brute force approach only gets you so far \u2013 the GPU is as brutish as can be \u2013 so here\u2019s Nvidia saying that subsequent gaming architectures will double down on Tensor cores and technologies akin to MFG.\n\nI strongly feel this approach is the only sane way of instigating fundamental step changes in gaming performance \u2013 wait until you see MFG in action on Alan Wake 2 and Cyberpunk 2077 \u2013 alongside upscaling IQ improvements emanating from using better models.\n\nMemory and Titan\n\nNvidia\u2019s well aware that more powerful and numerous SM units require significantly more memory bandwidth than available to even RTX 4090. Two key changes are enacted. Compared to its predecessor, bus width increases from 384 bits to 512 bits and memory from 21Gbps to 28Gbps \u2013 the latter made possible by the move from GDDR6 to GDDR7. Combining the two lends RTX 5090 a 78% on-paper advantage, together with a frame buffer that at 32GB is 33% larger.\n\nAdding more of everything inevitably leads to higher power consumption, too, notwithstanding GDDR7\u2019s better energy efficiency at any given frequency over its predecessor. Accordingly, Nvidia states a lofty 575W TGP, representing a 28% increase over already hot and bothered RTX 4090. I wonder what it would have been if GDDR6 was still in service?\n\nThough I\u2019ve dismissed talk of an RTX 50 Series Titan card carrying the full GB202 die, I can\u2019t help but conjecture. Run at the same frequencies as RTX 5090, I hazard it\u2019ll need higher voltage to maintain all SMs. A Titan pulling 750W certainly isn\u2019t out of the question. This is all academic talk until, and very much if, Nvidia pulls one out of the hat for no other reason than it can.\n\nOther Titbits\n\nVideo encoding/decoding ability is tied in with RTX 50 Series card. For top dog RTX 5090, Nvidia equips three encoders and two decoders. RTX 5080 goes two and two for both, RTX 5070 Ti uses two and one, while RTX 5070 slims it all the way down to one each. They all support the 4:2:2 format, meaning twice the colour data as 4:2:0, so a good fit for videos that feature lots of hues and contrasting imagery. Furthermore, Nvidia says GeForce RTX 5090 can export video 60% faster than the GeForce RTX 4090 and at 4x speed compared with the GeForce RTX 3090.\n\nA natural bump in specification and throughput means PCIe 5.0 is the conduit between card and system. I don\u2019t imagine much performance improvement over PCIe 4.0 because the older standard has plenty of bandwidth.\n\nFounders Edition\n\nBefore I get to the all-important benchmarks, it\u2019s absolutely worth taking a moment to reflect on the beautiful engineering of RTX 5090 Founders Edition. This, for me, supplants RTX 4070 Super as the prettiest card Nvidia has ever produced.\n\nAnd make no mistake, this isn\u2019t a case of form over function. Rather, this is a design years in the making, and one that rectifies multiple shortcomings of unwieldy, gargantuan RTX 4090.\n\nMeasuring 304mm x 137mm, RTX 5090 FE is exactly the same length and height as its predecessor but slims from a three-slot to two-slot form factor. The end result is dramatic, resulting in a far sleeker product better suited to a wider range of cases. I didn\u2019t find the chunky RTX 4090 particularly offensive, but it looks comically large by comparison.\n\nThe ability to increase TGP and decrease overall thickness is a neat trick, made possible by a wholly new cooling setup based around a dual-blowthrough configuration. Both fans showcase ultra-tight tolerances and funnel air through fin stacks whose surface is subtly curved to ensure the longest fins are where airflow is greatest, while angled vents on the cards\u2019 thinner sides help prevent hot air from reentering from below.\n\nThis, for me, supplants RTX 4070 Super as the prettiest card Nvidia has ever produced.\n\nAirflow is but one part of the puzzle. Beneath that shiny, curvy, all-metal shell is a central PCB, behind the X pattern as you look at it. Densely packed with 32GB of memory, this is Nvidia\u2019s smallest range-topping PCB to date, and unique insofar as its central position means connectivity to IO and PCIe takes an unusual approach.\n\nThis time around, a proprietary internal cable attaches PCB to PCIe connector, which is clamped in place using a series of screws. Similarly, the IO ports connect to the PCB via a ribbon cable tasked with driving three DisplayPort 2.1b and a single HDMI 2.1b. All told, the card, in tandem with display stream compression (DSC), can drive up to 4K 12-bit HDR at 480Hz or 8K 12-bit HDR at 165Hz.\n\nThe intrigue doesn\u2019t stop there. Nvidia\u2019s 750mm2 GPU is now coated with a thin layer of Liquid Metal as opposed to conventional thermal interface material (TIM). In an effort to maximise performance and longevity, the GPU is surrounded by a slim rubber ring to prevent leakage, and the contact plate is nickel-plated to eliminate any unwanted corrosion and subsequent drying out.\n\nSpeaking of contact plate, the customary copper block is now a purpose-built vapour chamber whose heatpipes extend horizontally through the fin stacks each side of the central PCB. It\u2019s an intricate setup that shows the lengths Nvidia has gone to in order to shoehorn a 575W chip into a dual-slot design. The drawback is that add-in-board partners with simpler triple- or quad-fan configurations are going to be humongous by comparison. Expect custom 5090s to occupy four or even five slots.\n\nPower is sourced via a tweaked 12VHPWR connector that feels a tad more secure. Nvidia includes a four-way, eight-pin splitter as part of the bundle, and even that is upgraded, with a solid reinforcement block doing a better job of preventing flex than the old piece of tape. Handy to have, however you will want a modern PSU with an available 12VHPWR header; a single cable is so much tidier, and less likely to obstruct the backlit GeForce RTX logo.\n\nIt\u2019s impressive to see a 575W chip tamed by such a sleek cooler, yet there are ramifications. To my memory this is the most power-hungry GeForce graphics card ever produced, and in an era of high energy bills, it\u2019s worth knowing the true cost of ownership.\n\nInstalled in a high-end test platform, system-wide power consumption balloons to 707W when gaming. Putting that into perspective, at the current UK energy price cap rate of \u00a30.25 per kWh, a single six-hour gaming session adds \u00a31.05 to your bill. Play a conservative six hours a week and you\u2019ll pay \u00a354.85 in electricity for the year. Stay glued to the screen three hours a day, every day, and your annual \u2018leccy cost will hit \u00a3191.93 for the PC alone.\n\nExtreme power usage is reflected in Nvidia recommending at least a 1,000W PSU, yet also serves to showcase the robustness of Founders Edition cooling. In-game temperature peaked at a balmy 73\u00b0C throughout our testing, and fan noise remained below 40dB. Mighty impressive results for a dual-slot card.\n\nI\u2019m of the opinion no consumer graphics card should fetch \u00a31,939 \u2013 it alone costs more than my current rig! \u2013 yet if you are able to afford an RTX 5090 Founders Edition, I will cede this feels like an ultra-premium product. From unboxing to installation, it\u2019s an exquisite piece of kit and a joy to handle.\n\nThe bad news? Well, you\u2019ll do well to ever lay hands on one. This isn\u2019t just the fastest, most beautiful graphics card on the market, it\u2019s also a 32GB AI and content creation accelerator, making it highly sought after by a vast audience. News of stock shortages has already spread, but I\u2019d go as far as to say RTX 5090 Founders Edition will be hard to come by for the entirety of 2025.\n\nPerformance\n\nNew year, new start. I\u2019ve tested RTX 5090, 4090 and 3090 all from scratch for your benchmark perusal, and I\u2019ve also re-tested the best Radeon of today, RX 7900 XTX. This is a heavyweight showdown fit for the modern era, today\u2019s equivalent of the fight of the century. Let\u2019s get ready to rumble.\n\nApplication & AI\n\nWhat better place to start than industry-standard 3DMark. A 39% uptick over already-blistering RTX 4090 is eye opening, while the likes of RX 7900 XTX and RTX 3090 cower in the corner.\n\nHigh-end Speed Way or non-raytraced Steel Nomad? Whichever medicine you take, RTX 5090 administers a whole new level of dosage. 173% faster than RTX 3090, 105% faster than RX 7900 XTX, and 48% faster than RTX 4090.\n\nDon\u2019t want to alarm you, fellow gamers, but you\u2019ll be vying for a place in the queue alongside content creators. The previous best do-it-all graphics card is soundly beaten to the tune of 35% in Blender.\n\nEver heard of that thing called AI? Well, yep, everyone who\u2019s anyone wanting to get a jumpstart on artificial intelligence will also be in the market for RTX 5090. It is the new standard bearer.\n\nSignalling real-world implications of the card\u2019s onboard AI chops, Procyon\u2019s AI Text Generation benchmark tests multiple large language models for measuring the inference performance of on-device accelerators. I\u2019m charting the newest Llama 3.1 model and seeing a near-40% uplift over RTX 4090.\n\nGame Rasterisation\n\nI hear you, it\u2019s games you care about. There\u2019s plenty to this tale, and I\u2019ll begin with good ol\u2019 fashioned rasterisation and some light ray tracing across a slew of modern titles at three popular resolutions \u2013 1920\u00d71080 (FHD), 2560\u00d71440 (QHD) and 3840\u00d72160 (UHD).\n\nExpecting a little more frame rate for your buck? A 33% hike in both cores and memory translates to only a 17% performance improvement in Assassin\u2019s Creed Mirage at 4K UHD.\n\nIt\u2019s hard to quibble with 150 frames per second at the highest resolution with image quality dialled up to maximum, yet for simple gaming\u2019s sake, a Radeon RX 7900 XTX can muster over 100fps and costs half as much.\n\nFinal Fantasy XIV: Dawntrail delivers a similar in-game performance bump. RTX 5090 naturally takes its place at the top of the chart, but pure rasterisation isn\u2019t a giant leap from 2022\u2019s RTX 4090.\n\nAs a console-first title, Forza Motorsport is surprisingly punishing on high-end GPUs. With image quality cranked right up, even RTX 5090 can\u2019t break the 100fps barrier at 4K UHD. Still, if you\u2019re wondering where to go from your RTX 3090, Nvidia\u2019s latest delivers meaningful gain; the game goes from stuttery to buttery.\n\nA 31% performance bump at 4K UHD in Mount & Blade II: Bannerlord is more in keeping with the card\u2019s underlying specifications.\n\nAs expected, the common pattern of 32GB RTX 5090 extending its lead at 4K UHD continues in Tom Clancy\u2019s Rainbow Six Extraction.\n\nThe tale of the tape thus far is that RTX 5090 is up to 30% quicker than RTX 4090 in the rasterisation stakes. A healthy step forward, yet that alone isn\u2019t enough to warrant an upgrade. In fact, the generational uplift is markedly lower than from RTX 3090 to RTX 4090.\n\nBetween Generations Average improvement at 4K RTX 3090 to RTX 4090 78.02% RTX 4090 to RTX 5090 24.84% RX 7900 XTX to RTX 5090 70.14%\n\nThis table takes the mainly rasterised games and calculates the performance delta between select combinations. Results are averaged in getting the figures above.\n\nLet me not beat around the bush. Gains from RTX 3090 to RTX 4090 are immense, averaging over 78% and peaking at 91% in Forza Motorsport, likely due to the heavy lifting required for ray tracing. A real performance step-change in the two years between launches. RTX 4090 to RTX 5090, meanwhile, is nowhere near this uplift. 25% or so isn\u2019t bad, per se, but it\u2019s abundantly clear focus is not on rasterisation for this generation.\n\nNvidia\u2019s latest architecture is geared differently, and adding new-and-improved frame generation to the mix is where the magic begins.\n\nFrame Generation\n\nAI-generation frames have grown to exist in many forms. One of last year\u2019s most popular games \u2013 Call of Duty: Black Ops 6 \u2013 supports DLSS Frame Generation on RTX 40 and 50 Series cards, while older 30 Series cards have to rely on AMD FSR3.\n\nThis is frame generation as we\u2019ve come to know it \u2019til now. Applying DLSS upscaling provides a minor uplift in performance, but it\u2019s those \u2018artificial\u2019 frames that make an impact. Turns out that was but the tip of the iceberg.\n\nI alluded to Multi Frame Generation being RTX 5090\u2019s secret weapon, and 3DMark\u2019s dedicated DLSS test provides evidence. This helpful benchmark is able to measure all of the available DLSS modes that now exist \u2013 from basic DLSS 2 upscaling, right through to DLSS 4x frame generation.\n\nIt\u2019s worth looking at the chart once more. RTX 4090 can scale to 140fps with DLSS 3 frame generation, which gets it to the base level of RTX 5090. Then things get wild. RTX 5090 frame generation ups frame rate to 200, DLSS 4 is a bit better still, but it\u2019s 3x and 4x Multi Frame Generation that really sends performance soaring up to 296 and 369fps, respectively.\n\nReady for more bonkers graphs? I had access to a pre-release patch for Alan Wake 2 which adds MFG support, and for a game running ultra-demanding path tracing, the results are nothing short of outstanding. Furthermore, GeForce RTX 5090\u2019s intrinsic power means a solid base frame rate enables MFG to really strut its stuff.\n\nYet there are words of caution. Higher frame rates availed by MFG don\u2019t necessarily translate into better gamer performance \u2013 fps and perceived performance are not exactly analogous in a game dominated by AI rendering. A true 244fps feels better than the outputted 244fps here, underscored by lower latency. The old qualitative vs quantitative debate rages on.\n\nI mentioned my old Diamond Monster 3D graphics card earlier in the review. That was based on the legendary Voodoo graphics chip, and I\u2019ve come full circle as RTX 5090 is performing voodoo of its own. There\u2019s surely no other way to explain going from 76fps to 244fps at the flick of a switch.\n\nOf course, there\u2019s much more to frame generation than first meets the eye. Look ever so closely and you will see the odd instance of ghosting or shimmering, and these unwanted artefacts affect some titles more than others. In an ideal world you wouldn\u2019t want to have to make that trade-off, yet when the frame rate boost is as dramatic as this, it\u2019s an evil I suspect a growing number of gamers will be willing to accept. And let\u2019s be clear, in real-time, a lot of those artefacts are hard to spot.\n\nThere\u2019s also the question of latency. Nvidia has a growing number of tools in its arsenal to prevent latency from spiralling out of control as more frames are inserted, and the tech appears to be working well.\n\nNative Latency FG Latency FG 3x Latency FG 4x Latency 5090 @ UHD 40.9 49.9 52.1 53.4 5090 @ QHD 25.4 32.3 34.8 36.1 5090 @ FHD 21.2 25.5 26.0 27.1 4090 @ UHD 51.3 63.6 \u2013 \u2013 4090 @ QHD 34.9 42.4 \u2013 \u2013 4090 @ FHD 26.9 31.9 \u2013 \u2013\n\nRTX 5090\u2019s inherent horsepower ensures latency across resolutions is lower than RTX 4090, but what\u2019s interesting is that the impact when adding MFG to the equation isn\u2019t as dramatic as one might think.\n\nIs an increase in latency from 41ms to 53ms worth it when a dramatically higher frame rate is on the table? That\u2019s a personal choice gamers are going to have to make, but frankly, it\u2019s nice to have the option.\n\nCyberpunk 2077\u2019s upcoming implementation of MFG is equally impressive, but I must caveat these results by stating the benchmarks are carried out on pre-release versions of the game. I expect the final release to deliver similar numbers, if not better, but MFG is a new tool for developers to get to grips with, and it\u2019ll be a while before we know how well it works across a much broader spectrum of titles.\n\nStill, I can\u2019t help but marvel at what\u2019s already being achieved. Playing using Cyberpunk\u2019s notorious Overdrive preset is brutal for any and every graphics card. Even RTX 5090 can muster only 33fps natively. I\u2019d prefer this to be higher, as MFG works best with a base 60fps, but flick the switch for 4x frame generation and that lowly number is propelled to 219.\n\nLooking at it all this way, it\u2019s only fair to tabulate the percentage increase as I\u2019ve done before.\n\nBetween Generations Improvement at 4K RTX 3090 to RTX 4090 105% RTX 4090 to RTX 5090 167% RX 7900 XTX to RTX 5090 492%\n\nComparing best versus best \u2013 50 Series MFG to 40 Series and 7900 FG \u2013 there\u2019s a truly staggering increase, and one has to wonder, if not for innovative frame generation, how else would developers get the best-looking games to scale to over 200fps? Even quad SLI wouldn\u2019t get the job done.\n\nIt\u2019s hard to disagree with Nvidia\u2019s prediction that one day all games will use AI technology for rendering, and that claim lends excitement to other categories. Seeing frame generation at its very best on high-end desktop makes me eager to explore what the technology could do for gaming handhelds, VR headsets, and beyond.\n\nConclusion\n\nGeForce RTX 4090 has stood undiminished as the PC gaming weapon of choice since its arrival in October 2022. Embodying a heady mix of brute power and novel frame rate-boosting technologies such as Frame Generation, it\u2019s rightfully been the go-to GPU for gaming at 4K with eye candy turned all the way up. Marching in with size 12s today, GeForce RTX 5090 rips the erstwhile champion\u2019s performance crown right off and gets comfy on the throne.\n\nNvidia looks to the future rather than peer over its shoulder. Significant generational gains in traditional rasterisation are deliberately compromised in favour of what AI can do for a more immersive gaming experience, running from stratospheric frame rates through to smarter NPCs. Nvidia has built the infrastructure; now it\u2019s up to developers to grasp the nettle and release better games.\n\nUp until then, no matter how I dice it up, GeForce RTX 5090 is the absolute pinnacle of PC gaming.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "I fit the world\u2019s most powerful graphics card in my aging miniature SFF PC",
            "link": "https://www.theverge.com/2025/1/23/24349976/nvidia-rtx-5090-fe-small-form-factor-sff-hands-on",
            "snippet": "We put an Nvidia RTX 5090 Founder's Edition, a two-slot graphics card, into a small form factor PC case: the Ncase M1, a 12.7-liter chassis.",
            "score": 0.8660327792167664,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and founding member of The Verge who covers gadgets, games, and toys. He spent 15 years editing the likes of CNET, Gizmodo, and Engadget.\n\nIn 2022, I wrote that GPUs were headed in the wrong direction \u2014 their price, size, and power consumption were off the charts. And while I still believe that\u2019s true, I can now confirm Nvidia has at least made one phenomenal exception in the size category: the two-slot \u201cFounder\u2019s Edition\u201d of its RTX 5090 graphics card, on sale January 30th.\n\nThe last time Nvidia made a two-slot flagship graphics card, it was the 2021 RTX 3080 Ti FE \u2014 the 3090, 4080 and 4090 were gigantic by comparison.\n\nRelated Nvidia GeForce RTX 5090 review\n\nSo, while my colleague Tom Warren was busy writing his full review of the new 5090 using the best gaming CPU, I wedged one of the $2,000 cards into my own aging mini desktop. I wanted to see whether the surprisingly small flagship GPU was truly ready for small form factor (SFF) cases \u2014 or, whether my beloved 12.7-liter Ncase M1 chassis is well and truly obsolete.\n\nPrevious Next\n\n\n\n\n\n\n\n\n\n\n\n1 / 7 Gallery: The 5090 may be a two-slot card, but it\u2019s not as small as two-slot cards of yesteryear like this RTX 3080...\n\nTo my surprise, it worked: all I needed was a new power supply to turn my backpack-sized daily driver into one of the most powerful gaming PCs in the world. At 4K resolution, I\u2019m typically seeing more than double the framerate I get with an RTX 3080 Founder\u2019s Edition, one of the last cards that could comfortably fit in the Ncase M1, to give you some idea.\n\nBut I\u2019m not going to suggest you do the same! For starters, we\u2019re talking about a two thousand dollar graphics card and a one thousand watt power supply \u2014 which I actually saw consuming up to one whole kilowatt (as measured by my trusty Kill A Watt at the wall) in my Cyberpunk 2077 tests. With an RTX 3080, my system consumed over 200 fewer watts. Not that I minded having a space heater on these cold January days!\n\nBut I literally had to wedge the 5090 into my Ncase M1 to make it fit, and even remove and reattach the video card\u2019s bracket inside my case. And even then I couldn\u2019t fully seal my desktop because the GPU\u2019s new 12V-2x6 power connector occupies a chunk of space where my case\u2019s side panel is supposed to go. You\u2019ll want an SFF-ready case with more clearance than I have.\n\nIt took a bit of elbow grease to get it in there.\n\nIf not for that power cable...\n\nStill, leaving my desktop\u2019s guts exposed was a small price to pay to toy with this much power! It\u2019s enough to play games at 4K at their maximum settings, save for full ray tracing (aka path tracing). It\u2019s even got enough horsepower to turn on path tracing, too, if you combine it with dynamic upscaling and/or fake frame generating tech.\n\nI normally play Helldivers 2 on an old 3060 Ti graphics card I bought for just $400, where I\u2019m forced to rely on those tricks just to get smooth 4K-ish gameplay. It was quite nice, if expected, to finally max out that game on the 5090 instead.\n\nWhat I didn\u2019t expect: my aging, space-constrained AMD 5800X desktop delivered the same performance as Tom\u2019s open-air testing rig in quite a few of our 4K gaming benchmarks. I knew it was possible, but it goes to show that Nvidia\u2019s fancy two-slot \u201cdouble flow through\u201d cooler really is suitable for SFF PCs.\n\nRTX 5090 SFF test at 4K Game Sean\u2019s SFF 5090 Tom\u2019s Bench 5090 Percent Difference Assassin's Creed Mirage (Ultra High, Native) 135 144 6.67% Black Myth: Wukong (100% resolution) 63 62 -1.59% Black Myth: Wukong (DLSS + Frame Gen) 147 146 -0.68% Call of Duty: Black Ops 6 (Extreme) 140 145 3.57% Cyberpunk 2077 (Ultra, no RT) 108 109 0.93% Cyberpunk 2077 (Ultra RT + DLSS Quality + FG) 152 153 0.66% Horizon Zero Dawn Remastered (Very High, Native) 112 153 36.61% Horizon Zero Dawn (DLSS Quality + Frame Gen) 206 237 15.05% Metro Exodus Enhanced (Extreme) 92 91 -1.09% Returnal (Epic) 138 142 2.90% Shadow of the Tomb Raider (Highest) 207 238 14.98% Average framerates at 4K resolution.\n\nIt depends on whether your games are CPU limited, of course, as my older PC does have a slower CPU \u2014 and most of today\u2019s games tend to be at least somewhat CPU limited at 1440p resolution, where Tom\u2019s system often pulled far ahead by 20 to 60 percent.\n\nRTX 5090 SFF results at 1440p Game Sean\u2019s SFF 5090 Tom\u2019s Bench 5090 Percent Difference Assassin's Creed Mirage (Ultra High, Native) 137 188 37.23% Black Myth: Wukong (100% resolution) 91 91 0.00% Black Myth: Wukong (DLSS + Frame Gen) 180 181 0.56% Call of Duty: BO6 (Extreme) 161 196 21.74% Cyberpunk 2077 (Ultra, no RT) 148 207 39.86% Cyberpunk 2077 (Ultra RT + DLSS Quality + FG) 207 246 18.84% Horizon Zero Dawn Remastered (Very High, Native) 123 200 62.60% Horizon Zero Dawn (DLSS Quality + Frame Gen) 222 299 34.68% Metro Exodus Enhanced (Extreme) 120 145 20.83% Returnal (Epic) 167 201 20.36% Shadow of the Tomb Raider (Highest) 223 354 58.74% Average framerates at 1440p.\n\nFacing down the alien swarms and flying particles in Returnal, for example, Tom pulled 201 frames per second at 1440p while my diminutive desktop managed just 169fps; in Horizon Zero Dawn Remastered, the most CPU-limited game we\u2019re testing, my bottlenecked system averaged just 123fps to Tom\u2019s 200fps.\n\nBut that\u2019s still over 120fps on max settings, more than enough for butter-smooth sessions of these single-player games! And if I were to pair my tiny tower with a 4K TV in my living room instead of a 1440p monitor, as many SFF PC builders might like to do, I\u2019d have a blast \u2014 my 4K results always averaged over 60fps, and were often within just a few FPS of Tom\u2019s open bench.\n\nRTX 5090 vs 3080 4K SFF test Game Sean\u2019s 5090 SFF Sean\u2019s 3080 SFF Roughly how much faster Assassin's Creed Mirage (Ultra High, Native) 135 65 2.1x as fast Black Myth: Wukong (100% resolution) 63 22 2.8x as fast Black Myth: Wukong (DLSS Quality) 93 39 2.4x as fast Call of Duty: Black Ops 6 (Extreme) 140 58 2.4x as fast Cyberpunk 2077 (Ultra, no RT) 108 40 2.7x as fast Cyberpunk 2077 (Ultra RT + DLSS Quality) 89 38 2.3x as fast Horizon Zero Dawn Remastered (Very High) 112 59 1.9x as fast Metro Exodus Enhanced (Extreme) 92 33 2.8x as fast Returnal (Epic) 138 61 2.3x as fast Shadow of the Tomb Raider (Highest) 207 89 2.3x as fast Average framerates at 4K resolution.\n\nAnd again, I\u2019m typically seeing the RTX 5090 delivering more than twice the horsepower of an RTX 3080, making it quite the upgrade for SFF fans with deep pockets. That\u2019s not necessarily something to celebrate, though: the $2,000 RTX 5090 admittedly costs more than twice as much as a $700 RTX 3080 did at its 2020 launch, and will be out of reach for most gamers even if shortages and scalpers don\u2019t rear their ugly heads.\n\nWhen it comes down to it, I think the Nvidia GeForce RTX 5090 is a damn cool piece of kit. It makes me want to quote Ferris Bueller\u2019s Day Off because it is so choice. It\u2019s a noteworthy exception to the very annoying trend of GPUs expanding in every direction. But at $2,000, 575 watts of power by its lonesome, and with no other Nvidia board partner offering anything nearly as compact, it\u2019s the exception that proves the rule.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia GeForce RTX 5090 review: more graphics card power in a smaller GPU",
            "link": "https://www.theshortcut.com/p/nvidia-geforce-rtx-5090-review-graphics-card-gpu",
            "snippet": "The Nvidia RTX 5090 is a higher-performance graphics card than the RTX 4090. There's no doubt about it from the benchmark numbers I recorded.",
            "score": 0.5511658191680908,
            "sentiment": null,
            "probability": null,
            "content": "(credit: Kevin Lee / The Shortcut)\n\n\ud83c\udfc6 Review score: 4.5 out of 5\n\n\ud83c\udfc5Editor\u2019s Choice\n\nPros:\n\n\u2705\ud83d\udc6f\u200d\u2642\ufe0f Multi-frame generation is a true revelation that makes 200, 300, and even 400 fps gaming possible at 4K\n\n\u2705 \u26a1\ufe0f Finally a graphics card fast enough for 4K 240fps and possibly 8K 120fps displays\n\n\u2705 \ud83d\udcd0 New DLSS 4 transformer model improves lighting, scaling, and motion\n\n\u2705 \ud83e\udd0f Smaller, two-slot design makes this GPU a better fit for more PC builds\n\nCons\n\n\u274c\ud83e\udead Double flow-through design doesn\u2019t work for vertical GPUs or dual-chamber cases\n\n\u274c\ud83e\udd11 $2,000 price tag will be a steep investment for most people\n\n\u274c\ud83d\udc7e Multi-frame generation can introduce large blotches of artifacts\n\nBest Buy: Nvidia RTX 5090 FE\n\nShortcut review\n\nThe Nvidia Geforce RTX 5090 isn\u2019t the Jesus messiah savior of graphics cards, but it is a very impressive graphics card all around. Nvidia has managed to pack in more graphical power than ever into a smaller GPU. I love the new dual-slot, double flow-through design of the card. It\u2019s a marvel of industrial to look at and hold, and it\u2019s the closest thing to art I\u2019ve seen from computer components.\n\nNvidia\u2019s new flagship RTX 50-series GPU can deliver 4K 60fps or higher gaming with full ray-tracing without any supporting AI-based DLSS or frame generation. With DLSS 4 and its new multi-frame generation technology, the RTX 5090 makes 200 to 300 \u2013 and even 400 fps 4K gaming a regular affair. Some may argue those frames aren\u2019t real and don\u2019t count, but DLSS and AI-enhanced frame rates are becoming more necessary everyday with higher frame rate and resolution displays.\n\nThe greater AI focus Nvidia is putting into its graphics cards is also just the first step to a total revolution in how computer graphics are rendered. DLSS and AI has always been a value add for Nvidia\u2019s graphics cards, but now its a headlining feature for the RTX 5090, that also helps futureproof it while making it potentially the first GPU to support 8K 120fps displays.\n\n(credit: Kevin Lee / The Shortcut)\n\n\ud83e\udd0f Smaller two-slot card. The Nvidia RTX 5090 is finally back to being a dual-slot-sized graphics card. That should make Nvidia\u2019s flagship GPU a better fit for Mini-ITX and SFF builds since the xx90 cards ballooned to a triple-slot size with the RTX 3090. It\u2019s also an amazing feat that Nvidia has managed to pack the RTX 5090\u2019s greater number of transistors into a smaller card and PCB.\n\n\ud83d\udca8 Double flow-through design. As I just mentioned, the Nvidia RTX 5090\u2019s overall PCB is even smaller now, so almost the whole card is fully see-through. Shinning a light through the card reveals both fans can freely push air through a series of heat pipes and the radiator grills. Comparatively, the last RTX 4090 Founders Edition only had a flow-through heatsink on the front half of it. It\u2019s an effective cooler design that kept temps between 70-81\u00b0F on full load. Even though it ran hotter than the RTX 4090, that\u2019s to be expected when the TGP goes from 450W to 575W. and I saw it maintained 5-8 lower temperatures than the RTX 4090 while emitting less of a whine when the fans went full speed.\n\n(credit: Kevin Lee / The Shortcut)\n\n\ud83e\udead Watch your airflow. Now, this new double flow-through cooler does remove the usual back exhaust port seen on virtually every other GPU. Since this graphics card only pushes up and through the fins, it works well if there\u2019s an exhaust fan directly above it. However, this cooler design won\u2019t work well if you have it vertically oriented, as it will just blow hot air onto your motherboard. It also might not work well in dual-chamber setups or most SFF cases, as the exhaust will just hit the separator or other components positioned right behind the GPU.\n\n\ud83d\udccb Core specs. The Nvidia RTX 5090 features 92 billion transistors with 21,760 CUDA Cores. Beyond that, Blackwell brings bigger architectural improvements, including 5th Generation Tensor cores, 4th Generation Ray Tracing cores, and 32GB of GDDR7 video memory. Nvidia touts that the RTX 5090 delivers almost three times the AI Tops of 3,352 than the RTX 4090\u2019s 1,321 AI Tops. That largely plays into Nvidia\u2019s latest AI supersampling and frame generation technology, DLSS 4, which promises to help the RTX 5090 outperform the RTX 4090 by two times.\n\n\ud83d\udcca Down to the numbers. The Nvidia RTX 5090 is a higher-performance graphics card than the RTX 4090. There\u2019s no doubt about it from the benchmark numbers I recorded. It\u2019s not quite two times more when looking at the natively rendered frame rates or even with DLSS factored in. The real game changer is DLSS 4 and multi-frame generation that lets you play games at 4K maxed out with all the ray tracing at 200, 300, and even up to 400 frames per second.\n\n\ud83d\udc6f\u200d\u2642\ufe0f DLSS 4 multi-frame gen. The biggest changes the RTX 5090 and Blackwell architecture introduces is multi-frame generation to boost frame rates by up to four times. Frame generation first debuted in DLSS 3.0 on RTX 40-series cards, allowing Nivida\u2019s GPU to create a virtual frame for every real frame rendered. DLSS 4 takes frame generation to the next level, producing two to three virtual frames for every real one.\n\n\ud83e\udd16 Necessary AI enhancement. DLSS 4 multi-frame gen allows 4K games like Cyberpunk 2077, which normally run on the RTX 5090 at 57 fps without any DLSS, to suddenly shoot up to 243 fps. That\u2019s a lot of frames, but it feels almost necessary for performance-punishing games like Alan Wake II and Silent Hill 2. Black Myth 2 struggled to run at a solid 60 fps at 4K with very high ray tracing and high settings, but then we saw it running at almost 250 fps with DLSS 4. Nvidia\u2019s latest multi-frame generation technology comes at the right time when 4K 240fps gaming monitors become more readily available. Plus, the addition of DisplayPort 2.1 on the back of the RTX 5090, means these cards will be able to support up to 8K 120fps displays in the future.\n\nYou can peer through the Nvidia RTX 5090's double flow-through cooler (credit: Kevin Lee / The Shortcut)\n\n\ud83e\ude84 Works like magic. Amazingly, it\u2019s impossible to distinguish between a real frame and a virtual one, even with DLSS 4 turned up to max. DLSS frame generation does add a few more milliseconds of latency, but it only peaked at 20-45ms in games like Marvel Rivals and Hogwarts Legacy. One. Jumping to higher levels of frame generation doesn\u2019t increase latency either. I was only able to enjoy multi-frame gen in a few games ahead of RTX 50-series launch, but Nvidia has promised 75 games will support DLSS 4 (including God of War Ragnar\u00f6k and Indiana Jones and the Great Circle) on January 30.\n\n\ud83d\udc94 Frame tearing. The only unfortunate thing about DLSS 4 multi-frame gen is it can sometimes create unsightly frame tearing. I call it frame tearing because it looks like large blocks of frames are popping out in large rectangular artifacts. It rarely occurred during my testing and only when I quickly whipped the camera from looking at an intricate texture (like a mountain or a bookshelf) to a completely other uniform texture (like a blue sky). I also only saw it occur in Hogwarts Legacy, which leads me to believe you may encounter it in open-world games like Dragon\u2019s Dogma or Elden Ring. Hopefully, this is just a minor bug that Nvidia can fix in the near future.\n\nThis 575W GPU requires some serious power and four eight-pin PCIe power connectors (credit: Kevin Lee / The Shortcut)\n\n\ud83d\udee0\ufe0f Another DLSS 4 upgrade. Aside from multi-frame generation, DLSS 4 is also giving plain-old AI super sampling, ray reconstruction, and deep-learning anti-aliasing a shot in the arm. DLSS 4 introduces a new transformer model that basically looks at more of the overall image to improve detail, lighting, and motion. During my review, I was able to test this out with CyberPunk 2077, and I saw improvements in ray-traced sun streaks. I could also see smoother motion in Alan Wake II.\n\n\ud83d\udcc8 About that higher price. There\u2019s no beating around the bush about it: the $2000 RTX 5090 is a more expensive card than the $1,600 RTX 4090 it\u2019s replacing. That\u2019s in contrast to how Nvidia actually drove down the prices of the rest of its 50-series cards, including the $549 Nvidia RTX 5070. The performance gulf between the Nvidia RTX 5090 and RTX 5080 has never been wider. In my Nvidia RTX 5080 review I found it to be a fantastic GPU for 4K gaming, but the RTX 5090 lets you do far more ridiclous\n\n\ud83d\udcc5 When can I buy it? The Nvidia RTX 5090 (and RTX 5080) will launch soon on January 30. Preorders haven\u2019t opened or been announced yet, but hopefully, that will change in just a few days.\n\nShould you buy the Nvidia Geforce RTX 5090?\n\nYes, if\u2026\n\n\u2705 \ud83d\udcfa You own a high-frame-rate gaming monitor or TV\n\n\u2705 \ud83c\udfc3\u200d\u2642\ufe0f You want the a faster 4K gaming experience\n\n\u2705 \ud83d\udddc\ufe0f You\u2019re itching to build a smaller, more powerful gaming PC\n\nNo, if\u2026\n\n\u274c \ud83d\udc40 You\u2019re taken out of your gaming immersion at the slightest visual artifacts\n\n\u274c \ud83d\udcb8 You\u2019re not ready to drop $2K on one PC part (get the Nvidia RTX 5080 instead)\n\n\u274c \ud83d\udda5\ufe0f You prefer to build small mini-itx systems\n\nKevin Lee is The Shortcut\u2019s Creative Director. Follow him on Twitter @baggingspam.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia RTX 5090 Graphics Card Review \u2014 Get Neural Or Get Left Behind",
            "link": "https://www.forbes.com/sites/moorinsights/2025/01/23/nvidia-rtx-5090-graphics-card-review---get-neural-or-get-left-behind/",
            "snippet": "With its AI capabilities enabled, the RTX 5090 is the fastest and best-performing graphics card in the world. Can Nvidia's competitors catch up?",
            "score": 0.5613154768943787,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Nvidia GeForce RTX 5090 Founders Edition Review",
            "link": "https://www.ign.com/articles/nvidia-geforce-rtx-5090-founders-edition-review",
            "snippet": "The Nvidia GeForce RTX 5090 is one of the most powerful (and expensive) graphics cards ever, and uses AI to further enhance its gaming performance.",
            "score": 0.9417252540588379,
            "sentiment": null,
            "probability": null,
            "content": "Every couple of years, Nvidia launches an extremely expensive, extremely powerful graphics card that brings PC gaming into a new generation. That is what the Nvidia GeForce RTX 5090 ultimately is, but the way it brings next-generation performance is unconventional, to say the least. Because in a lot of games, the performance uplift over the RTX 4090 isn\u2019t quite as steep as you\u2019d expect \u2013 at least when DLSS Frame Generation is taken out of consideration. With the next generation of Nvidia\u2019s DLSS for both upscaling and frame generation, however, we get leaps in image quality and performance that feel even greater than what we see with a typical graphics generation.\n\nHow much of an upgrade the Nvidia RTX 5090 is going to be for you, then, is ultimately going to depend on the games you play, the resolution you play those games at, and whether you\u2019re ok with an AI algorithm generating extra frames. For a lot of people playing games on anything less than a 4K monitor with a 240Hz refresh rate, this upgrade is simply not going to make a lot of sense. But if you do have a high-end display, these AI-generated frames are going to feel like a taste of the future.\n\nRTX 5090 too rich for you? Check out the RTX 5080 review.\n\nNvidia GeForce RTX 5090 \u2013 Photos 5 Images\n\nRTX 5090 \u2013 Specs and Features\n\nThe Nvidia GeForce RTX 5090 is built on Blackwell, Nvidia\u2019s high-end architecture that\u2019s already powering the data centers and supercomputers behind many of the most popular AI models. That should give you an idea of what the RTX 5090 is especially good at, but Nvidia didn\u2019t neglect the, well, non-AI parts of the card.\n\nWith the 5090, Nvidia found a way to shove more Streaming Multiprocessors (SMs) into the same amount of GPCs (Graphics Processing Clusters), which means more CUDA cores \u2013 21,760, up from 16,384 in the RTX 4090. That makes up for a 32% uplift in the amount of shader cores over the previous generation, and is where a bulk of the raw gaming performance comes from.\n\nEach SM also has four Tensor Cores and one RT Core, just like its predecessor. That means you get 680 Tensor Cores and 170 RT cores, compared to 512 and 128, respectively, for the RTX 4090. The 5th-generation Tensor Cores are tailor-made to boost AI performance, with this generation adding support for FP4 operations, which should make AI workloads less dependent on VRAM.\n\nAll of this silicon is coupled with 32GB of GDDR7 VRAM, or video memory. This is a generational shift from the GDDR6X memory in the RTX 4090, and should be faster and more power efficient than the previous generation. But because the RTX 5090 requires a staggering 575W of power, a huge increase over the already power hungry 4090, power efficiency isn\u2019t exactly Nvidia\u2019s main goal with this graphics card.\n\nBecause the new Tensor Cores are more efficient, Nvidia shifted the entire DLSS algorithm to run on a Transformer Neural Network (TNN), rather than a Convolutional Neural Network (CNN). This shift won\u2019t necessarily improve your framerate when you enable DLSS, but Nvidia claims it will improve image quality, and mitigate issues like ghosting and other unwanted artifacts.\n\nNvidia did more than just make an under-the-hood change to the way DLSS works. Team Green also introduces Multi-Frame Generation, which takes the Frame Gen tech introduced with the RTX 4090, makes it more efficient and smooth, and allows it to generate multiple frames off of each rendered image. This drastically improves frame rate, but should probably only be enabled if you\u2019re already getting a decent frame rate, just like the last generation version.\n\nPurchasing Guide The Nvidia GeForce RTX 5090 is available January 30, starting at $1,999. Keep in mind that this is just a starting price for the Founders Edition, and third-party cards can be much more expensive. Check out our Nvidia 50-series Preorder Guide for the latest info on where to buy across retailers.\n\nThe Founders Edition\n\nThe Nvidia GeForce RTX 5090 requires 575W of power, which is much more than the 450W of the RTX 4090. More power inherently means more heat, which means increasingly powerful cooling solutions are needed. Looking back at the RTX 4090 and even the RTX 3090, the Founders Editions were these giant, triple-slot graphics cards that took up a ton of room, and straight up wouldn\u2019t fit in some PC cases.\n\nBefore I saw the RTX 5090, I was expecting something even bigger and more unwieldy. However, somehow, it\u2019s smaller. Nvidia was able to make a 575W graphics card fit in a dual-slot chassis with a dual fan configuration. And it works.\n\nThroughout my time with the RTX 5090, which included both my standard testing suite and playing games with DLSS 4 enabled to test multi frame generation, the temperature maxed out at around 86\u00b0C, even while its power consumption peaked at 578W. That\u2019s a high temperature, to be sure, and higher than the RTX 4090\u2019s 80\u00b0C, but it\u2019s not high enough to throttle, and that\u2019s all that matters.\n\nNvidia was able to do this by shrinking down the PCB (printed circuit board) to a little square and placing it in the middle of the graphics card. The two fans are placed on each side of where the PCB is, with a heatsink that runs through the width of the card. These fans then take air in through the bottom of the card and shoot it straight through the top of the card and out through your exhaust fans in your PC case. In fact, this graphics card doesn\u2019t even have exhaust vents under the output ports at the rear of the card, unlike previous generation designs.\n\nBut it\u2019s immediately apparent that the RTX 5090 Founders Edition follows a similar design language to the last couple of generations. The center of the card has the same silver \u2018X\u2019 design as the RTX 4090, with a gunmetal-gray chassis surrounding the black heatsinks. On the outer edge of the graphics card, you get a \u2018GeForce RTX\u2019 logo that lights up with white LEDs, too.\n\nNext to that logo, you\u2019re going to find the power connector. While it looks very similar to the 12VHPWR connector of the last generation, it\u2019s actually a new 12V-2x6 power connector. The difference is minor, but it\u2019s supposedly more efficient than the last-generation connector. Maybe Nvidia will be available to avoid controversy around its power connectors melting with this generation \u2013 only time will tell.\n\nNvidia does include an 12V-2x6 adapter in the box, which takes four 8-pin PCIe power connectors in order to provide the required 575W of power. But what\u2019s nice is that the connector on the graphics card itself is now angled, and facing the back of the graphics card, which should make connecting the cable much easier. The power connector seems more secure this time around, too.\n\nThe hidden benefit of this design is its ability to be slotted into smaller PC builds. You don\u2019t need a giant case to run the RTX 5090, like you did with the RTX 4090 and 3090. However, it\u2019s very likely that third-party designs from the likes of Asus and MSI are going to be much larger than Nvidia\u2019s Founders Edition.\n\nDLSS 4: Fake Frames?\n\nWhen Nvidia revealed the RTX 5090, it claimed that it could boost performance by as much as 8x. The actual number isn\u2019t that high, but the RTX 5090 can deliver extremely high frame rates in the most demanding games, but not exactly through traditional rendering. Because while the RTX 5090 does deliver a decent increase in raw rasterization performance, the real next-generation benefit is in its ability to generate extra frames to increase your frame rate.\n\nDLSS 4 introduces \u2018Multi-Frame Generation\u2019, a next-gen version of the Frame Generation introduced with DLSS 3 and the RTX 4090. But it\u2019s more than just using the same method to just produce more frames. And the secret behind it is a new AI Management Processor, or AMP core in the RTX 5090 \u2013 along with other RTX 5000 graphics cards. The AMP allows the graphics card to essentially assign work to different parts of the GPU, something that was traditionally handled by your CPU. But because it\u2019s physically located on the GPU, it\u2019s able to do this much more efficiently.\n\nCourtesy of Nvidia\n\nAccording to Nvidia, the AMP and the 5th-generation Tensor Cores allowed it to create a new frame generation model that\u2019s both 40% faster than the original frame generation model, while requiring 30% less memory. This new model only needs to run once on each rendered frame, which then can create 3 AI frames. Something like this would naturally introduce latency, but Nvidia found a way around that, too.\n\nThe AMP runs a Flip Metering algorithm, which paces out the frames in order to reduce input lag. Nvidia claims this is why multi-frame generation won\u2019t work on RTX 4000 graphics cards, as the last-generation frame generation relied on the CPU for frame pacing, which would introduce much more latency than the new model, which runs entirely on the GPU itself.\n\nTo be clear, this isn\u2019t a magic button to get good performance. Just like the previous generation, you only really want to enable this if you\u2019re already getting a passable frame rate. If you\u2019re not already getting around 60 fps with Frame Gen disabled, turning it on can introduce significant latency problems. That\u2019s why it pairs best with DLSS upscaling also enabled, in order to maximise your performance.\n\nWhen the RTX 5090 hits store shelves on January 30, DLSS 4 will work in a wide array of PC games that already support DLSS 3 Frame Generation. However, while working on this review, I only had access to two games with this technology enabled, and both were on beta builds \u2013 Cyberpunk 2077 and Star Wars Outlaws.\n\nAnd I was surprised how well it worked. In Cyberpunk 2077 at 4K, on the Ray Tracing Overdrive Preset, with DLSS on Performance mode, the RTX 5090 gets 94 fps. That\u2019s not bad for a game with full ray tracing. When I turned on DLSS 2x frame gen \u2013 the same as is supported by the RTX 4090 \u2013 that framerate increased to 162 fps. That\u2019s a 2x improvement over just plain ol\u2019 DLSS. However, when I cranked the frame generation to 4x, that\u2019s 3 AI frames per rendered frame, that number went all the way up to 286 fps \u2013 more than my 4K display can actually render.\n\nIt\u2019s a similar story with Star Wars Outlaws. Playing at 4K with all the settings cranked up to max, I was able to get up to around 300 fps with DLSS 4 enabled \u2013 and that\u2019s up from about 120 fps without frame generation.\n\n\u201c Multi-Frame Generation actually does work.\n\nWhen I saw these framerates, I was straight-up expecting to see artifacts and weird spikes of lag. However, I only really saw one broken texture in Star Wars Outlaws, and it\u2019s something I wouldn\u2019t have noticed if I wasn\u2019t actively looking for problems. It\u2019s hard to believe, but Multi-Frame Generation actually does work, you just need to have an extremely high-end 4K display to benefit from it, at least with the RTX 5090.\n\nIt\u2019s easy to write off this performance as \u2018Fake Frames\u2019, and you wouldn\u2019t necessarily be wrong, especially because you need good baseline performance to make it a good experience. But it is going to be genuinely useful for anyone with a high-refresh, high-resolution display. It\u2019s also important to keep in mind that I was only able to test it in a handful of games. Nvidia claims that 75 games will support DLSS 4 when the RTX 5090 hits store shelves on January 30, and there\u2019s a decent possibility that it won\u2019t work flawlessly in at least one of those games. For the time being, though, it looks like it works extremely well.\n\nRTX 5090 \u2013 Performance\n\nThe Nvidia GeForce RTX 5090 is an incredibly powerful graphics card, but testing this thing was a journey. In 3DMark, the RTX 5090 proved itself to provide a generational improvement over the RTX 4090 in terms of raw performance. However, things get a lot more complicated when I test actual games. In the vast majority of games, the RTX 5090 is bottlenecked by my CPU, even at 4K \u2013 and I paired it with the Ryzen 7 9800X3D, the fastest gaming processor on the market right now. For most people who already have a high-end graphics card, upgrading to this $1,999 GPU isn\u2019t going to make a world of difference \u2013 the games just aren\u2019t there yet. This is a graphics card you buy to set yourself up for future PC games, like The Witcher 4.\n\nI want to note that I did not enable DLSS 4 for any of these comparative benchmarks, and everything was tested on the public drivers available at the time. That means all non-5090 Nvidia cards were tested on Driver version 566.36, and all AMD cards were tested on AMD Adrenalin 24.12.1. All games were tested on their latest public builds, too.\n\nTest System CPU AMD Ryzen 7 9800X3D Motherboard Asus ROG Crosshair X870E Hero RAM 32GB G.Skill Trident Z5 Neo @ 6,000MHz SSD 4TB Samsung 990 Pro CPU Cooler Asus ROG Ryujin III 360\n\nIn 3DMark, the RTX 5090 is up to 42% faster than the RTX 4090. In the Speed Way benchmark, the RTX 5090 scores 14,399 points to the RTX 4090\u2019s 10,130, making for a 42% performance uplift. Similarly in Port Royal, the ray tracing test, the RTX 5090 scores 36,946 points to the 4090\u2019s 25,997 points, which is also a 42% performance leap. What\u2019s more impressive is how far Team Green has come since the RTX 3090. That graphics card got 5,619 points in Speed Way and 13,738 points in Port Royal, meaning anyone that skipped the last generation can get a 2.5x performance jump. That\u2019s just 3DMark, though, and not necessarily reflective of real-world gaming performance.\n\nIn Call of Duty Black Ops 6, however, we start to see the big issue with the RTX 5090 in today\u2019s games \u2013 a severe CPU bottleneck. At 4K Extreme settings, with DLSS set to \u2018performance\u2019, the RTX 5090 gets 161 fps, compared to 146 fps from the RTX 4090. That\u2019s just a 10% performance difference, and definitely not what I\u2019d call a \u2018next-generation\u2019 performance increase. However, when looking back at the RTX 3090, the 4-year-old graphics card gets 91 fps, which means a nearly 2 x performance increase.\n\nWhat\u2019s wild is that the RTX 5090 even shows signs of CPU bottleneck in Cyberpunk 2077, one of the most demanding PC games on the market right now. At 4K, with the Ray Tracing Ultra preset and DLSS set to performance, the RTX 5090 gets 125 fps, compared to 112 fps from the RTX 4090 with the same settings. Similarly to Black Ops 6, this is just a 10% performance increase. That scaling just gets worse at lower resolutions, though, with the RTX 5090 getting 153 fps at 1440p and 156 fps at 1080p.\n\nI test Metro Exodus: Enhanced Edition with DLSS disabled, because it\u2019s the only upscaling solution in that game, and I need to get an honest comparison with AMD cards. This makes it one of the most demanding tests in my suite, and gives the RTX 5090 a chance to stretch its legs a bit. At 4K with the Extreme preset, the RTX 5090 gets 95 fps, compared to 76 fps from the RTX 4090 and 44 fps from the Radeon RX 7900 XTX. But even without the upscaling, the RTX 5090 only gets a 25% improvement over the RTX 4090, even if it more than doubles the 39 fps from the RTX 3090.\n\nRed Dead Redemption 2 is getting up there in years, but it\u2019s still a gorgeous game. At 4K with every setting maxed out, and DLSS set to performance mode, the RTX 5090 gets 167 fps, compared to 151 from the RTX 4090 and 92 fps from the RTX 3090. That means in this game, which admittedly doesn\u2019t even use ray tracing, the RTX 5090 gets a paltry 6% performance uplift over the RTX 4090.\n\nNvidia GeForce RTX 5090 \u2013 Benchmarks 14 Images\n\nTotal War: Warhammer 3 is an interesting test these days, because it doesn\u2019t support ray tracing or upscaling, and gives a clear picture on raw rasterization performance. The RTX 5090 impresses here, delivering 147 fps to the RTX 4090\u2019s 107 fps. That\u2019s a 35% performance uplift and close to the potential performance difference demonstrated in 3DMark. However, it\u2019s still a far cry from the 67% performance difference enjoyed by the RTX 4090 over the 3090.\n\nAssassins Creed Mirage is a weird one. For some reason, when I first benchmarked this game, the RTX 5090 was getting terrible performance. Its game clock was limited to 772MHz, and was giving me around 50 fps at 4K. I was able to get around that problem, but even when it was resolved the 5090 only got 172 fps, which is lower than the RTX 4090 at 183 fps. What\u2019s worse, is that the framerate was extremely spikey with microstutters. That\u2019s bad, obviously, but it\u2019s very likely that this is a driver bug, and as such should be treated as an outlier.\n\nWhich new graphics card are you planning to buy? Nvidia GeForce RTX 5090 Nvidia GeForce RTX 5080 Nvidia GeForce RTX 5070 Ti Nvidia GeForce RTX 5070 I'm waiting to see AMD's new cards Not planning on upgrading this generation Answer See Results\n\nBlack Myth: Wukong, like Cyberpunk 2077, is an extremely demanding game that will push any GPU to its limits. The RTX 5090, however, averaged 104 fps at 4K, with the Cinematic Preset and DLSS set to 40%. The RTX 4090, with identical settings, got 84 fps. That\u2019s a 20% uplift in favor of the RTX 5090.\n\nIn Forza Horizon 5, the RTX 5090 averaged 216 fps, compared to 210 fps from the RTX 4090, which is essentially in the margin of error. This is an aging game, to be sure, but the CPU bottleneck means there\u2019s essentially no difference between these two cards at this resolution.\n\nNvidia really wants us to believe that Moore\u2019s Law is dead and GPUs that deliver a giant uplift over their previous-generation counterparts are going to grow more rare over time. I don\u2019t know if that\u2019s true \u2013 I\u2019m not an engineer \u2013 but regardless, in most games, the RTX 5090 doesn\u2019t exactly deliver next-generation performance over the RTX 4090 \u2013 at least not to the extent that the latter card thoroughly trounced the 3090 back in 2022.\n\nThat\u2019s not to say the RTX 5090 is a bad graphics card. No matter how you slice it, the RTX 5090 is now the fastest graphics card on the consumer market, and that\u2019s not nothing. The problem is that a lot of games can\u2019t really take advantage of the extra power offered by the Blackwell GPU. That\u2019s something that will absolutely change over time, but it also means there\u2019s little reason for someone with an RTX 4090 to upgrade to the new hotness.\n\nInstead, the Nvidia GeForce RTX 5090 is betting its existence on the future of AI-powered gaming. DLSS 4 uses AI to greatly increase frame rates, which is definitely a sight to behold. This graphics card is therefore best for gamers that want to be on the cutting edge, and are willing to bet $1,999 (at least) on an AI gaming future. For everyone else, the RTX 4090 is going to be more than powerful enough for the next few years.\n\nJackie Thomas is the Hardware and Buying Guides Editor at IGN and the PC components queen. You can follow her @Jackiecobra",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Could Nvidia Stock Be Due for a Crash in 2025? Here's What History Says.",
            "link": "https://www.fool.com/investing/2025/01/23/could-nvidia-stock-be-due-for-a-crash-in-2025-here/",
            "snippet": "Nvidia experienced four separate 50% drawdowns in its history. The company is booming right now, but a cyclical downturn could be around the corner.",
            "score": 0.6802759766578674,
            "sentiment": null,
            "probability": null,
            "content": "There are no guarantees in investing. That's the hard truth we all have to learn. Some investments come with low risks while others have a high risk of not working out, but none come with zero risks. When you buy a stock, you need to weigh its potential upside and money-making potential without ignoring the downside.\n\nToday, many people divided over Nvidia (NVDA 5.27%). After rocketing higher in the last few years and becoming one of the largest companies in the world by market value, bulls are pounding the table that the party will continue in 2025. Bears are calling for a crash in Nvidia stock and think it is overvalued. While both sides will talk with certainty, it's impossible to guarantee what will happen with Nvidia in 2025. If it's possible the stock will surge higher or crash, what's the likelihood of either scenario?\n\nUsing history as a guide, let's try to figure out the likelihood of an Nvidia stock crash in 2025 and if you should buy shares of the stock for your portfolio today.\n\nDecades of history for a cyclical company\n\nNvidia is one of the best-performing stocks in the last few decades. Shares of the stock are up 335,000% since going public in 1999, or a compound annual growth rate (CAGR) of 30%. However, there have been multiple stock crashes along the way. If we define a stock crash as a drawdown of 50% or more, Nvidia has gone through four stock crashes since going public: 2001, 2008, 2018, and most recently in 2022. Yes, it was just a few years ago that Nvidia was disliked by the investment community.\n\nWhy such erratic behavior from investors? Well for one, this is how markets tend to operate, even for the best businesses in the world. Two, Nvidia operates in the cyclical semiconductor industry. A cyclical industry is one with inconsistent end-market demand from customers, which causes volume fluctuations and volatility in the income statement. Semiconductors are cyclical because of inconsistent demand from computer chip buyers. You can see this in Nvidia's revenue chart, which has periodic dips and revenue declines.\n\nIt has been 26 years since Nvidia has gone public. The stock has crashed in four of these years. So it's certainly possible that Nvidia's stock price will reverse and turn south in 2025.\n\nIn fact, I think 2025 is a year that is more likely than most of a drop in stock price. Here's why.\n\nA profit margin jump to stay cautious about\n\nWhen looking at a stock that operates in a cyclical industry, you need to try to analyze what part of the cycle we are in. For Nvidia, we are definitely in an uptrend in the cycle. Revenue is soaring on the back of new demand in artificial intelligence (AI). Perhaps more importantly, its operating margin is at an all-time high of 63%, which shows that the company is implementing sizable price increases due to what seems like insatiable customer demand for its AI products.\n\nEventually, Nvidia's supply of semiconductors will balance out with customer demand. This will lead to price stabilization, slowing revenue growth, and perhaps a turning of the semiconductor cycle (at least in Nvidia's niche). That doesn't mean that it's a guarantee that Nvidia's revenue will turn over in 2025, but it isn't impossible. It has happened multiple times in the last 25 years, usually following periods of soaring revenue.\n\nThis is the nature of investing in a cyclical industry. Revenue may go up over the long term, but there will almost assuredly be lumps along the way.\n\nShould you buy Nvidia stock?\n\nAt today's stock price, Nvidia has a market cap of $3.5 trillion. It has a price-to-earnings ratio (P/E) of 54, which is close to twice the level of the S&P 500 index's average P/E of 30.\n\nI think it is likely that Nvidia's revenue will grow over the long term, which should lead to earnings growth. But today the company is sporting a record operating margin that will likely revert back to a lower level once this AI boom levels off. If supply catches up with demand -- as it always eventually does in semiconductors -- then Nvidia's selling prices and profit margins per chip are likely to fall. This is a double whammy of earnings headwinds from lower revenue and lower profit margins.\n\nNvidia currently has a net income margin of 55%, leading to $63 billion in earnings on $113 billion in revenue. If revenue goes through a downcycle in 2025 and falls to $100 billion but profit margins slip back to 40%, its net income will fall to $40 billion. Compared to a market cap of $3.37 trillion, that is a nosebleed P/E ratio of 84. Nvidia's stock is likely lower in this scenario.\n\nThis is a great business, but one trading with sky-high expectations and a chance for a cyclical downturn to occur in the near future. Skip out buying Nvidia stock today. If you like the company and think it is a great business, wait and buy during the eventual cyclical downturn for the semiconductor market, not close to the peak.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-22": {
        "0": {
            "title": "How AI Helps Fight Fraud in Financial Services, Healthcare, Government and More",
            "link": "https://blogs.nvidia.com/blog/how-ai-helps-fight-fraud/",
            "snippet": "Companies and organizations are increasingly using AI to protect their customers and thwart the efforts of fraudsters around the world.",
            "score": 0.9291395545005798,
            "sentiment": null,
            "probability": null,
            "content": "How AI Helps Fight Fraud in Financial Services, Healthcare, Government and More\n\nCompanies and organizations are increasingly using AI to protect their customers and thwart the efforts of fraudsters around the world.\n\nVoice security company Hiya found that 550 million scam calls were placed per week in 2023, with INTERPOL estimating that scammers stole $1 trillion from victims that same year. In the U.S., one of four noncontact-list calls were flagged as suspected spam, with fraudsters often luring people into Venmo-related or extended warranty scams.\n\nTraditional methods of fraud detection include rules-based systems, statistical modeling and manual reviews. These methods have struggled to scale to the growing volume of fraud in the digital era without sacrificing speed and accuracy. For instance, rules-based systems often have high false-positive rates, statistical modeling can be time-consuming and resource-intensive, and manual reviews can\u2019t scale rapidly enough.\n\nIn addition, traditional data science workflows lack the infrastructure required to analyze the volumes of data involved in fraud detection, leading to slower processing times and limiting real-time analysis and detection.\n\nPlus, fraudsters themselves can use large language models (LLMs) and other AI tools to trick victims into investing in scams, giving up their bank credentials or buying cryptocurrency.\n\nBut AI \u2014 coupled with accelerated computing systems\u2014 can be used to check AI and help mitigate all of these issues.\n\nBusinesses that integrate robust AI fraud detection tools have seen up to a 40% improvement in fraud detection accuracy \u2014 helping reduce financial and reputational damage to institutions.\n\nThese technologies offer robust infrastructure and solutions for analyzing vast amounts of transactional data and can quickly and efficiently recognize fraud patterns and identify abnormal behaviors.\n\nAI-powered fraud detection solutions provide higher detection accuracy by looking at the whole picture instead of individual transactions, catching fraud patterns that traditional methods might overlook. AI can also help reduce false positives, tapping into quality data to provide context about what constitutes a legitimate transaction. And, importantly, AI and accelerated computing provide better scalability, capable of handling massive data networks to detect fraud in real time.\n\nHow Financial Institutions Use AI to Detect Fraud\n\nFinancial services and banking are the front lines of the battle against fraud such as identity theft, account takeover, false or illegal transactions, and check scams. Financial losses worldwide from credit card transaction fraud are expected to reach $43 billion by 2026.\n\nAI is helping enhance security and address the challenge of escalating fraud incidents.\n\nBanks and other financial service institutions can tap into NVIDIA technologies to combat fraud. For example, the NVIDIA RAPIDS Accelerator for Apache Spark enables faster data processing to handle massive volumes of transaction data. Banks and financial service institutions can also use the new NVIDIA AI workflow for fraud detection \u2014 harnessing AI tools like XGBoost and graph neural networks (GNNs) with NVIDIA RAPIDS, NVIDIA Triton and NVIDIA Morpheus \u2014 to detect fraud and reduce false positives.\n\nBNY improved fraud detection accuracy by 20% using NVIDIA DGX systems. PayPal improved real-time fraud detection by 10% running on NVIDIA GPU-powered inference, while lowering server capacity by nearly 8x. And Swedbank trained generative adversarial networks on NVIDIA GPUs to detect suspicious activities.\n\nUS Federal Agencies Fight Fraud With AI\n\nThe United States Government Accountability Office estimates that the government loses up to $521 billion annually due to fraud, based on an analysis of fiscal years 2018 to 2022. Tax fraud, check fraud and improper payments to contractors, in addition to improper payments under the Social Security and Medicare programs have become a massive drag on the government\u2019s finances.\n\nWhile some of this fraud was inflated by the recent pandemic, finding new ways to combat fraud has become a strategic imperative. As such, federal agencies have turned to AI and accelerated computing to improve fraud detection and prevent improper payments.\n\nFor example, the U.S. Treasury Department began using machine learning in late 2022 to analyze its trove of data and mitigate check fraud. The department estimated that AI helped officials prevent or recover more than $4 billion in fraud in fiscal year 2024.\n\nAlong with the Treasury Department, agencies such as the Internal Revenue Service have looked to AI and machine learning to close the tax gap \u2014 including tax fraud \u2014 which was estimated at $606 billion in tax year 2022. The IRS has explored the use of NVIDIA\u2019s accelerated data science frameworks such as RAPIDS and Morpheus to identify anomalous patterns in taxpayer records, data access and common vulnerability and exposures. LLMs combined with retrieval-augmented generation and RAPIDS have also been used to highlight records that may not be in alignment with policies.\n\nHow AI Can Help Healthcare Stem Potential Fraud\n\nAccording to the U.S. Department of Justice, \u200b\u200bhealthcare fraud, waste and abuse may account for as much as 10% of all healthcare expenditures. Other estimates have deemed that percentage closer to 3%. Medicare and Medicaid fraud could be near $100 billion. Regardless, healthcare fraud is a problem worth hundreds of billions of dollars.\n\nThe additional challenge with healthcare fraud is that it can come from all directions. Unlike the IRS or the financial services industry, the healthcare industry is a fragmented ecosystem of hospital systems, insurance companies, pharmaceutical companies, independent medical or dental practices, and more. Fraud can occur at both provider and patient levels, putting pressure on the entire system.\n\nCommon types of potential healthcare fraud include:\n\nBilling for services not rendered\n\nUpcoding: billing for a more expensive service than the one rendered\n\nUnbundling: multiple bills for the same service\n\nFalsifying records\n\nUsing someone else\u2019s insurance\n\nForged prescriptions\n\nThe same AI technologies that help combat fraud in financial services and the public sector can also be applied to healthcare. Insurance companies can use pattern and anomaly detection to look for claims that seem atypical, either from the provider or the patient, and scrutinize billing data for potentially fraudulent activity. Real-time monitoring can detect suspicious activity at the source, as it\u2019s happening. And automated claims processing can help reduce human error and detect inconsistencies while improving operational efficiency.\n\nData processing through NVIDIA RAPIDS can be combined with machine learning and GNNs or other types of AI to help better detect fraud at every layer of the healthcare system, assisting patients and practitioners everywhere dealing with high costs of care.\n\nAI for Fraud Detection Could Save Billions of Dollars\n\nFinancial services, the public sector and the healthcare industry are all using AI for fraud detection to provide a continuous defense against one of the world\u2019s biggest drains on economic activity.\n\nThe NVIDIA AI platform supports the entire fraud detection and identity verification pipeline \u2014 from data preparation to model training to deployment \u2014 with tools like NVIDIA RAPIDS, NVIDIA Triton Inference Server and NVIDIA Morpheus on the NVIDIA AI Enterprise software platform.\n\nLearn more about NVIDIA solutions for fraud detection with AI and accelerated computing.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Opinion: The Biggest Risk Facing Nvidia Stock, and How the Company Will Solve It",
            "link": "https://www.fool.com/investing/2025/01/22/opinion-biggest-risk-nvidia-stock-company-will-sol/",
            "snippet": "The inevitable slowdown could be the biggest risk to Nvidia's stock price. However, CEO Jensen Huang might have already revealed the solution.",
            "score": 0.9245566725730896,
            "sentiment": null,
            "probability": null,
            "content": "The semiconductor industry has always been cyclical, because whether it's computers, smartphones, or even data centers, consumers and businesses only upgrade their physical hardware once every few years. That means revenue comes in big waves, followed by lengthy troughs.\n\nArtificial intelligence (AI) recently changed that. Since 2023, spending on data center chips and components appears to be growing exponentially, as some of the world's biggest technology companies race to develop the most powerful AI software.\n\nNvidia (NVDA 5.27%) has been the biggest beneficiary of that spending boom, because it supplies the most advanced data center graphics processors (GPUs) for developing AI. Nvidia is selling so many chips that it has become the second-largest company in the entire world, adding $3 trillion to its market capitalization over the last two years alone.\n\nBut data center spending can't continue at this pace forever, and the inevitable slowdown could be the biggest risk to Nvidia's stock price. However, CEO Jensen Huang might have already revealed the solution.\n\nData center spending could slow significantly in a few years\n\nThe ultimate goal for every company developing AI is to achieve artificial general intelligence (AGI), which is the point at which the technology matches human intelligence in most cognitive tasks. A researcher who used to work for ChatGPT creator OpenAI predicts AGI could arrive as soon as 2027. Tesla CEO Elon Musk believes 2029 is a more realistic target. In any case, it could be just a few years away.\n\nDeveloping AI beyond the point of AGI will almost certainly yield diminishing returns, because very few commercial workloads would benefit from such a high degree of machine intelligence. If that's the case, demand for Nvidia's data center GPUs could plunge a few years from now because the pool of developers who want (or who can afford) further performance increases will be very small.\n\nToday, the bulk of data center infrastructure spending comes from just a handful of trillion-dollar tech giants (more on that in a moment), and Nvidia is launching new generations of data center GPUs almost on an annual basis to meet their needs. It released the Hopper architecture in September 2022, the Blackwell architecture in March 2024, and reports suggest a new architecture called \"Rubin\" will be revealed by the end of 2025.\n\nThat isn't sustainable over the long term, not only because demand is likely to slow in a few years, but also because it cost Nvidia $10 billion to develop Blackwell alone, and research and development costs will only climb from there.\n\nBlackwell demand is so strong right now that Nvidia can't keep up, so there's nothing to worry about in the short term.\n\nNvidia's growth relies heavily on just a handful of customers\n\nNvidia is on track to generate a record $128.6 billion in total revenue during its fiscal year 2025, which ends later this month. That would be a whopping 112% increase from the prior year. If the first three quarters are anything to go by, Nvidia's data center segment will account for around 88% of that total revenue figure, primarily driven by GPU sales.\n\nAs I mentioned earlier, a small number of tech giants are responsible for most of the AI data center infrastructure spending. That's why during Nvidia's recent third quarter (ended Oct. 27), 36% of its $35 billion in total revenue came from just three unnamed customers:\n\nCustomer Proportion of Nvidia's Q3 Revenue Customer A 12% Customer B 12% Customer C 12%\n\nMorgan Stanley estimates just four companies alone will spend a combined $300 billion building AI data centers in 2025: Microsoft, Amazon, Alphabet, and Meta Platforms. Those names are likely some of Nvidia's mystery customers in the table above.\n\nIn my opinion, this heightens the risk Nvidia faces if AI spending slows down, because if even one of its top three customers abruptly pulls back, it will leave a massive hole in the company's revenue base that will be practically impossible to fill. That would threaten Nvidia's ability to maintain its incredible growth rates, which could result in a significant correction in its stock price.\n\nThe solution: Nvidia will materially diversify its revenue\n\nJensen Huang presented at the CES 2025 technology conference on Jan. 7, where he highlighted several new opportunities for Nvidia as the AI industry expands beyond the data center. Autonomous vehicles will be one of them, and Huang says it could be the first multitrillion-dollar segment of the emerging AI-powered robotics industry.\n\nNvidia already has an automotive business that is more than two decades old. It's currently home to the Drive platform, which is an end-to-end solution for car manufacturers seeking to install self-driving capabilities into their new vehicles. It includes all of the hardware and software manufacturers need, including Nvidia's powerful Thor chip, which processes data from the car's sensors to facilitate real-time decisions on the road.\n\nAutomotive giants like Toyota, Mercedes-Benz, Hyundai, BYD, Volvo, and more have already partnered with Nvidia. Aside from using the Drive platform, Huang says some of those companies are also buying DGX data center systems featuring Blackwell-based GP200 GPUs, so they have enough computing power to continuously train their self-driving models.\n\nMoreover, Nvidia just launched a new multimodal foundation model called Cosmos, which is trained on 20 million hours of video so it's equipped with an understanding of the real physical world. Car manufacturers can use it to run millions of simulations with synthetic data, which serves as training material for their autonomous driving software. This is yet another way for Nvidia to attract customers into its ecosystem.\n\nNvidia is on track to generate around $1.5 billion in revenue from its automotive segment in fiscal 2025. However, in fiscal 2026, Huang says that figure could more than triple to $5 billion. Wall Street's consensus forecast (provided by Yahoo!) suggests Nvidia will deliver $196 billion in total revenue in fiscal 2026, so the automotive segment will still be very small.\n\nHowever, if it grows at that pace consistently for the next few years, it could become a major part of Nvidia's business just in time for the potential slowdown in data center spending.\n\nPlus, Cathie Wood's Ark Investment Management believes autonomous ride-hailing alone -- just one use case for self-driving vehicles -- will create $14 trillion in enterprise value by calendar year 2027. The firm believes most of that value will come from autonomous platform providers like Nvidia, so it's possible this opportunity is even bigger than Huang expects right now.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Stock Rallies As Chipmaker Named To Stargate AI Venture",
            "link": "https://www.investors.com/news/technology/nvidia-stock-rallies-stargate-ai-project/",
            "snippet": "Nvidia (NVDA) stock rallied on Wednesday after the chipmaker was named a technology partner for a massive US artificial intelligence infrastructure initiative...",
            "score": 0.7154006361961365,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock rallied on Wednesday after the chipmaker was named a technology partner for a massive U.S. artificial intelligence infrastructure initiative called Project Stargate.\n\nSoftBank and OpenAI are the lead partners for Stargate, a new company that intends to invest $500 billion over the next four years building AI infrastructure for OpenAI in the United States. SoftBank has financial responsibility for the venture while ChatGPT creator OpenAI has operational responsibility. The initial equity funders in Stargate are SoftBank, OpenAI, Oracle (ORCL), and investment fund MGX.\n\nKey initial technology partners in Project Stargate are Arm (ARM), Microsoft (MSFT), Nvidia, Oracle and OpenAI.\n\nOn the stock market today, Nvidia stock popped 4.4% to close at 147.07.\n\nArm, Microsoft and Oracle also saw their shares jump on the news.\n\nOther stocks moving higher on enthusiasm for Project Stargate included chip foundry Taiwan Semiconductor Manufacturing (TSM) and server makers Dell Technologies (DELL) and Hewlett Packard Enterprise (HPE). Data center infrastructure provider Vertiv (VRT) also gapped up on the news.\n\n\"We expect Nvidia will remain a primary beneficiary of forthcoming spend which should if anything drive future revenue expectations higher,\" Wedbush Securities analyst Matt Bryson said in a client note. He rates Nvidia stock as outperform with a price target of 175.\n\nThe Stargate announcement should ease some investor about AI spending, UBS analyst Mayur Ramdhani said in a client note.\n\n\"While investors have become increasingly concerned about peak compute demand, this should go a long way to quelling these concerns and potentially adding growth runway for Nvidia beyond calendar 2026,\" Ramdhani said.\n\nMelius Research analyst Ben Reitzes said the Project Stargate news helps Nvidia's long-term growth story. He maintained his buy rating on Nvidia stock with a price target of 195.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nOracle Stock Surges As Trump Unveils Stargate AI Plan With OpenAI, SoftBank\n\nHow S&P 500 Nuclear Power Giants Are Responding To Trump's Stargate AI Announcement\n\nElon Musk And Sam Altman Squabble Over Stargate AI Project\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "3": {
            "title": "Nvidia Stock Climbs 4% as $500 Billion AI Initiative Gains Momentum",
            "link": "https://finance.yahoo.com/news/nvidia-stock-climbs-4-500-160020959.html",
            "snippet": "Billion AI Project Powers Nvidia's Dominance in Tech Stocks.",
            "score": 0.908093273639679,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's (NVDA, Financial) stock surged 4% on Wednesday as investors cheered news about Stargate's $500 billion AI infrastructure project. Major tech firms SoftBank OpenAI Oracle MGX SoftBank and OpenAI have selected Nvidia as their main technology ally alongside Microsoft (MSFT, Financial).\n\nNvidia Stock Climbs 4% as $500 Billion AI Initiative Gains Momentum\n\nStargate will put $100 billion into operations right away with its full funding deployment schedule over four years. To support AI computing needs, the venture will build state-of-the-art US data centers across the country. By participating in Stargate's data center project, Nvidia established its top position in AI hardware technology, especially for GPUs designed to handle large data processing.\n\nNvidia saw its market value increase to $3.58 trillion through the announcement, which sent it past Apple's $3.35 trillion worth. The market's reaction shows how Nvidia has gained greater power in AI development while moving technology infrastructure into the future.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia Is World\u2019s Most Valuable Company Again. How Stargate and Trump Helped the Stock.",
            "link": "https://www.barrons.com/articles/nvidia-stock-stargate-trump-ai-c2b67cfc",
            "snippet": "Nvidia stock rises as President Donald Trump sends another signal to markets that AI will be a priority under his second administration.",
            "score": 0.6725917458534241,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia confident that RTX 50 series power connectors unlikely to melt despite higher TDP",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-confident-that-rtx-50-series-power-connectors-unlikely-to-melt-despite-higher-tdp",
            "snippet": "Nvidia execs say that the RTX 5090 will not experience the melting connector issues reported in the RTX 4090.",
            "score": 0.9420329928398132,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia says that its flagship RTX 5090 GPUs will not experience the melting 16-pin connectors that plagued RTX 4090 graphics cards. Company representatives made this announcement during Nvidia RTX AI Day 2025 in South Korea, as reported by Quasar Zone (machine translated). Nvidia is confident that the changes it made to the connector design will avoid the issue, despite the RTX 5090 having a higher TDP of 575 watts (versus the 4090\u2019s 450 watts).\n\nThe company, represented by Nvidia APAC Director of Tech Marketing Jeff Yen, GeForce Tech Director for Marketing Sean Cleveland, and Nvidia Korea Senior VP Sunwook Kim, was answering some questions during the Nvidia RTX AI Day 2025 event. Someone in the audience asked if the RTX 5090 solved the problem that plagued the RTX 4090. The issue being \"where the connector overheated and melted\u201d. Nvidia answered, \u201cWe don\u2019t expect that to happen with the RTX 50 series. We made some changes to the connector to respond to the issue at the time, and we know that it is not happening now, about two years later.\u201d\n\nThe root cause of the overheating connector is often seen as an improperly or loosely attached cable, or if the cable is bent near the plug, causing too much stress at the connection point. Because of this, the industry developed the new 12V-2x6 connector, which replaced the 12VHPWR connector.\n\nThis new connector seemingly solved the overheating issue, although it\u2019s been reported as late as April 2024 that many RTX 4090s continue to melt\u2014but these were often older cards that used the older standard or those that used defective Cablemod 16-pin GPU power adapters that have since been recalled and discontinued.\n\nSince Nvidia has released the RTX 5090 some two years after it started using the 12V-2x6 connector, we can safely assume that overheating power connectors will no longer be an issue. Still, some AIBs are erring on the side of caution to avoid a repeat of the melting RTX 4090 connectors.\n\nZotac added a Safety Light feature that prevents the GPU from turning on until the power cable is fully and securely inserted. On the other hand, MSI introduced a simpler solution: it used a yellow-tipped 16-pin power adapter as a visual indicator. A fully seated power cable wouldn\u2019t show any of the yellow parts\u2014so if you still spot it after plugging it into your GPU, then the power cable is not properly attached.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Why Nvidia, Microsoft, Oracle, Arm, and Other AI Stocks Are Surging Wednesday",
            "link": "https://www.investopedia.com/why-nvidia-microsoft-oracle-arm-and-other-ai-stocks-are-surging-wednesday-8778471",
            "snippet": "Shares of Nvidia, Microsoft, Oracle, and other AI-related companies surged Wednesday after the Trump administration's announcement of a $500 billion joint...",
            "score": 0.6715333461761475,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Shares of AI-related companies surged Wednesday after President Trump announced a $500 billion joint venture with Oracle, OpenAI, and SoftBank to build AI infrastructure in the U.S.\n\nAs key technology partners for the initiative, Nvidia, Microsoft, and Arm saw their shares gain.\n\nWedbush analysts said they believe the project could represent \u201cthe start of a wave of massive AI investments,\" anticipating more announcements in the coming weeks.\n\n\n\nShares of AI-related companies surged Wednesday after the Trump administration\u2019s announcement of a $500 billion joint venture with Oracle (ORCL), OpenAI, and SoftBank spurred optimism about a rising tide of AI investments.\n\nFor Oracle, the project could mean \"significant revenue opportunity ahead starting potentially this year,\u201d Morgan Stanley analysts told clients Wednesday, with a portion of the companies' initial $100 billion investment appearing bound for Oracle\u2019s data centers in Abilene, Texas.\n\nThe project, known as Stargate, also \"reads positively\" for Microsoft (MSFT) as a backer for OpenAI and partner for the initiative, the analysts said.\n\nOracle shares jumped close to 8% in intraday trading Wednesday, extending Tuesday's gains ahead of the announcement, while Microsoft shares rose nearly 4%.\n\nAs key technology partners for the initiative, Nvidia (NVDA) and Arm Holdings (ARM) were also among the companies that saw their shares pop, with Arm shares soaring 15%, and Nvidia climbing close to 5%.\n\nShares of several Nvidia partners and other companies in the AI chipmaker's ecosystem, including Dell (DELL), TSMC (TSM), and Hewlett Packard Enterprise (HPE), gained as well.\n\nWedbush analysts said they believe the project could also represent \u201cthe start of a wave of massive AI investments to take place in the U.S.,\u201d anticipating more announcements in the coming weeks.\n\n",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "As Nvidia\u2019s stock gains, one analyst shows why Stargate could be so significant",
            "link": "https://www.marketwatch.com/story/as-nvidias-stock-gains-one-analyst-shows-why-stargate-could-be-so-significant-39b95d61",
            "snippet": "Melius Research says the new Stargate joint venture may signal more AI-friendly moves ahead and help Nvidia's long-term growth.",
            "score": 0.7389459013938904,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "How Stargate Partners Oracle, Open AI, Nvidia And Softbank Aim To Transform Healthcare",
            "link": "https://www.forbes.com/sites/saibala/2025/01/22/project-stargate-a-500-billion-ai-venture-between-oracle-open-ai-nvidia--softbank-that-will-revolutionize-healthcare/",
            "snippet": "The $500 billion Stargate project will be critical to \"maintain American leadership in AI,\" one of the partners said in a statement.",
            "score": 0.9063069820404053,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia Ingest Aims to Make it Easier to Extract Structured Information from Documents",
            "link": "https://www.infoq.com/news/2025/01/nvidia-ingest-document-extract/",
            "snippet": "Nvidia Ingest is a new microservice aimed at processing document content and extracting metadata into a well-defined JSON schema. Ingest is able to process...",
            "score": 0.8354917764663696,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia Ingest is a new microservice aimed at processing document content and extracting metadata into a well-defined JSON schema. Ingest is able to process PDFs, Word, and PowerPoint documents and extract structured information from tables, charts, images, and text using optical character recognition.\n\nTo use Nvidia Ingest, you provide it with a JSON job description of the payload to ingest. You can then retrieve the results as a JSON dictionary with metadata for all extracted objects, processing annotations, and timing/trace information.\n\nNvidia has not provided figures about Ingest performance but says it is scalable and can use multiple processing methods to improve accuracy or increase throughput. For PDF documents, Ingest can use pdfium, Unstructured.io, or Adobe Content Extraction Services.\n\nFor example, using nv-ingest-cli , the command line tool used to interact with Nvidia Ingest, you specify how to process a document using the --task argument, which includes an extract_method option:\n\nnv-ingest-cli \\ ... \\ --task='extract:{\"document_type\": \"pdf\", \"extract_method\": \"pdfium\", \"extract_text\": true, \"extract_images\": true, \"extract_tables\": true, \"extract_tables_method\": \"yolox\"}' \\ ...\n\nNvidia explicitly states that you cannot use Ingest to create a pipeline to carry through a sequence of operations on the documents in the payload. Yet, you can run various pre- or post-processing transformations, including text splitting and chunking, filtering, embedding generation, and image offloading. This means you can use multiple --task arguments for the same nv-ingest-cli execution. For example, you can add a dedup (de-duplication) step by using:\n\nnv-ingest-cli \\ ... \\ --task='extract:{...} \\ --task='dedup:{\"content_type\": \"image\", \"filter\": true}' \\ ...\n\nThe tool can be used on a single document specified with the --doc argument or on a set of documents simultaneously by providing a JSON-formatted dictionary describing the batch payload.\n\nAll extracted data are stored in an output directory containing a subdirectory for each document type, e.g., image, text, structured, etc. Each ingested document generates a JSON metadata file with the extracted content; source metadata including source name, location, type, etc.; and content metadata. Content metadata includes both general and type-specific content metadata. For example, for images, you get the image type, any caption, the location, size, and so on; for text, you get a summary, a list of keywords, the language, etc.; for tables, you get the format, location, the content as text, any caption or title, etc.\n\nNvidia Ingest requires a number of supporting services, both from Nvidia and open-source projects, including redis, yolox, otel-collector for open telemetry, prometheus, grafana, and more. They are packaged as a Docker Compose application to make deployment easier. It also requires support for CUDA and the Nvidia Container Toolkit and a minimum of two H100 or A100 GPUs with at least 80GM memory.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-21": {
        "0": {
            "title": "NoTraffic Reduces Road Delays, Carbon Emissions With NVIDIA AI and Accelerated Computing",
            "link": "https://blogs.nvidia.com/blog/notraffic-reduces-road-delays-carbon-emissions/",
            "snippet": "On average, the startup helps to save over 12 tons of carbon dioxide and generate $45 million in economic productivity per year for a typical city of 100000...",
            "score": 0.6884901523590088,
            "sentiment": null,
            "probability": null,
            "content": "On average, the startup helps to save over 12 tons of carbon dioxide and generate $45 million in economic productivity per year for a typical city of 100,000 residents.\n\nMore than 90 million new vehicles are introduced to roads across the globe every year, leading to an annual 12% increase in traffic congestion \u2014 according to NoTraffic, a member of the NVIDIA Inception program for cutting-edge startups and the NVIDIA Metropolis vision AI ecosystem.\n\nStill, 99% of the world\u2019s traffic signals run on fixed timing plans, leading to unnecessary congestion and delays.\n\nTo reduce such inefficiencies, mitigate car accidents and reduce carbon emissions from vehicles, NoTraffic\u2019s AI Mobility platform predicts road scenarios, helps ensure continuous traffic flow, minimizes stops and optimizes safety at intersections across the U.S., Canada and elsewhere.\n\nThe platform \u2014 which enables road infrastructure management at both local-intersection and city-grid scale \u2014 integrates NVIDIA-powered software and hardware at the edge, under a cloud-based operating system.\n\nIt\u2019s built using the NVIDIA Jetson edge AI platform, NVIDIA accelerated computing and the NVIDIA Metropolis vision AI developer stack.\n\n\u201cWith NVIDIA accelerated computing, we achieved a 3x speedup in AI training and doubled AI Mobility\u2019s energy efficiency,\u201d said Uriel Katz, cofounder and chief technology officer of NoTraffic. \u201cThese optimizations in time, money and energy efficiency are all bolstered by NVIDIA Jetson, which sped our image preprocessing tasks by 40x compared with a CPU-only workflow. Plus, GPU-accelerated NVIDIA CUDA libraries increased our model throughput by 30x.\u201d\n\nThese libraries include the NVIDIA TensorRT ecosystem of application programming interfaces for high-performance deep learning inference and the NVIDIA cuDNN library of primitives for deep neural networks.\n\nTaming Traffic in Tuscon, Vancouver and Beyond\n\nIn Tuscon, Arizona, more than 80 intersections are tapping into the NoTraffic AI Mobility platform, which has enabled up to a 46% reduction in road delays during rush hours \u2014 and a half-mile reduction in peak queue length.\n\nThe work is an expansion of NoTraffic\u2019s initial deployment on Tuscon\u2019s West Ajo Way. That effort led to an average delay reduction of 23% for drivers.\n\nSince installation, NoTraffic technology has helped free Tucson drivers from over 1.25 million hours stuck in traffic, the company estimates, representing an economic benefit of over $24.3 million. The company has also tracked a nearly 80% reduction in red-light runners since its platform was deployed, helping improve safety at Tucson intersections.\n\nBy reducing travel times, drivers have also saved over $1.6 million in gas, cutting emissions and improving air quality to make the equivalent impact of planting 650,000 trees.\n\nIn Vancouver, Canada, the University of British Columbia (UBC) is using the NoTraffic platform and Rogers Communications\u2019 5G-connected, AI-enabled smart-traffic platform to reduce both pedestrian delays and greenhouse gas emissions.\n\nRogers Communications\u2019 5G networks provide robust and stable connectivity to the sensors embedded on the traffic poles.\n\nThis advanced network infrastructure enhances the NoTraffic platform\u2019s efficacy and scalability, as the improved speed and reduced latency of 5G networks means traffic data can be processed in real time. This is critical for predicting numerous potential traffic scenarios, adjusting signal timings and prioritizing road users accordingly.\n\nWith AI Mobility deployed at seven intersections across the campus, the university experienced an up to 40% reduction in pedestrian delays and significant decreases in vehicle wait time.\n\nIn addition, UBC reduces 74 tons of carbon dioxide emissions each year thanks to the NoTraffic and Rogers solution, which is powered by NVIDIA edge AI and accelerated computing.\n\nThe platform is also in action on the roads of Phoenix, Arizona; Baltimore, Maryland; and in 35 states through 200+ agencies across the U.S. and Canada.\n\nHonk If You Love Reducing Congestion, Carbon Emissions\n\nThe NoTraffic AI Mobility platform offers local AI-based predictions that, based on sensor inputs at multiple intersections, analyze numerous traffic scenarios up to two minutes in advance.\n\nIt can adapt to real-time changes in traffic patterns and volumes, send messages between intersections and run optimization algorithms that control traffic signals to improve overall transportation efficiency and safety through cloud connectivity.\n\nSpeedups in the AI Mobility platform mean quicker optimizations of traffic signals \u2014 and reduced congestion on the roads means reduced carbon emissions from vehicles.\n\nNoTraffic estimates that for every city optimized with this platform, eight hours of traffic time could be saved per driver. Plus, with over 300,000 signalized intersections in the U.S., the company says this could result in a total of $14 billion in economic savings per year.\n\nLearn more about the NVIDIA Metropolis platform and how it\u2019s used in smart cities and spaces.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Stock Treading Water Ahead Of These Risks, Potential Catalysts",
            "link": "https://www.investors.com/news/technology/nvidia-stock-treading-water-risks-catalysts/",
            "snippet": "Nvidia Stock Treading Water Ahead Of These Risks, Potential Catalysts ... Nvidia (NVDA) stock has been trending sideways for about four months as investors debate...",
            "score": 0.7138727307319641,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA) stock has been trending sideways for about four months as investors debate uncertainties ranging from the sustainability of data center demand for artificial intelligence chips to the impact of the new Trump administration.\n\nNvidia shares have wavered up and down on every news report about supply and demand for the chipmaker's current-generation Blackwell series products.\n\nStill, Wall Street analysts are generally positive about Nvidia's prospects this year. According to FactSet, 61 analysts have buy ratings on Nvidia stock, while five have hold ratings. No analysts have a sell rating on it.\n\nNvidia stock hit a record high of 153.13 on Jan. 7 after Chief Executive Jensen Huang gave the opening keynote address at the CES 2025 technology conference in Las Vegas. During his speech, Huang outlined the coming shift from generative artificial intelligence to agentic AI and physical AI.\n\nBut the euphoria over Huang's speech quickly faded over lingering concerns about whether hyperscale cloud computing customers will slow their purchases of AI processors.\n\nInvestors will glean insights on AI data center demand from upcoming Big Tech earnings reports. Meta Platforms (META) and Microsoft (MSFT) are due to report December-quarter results on Jan. 29. Apple (AAPL) will follow on Jan. 30. Google parent Alphabet (GOOGL) will release its fourth-quarter results on Feb. 4. Amazon.com (AMZN) will post its fourth-quarter results in early February.\n\nPossible Catalysts For Nvidia Stock\n\nNvidia's next earnings report \u2014 for its fiscal fourth quarter \u2014 is due on Feb. 26.\n\nAfter that, the next potential catalyst for Nvidia stock will be announcements at the company's GTC conference, which starts March 18, in San Jose, Calif.\n\nMeanwhile, Nvidia faces potential business challenges from changes in U.S. policy under President Donald Trump, who took office on Monday.\n\nNvidia was critical of recent export restrictions on AI chips and systems under former President Joseph Biden. And President Trump has threatened to impose steep tariffs on imports that could impact Nvidia's products.\n\nAI Chip Competition\n\nNvidia also faces the threat of increased competition from custom AI chips from Broadcom (AVGO) and Marvell Technology (MRVL).\n\nThe AI chip market is big enough to support graphics processing units, or GPUs, from Nvidia as well as application-specific integrated circuits, or ASICs, Barclays analyst Tom O'Malley said in a client note Friday.\n\n\"Nvidia GPU sales reached nearly $100 billion in calendar 2024 and are expected to grow to about $160 billion in calendar 2025,\" O'Malley said. \"Custom silicon is just starting to become more significant and should grow at a faster compounded rate over the next three years (55%). We think the AI TAM (total addressable market) is big enough to support both through 2026.\"\n\nCustomers are clamoring for \"second-source options\" for AI computing systems, O'Malley said. That includes merchant GPUs from Advanced Micro Devices (AMD) and custom chips from Broadcom and Marvell, he said.\n\n\"While we continue to expect Nvidia to dominate training workloads, we see custom ASICs as an increasingly compelling solution for inference workloads,\" he said.\n\nAI training is the process of teaching an AI model to recognize patterns and make predictions, while AI inference is when the model uses that training to make predictions on new data.\n\nO'Malley predicts that the custom ASIC portion of the inference accelerator market will reach 45% of total spending in the segment by 2028, up from 7% now.\n\nNvidia Stock Gets Price-Target Hike\n\n\"Given the ongoing focus on the AI compute end-market, we would be remiss to not mention Nvidia's continued upside potential,\" O'Malley said. \"For calendar 2025, we expect Nvidia to grow its data-center compute business by nearly 60% year over year.\"\n\nO'Malley reiterated his overweight, or buy, rating on Nvidia stock and upped his price target to 175 from 160.\n\nOn the stock market today, Nvidia stock advanced 2.3% to close at 140.83.\n\nInvestor fears around a range of issues hanging over Nvidia stock \"seem overdone,\" O'Malley said. Those issues include rumored production snags and slowing demand for prior-generation Hopper systems.\n\nNvidia A Full-Stack AI System Provider\n\nEvercore ISI analyst Mark Lipacis maintained his outperform rating on Nvidia stock with a price target of 190.\n\n\"We continue to favor Nvidia, as a dominant, full-stack, AI-ecosystem play, but also expect upside surprises in 2025 from the custom-AI chip plays \u2014 Marvell, Broadcom, Arm (ARM), Teradyne (TER) and Astera Labs (ALAB),\" Lipacis said in a client note Friday.\n\nIn 2024, Nvidia stock rose 171% to end the year at 134.29.\n\nD.A. Davidson analyst Gil Luria has been neutral on Nvidia stock for the past year. He says he has taken flak for his guarded stance on the AI chipmaker.\n\n\"Another takeaway from this past year is that when your appearance relaying a cautious outlook on Nvidia gets posted online never check the comments,\" he said in a Jan. 15 report. \"People are not very nice online.\"\n\nHe maintains a neutral rating on Nvidia stock with a price target of 135.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nTSM Stock Jumps As Chip Foundry Tops Q4 Estimates On AI Chip Sales\n\nBroadcom Stock Showing Strength After Breakout\n\nSee Stocks On The List Of Leaders Near A Buy Point\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "What Is Nvidia Cosmos?",
            "link": "https://builtin.com/articles/nvidia-cosmos",
            "snippet": "Nvidia Cosmos is a cloud-based platform that helps developers build and deploy AI for robots and autonomous vehicles, using a suite of specialized...",
            "score": 0.9114540815353394,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia Cosmos is a platform built to accelerate the development of \u201cphysical AI,\u201d the artificial intelligence powering anything robotic. Developed by chip manufacturer Nvidia, Cosmos includes a video data curation pipeline, various tools for customizing data to specific use cases and \u2014 perhaps most importantly \u2014 a family of specialized AI models called \u201cworld foundation models.\u201d\n\nTraining physical AI systems is incredibly labor-intensive, often requiring the collection, labeling and categorization of millions of hours of real-world footage. Cosmos\u2019 world foundation models (WFMs) aim to tackle this issue by generating three-dimensional simulations of the physical world to train robots \u2014 whether it\u2019s a factory robot assembling a product, a humanoid robot interacting with humans or an autonomous vehicle navigating the road. In other words: WFMs make synthetic training data so developers don\u2019t have to rely entirely on costly real-world data collection and processing.\n\nWhat Is Nvidia Cosmos? Nvidia Cosmos is a cloud-based platform that helps developers build and deploy AI for robots and autonomous vehicles, using \u201cworld foundation models\u201d to generate synthetic training data.\n\nNvidia unveiled Cosmos at the 2025 CES conference in Las Vegas, alongside several other product launches and upgrades. Among them is a new feature on its Isaac simulation platform, enabling robot builders to generate large volumes of synthetic training data from just a few examples. The company also upgraded its Omniverse platform with a \u201cMega\u201d operating system that lets developers create, test and optimize their robot fleets in a digital twin environment before deploying them in the real-world.\n\nTogether, Nvidia hopes these products will expand what is possible in physical AI \u2014 an industry projected to be worth $45 billion by 2029.\n\n\u201cEverything that moves \u2014 from cars and trucks to factories and warehouses \u2014 will be robotic and embodied by AI,\u201d Jensen Huang, Nvidia\u2019s founder and CEO, said in a statement. \u201cNvidia\u2019s Omniverse digital twin operating system and Cosmos physical AI serve as the foundational libraries for digitalizing the world\u2019s physical industries.\u201d\n\nA demonstration of what Nvidia Cosmos can do, and its potential impact on physical AI development. Video: Nvidia\n\nWhat Is Nvidia Cosmos?\n\nNvidia Cosmos offers a suite of foundation models that can generate \u201ccontrollable, high-quality\u201d synthetic data to train robots, driverless cars and more, according to Nvidia. The models are divided into three categories:\n\nNano, for real-time, low-latency inference and edge deployment. Super, a \u201chighly performant baseline\u201d model for \u201cout-of-the box\u201d fine-tuning and deployment. Ultra, for maximum accuracy and quality outputs; provides best fidelity knowledge transfer for distilling custom models.\n\nThe size of the models range from 4 billion to 14 billion parameters, with Nano being the smallest and Ultra being the largest. Essentially, parameters correspond to an AI model\u2019s problem-solving skills, so models with more parameters generally perform better than those with fewer parameters.\n\nIn addition to the WFMs, Cosmos includes a 12-billion-parameter \u201cunsampling\u201d model for refining text prompts, a 7-billion-parameter video decoder optimized for augmented reality and a guardrail model to ensure \u201cresponsible\u201d use. It also includes fine-tuned models for applications like generating multisensor views for autonomous vehicles.\n\nNvidia says all of its Cosmos models were trained on 9,000 trillion tokens (bits of raw data \u2014 in this case, video footage) from 20 million hours of real-world environmental, industrial, robotics, and driving data, as well as human activities like walking and manipulating objects. Users can also fine-tune the models with their own data.\n\nNvidia is joining a growing roster of companies building tools to simplify the integration of artificial intelligence into robots. Other top players include cloud provider AWS, game development company Unity and Genesis, an open source computer simulation system created by a group of university and private industry researchers (including from Nvidia) that can reportedly train robots 430,000 times faster than reality.\n\nNvidia Cosmos is now available for free on both Hugging Face and Nvidia\u2019s own NGC catalog under an open model license, meaning developers can freely access and use all of the models, so long as they have Nvidia hardware. Several companies have begun using the platform, according to Nvidia, including humanoid robot makers Galbot, Agility Robotics and Figure AI, as well as self-driving car companies Uber, Wayve and Waabi.\n\nMore on NvidiaWhat Is Nvidia\u2019s Chat with RTX?\n\nWhat Can Nvidia Cosmos Do?\n\nHere are some of the key features and capabilities of Nvidia Cosmos.\n\nVideo Search\n\nNvidia says Cosmos can simplify the video tagging and search process, helping to make robots that are ready for the real world. So, whether it\u2019s footage of a snowy road in front of a self-driving car or a busy warehouse, the platform is able to understand spatial and temporal patterns, saving developers time and costs associated with training data preparation.\n\n\u20183D-to-Real\u2019 Synthetic Data Creation\n\nWith Cosmos, developers can transform their 3D simulation data into \u201cphotoreal\u201d synthetic video, according to Nvidia. By pairing the platform with Omniverse, users can create 3D environments that represent their training needs. Then, they can generate photorealistic videos that are \u201cprecisely controlled\u201d by the 3D scenes, creating \u201chighly tailored\u201d synthetic datasets.\n\nPolicy Model Training and Evaluation\n\nCosmos\u2019 world foundation models \u2014 when fine-tuned for \u201caction conditioned video prediction\u201d \u2014 enable scalable and repeatable training and evaluation of policy models, according to Nvidia, which define strategies for physical AI systems by mapping states to actions. The company says developers can use these models instead of relying on \u201crisky real-world tests\u201d or complex simulations of tasks like obstacle navigation and object manipulation, which can help optimize machines\u2019 performance and improve their reliability.\n\nPredictive Intelligence\n\nCosmos is designed to bring advanced predictive intelligence \u2014 or \u201cforesight,\u201d as Nvidia puts it \u2014 to physical AI, enabling systems to anticipate future scenarios and make decisions based on those predictions. The platform can do this by generating predictive videos based on past data and text prompts, enabling robots and other automated machines to navigate and adapt to dynamic physical environments safely and efficiently.\n\n\u2018Multiverse\u2019 Simulation\n\nWhen paired with Omniverse, Cosmos can help developers simulate multiple outcomes of real-time scenarios, accelerating the decision making process for robots, autonomous vehicles and other artificially intelligent machines. Creating a sort of \u201cmultiverse,\u201d Nvidia says Cosmos and Omniverse can work together to explore \u201call possible future outcomes,\u201d ultimately selecting the best path for \u201cenhanced precision and reliability in complex environments.\u201d\n\nRelated ReadingThe Future of Robots and Robotics\n\nHow Does Nvidia Cosmos Work?\n\nNvidia Cosmos\u2019 world foundation models work similarly to large language models (LLMs), insofar as both process vast amounts of data and produce complex outputs based on the patterns they learned from that data.\n\nHowever, while LLMs are trained on text data (books, articles, social media posts, etc.) to perform tasks related to natural language processing and generation, WFMs are designed to make photorealistic, \u201cphysics-aware\u201d simulations for training robots in real-world applications. Using data from text, images, video and motion sensors, these models render detailed worlds, and then identify and label the physical attributes within those worlds \u2014 maintaining accurate spatial awareness, physics and object permanence throughout.\n\nThe focus isn\u2019t just on generating AI content, but on teaching AI to understand the physical world well enough to operate in it effectively. Cosmos can, for example, make video footage of boxes falling from shelves, which can be used to train a robot to recognize accidents. Or it can simulate a self-diving car encountering an animal on the side of the road, teaching the vehicle to anticipate the animal\u2019s movements and slow down if necessary. Meanwhile, all of this training data is synthetic, making it more accessible to a broader range of developers.\n\n\u201cThe ChatGPT moment for robotics is coming. Like large language models, world foundation models are fundamental to advancing robot and [autonomous vehicle] development, yet not all developers have the expertise and resources to train their own,\u201d Huang said in a statement. \u201cWe created Cosmos to democratize physical AI and put general robotics in reach of every developer.\u201d\n\nFrequently Asked Questions\n\nWhat is Nvidia Cosmos? Nvidia Cosmos is a cloud-based platform designed to help developers build and deploy AI models for physical AI systems, such as robots and automated vehicles.\n\nNvidia Omniverse vs. Nvidia Cosmos Nvidia Omniverse functions as a digital twin operating system, enabling developers to create, test and optimize their robot fleets in a simulated environment before deploying them in the real world. The Nvidia Cosmos platform, on the other hand, has a family of specialized AI models that can generate realistic, three-dimensional videos, maintaining accurate spatial awareness, physics and object permanence throughout. So, while Omniverse provides a space to create and simulate virtual environments, Cosmos has the tools necessary to produce the synthetic data required for training robots within those environments.\n\nIs Nvidia Cosmos available? Yes, Nvidia Cosmos is now available for free on both Hugging Face and Nvidia\u2019s own NGC catalog under an open model license. Cosmos also provides an end-to-end pipeline to fine-tune Cosmos\u2019 world foundation models with Nvidia NeMo. Developers can use the Cosmos tokenizer from /NVIDIA/cosmos-tokenizer on GitHub and Hugging Face.\n\nIs Nvidia Cosmos free? Yes, Nvidia Cosmos is available for free on both Hugging Face and Nvidia\u2019s own NGC catalog under an open model license.\n\nIs Nvidia Cosmos open source? No, the world foundation models on Nvidia Cosmos are not technically open source. Nvidia has not disclosed what specific data was used to train these models, nor has it made available all the tools needed to recreate the models from scratch. Hence, the company calls these models \u201copen\u201d \u2014 not \u201copen source.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Hands-On With the RTX 5090, Nvidia's Big, New GPU",
            "link": "https://gizmodo.com/the-nvidia-rtx-5090-is-a-thick-slab-of-graphics-processing-promise-2000552978",
            "snippet": "Nvidia's RTX 5090 Founders Edition is a two-fan GPU that's seriously hefty, though somehow smaller and more menacing than the RTX 4090.",
            "score": 0.5418670773506165,
            "sentiment": null,
            "probability": null,
            "content": "I don\u2019t find many graphics cards appealing. Without some wild case designs or pre-built desktops made to offer looks and power in equal measure, they seem like excess compared to the other sleek, low-profile components. The Nvidia RTX 5090 Founders Edition, on the other hand, appears downright menacing. It\u2019s a slab of dark grey metal, a demanding, brick-like beetle ready to leech onto your PSU and fully consume your PC.\n\nNvidia sent me the RTX 5090 to review but stipulated that I can\u2019t yet show you it powered on. I can\u2019t even show it snuggled up warm in a desktop case. So with those limitations, here are my first impressions.\n\nThe RTX 5090 Founders Edition is about 12 inches by 5.3 inches and 2 inches deep. That\u2019s roughly the same length and width of the RTX 4090 Founders Edition with the same two-fan variation. However, the new card is shorter than the previous. This is a two-slot card, though that doesn\u2019t mean any future OEMs will keep to those dimensions.\n\nBut for that, it\u2019s a heavy card. It had been out in a cold FedEx truck for so long the metal was nearly painful to touch. Even when I had it up to room temperature, the GPU felt chunky, with practically no room wasted in this slab of metal and silicon. It\u2019s the kind of card that makes me nervous to put into my case supported by nothing but a PCIe slot and two screws.\n\nWhen compared to the other cards we have on hand, namely a PNY RTX 4080 Super, an Asus RTX 4080 Super, and a MSI RTX 4070 Ti Super\u2014all with three fans\u2014the 5090 seems more self-contained. It takes up fewer slots inside your case, but it desperately demands some kind of extra support for any end sticking outside your motherboard.\n\nNvidia swears its new cooling apparatus will manage to keep the card cool under pressure. The card\u2019s intake is below, and the exhaust comes out the vents above. That should be good for most setups where airflow normally goes from bottom to top.\n\nThe weight adds to the overall aesthetics of the Founders Edition. It\u2019s intimidating in grey metallics, especially compared to the RTX 4090\u2019s silver sides. You can see into the GPU when you shine a light on it, enough to see the heat pipes running through it. Otherwise, it still includes the glowing GeForce RTX logo when the card is turned on.\n\nThe Founders Edition card now comes in a corrugated cardboard box that\u2019s supposed to emulate the footprint of the 5090. Inside is the GPU without any anti-static bag or sleeve. It didn\u2019t seem necessary, as the packaging was tight enough to keep it from moving around. Other than the card, the box came with a single, short 4x PCIe 5.0 adapter. Nvidia has said you\u2019ll either need the adapter hooked up to four PCIe 8-pin connectors or a 600W PCIe Gen 5 cable.\n\nIt also requires at least a 1000 W of system power, so if you felt okay running a RTX 4080 Super with an 850 W PSU, it\u2019s yet another piece you\u2019ll need to upgrade to support Nvidia\u2019s hefty new GPU.\n\nThe 16-pin power connection now sits at an angle to the card itself. This may be a boon for smaller cases, but I know in my current setup with the Origin PC it will actually mean I need to twist the power connection up to fit it in the correct slots. Behind it, the only ports you\u2019ll find are three DisplayPort 2.1 and a single HDMI 2.1b.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Is Nvidia a Buy?",
            "link": "https://www.fool.com/investing/2025/01/21/is-nvidia-a-buy/",
            "snippet": "Yes, the cat's out of the bag on Nvidia and AI. Today, the stock trades over 50 times trailing-12-month earnings, so buyers are banking on the future rather...",
            "score": 0.9214902520179749,
            "sentiment": null,
            "probability": null,
            "content": "Buying Nvidia at these prices is a bet on the company's future. Here's a look at Nvidia's growth prospects to determine whether the stock deserves your investment.\n\nAt this point, most investors interested in artificial intelligence (AI) know the critical role Nvidia's (NVDA 5.27%) AI accelerator chips have played. The resulting growth spurt has propelled the company and its stock to staggering heights. If you're reading this, you're probably wondering whether Nvidia's AI tailwinds can continue and whether the stock is still buyable after soaring over 800% since early 2023.\n\nYes, the cat's out of the bag on Nvidia and AI. Today, the stock trades over 50 times trailing-12-month earnings, so buyers are banking on the future rather than the past.\n\nWhat might the future hold for Nvidia? I researched the company's growth prospects to determine whether the stock is a buy today.\n\nPeering into Nvidia's leadership in AI chips amid massive market expansion\n\nNvidia rose to prominence in AI thanks to its Hopper AI accelerator chip architecture, which became the gold standard for training today's AI models. According to industry estimates, the company owns between 70% and 95% of the market. Owning that much of the market means that future growth depends on market expansion because there's little room to gain additional business from competitors. On the other hand, it means Nvidia risks ceding market share to other chip companies.\n\nHowever, the market is growing so fast that it could realistically offset any modest market share losses. The global AI chip industry was estimated at $123 billion in 2024 and could grow to $311 billion by 2029. Nvidia's trailing-12-month revenue was $113 billion, most of which came from AI data center sales. The company could lose market share and double its revenue over the next five years.\n\nThe underlying factors driving AI chip growth seem pretty intact. AI will need more (and better) chips to train and operate more intelligent models, and increased demand for AI services will require additional computing resources to create the capacity to accommodate that demand.\n\nBlackwell looks like a successful continuation of AI momentum\n\nNvidia has begun transitioning to Blackwell, the successor to Hopper. The new architecture represents significant breakthroughs in performance and, arguably more importantly, efficiency. The chip has 208 billion transistors, up from 80 billion on the H100. The B200 can achieve up to 20 petaflops of computing power (like horsepower for computing) versus four for the H100.\n\nThe B200 will deliver three times faster large-model training than the H100 and 15 times the AI inference performance. It's also 25 times more energy efficient, translating to a significantly lower cost of ownership. This is a key point because companies investing billions of dollars in AI data centers need to monetize these investments over time. Low operating costs can be very helpful.\n\nBlackwell has already shown signs of rampant demand. A few months back, Nvidia announced that it had sold out its entire Blackwell capacity for 2025. The lack of Blackwell supply should also continue trickling down to ongoing Hopper sales, as some companies may opt to build with Hopper chips rather than wait 12 months or longer for chips.\n\nIs Nvidia a buy?\n\nAssuming the prominent companies building up AI data centers, known as hyperscalers, continue investing in AI infrastructure, Nvidia's growth prospects look fantastic.\n\nAnalysts estimate Nvidia will grow earnings by an average of 38% annually over the next three to five years, which makes sense within the broader growth forecast for the AI chip market. There doesn't seem to be much reason to doubt the growth forecast until competition shows tangible evidence of eating into Nvidia's business. So far, Blackwell looks like a smashing success.\n\nI like using the PEG ratio to weigh a stock's valuation versus its growth rate. Typically, I'll buy high-quality stocks up to a PEG ratio of 2.0 to 2.5, and Nvidia's (PEG ratio of 1.4) falls comfortably below that threshold. I'd much rather buy Nvidia here than a similarly priced retail stock like Costco, which has much slower growth.\n\nSo yes, Nvidia is a buy in my book.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "HEAVY.AI Announces Availability of Analytics Platform Accelerated by NVIDIA Grace Hopper Superchip",
            "link": "https://www.businesswire.com/news/home/20250121115004/en/HEAVY.AI-Announces-Availability-of-Analytics-Platform-Accelerated-by-NVIDIA-Grace-Hopper-Superchip",
            "snippet": "HEAVY.AI, the leader in GPU-accelerated analytics, today announced the general availability of the HEAVY.AI analytics platform with the NVIDIA GH200 G.",
            "score": 0.7529497146606445,
            "sentiment": null,
            "probability": null,
            "content": "SAN FRANCISCO--(BUSINESS WIRE)--HEAVY.AI, the leader in GPU-accelerated analytics, today announced the general availability of the HEAVY.AI analytics platform with the NVIDIA GH200 Grace Hopper Superchip, an advanced hardware architecture that features an NVIDIA-designed Arm-based CPU with an ultra-fast NVIDIA Hopper GPU. The launch is part of the broader 8.2 release of the HEAVY.AI accelerated analytics platform.\n\nSupport for running HEAVY.AI on the Grace Hopper Superchip means that users of the platform will be able to process faster and cheaper than previously possible. By leveraging the ultra-fast NVIDIA NVLink-C2C interconnect between the CPU and GPU, which features 900GB/sec of bidirectional bandwidth, data can be transferred between CPU and GPU at speeds of up to 7X faster than traditional PCIe-based systems. This means that users of HEAVY.AI can now query and visualize massive datasets that exceed GPU memory capacity at interactive speeds. Furthermore, users of HEAVY.AI will be able to also take advantage of the NVIDIA GB200 Grace Blackwell Superchip and GB200 NVL72, a liquid-cooled, rack-scale solution that boasts a 72-GPU NVLink domain that acts as a single massive GPU and delivers 30X faster real-time trillion-parameter LLM inference.\n\nThe release of the HEAVY.AI platform for the Grace Hopper and Blackwell architectures also offers significant cost savings for customers, who now can now scale to larger datasets with less GPU resources. As an illustration of these cost savings, with this release, HEAVY.AI has launched a publicly available demo featuring over 20 billion records of ship locations (AIS data) in US coastal waters spanning the last 7 years, running on a single NVIDIA GH200 Superchip on Vultr Cloud. Previously, this demo would have required at least 4 NVIDIA A100 GPUs with 320GB of combined GPU VRAM to cache the relevant data and ensure interactive performance, representing nearly 70% hardware cost savings.\n\n\u201cThe NVIDIA Grace Hopper Superchip changes the game in terms of being able to provide best-in-class performance over our customers\u2019 largest datasets,\u201d said Todd Mostak, Co-Founder and CEO of HEAVY.AI. \u201cNow customers no longer have to choose between faster performance and lower cost; with HEAVY.AI running on NVIDIA Grace systems, they can have both.\u201d\n\n\u201cCustomers worldwide are looking to boost performance and reduce cost when analyzing large datasets,\u201d said Ivan Goldwasser, director of data center CPUs at NVIDIA. \u201cBy accelerating HEAVY.AI\u2019s analytics platform with NVIDIA Grace Hopper and Grace Blackwell Superchips, customers can speed up high-performance data processing and visualization for big data analytics.\u201d\n\nUnderscoring the performance and cost benefits of the Grace Hopper architecture, HEAVY.AI recently released benchmarks comparing the performance of the HeavyDB GPU-accelerated database on the industry standard TPC-H SQL Data Warehouse benchmark against three of the most popular CPU-based data warehouses. Running on a NVIDIA GH200 Grace Hopper system, HeavyDB was up to 21X faster on average than its CPU-based competitors while being up to 9X cheaper per hour to operate. More details from the benchmark can be found here.\n\nThe release of the HEAVY.AI analytics platform on NVIDIA Grace architecture is part of a larger effort by the company to further increase the speed and cost advantages of the platform compared to CPU-based data warehouses and analytics systems. \u201cOur overarching goal is to make the performance and cost advantages of GPU-accelerated analytics so compelling that it becomes the default means of analyzing large datasets,\u201d said Mostak.\n\nAbout HEAVY.AI\n\nHEAVY.AI provides a groundbreaking GPU-accelerated analytics platform that empowers organizations to instantly query and visualize multi-billion-record datasets, including geospatial and time-series data, delivering a complete view of what is happening, when, and where. By integrating massive data volumes from multiple sources, HEAVY.AI provides an immersive, real-time interactive visual analytics experience. Industry leaders in telecommunications, energy, government, utilities, and higher education rely on HEAVY.AI to drive high-impact decisions at unprecedented speeds. Born from research at Harvard and MIT\u2019s Computer Science and Artificial Intelligence Laboratory (CSAIL), HEAVY.AI is backed by GV, In-Q-Tel, New Enterprise Associates (NEA), NVIDIA, Tiger Global Management, Vanedge Capital, and Verizon Ventures. Headquartered in San Francisco, HEAVY.AI is reshaping data analytics. Learn more about HEAVY.AI.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia CEO's whirlwind Taiwan tour: from Lunar New Year banquets to night market visits",
            "link": "https://finance.yahoo.com/news/nvidia-ceos-whirlwind-taiwan-tour-093000991.html",
            "snippet": "Nvidia CEO Jensen Huang wrapped up a fast-paced 55-hour visit to Taiwan, which included engagements with top industry leaders and visits to key...",
            "score": 0.9307864904403687,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang wrapped up a fast-paced 55-hour visit to Taiwan, which included engagements with top industry leaders and visits to key manufacturing sites, underscoring the island's role in the global artificial intelligence (AI) and semiconductor industries.\n\nOn Sunday, Huang hosted a high-profile lunch in Taipei, attended by 36 top executives from Taiwan's leading technology companies.\n\nThis rare gathering included Foxconn Technology Group chairman Liu Young-way, Foxconn industrial internet chairman Brand Cheng, Acer chairman Jason Chen, Quanta Computer chairman Barry Lam and vice-chairman C.C. Leung, Wistron chairman Simon Lin, Asus chairman Jonney Shih, ASRock president Hsu Lung-luen, Pegatron chairman TH Tung, MSI chairman Joseph Hsu, Inventec chairman Sam Yeh and president Jack Tsai, and more.\n\nDo you have questions about the biggest topics and trends from around the world? Get the answers with SCMP Knowledge, our new platform of curated content with explainers, FAQs, analyses and infographics brought to you by our award-winning team.\n\nTaiwan Semiconductor Manufacturing Company (TSMC) chairman Mark Liu, who had dined with Huang the day before, also joined the event.\n\nHuang and Wistron president Simon Lin (centre) draw lottery balls during an end-of-year party in Taipei on Friday. Photo: AFP alt=Huang and Wistron president Simon Lin (centre) draw lottery balls during an end-of-year party in Taipei on Friday. Photo: AFP>\n\nDuring the lunch, Huang joked about the group photo taken after the meal, where he and several others squatted in the front row. \"The front row exercises. The back row does not exercise,\" he quipped, drawing laughter from the attendees.\n\nHuang emphasised the critical contributions of these supply chain partners to Nvidia's technological innovations. He highlighted that 45 factories worldwide were currently operating 24/7 to build supercomputers powered by Nvidia's Grace and Blackwell chips.\n\nHis primary goal for this visit was to personally thank these partners for their efforts and lay the groundwork for an even more ambitious year ahead, he said.\n\nHuang's visit began on Thursday when he arrived in Taichung and attended the ribbon-cutting ceremony for Siliconware Precision Industry's new factory at the Tanzi Technology Industrial Park. It was his first visit to the city.\n\nLater that evening, he visited the bustling Fengjia Night Market, though his stay was brief because of the large crowds.\n\nHuang and Siliconware Precision Industry chairman C.W Tsai at the opening ceremony of the company's new plant in Taichung. Photo: Reuters alt=Huang and Siliconware Precision Industry chairman C.W Tsai at the opening ceremony of the company's new plant in Taichung. Photo: Reuters>",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Is Nvidia Stock A Buy, Sell, Or Hold Before Its March 20 Quantum Day?",
            "link": "https://www.barchart.com/story/news/30542817/is-nvidia-stock-a-buy-sell-or-hold-before-its-march-20-quantum-day",
            "snippet": "Technology giant Nvidia (NVDA) is an American semiconductor company that designs and manufactures computer chips. The company is widely known for inventing...",
            "score": 0.915562629699707,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia passes Apple again to become world's most valuable company",
            "link": "https://www.cnbc.com/2025/01/21/nvidia-passes-apple-again-to-become-worlds-most-valuable-company-.html",
            "snippet": "Nvidia passed Apple in market value on Tuesday, once again becoming the most valuable publicly-traded company in the world.",
            "score": 0.8651366233825684,
            "sentiment": null,
            "probability": null,
            "content": "Jensen Huang, Nvidia's founder, president and CEO, speaks about the future of artificial intelligence and its effect on energy consumption and production at the Bipartisan Policy Center in Washington, D.C., on Sept. 27, 2024.\n\nNvidia passed Apple in market value on Tuesday, once again becoming the most valuable publicly-traded company in the world.\n\nShares of the chipmaker rose over 2% on Tuesday, and shares are up about 5% so far in 2025 after rising 171% in 2024 and nearly 239% in 2023, reflecting insatiable demand for the company's artificial intelligence chips.\n\nMeanwhile, Apple shares slid 3% on Tuesday. They're now down 11% this year after gaining 30% in 2024. The iPhone maker has developed its Apple Intelligence suite of AI features for its phones and laptops, but its business doesn't have the same level of exposure to the AI boom.\n\nNvidia has the vast majority of market share for graphics processing units, or GPUs, which have become essential for developing and deploying AI software such as OpenAI's ChatGPT. While revenue growth has slowed, it still nearly doubled to $35.08 billion in the most recent quarter.\n\nApple was the first company to reach the $1 trillion, $2 trillion and $3 trillion market cap milestones. Nvidia previously passed Apple in June and then again in November.\n\nAt Tuesday's close, Nvidia had a market cap of about $3.45 trillion, versus Apple at $3.35 trillion.Microsoft is just behind them at $3.2 trillion. A major buyer of Nvidia's GPUs, Microsoft said earlier this month that it expected to spend $80 billion on AI data centers in fiscal 2025.\n\nIn November, Nvidia joined the Dow Industrial Average, replacing Intel, and joining Apple and Microsoft in the blue-chip index.\n\nWATCH: Nvidia's outperformance will continue and growth rates will remain higher",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "Nvidia and Mayo Clinic Partner on AI-Powered Digital Pathology",
            "link": "https://www.genengnews.com/topics/artificial-intelligence/nvidia-and-mayo-clinic-partner-on-ai-powered-digital-pathology/",
            "snippet": "Reviving the tradition of unveiling game-changing announcements at the J.P. Morgan Healthcare Conference, Nvidia made a major announcement to kick off the...",
            "score": 0.5518054962158203,
            "sentiment": null,
            "probability": null,
            "content": "Reviving the tradition of unveiling game-changing announcements at the J.P. Morgan Healthcare Conference, Nvidia made a major announcement to kick off the 2025 conference, a significant departure from the last such event. By equipping the world-renowned Mayo Clinic with its state-of-the-art artificial intelligence platforms, Nvidia and its partner will greatly speed up the creation of next-generation digital pathology. In addition to improving the hospital\u2019s digital pathology tools and infrastructure, the partnership will push precision medicine to new heights by creating human digital twins and physical AI.\n\n\u201cOur primary goal is to enable the broad-scale transformation of pathology data into meaningful insights,\u201d said Jim Rogers, CEO of Mayo Clinic Digital Pathology. \u201cWe will unlock the data currently inaccessible in the current standard of practice, which involves reading glass slides. Our collaboration with Nvidia allows us to build and power the necessary infrastructure to make this a reality. Ultimately, we are invested in creating an ecosystem, a whole new way to deliver value to patients and caregivers. Through this initiative, Mayo Clinic will catalyze digitization and the delivery of impactful AI-driven pathology solutions to patients and caregivers.\u201d\n\nNext-generation digital pathology\n\nMayo Clinic has led in transforming pathology\u2014a field traditionally reliant on slow and manual methods\u2014into a faster, more precise digital process to enhance how doctors diagnose and treat complex diseases like cancer.\n\nMayo Clinic\u2019s Digital Pathology platform, developed in the last few years, uses state-of-the-art imaging technology and automated robotic labs to compile an unparalleled database of medical data. This database includes 10 million patient records and 20 million high-resolution slide images. Such a comprehensive dataset is perfect for developing foundational AI models and robust, pre-trained systems to tackle various tasks across different fields. By leveraging this data, Mayo Clinic is paving the way for groundbreaking AI tools to revolutionize the speed and accuracy of disease diagnosis and treatment.\n\n\u201cSince the 1800s through to today, pathologists have worked with microscopes and glass slides,\u201d said Rogers. \u201cEven now, fewer than 10% of pathology practices in the world are digitized, preventing the necessary transformation of pathology\u2014the practice that is the front end of much of the serious and complex care in medicine\u2014for the benefit of our patients and clinicians. This is unacceptable. Mayo Clinic Digital Pathology is building the tools and infrastructure so that this inaccessible data can be used to create increasingly personalized and accurate diagnoses, predictions, and treatments for patients.\u201d\n\nMayo Clinic plans to use Nvidia\u2019s new DGX B200 platform, which uses super-powerful computers with 1.4 terabytes of memory specifically designed to handle large medical image files like digital pathology slides. They will also use MONAI, Nvidia\u2019s free, open-source platform for developing AI tools for medical imaging. This combination will help them process and analyze complex medical images more effectively.\n\nRogers explained, \u201cIn the short term, we will define, build, and optimize the tools and techniques necessary for extracting and refining actionable insights from pathology slides and related data. A great example of this is our collaboration with Aignostics, which, within two months, has delivered a leading digital pathology foundation model. We will enable innovators to create the best solutions for patients and caregivers. In the longer term, we believe edge computing for AI, where partners such as Nvidia have incredible capabilities, will be valuable to provide impactful AI at the point of care while simultaneously addressing some of the cost and complexity factors that prevent the necessary scaling and adoption of digital pathology.\u201d\n\nHuman digital twins and physical AI\n\nThe Nvidia-Mayo collaboration doesn\u2019t stop at just beefing up the hospital\u2019s leading digital pathology platform. Nvidia and Mayo Clinic also aim to create human digital twins\u2014a virtual representation of a person\u2014to simulate, predict, and optimize patient health outcomes. Human digital twins can be used in clinical trials, medical training, and surgical intervention planning.\n\n\u201cOur ultimate goal is to create a human digital twin,\u201d said Kimberly Powell, vice president of healthcare at Nvidia. \u201cThis dynamic digital representation includes medical imaging, pathology, health records, and wearables. To achieve this, Mayo Clinic and Nvidia will leverage some of the latest AI and vision language models like Cosmos Nemotron and our microservices. This collaboration will be a cornerstone for new drug discovery and diagnostic medicine applications.\u201d\n\nIf that isn\u2019t futuristic enough, Mayo Clinic and Nvidia will continue to expand this collaboration to advance the use of physical AI, such as robots, in healthcare and life sciences. Physical AI models are costly and require vast real-world data and testing. Mayo Clinic combines its medical knowledge and AI expertise with Nvidia\u2019s new platform, Nvidia Cosmos, to create advanced computer models. These models are designed to simulate real-world environments, helping robots learn how to move around and interact with the physical world. Announced at the CES 2025 tech conference, Nvidia Cosmos makes it easier for developers to create large amounts of realistic, physics-based data that can improve and test their AI systems.\n\nNvidia\u2019s investment in life sciences and healthcare\n\nThe news of Nvidia\u2019s partnership with Mayo Clinic was just one of four such announcements. The computing technology developer and manufacturer is also partnering with IQVIA, Illumina, and Arc Institute to revolutionize the healthcare and life sciences industries with AI and accelerated computing.\n\n\u201cHealthcare is a $10 trillion industry with over 30% of its operating expenses dedicated to meeting the growing demand,\u201d said Powell. \u201cTo address this challenge, we need AI solutions of all kinds. No single company can achieve this alone. The incredible ecosystem and partnerships we\u2019ve just announced here at JP Morgan will demonstrate this rapid adoption of Nvidia AI by the world-leading companies in the industry. We\u2019re on a mission to improve patient care and increase accessibility, and together, we will write the next chapter in medicine.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-20": {
        "0": {
            "title": "One Leadership Trait You Don\u2019t Want to Learn From Nvidia\u2019s Jensen Huang",
            "link": "https://www.inc.com/jeff-haden/one-leadership-trait-you-dont-want-to-learn-from-nvidias-jensen-huang/91107568",
            "snippet": "A story about Nvidia co-founder Jensen Huang from Tae Kim's book The Nvidia Way, about the rise (and rise) of the AI hardware and software giant.",
            "score": 0.9112136363983154,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "What US Chip Export Restrictions Mean For Nvidia",
            "link": "https://technologymagazine.com/articles/what-us-chip-export-restrictions-mean-for-nvidia",
            "snippet": "Biden's last-minute system for GPU exports imposes controls on global advanced AI chips including Nvidia's products for US dominance.",
            "score": 0.723394513130188,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Forget Tesla and Nvidia Stocks. Check Out These Unloved AI Bargains.",
            "link": "https://www.barrons.com/articles/tesla-nvidia-stocks-ai-buy-59e3eb42",
            "snippet": "Tesla stock and Nvidia stock may be the obvious artificial intelligence plays\u2014but here are some AI names that still look undervalued.",
            "score": 0.8668388724327087,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia and TSMC to collaborate on silicon photonics technology",
            "link": "https://www.datacenterdynamics.com/en/news/nvidia-and-tsmc-to-collaborate-on-silicon-photonics-technology/",
            "snippet": "Nvidia has partnered with TSMC for the development of silicon photonics. Speaking to reporters after meeting with TSMC chairman CC Wei in Taiwan.",
            "score": 0.7852309346199036,
            "sentiment": null,
            "probability": null,
            "content": "Speaking to reporters after meeting with TSMC chairman C.C. Wei in Taiwan, Nvidia CEO Jensen Huang said that the partnership was still in its earlier stages, with local media reported Huang said any results would \u201ctake a few years.\u201d\n\nSilicon photonics has long been touted as a solution to help data centers deal with increasing bandwidth demands and improve connectivity between servers and switches. The technology involves using silicon instead of glass as the light-transport medium to create photonic integrated circuits.\n\nThis is not the first time TSMC has been tied to a silicon photonics project. In May 2024, chipmaker TSMC announced it was partnering with Ansys, Synopsys, and Cadence to develop its silicon photonics integration system capabilities.\n\nLater that same year, Ansys and TSMC teamed up with Microsoft to enhance the simulation and analysis of silicon photonic components using Microsoft Azure NC A100v4-series virtual machines to speed up the Ansys Lumerical FDTD photonics simulation by more than 10x.\n\nThe company is also a founding member of the Silicon Photonics Industry Alliance, a group comprised of more than 30 Taiwanese companies focused on the development of silicon photonics and co-packaged optics (CPO) technology \u2013 the advanced heterogeneous integration of optics and silicon onto a single packaged substrate.\n\nMeanwhile, in October 2024, Nvidia participated in a $44 million Series A funding round for silicon photonics startup Xscape Photonics. According to the company, the funds would be used to accelerate the development of its ChromX platform, which it claims can maximize GPU \u201cescape bandwidth,\u201d reduce power consumption, and improve overall inference performance for AI workloads.\n\nChinese-language media has also recently reported that Nvidia plans to unveil optical switches for CPO technology at its GTC conference in March, in an effort to reduce power consumption and heat in AI servers. Sources cited in said media reports claim TSMC will begin mass production of the Switch ASIC chip in August 2025.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia's AI Throne Under Siege? Rivals Marvell & Broadcom Are Closing the Gap",
            "link": "https://finance.yahoo.com/news/nvidias-ai-throne-under-siege-170116324.html",
            "snippet": "Nvidia's AI dominance remains strong, but Barclays warns the semiconductor race is shifting--Marvell, Broadcom, and Lumentum are rising fast.",
            "score": 0.8165833950042725,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NASDAQ:NVDA) isn't slowing down. Barclays analyst Thomas O'Malley just reaffirmed his Buy rating and hiked his price target to $175, signaling confidence in the AI giant's momentum. With $35.08 billion in quarterly revenuenearly doubling last year's $18.12 billionNvidia is printing money. But not everyone's bullish. Insider sentiment has turned cautious, with key executives cashing out big. Case in point: Director Tench Coxe offloaded 1 million shares, raking in $131 million. Meanwhile, Barclays warns that while Nvidia's GPU dominance remains intact, customers are actively seeking cheaper alternativesa trend that could shake up the semiconductor hierarchy.\n\nEnter Marvell (NASDAQ:MRVL) and Broadcom (NASDAQ:AVGO)two AI semiconductor contenders gaining serious traction. Barclays raised Marvell's price target to $150, betting big on its proprietary serializer/deserializer tech and a projected $4 billion ASIC market by 2026. Broadcom got a boost, tooits target jumped to $260, as analysts expect $21 billion to flow into its custom chip division by FY26. Lumentum Holdings (NASDAQ:LITE) is another dark horse, scoring a double upgrade to Overweight thanks to explosive hyperscaler adoption beyond Google (NASDAQ:GOOG). Barclays is drawing battle lines: the AI chip market is splitting into haves and have-nots, and traditional semis, analog, and PC chips are getting left in the dust.\n\nThe big picture? AI semiconductors are heading into a $1 trillion total addressable market by the end of the decade, and Nvidia won't have the whole pie to itself. While NVDA is still king, demand for lower-cost, high-efficiency AI chips is surging. Marvell, Broadcom, and Lumentum are moving in fast, and the semiconductor shake-up is just beginning. Investors looking for AI exposure might want to pay attentionbecause this market is about to get a whole lot more competitive.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "Nvidia Stock Investors Just Got Great News From Wall Street That Could Send Shares Soaring",
            "link": "https://www.fool.com/investing/2025/01/20/nvidia-stock-investors-great-news-from-wall-street/",
            "snippet": "Nvidia's Blackwell GPU could a bigger success than most analysts anticipate. Tom O'Malley at Barclays last week raised his target price on Nvidia to $175 per...",
            "score": 0.838508129119873,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) has been one of the hottest stocks on the market in recent years. Shares have advanced 840% since December 2022 amid tremendous demand for the company's graphics processing units (GPUs), chips that power essentially all of the most advanced artificial intelligence systems.\n\nNvidia is currently worth $3.3 trillion, but certain Wall Street analysts see upside arising from its Blackwell GPUs and robotics computing solutions. Here's the good news for Nvidia shareholders.\n\nNvidia's Blackwell GPU could a bigger success than most analysts anticipate\n\nTom O'Malley at Barclays last week raised his target price on Nvidia to $175 per share, up from $160 per share. That forecast implies nearly 28% upside from its current share price of $137. He cited expectations that Nvidia's next-generation Blackwell GPUs will drive a meaningful increase in sales growth as the reason for the upward revision.\n\nRemember, Nvidia GPUs are the industry standard in accelerating data center tasks like artificial intelligence (AI). Compared to the previous Hopper architecture, Blackwell GPUs can handle AI training tasks up to four times faster and AI inference tasks up to 30 times faster. O'Malley estimates Blackwell GPUs will add $15 billion to sales in the current quarter, and thinks that number could more than double in the next quarter.\n\nBarclays is not the only Wall Street firm to revise its outlook higher. LSEG data shows the average estimate regarding Nvidia's earnings in the fourth quarter of fiscal 2025 (which ends in January 2025) has increased 5% in the last 90 days, and the average estimate for fiscal 2026 has increased 9% during the same period. Many analysts attribute their upward revisions to increased confidence in Blackwell.\n\nBeth Kindig, lead technology analyst at the I/O Fund, is particularly optimistic. She believes data center sales will increase at least 50% in fiscal 2026, driven by Blackwell GPU sales of at least $200 billion. Looking further ahead, Kindig estimates Nvidia will be worth $10 trillion by 2030, which implies 200% upside from its current market value of $3.3 trillion.\n\nAdditionally, Dan Ives at Wedbush says demand for Blackwell GPUs is currently outpacing supply by a factor of 15. Put differently, he believes Nvidia can only supply a single chip for every 15 chips that customers want to buy. That adds context to recent commentary from Nvidia CFO Colette Kress, who said during the third-quarter earnings call that Blackwell \"demand greatly exceeds supply.\"\n\nNvidia has an underappreciated opportunity in autonomous driving and robotics\n\nDan Ives recently told CNBC the Wall Street consensus underestimates Nvidia's earnings growth by 30% over the next few years. He attributes some of that shortfall to Blackwell GPU revenue, which he believes will beat expectations in the near term. But Ives also sees upside to the consensus arising from robotics solutions in the long term.\n\nThat last point is particularly important. The market is currently focused on generative AI, but Nvidia CEO Jensen Huang says the next wave of AI is robotics powered by physical AI. For context, generative AI can create text, images, and other media content in response to prompts, whereas physical AI can understand, navigate, and interact with the real world.\n\nNvidia has been very successful in monetizing generative AI, which is good reason to believe the company will be just as successful with physical AI. Nvidia has the distinct advantage of providing a full-stack computing solution for autonomous robots. Its robotics platform comprises the supercomputing chips and networking gear needed to train AI models, and the software development tools needed to build industrial manipulation arms, autonomous vehicles, and humanoid robots.\n\nImportantly, Nvidia's automotive and robotics sales totaled just $449 million in the most recent quarter, which pales in comparison to the company's $30.7 billion in data center revenue. However, Jensen Huang believes autonomous vehicles and robotics will eventually become two of the largest computing industries in the world, which hints at strong growth in the coming years.\n\nIndeed, Ives estimates that Nvidia has a $1 trillion opportunity in autonomous vehicles and robotics, and he believes the company will ultimately attain a market value of $5 trillion as it monetizes those opportunities. His forecast implies 52% upside from its current market value of $3.3 trillion. That is undoubtedly great news for Nvidia shareholders.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Where does Microsoft's NPU obsession leave Nvidia's AI PC ambitions?",
            "link": "https://www.theregister.com/2025/01/20/microsoft_nvidia_ai_pcs/",
            "snippet": "Comment Nvidia is the uncontested champion of AI infrastructure \u2014 at least in the datacenter. In the emerging field of AI PCs, things aren't so clear cut.",
            "score": 0.8123357892036438,
            "sentiment": null,
            "probability": null,
            "content": "Comment Nvidia is the uncontested champion of AI infrastructure \u2014 at least in the datacenter. In the emerging field of AI PCs, things aren't so clear cut.\n\nIn early 2024, it became plain that, for better or worse, the future of Windows would be imbued with AI-augmented features and experiences. Headline features included live captions and translation, image generation in MS Paint, and, eventually, the somewhat dubious Recall feature that captures periodic screenshots and uses them to keep track of past activity.\n\nFor the moment, these new features are exclusive to so-called Copilot+ PCs, but in order to qualify for that designation, the computers have to meet Microsoft's minimum performance targets.\n\nAccording to the Windows titan's documentation, Copilot+ PCs require a neural processing unit (NPU) capable of 40 or more TOPS, or 40-plus trillion INT8 AI operations per second, along with at least 16GB of RAM and 256GB of storage. When this all launched, only Qualcomm had a processor capable of meeting Redmond's NPU requirement, and so only PCs with that silicon were allowed as Copilot+ PCs to run the aforementioned AI-augmented features.\n\nSince then, Qualcomm's qualifying Arm-compatible X chips were joined by Intel's Lunar Lake and AMD's Strix Point and Halo processor families as Copilot+ PC compliant.\n\nYet, somehow, a $2,000 Nvidia RTX 5090, as announced at CES 2025 this month, with more than 3.3 petaFLOPS of AI compute (that's at FP4 by the way) still isn't good enough for Redmond. No matter how many FLOPS or TOPS your GPU can muster, it only matters to Microsoft if it's an NPU churning them out \u2014 for now anyway.\n\nNvidia hasn't been slacking on the AI PC\n\nMuch of the marketing hype around AI PCs has revolved around Microsoft's Copilot+ spec and understandably so. Nearly every PC sold today runs Windows. This dominance of the PC software ecosystem makes Microsoft's obsession with NPUs hard to ignore, but, that doesn't mean Nvidia has been resting on its laurels, content with lording it over the datacenter, workstation graphics, and discrete gaming GPUs.\n\nIn fact, Nvidia has been working to bring AI features to the PC for years, Jesse Clayton, who leads product marketing for Windows AI at Nvidia, told The Register.\n\n\"We kind of started the movement with AI on the PC back in 2018 when we launched the first GeForce GPUs and Nvidia GPUs with dedicated AI hardware \u2014 our tensor cores,\" Clayton said. \"Along with that, we announced the first widely deployed PC AI, which was DLSS, which is used in games to accelerate frame rates by using AI to generate pixels and now generating frames for the games.\"\n\nSince then the GPU giant has rolled out the RTX AI Toolkit, a suite of tools and software for optimizing and deploying genAI models on Windows PCs, brought Nvidia Inference Microservices (NIMs) to PCs, and rolled out a number of blueprints for things like state-of-the-art image generation, and converting PDFs to podcasts.\n\n\"Our strategy is where we can deliver interesting and differentiated experiences, either as a gamer because it enhances your gameplay, or as a creator because it saves you time and reduces the repetitive, tedious work,\" Clayton explained.\n\nAnd, while some of these experiences are aimed directly at end users \u2014 ChatRTX and RTX Voice, for example \u2014 many of Nvidia's more recent software launches have been aimed at the developer community.\n\nCompetition or opportunity\n\nSay what you will about Copilot+'s actual value, Microsoft has successfully forced chipset designers to offer some form of NPU that satisfies the Windows giant while also setting a new minimum standard for machine-learning performance.\n\nConsidering Windows' market share and Microsoft's ongoing efforts to shoehorn AI into every corner of its software, it's only a matter of time before NPUs trickle down to even the lowliest of budget builds.\n\nWhat's more, adoption of frameworks such as Microsoft's DirectML and ONNX Runtime have helped to simplify application development and allow code to run across a diverse set of hardware with minimal retooling.\n\nJust as your LLM once again goes off the rails, Cisco, Nvidia are at the door smiling READ MORE\n\nThis poses a potential problem for Nvidia. The Silicon Valley goliath may dominate the discrete graphics processor market, surrounded by its CUDA moat, yet its GPUs are only found in about 18 percent of PCs sold with the vast majority of systems using integrated graphics from Intel, AMD, or others.\n\nThe case can be made that, before too long, NPUs will become a much larger target for developers building AI apps. And while Nvidia won't necessarily be left out of the conversation as its accelerators also support many of the more popular software frameworks, at least some of its competitive edge revolves around convincing developers to use its libraries and microservices, which promise easier integration and higher performance and efficiency.\n\nUltimately, Clayton says, developers will have to make a decision on whether they want to bring their app to market quickly using something like NIMs or if they want to support the largest possible install base.\n\nBut, while Nvidia may face competition from NPUs eventually \u2014 AI PCs are still a pretty niche market \u2014 it isn't necessarily all bad news. Even if models don't end up running on Nvidia's PC hardware, it's highly likely they were trained on its GPUs.\n\nEven then, Clayton makes the case that NPUs won't be appropriate for every workload. Forty TOPS is a decent amount of compute, but, as we mentioned earlier, it pales in comparison to the performance of high-end graphics silicon.\n\n\"NPUs are going to be where you can run your lightweight AI workloads, and they're going to be really power efficient,\" he said. \"A GPU is where you run your more demanding AI use cases, and that's where we've been pushing and focusing our efforts.\"\n\n\"For stuff that simply doesn't fit on a PC, you run those on GPUs in the cloud, where you have effectively unlimited performance,\" Clayton added.\n\nGPUs may get some Copilot+ love after all\n\nThere's already some evidence to suggest that Microsoft may extend some Copilot+ functionality to GPUs to support more computationally challenging workloads in the future.\n\nMicrosoft didn't address our questions regarding its plans to harness GPUs. However, in an anouncement from June, Nvidia said it was working with Microsoft to add GPU acceleration for small language models (SLMs) via the Windows Copilot Runtime.\n\nThe tech was supposed to materialize by the end of 2024, though Microsoft's own docs \u2014 last updated December 5 \u2014 make no mention of GPUs and specifically cite NPUs as a requirement for its as-yet unavailable Phi Silica project for SLMs.\n\nClayton declined to provide any updates on the collaboration, saying that \"ultimately, it's Microsoft's decision for where they're going to run which workloads.\"\n\nIf and when Microsoft chooses to embrace GPUs for local AI may ultimately come down to hardware availability. As of writing, the number of NPU-toting Copilot+ PCs with dedicated graphics is rather small.\n\nOn the desktop, the situation is even trickier. Desktop chips with NPUs do exist, but none of them \u2014 at least to our knowledge \u2014 meet Microsoft's 40 TOPS performance requirement. We don't expect it'll be long before beefier NPUs make their way into desktop silicon. All it would take is Intel or AMD finding a way to cram the NPUs from their mobile chips into a desktop form factor. \u00ae",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Most GeForce Now tiers are currently sold out \u2014 Nvidia blames high demand for unavailability",
            "link": "https://www.tomshardware.com/video-games/cloud-gaming/most-geforce-now-tiers-are-currently-sold-out-nvidia-blames-high-demand-for-unavailability",
            "snippet": "The free-tier users are the most affected, as Nvidia is likely prioritizing premium users first. Nvidia says they are restricting supply so existing users can...",
            "score": 0.9279289841651917,
            "sentiment": null,
            "probability": null,
            "content": "Most of Nvidia's GeForce Now game streaming tiers are currently unavailable, reportedly due to high demand. Gamers looking to opt for Nvidia's cloud gaming platform will be left disappointed, as five of the eight subscription tiers are currently sold out. Nvidia (via ComputerBase) attributes the current unavailability to high subscriber demand.\n\nFurther investigation suggests this is likely a global issue, as the same tiers remain inaccessible for several regions. Last year, Nvidia introduced Day Passes, which provide 24-hour access to Nvidia's high-performance machines without the commitment of a long-term subscription. Likewise, Nvidia also offers one-month and six-month memberships. These one-month and six-month plans are divided into Free (Basic Rig with Ads), Premium (1440p + RTX ), and Ultimate (4K HDR + 240 FPS + RTX) packages.\n\nThe affected plans include all the Day Passes, Free, and Performance tiers of the 1-month and the Free tier of the 6-month plan. The free-tier users are the most affected, as Nvidia is likely prioritizing premium users first. Nvidia says they are restricting supply so existing users can enjoy a seamless experience. GeForce Now isn't accepting new customers across several tiers to prevent the servers from being overloaded.\n\nImage 1 of 3 (Image credit: Nvidia) (Image credit: Nvidia) (Image credit: Nvidia)\n\nThere could be several possibilities, but based on the given statement, it is fair to assume that Nvidia's servers are likely at full capacity and unable to keep up with the influx of new users. Back in November, Nvidia introduced a 100-hour limit per month for all tiers, roughly equivalent to around three hours per day. Any more, and you'll have to pay extra: $5.99 for 15 additional hours on the Ultimate tier, down to $2.99 for the Performance tier.\n\nThinking out of the box, if this was a capacity problem, it's improbable all servers would be affected similarly. Nvidia may be upgrading the underlying hardware while the existing hardware continues to function in parallel, but that's speculation.\n\nWhile this may inconvenience newcomers, Nvidia is prioritizing service quality for existing subscribers. Moreover, this isn't the first time this has happened. Similar issues arose in 2020, so it's likely only a matter of time before Nvidia resolves this problem. It could be as simple as upgrading the existing infrastructure to accommodate more users, but we'll wait for more information from Nvidia before jumping to conclusions.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "8": {
            "title": "Frame Generation May Come to Nvidia\u2019s 30-series GPUs",
            "link": "https://gamerant.com/frame-generation-nvidia-30-series-gpus-rumor/",
            "snippet": "Frame Generation (FG) was introduced for the Nvidia RTX 40-series GPUs, and it uses AI to generate frames for a significant bump to FPS in a lot of games.",
            "score": 0.8601294159889221,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Aetina Announces Support for NVIDIA's Super Mode",
            "link": "https://www.manufacturing.net/artificial-intelligence/news/22931146/aetina-announces-support-for-nvidias-super-mode",
            "snippet": "Aetina announced its comprehensive support for Super Mode, a feature introduced in the latest NVIDIA Jetson Orin NX and Jetson Orin Nano modules.",
            "score": 0.5508874654769897,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-19": {
        "0": {
            "title": "Jensen Huang Just Delivered Incredible News for Nvidia Stock Investors",
            "link": "https://www.fool.com/investing/2025/01/19/jensen-huang-delivered-news-nvidia-stock-investors/",
            "snippet": "At the CES 2025 technology conference on Jan. 7, Nvidia CEO Jensen Huang highlighted a new multitrillion-dollar opportunity in autonomous vehicles.",
            "score": 0.7812424302101135,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) was founded in 1993, and it went on to create the world's first graphics processing units (GPUs) for computing, media, and gaming applications. Now, decades later, the company has adapted those powerful chips for data centers, where they are used to develop advanced artificial intelligence (AI) models.\n\nNvidia CEO Jensen Huang believes data center operators will spend $1 trillion over the next four years on upgrading their infrastructure to meet demand from AI developers. Since the data center segment currently accounts for 88% of Nvidia's total revenue, that spending will be instrumental to the company's future success.\n\nHowever, the semiconductor industry has always been cyclical, so the data center boom won't last forever. That's why it's critical for Nvidia to diversify its revenue streams, and at the CES 2025 technology conference on Jan. 7, Huang delivered some incredible news for investors on that front.\n\nMeet Nvidia's next multitrillion-dollar opportunity: Autonomous vehicles\n\nNvidia saw the autonomous driving revolution coming. In fact, the company's automotive business is more than two decades old, but its revenues were so tiny that it lived in the shadow of the gaming and data center segments. That's all about to change, because global car brands like Mercedes-Benz, Hyundai, BYD, Volvo, Toyota, and more are adopting Nvidia's Drive platform to power their autonomous ambitions.\n\nDrive provides all of the internal hardware and software a car needs for self-driving capabilities. That includes Nvidia's latest chip called Thor, which processes all of the incoming data from the car's sensors to determine the best course of action on the road. But Nvidia's opportunity doesn't end there, because it also sells the infrastructure a car company needs to maintain and improve its autonomous models, so it can differentiate itself from the competition.\n\nIn addition to Drive, Huang says car companies are buying DGX data center systems featuring its latest Blackwell-based GB200 GPUs, which deliver the necessary computing power to continuously train self-driving software. Then there is Nvidia's new Cosmos multimodal foundation model, which allows companies to run millions of real-world simulations using synthetic data, serving as training material for the software.\n\nOverall, Huang says autonomous vehicles could be the first multitrillion-dollar opportunity in the emerging robotics space. He's not alone, because Cathie Wood's Ark Investment Management thinks technologies like autonomous ride-hailing could create $14 trillion in enterprise value by 2027, with the majority of that value attributed to autonomous platform providers -- in this case, that would be Nvidia.\n\nNvidia's fiscal year 2025 will finish at the end of January, but the company generated $1.1 billion in automotive revenue through the first three quarters (if we extrapolate that result, full-year revenue will probably be around $1.5 billion). Huang says in fiscal 2026, Nvidia's automotive revenue could soar to $5 billion, so it's going to ramp up insanely fast.\n\nData center chips are the main story for now\n\nWall Street's consensus forecast (provided by Yahoo) suggests Nvidia could generate a whopping $196 billion in total revenue during fiscal 2026, so the automotive segment's potential $5 billion contribution would still be relatively tiny. It's a longer-term story that could secure Nvidia's future growth, but in the here and now, it's all about the data center.\n\nNvidia just started shipping its new Blackwell GB200 GPUs to customers, but sales are expected to grow quickly. By April this year, revenue from Blackwell chips could overtake revenue from the previous generation of chips built on the Hopper architecture, which highlights how quickly Nvidia's business is evolving.\n\nThe GB200 NVL72 system is capable of performing AI inference up to 30 times faster than the equivalent H100 GPU system, so Blackwell will pave the way for the most advanced AI models to date. Therefore, over the next year or so, consumers and businesses might have access to the \"smartest\" AI software applications (like chatbots and virtual assistants) so far.\n\nDemand for Blackwell chips is outstripping supply, which should support further strength in Nvidia's revenue and earnings during fiscal 2026. Plus, some reports suggest a Blackwell successor called \"Rubin\" might be unveiled later in the year, which would further cement the company's chokehold on the market for data center GPUs.\n\nIt's probably not too late to buy Nvidia stock, despite its incredible run\n\nNvidia stock has soared by 830% since the start of calendar year 2023, lifting the company's value from $360 billion to an eye-popping $3.3 trillion in just two years. Despite the amazing run, the stock might still be cheap.\n\nIt currently trades at a price-to-earnings (P/E) ratio of 53.6, which is a discount to its 10-year average P/E ratio of 59. But Wall Street's consensus estimate suggests Nvidia could generate $4.44 in earnings per share in fiscal 2026, placing its forward P/E ratio at just 30.6.\n\nIn other words, Nvidia stock would have to soar by 92% over the next 12 months just to trade in line with its 10-year average P/E ratio of 59.\n\nNvidia has a habit of beating Wall Street's forecasts, so it's possible the stock has even more upside potential. On the flip side, there is some competition emerging from other chipmakers like Advanced Micro Devices, which plans to release a Blackwell rival in a few months. That's a risk investors should keep an eye on as this year progresses.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "Understanding The Physics-Aware Systems That Nvidia Is Working On",
            "link": "https://www.forbes.com/sites/johnwerner/2025/01/19/understanding-the-physics-aware-systems-that-nvidia-is-working-on/",
            "snippet": "As just one part of what came out of this year's CES earlier this month, Nvidia announced its development of something called Nvidia Cosmos.",
            "score": 0.882931649684906,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "NVIDIA DLSS Boss Doesn\u2019t Rule Out Possible Frame Generation Support for RTX 30 Series",
            "link": "https://wccftech.com/nvidia-dlss-boss-doesnt-rule-out-possible-frame-generation-support-for-rtx-30-series/",
            "snippet": "The announcement of NVIDIA DLSS 4 and its many improvements to Super Resolution, Ray Reconstruction, and Frame Generation.",
            "score": 0.8529733419418335,
            "sentiment": null,
            "probability": null,
            "content": "At CES 2025, Digital Foundry interviewed Bryan Catanzaro, VP of Applied Deep Learning Research, about the announcement of NVIDIA DLSS 4 and its many improvements to Super Resolution, Ray Reconstruction, and Frame Generation.\n\nCatanzaro talked about the new transformer models that are replacing CNN (convolutional neural networks) for Super Resolution and Ray Reconstruction. These are simply much smarter, can be trained on larger datasets and make better choices as a result, improving historic NVIDIA DLSS shortcomings like shimmering or ghosting. The new Super Resolution model, for instance, has four times more compute compared to the previous one. Catanzaro didn't provide an estimate on how much more rendering time it will cost, but he did say NVIDIA believes it's the best way to play on the new Blackwell-powered GeForce RTX 50 graphics cards that are launching later this month.\n\nFrame Generation is also being overhauled, abandoning the previous model based on the Optical Flow hardware accelerator for an entirely AI-powered solution. Here's why NVIDIA is doing that:\n\nWhen we built NVIDIA DLSS 3 Frame Generation, we absolutely needed hardware acceleration to compute Optical Flow. We didn't have enough Tensor Cores and we didn't have an Optical Flow algorithm that was good enough. We hadn't developed a real-time Optical Flow algorithm that ran on Tensor Cores that could fit our compute budget. We had the Optical Flow accelerator, which NVIDIA had been building for years as an evolution of our video encoder technology, and it's also been a part of our automotive computer vision acceleration for self-driving cars.\n\nIt made sense for us to use that for NVIDIA DLSS 3 Frame Generation. But the difficult part about any sort of hardware implementation of an algorithm like Optical Flow is that it's really difficult to improve it. It is kind of what it is and the failures that arose from that hardware Optical Flow, we couldn't undo them with a smarter neural network until we decided to just replace it and go with a fully AI-based solution, so that's what we've done for Frame Generation in DLSS 4.\n\nThe new Frame Generation model is heavier on Tensor Cores, but it uses less VRAM, offers improved image quality (which Catanzaro deemed critical, especially for the new Multi Frame Generation available on the new RTX 50 GPUs), and is also more efficient as the cost has been amortized over multiple frames.\n\nDF's Alex Battaglia asked then whether the new model could be ported to older hardware like the GeForce RTX 30 Series, and the head of NVIDIA DLSS didn't close that door.\n\nI think this is primarily a question of optimization and also engineering and then the ultimate user experience. We're launching this Frame Generation, the best Multi Frame Generation technology, with the 50 Series, and we'll see what we're able to squeeze out of older hardware in the future.\n\nAs a reminder, when NVIDIA introduced Frame Generation with the GeForce RTX 40 graphics cards, Catanzaro himself explained that the feature was exclusive to then-new GPUs because they had a much improved Optical Flow hardware accelerator than the RTX 30 Series. Back then, he also said it was theoretically possible to port it to older hardware, although it probably wouldn't be as beneficial.\n\nWith the new model removing the Optical Flow hardware accelerator, it seems like the door might be open for that to happen. However, Catanzaro also said the Tensor Core requirements are higher, and obviously, the older GPU architecture has worse Tensor Core performance. We'll see if NVIDIA can really make it happen.\n\nElsewhere in the interview, Bryan Catanzaro highlighted the importance of decoupling the updated flip metering from the CPU to reduce frame time variability (and, therefore, improve frame pacing) by a factor of five to ten compared to before. Last but not least, he claimed that playing a game with Reflex 2 (which is also AI-based) feels much more 'connected' and he believes that especially latency-sensitive gamers will love it.\n\nStay tuned for more NVIDIA DLSS 4 coverage on Wccftech as we get closer to the launch of the new GeForce RTX 50 graphics cards.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Better Artificial Intelligence (AI) Stock for 2025: Nvidia vs. Microsoft",
            "link": "https://www.nasdaq.com/articles/better-artificial-intelligence-ai-stock-2025-nvidia-vs-microsoft",
            "snippet": "The 12-month median price targets of both companies suggest that Nvidia stock could rise 33% while Microsoft is expected to deliver 20% gains.",
            "score": 0.7921064496040344,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NASDAQ: NVDA) and Microsoft (NASDAQ: MSFT) are pioneers in artificial intelligence (AI) as both companies have played central roles in bringing this technology mainstream.\n\nWhile Microsoft-backed OpenAI kicked off the AI craze when it launched the highly popular ChatGPT in November 2022, the chatbot may not have seen the light of the day without Nvidia's graphics processing units (GPUs), which were used for training the large language model (LLM) powering ChatGPT. As other major tech giants joined the AI race, the demand for Nvidia's chips went through the roof, and sent its revenue and bottom line soaring.\n\nWhere to invest $1,000 right now? Our analyst team just revealed what they believe are the 10 best stocks to buy right now. See the 10 stocks \u00bb\n\nMeanwhile, the impact of AI on Microsoft's business has been gradual. The company is spending billions to build its AI data center infrastructure -- benefiting Nvidia in the process -- and believes its AI-focused investments will \"support monetization over the next 15 years and beyond.\" This explains why Nvidia stock has outperformed Microsoft's by a massive margin in the past couple of years.\n\nNVDA data by YCharts.\n\nBut will Nvidia continue to outperform Microsoft in 2025? Let's find out.\n\nMicrosoft's AI-driven growth should gain momentum, while Nvidia faces challenges\n\nAs already mentioned, Microsoft is benefiting gradually from AI. The company's revenue in fiscal year 2024 (that ended June 30) was up 16% from the previous year to $245 billion. Its adjusted earnings increased 20% year over year to $11.80 per share. The company's estimated revenue growth rate of 14% for fiscal 2025 to $278.6 billion isn't all that great, while its earnings are expected to increase by 10.5% to $13.04 per share.\n\nThe forecasts for fiscal 2026, which will begin in July 2025, don't point toward a major improvement either. Consensus estimates are projecting a 14% increase in Microsoft's revenue in the next fiscal year, along with a 15% jump in earnings.\n\nMSFT Revenue Estimates for Next Fiscal Year data by YCharts.\n\nNvidia, on the other hand, is expected to finish the current fiscal year (ending January 2025) with outstanding revenue growth of 112% to $129 billion, followed by a 52% increase in the next fiscal year to $196 billion. Nvidia's bottom line is also expected to jump an impressive 128% in the ongoing fiscal year followed by a 50% jump in the next one.\n\nAll this indicates that Nvidia could continue to grow at a much faster pace than Microsoft over the course of the next year. What's more, the 12-month median price targets of both companies suggest that Nvidia stock could rise 33% while Microsoft is expected to deliver 20% gains. So, the odds seem in favor of Nvidia outperforming Microsoft in 2025, and that's not surprising considering that the chipmaker continues to witness outstanding demand for its AI data center GPUs.\n\nHowever, investors would do well to note that a few concerns could weigh on Nvidia stock. From potential restrictions that could be imposed on sales of its AI chips to foreign countries to an expensive valuation to the efforts being undertaken by major Nvidia customers to reduce their reliance on its chips, there are multiple reasons why Nvidia shares may remain under pressure.\n\nMoreover, the company's growth rate -- though still impressive -- is gradually slowing down. These are the reasons why Nvidia stock may lose its shine in 2025. Microsoft, on the other hand, is slowly but steadily stepping up its game in key AI-focused niches that could set it up for outstanding growth in the long run.\n\nThe tech giant is already gaining share in the cloud computing market thanks to AI. Microsoft's Azure cloud and other services revenue increased by 33% in the first quarter of fiscal 2025, with AI contributing 12 percentage points of that growth. More importantly, AI is helping Microsoft build a solid revenue pipeline by driving an increase in the number of big contracts that the company is signing for its Azure cloud solutions.\n\nThis is evident from the 22% increase in Microsoft's commercial remaining performance obligations (RPO) in fiscal Q1 2025 to $259 billion. The faster growth in RPO as compared to the revenue bodes well for Microsoft, as this metric refers to the total value of a company's unfulfilled contracts. Microsoft expects to recognize 40% of its RPO as revenue in the next 12 months, which would be an increase of 17% on a year-over-year basis.\n\nAs a result, the possibility of Microsoft growing at a faster pace than Wall Street's expectations in the coming year cannot be ruled out. A similar story could unfold at Nvidia, and the company could overcome the adversity that it is facing right now.\n\nThough the outgoing administration's potential curbs on Nvidia chip exports could post a threat to its remarkable revenue growth -- 56% of its revenue is from customers outside the U.S. -- investors should note the rules will be enforced 120 days from now, giving the incoming administration time to weigh in on the new rules.\n\nLooking at the positive side of things, Nvidia's focus on significantly enhancing the production capacity of its latest Blackwell AI processors to meet the red-hot demand from its customers, along with the massive increase in AI infrastructure spend by its U.S. customers, could be enough to help the company grow at an impressive pace once again in 2025. As such, the possibility of Nvidia stock retaining its mojo despite the potential challenges it faces cannot be ruled out.\n\nWhat should investors do?\n\nThe uncertainties surrounding Nvidia may tempt investors to put their money into Microsoft stock to capitalize on the AI boom, especially considering the latter's valuation. After all, Microsoft is trading at a much cheaper 34 times earnings as compared to Nvidia's earnings multiple of 52.\n\nHowever, Nvidia's forward earnings multiple of 31 is in line with Microsoft's, and that's not surprising considering the outstanding growth that the company is expected to deliver. This makes Nvidia an attractive AI stock to buy right now, and investors can consider using the negative press surrounding it as a buying opportunity considering that its huge addressable market could be enough to help it overcome any potential regulatory challenges.\n\nBut at the same time, even Microsoft looks like a top AI stock to buy for the long run considering the massive opportunities in cloud computing and workplace collaboration that could supercharge its growth.\n\nSo, investors can consider buying any one of these two AI stocks right now depending on their risk appetite, and there is a good chance that they may not go wrong in 2025, as well as in the long run, as both Microsoft and Nvidia are sitting on lucrative end markets thanks to AI.\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nEver feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia: if you invested $1,000 when we doubled down in 2009, you\u2019d have $357,084 !*\n\nif you invested $1,000 when we doubled down in 2009, !* Apple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $43,554 !*\n\nif you invested $1,000 when we doubled down in 2008, !* Netflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $462,766!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nSee 3 \u201cDouble Down\u201d stocks \u00bb\n\n*Stock Advisor returns as of January 13, 2025\n\nHarsh Chauhan has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Microsoft and Nvidia. The Motley Fool recommends the following options: long January 2026 $395 calls on Microsoft and short January 2026 $405 calls on Microsoft. The Motley Fool has a disclosure policy.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "TSMC and Nvidia team up on silicon photonics, CEO Jensen Huang says",
            "link": "https://www.taipeitimes.com/News/front/archives/2025/01/19/2003830461",
            "snippet": "Taiwan Semiconductor Manufacturing Co (TSMC, \u53f0\u7a4d\u96fb) and US-based artificial intelligence (AI) chip designer Nvidia Corp have partnered with each other on...",
            "score": 0.8079169392585754,
            "sentiment": null,
            "probability": null,
            "content": "TSMC and Nvidia team up on silicon photonics, CEO Jensen Huang says\n\nEXPRESSING GRATITUDE: Without its Taiwanese partners which are \u2018working around the clock,\u2019 Nvidia could not meet AI demand, CEO Jensen Huang said\n\nStaff writer, with CNA\n\n\n\n\n\nTaiwan Semiconductor Manufacturing Co (TSMC, \u53f0\u7a4d\u96fb) and US-based artificial intelligence (AI) chip designer Nvidia Corp have partnered with each other on silicon photonics development, Nvidia founder and CEO Jensen Huang (\u9ec3\u4ec1\u52f3) said.\n\nSpeaking with reporters after he met with TSMC chairman C.C. Wei (\u9b4f\u54f2\u5bb6) in Taipei on Friday, Huang said his company was working with the world\u2019s largest contract chipmaker on silicon photonics, but admitted it was unlikely for the cooperation to yield results any time soon, and both sides would need several years to achieve concrete outcomes.\n\nTo have a stake in the silicon photonics supply chain, TSMC and Taiwan-based IC packaging and testing services provider ASE Technology Holding Co (\u65e5\u6708\u5149\u6295\u63a7) have organized the Silicon Photonics Industry Alliance, which is comprised of more than 30 Taiwanese companies.\n\nTaiwan Semiconductor Manufacturing Co chairman and CEO C.C. Wei, front left, looks on as Nvidia founder and CEO Jensen Huang, front right, speaks to reporters in Taipei yesterday. Photo: CNA\n\nSilicon photonics is a technology that uses silicon as a medium for optical transmission, which has some promising features, such as low power consumption, extensive transmission distance and lower costs.\n\nAt a time when AI applications are booming, finding ways to cut energy consumption has become a focus.\n\nHuang expressed gratitude to all TSMC\u2019s employees for their support for Nvidia, which he said has enabled his company to make a significant improvement in production, as AI applications are in great demand.\n\nHuang also visited several of Nvidia\u2019s business partners in Taiwan.\n\nDuring the visits, Huang said global demand for Nvidia\u2019s advanced Grace and Blackwell graphics processing units (GPUs) was strong as he also thanked Taiwanese partners for providing about 45 factories to help his company.\n\nHe named Nvidia\u2019s partners in expressing his gratitude, including AI server makers Hon Hai Precision Industry Co (\u9d3b\u6d77\u7cbe\u5bc6), Quanta Computer Inc (\u5ee3\u9054\u96fb\u8166) and Wistron Corp (\u7def\u5275), cloud-enabled services provider Wiwynn Corp (\u7def\u7a4e), AI graphics card vendor Gigabyte Technology Co (\u6280\u5609), IC packaging and testing services provider Siliconware Precision Industries Co (\u77fd\u54c1\u7cbe\u5bc6) and PC component maker Cooler Master Co (\u8a0a\u51f1\u570b\u969b).\n\nNvidia has created GPUs and AI applications, and laid a new foundation for future computer development, Huang said, adding that he believed the AI industry would become the mainstream of the global industrial development, and that the technology would allow machines to learn 24 hours a day to help humans.\n\nHe said that now is just the beginning of the AI era, and he expected several trillion US dollars worth of business opportunities to come.\n\nNvidia has to grow rapidly to meet the trend, but without its Taiwanese partners, which are \u201cworking around the clock,\u201d it would be impossible for his company to make it, Huang said.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "This Nvidia Supplier's Stock Is Down 30% in 3 Months\u2014Here\u2019s Why Deutsche Bank Says It's Still a Buy",
            "link": "https://www.investopedia.com/nvidia-supplier-monolithic-power-systems-down-30-percent-still-a-buy-8776443",
            "snippet": "Shares of Monolithic Power Systems (MPWR) have slumped more than 30% since the semiconductor firm's last earnings report at the end of October, but analysts...",
            "score": 0.968675434589386,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Shares of Monolithic Power Systems are down over 30% since its last earnings report at the end of October.\n\nThe company's fourth-quarter outlook and reports that its sales to Nvidia could decline sent shares sharply lower in the last months of the year.\n\nAnalysts have stayed bullish, citing the expanding market of potential AI-related customers as a boost to revenue in the coming years.\n\nAfter reaching record highs last year, shares of Monolithic Power Systems (MPWR) have slumped more than 30% since the semiconductor firm's last earnings report at the end of October, but analysts have remained bullish on the stock.\n\nIn a note earlier this week, Deutsche Bank analysts added Monolithic's stock to its list of \"top picks,\" and called its recent slide a \"buying opportunity.\" The analysts reiterated their \"buy\" rating and a $900 price target, citing likely revenue growth and improving margins over the next two years as positive catalysts.\n\nModest Q4 Projections, Potential Reduction to Nvidia Sales Drove Shares Lower\n\nMonolithic beat earnings estimates in the third quarter, but the company's forecast that revenue growth would be \"roughly flat\" in the fourth quarter shook investors and its stock plunged 17% in a day.\n\nShares took another hit in November when a report suggested Nvidia (NVDA) was considering reducing the amount of components it buys from Monolithic for its Blackwell platform. Analysts had previously cited Blackwell as a positive catalyst for Monolithic stock.\n\nExpanding AI Market, 'Diversity of Growth Drivers' Could Help Monolithic\n\nDeutsche Bank analysts wrote that even if Monolithic loses share among Nvidia's suppliers, the company should \"continue to deliver solid AI-related growth\" as the market of potential customers continues to expand.\n\nWhile the company's valuation is \"at the high end of our comfort range,\" the analysts said it's justified because of Monolithic's \"consistent execution, above-industry growth, diversity of growth drivers, and sustainable margin-expansion potential.\"\n\nTen of the 11 analysts tracked by Visible Alpha rate the hardware maker's stock as a \"buy\" along with one \"hold\" rating. The average price target of $822.91 is more than 30% above Friday's closing price of $625.82, suggesting analysts think the stock will make up most of the ground it has lost since its third-quarter report.\n\nMonolithic is scheduled to release its fourth-quarter earnings report after the bell on Feb. 6.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Nvidia RTX 5080 and 5090 May Be in Extremely Short Supply at Launch",
            "link": "https://gamerant.com/nvidia-rtx-5080-5090-supply-shortage-launch-rumor/",
            "snippet": "A rumor suggests the RTX 5090 may be in very limited supply at launch. PC gamers may have a hard time getting their hands on Nvidia's 50 series of GPUs when...",
            "score": 0.8469966053962708,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Oppenheimer Analysts: Nvidia Stock Is Still The Best Semiconductor Buy",
            "link": "https://www.barchart.com/story/news/30508552/oppenheimer-analysts-nvidia-stock-is-still-the-best-semiconductor-buy",
            "snippet": "Analysts at Oppenheimer consider Nvidia as the chip stock to own for 2025. In a client note, Oppenheimer highlighted Nvidia as the largest volume producer of...",
            "score": 0.8824406862258911,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Nvidia's Huang enjoys 'trillion dollar banquet' with 35 Taiwanese semiconductor industry chiefs",
            "link": "https://www.tomshardware.com/tech-industry/nvidias-huang-enjoys-trillion-dollar-banquet-with-35-taiwanese-semiconductor-industry-chiefs",
            "snippet": "Jensen Huang invited around 35 chief executives from various Nvidia partners to lunch, which led to the media calling it the 'trillion dollar' banquet.",
            "score": 0.7222176194190979,
            "sentiment": null,
            "probability": null,
            "content": "Jensen Huang invited the chairpersons and chief executives of a host of Nvidia AI chip-building partners to a lavish meal on Saturday. According to the Central New Agency (machine translated), at least 35 execs attended what is being locally referred to as Huang\u2019s 'trillion dollar banquet.' Industry luminaries attending included the heads of Quanta, Asus, Acer, Inventec, Gigabyte, ASRock, and MSI. The Nvidia CEO also gave a short interview alongside TSMC chairman CC Wei, where both industry giants had nothing but praise for each other.\n\nThis lunch out is reminiscent of Huang\u2019s dinner with the heads of TSMC, MediaTek, and Quanta just before Computex 2024. However, the media did not have to speculate as to why the Nvidia CEO brought some of the biggest names in the computer industry here. He has previously said that he intends to meet with the CEOs of Nvidia\u2019s supply chain partners to thank them for building Nvidia, and this catered event is likely the culmination of that promise.\n\nOutside the restaurant, Taiwanese media interviewed the Nvidia CEO and the TSMC chairman. \u201c[It\u2019s] my pleasure and my honor to supply the chips to you and be a partner for more than 20 years,\u201d CC Wei said to Jensen. \u201cOn behalf of my friends inside the restaurant, we thank you for bringing business to Taiwan.\u201d\n\nRegarding Nvidia\u2019s partnership with TSMC, Huang said, \u201cMy hair was 100% black, now it\u2019s 100% silver. So, without TSMC, of course, Nvidia would not be possible.\u201d He also added, \u201cBecause of the PC revolution, there was Computex, and then the world started coming to Taiwan.\u201d Wei interjected, \u201cJensen Huang bring it to Taiwan, remember that.\u201d To which the Nvidia CEO replied with a smile, \u201cThat is not true!\u201d\n\nAside from being industry partners for several decades, it seems that Nvidia CEO Jensen Huang and TSMC Chairperson CC Wei built a friendship \u2014 something rarely seen by the public.\n\nHuang of the people\n\nAlthough Jensen Huang is one of the richest people on earth, what separates him from other Silicon Valley billionaires is how relatable he is to the people. For example, he was spotted in late 2023 eating street food in Vietnam and visiting a LAN party in Hanoi, and he has been acting much the same with night market visits during this latest visit to Taiwan.\n\nTrue enough, when he saw that there was a crowd waiting for him outside of the restaurant as he was eating with industry leaders, he came out for a moment to hand out sausages and sesame buns to the people. He also signed some autographs and got a couple of pictures taken with some lucky fans.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nA few lucky kids also received \u201chongbao\u201d or red envelopes containing NT$1,000 or about US$30 from Jensen. Aside from the monetary value, these gifts also symbolize love, blessings, and good luck, which many children expect to receive during the Chinese New Year. It\u2019s actions like these that make him so loved by the Taiwanese people, leading to the rise of \u2018Jensanity\u2019, especially as Huang himself was born in Tainan, Taiwan, although he is also an American citizen.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "Nvidia and AMD: Cowen Selects the Top AI Stocks to Buy Before Earnings",
            "link": "https://www.tipranks.com/news/nvidia-and-amd-cowen-selects-the-top-ai-stocks-to-buy-before-earnings",
            "snippet": "We should note that AMD has experienced sequential gains in both revenues and earnings over the last two quarterly results. In late October, when the company...",
            "score": 0.9428699016571045,
            "sentiment": null,
            "probability": null,
            "content": "With the earnings season now underway, we\u2019ll soon find out how publicly traded companies finished 2024. So far, analysts have set a high bar, projecting year-over-year growth of 11% or more. If achieved, this would make Q4 2024 the strongest quarter for earnings growth since 2021.\n\nAgainst that background, we can look at one of the driving themes behind the success of the past few years \u2013 artificial intelligence (AI). AI hasn\u2019t just grabbed the headlines and the hype, it has also provided a solid technological base for a fundamental change across a wide array of industries, from the digital world to healthcare to finance to education to entertainment. No matter where you look these days, AI is there.\n\nThis wave of innovation hasn\u2019t gone unnoticed by market analysts. Many are keeping a close eye on AI stocks, seeking insights into what lies ahead for investors. Among them is Cowen analyst Joshua Buchalter, who has highlighted two AI giants that deserve particular attention as this earnings season unfolds.\n\nNvidia (NASDAQ:NVDA) and Advanced Micro Devices (NASDAQ:AMD), Buchalter\u2019s top picks, are key players in the semiconductor space and integral to the AI revolution. Both companies are known for their high-end, AI-capable processors. Nvidia has been the industry leader for several years now, while AMD is making a concerted bid to expand its market share.\n\nAs both companies approach their upcoming earnings reports, Buchalter\u2019s analysis sheds light on what investors can expect, focusing on their growth trajectories and competitive positioning. Let\u2019s take a closer look.\n\nNvidia\n\nFirst up is Nvidia, the undisputed heavyweight of the semiconductor industry, with a $3.37 trillion market cap that crowns it as Wall Street\u2019s second-most valuable player, just behind Apple. Nvidia has reached that dizzying height after an astounding 450% share price gain over the past three years \u2013 a period that coincides with the AI boom.\n\nThe company built its reputation on high-speed processors, the GPUs that became so popular in the gaming industry \u2013 but those chips also proved adaptable to a wide range of other high-end computing applications, including high-speed computing, data center server stacks, and AI, particularly generative AI. Today, the company\u2019s data center business segment, which is closely tied to AI, accounts for the bulk of Nvidia\u2019s revenues, and the company commands 80% or more of the AI chip market.\n\nNvidia continues to benefit from continued demand for both its latest Blackwell series accelerators and its existing Hopper series. The Blackwell chips were announced last March but faced delays in production and delivery. Nvidia has begun to ship out Blackwell sample packs, and the company states that demand for the new chips is strong.\n\nLooking ahead, Nvidia is expected to release its fiscal 4Q25 results at the end of next month, with Wall Street anticipating ~$38 billion in revenue and $0.85 in EPS. If these forecasts hold, they\u2019ll mark an 8.4% increase in revenue and a 5% climb in EPS from the $35.08 billion and $0.81 recorded in fiscal 3Q25.\n\nTurning to the Cowen view, we find analyst Buchalter is upbeat on Nvidia, citing the company\u2019s consistent solid earnings and the high demand for its product lines.\n\n\u201c[We] anticipate a solid print & guide with a continuation of the company\u2019s recent trend to come in ~$1-2B above consensus on both. Our checks indicate that the Blackwell (largely B200 at this point with GB200 beginning more materially in AprQ/JulQ) ramp is progressing well, and we are observing some suppliers shifting allocation from Hopper-based platforms to Blackwell (indicating the health of the ramp and demand). With Blackwell demand still well in excess of supply, and the supply chain not yet shifted to Blackwell Ultra, we view NVIDIA revenue as largely on cruise control for F1H26,\u201d Buchalter opined.\n\nTo this end, Buchalter rates Nvidia stock as a Buy, designates it as a Top Pick, and sets a $175 price target, implying a potential one-year upside of 27%. (To watch Buchalter\u2019s track record, click here)\n\nOverall, Nvidia has picked up 39 analyst reviews, with a breakdown of 36 Buys and 3 Holds to give the stock a Strong Buy consensus rating. The shares are currently priced at $137.71 and their $176.86 average target price points toward a 28% upside in the next 12 months. (See NVDA stock forecast)\n\nAdvanced Micro Devices\n\nThe second stock we\u2019ll look at, AMD, may not be in Nvidia\u2019s league, but it is a solid operator in its own right. AMD has a strong reputation in the chip industry and, over the past year, has been making a concerted effort to erode Nvidia\u2019s market share. The company\u2019s challenge is built on its own lineup of high-quality chipsets, particularly its high-end PC processors and AI-capable accelerator chips.\n\nEarlier this month, AMD announced several new products in line with this challenge. These include several new Ryzen chips for both business and consumer PCs \u2013 all of which are designed to power next-generation AI applications. The new chips include the AMD Ryzen AI Max, AMD Ryzen AI 300 Series and AMD Ryzen 200 Series processors on the consumer side, and the AMD Ryzen AI Max PRO, AMD Ryzen AI 300 PRO and AMD Ryzen 200 PRO Series on the commercial side. In addition, the company also announced a stronger partnership with Dell. The Dell agreement will see the PC maker put AMD Ryzen AI PRO processors into new Dell Pro computers. This will mark the first shipment of Dell\u2019s commercial PCs with AMD Ryzen AI PRO processors.\n\nThese announcements mark an important expansion in AMD\u2019s push to expand its AI offerings. The company already has support from important customers such as Microsoft, Meta, and Oracle, all of which are major enterprise consumers of high-end AI processors.\n\nWe should note that AMD has experienced sequential gains in both revenues and earnings over the last two quarterly results. In late October, when the company released its third-quarter 2024 results, the top line reached $6.82 billion, marking an 18% year-over-year increase and surpassing the forecast by $110 million. At the bottom line, the non-GAAP EPS of 92 cents was in line with expectations.\n\nLooking ahead, AMD is set to release its fourth-quarter 2024 results on Tuesday, February 4. Wall Street expects $7.53 billion in revenue and an EPS of $1.09, reflecting a sequential 10% increase in revenue and an 18% rise in EPS compared to Q3 2024.\n\nEven though AMD\u2019s results have been ticking upwards, and its product lines are finding solid demand, the company\u2019s stock price has been falling. AMD shares are down 23% in the last 12 months.\n\nThat, however, leads Cowen analyst Buchalter to see AMD as an opportunity. Noting that the stock has underperformed, the analyst also cites AMD\u2019s success in expanding its market share.\n\n\u201cWe expect a sold [sic] in-line print/guide against a very high bar that\u2019s being set by the fervent backdrop surrounding AI and the \u2018insatiable\u2019 demand for compute and recent momentum in the ASIC space. The stock has underperformed over the last three months as AMD\u2019s progress has been steady, but not necessarily step-function like its AI-compute peers. Our checks point to continued positive demand for MI300/325 products at AMD\u2019s lead-customers. That said, a more seasonal gen-purpose Server CPU and stubborn softness in Embedded (and typically weak 1Q25 seasonality in Client) are partially muting AMD\u2019s ongoing share gains (eg. AMD\u2019s significant enterprise PC win with Dell). As a result, we take our above-Street estimates down slightly while maintaining our AI accelerator estimate of $5B/$10B/$15B for 2024/2025/2026,\u201d Buchalter stated.\n\nThese comments back up Buchalter\u2019s Buy rating on AMD, which he complements with a $150 price target that suggests a share appreciation of 23.5% on the one-year horizon.\n\nAll in all, AMD\u2019s consensus rating is a Moderate Buy, based on 32 recent analyst reviews that include 21 Buys, 10 Holds, and a lone Sell. The stock is trading for $121.46 currently, and shows a ~42% one-year upside based on the average price target of $172.24. (See AMD stock forecast)\n\nTo find good ideas for stocks trading at attractive valuations, visit TipRanks\u2019 Best Stocks to Buy, a tool that unites all of TipRanks\u2019 equity insights.\n\nDisclaimer: The opinions expressed in this article are solely those of the featured analyst. The content is intended to be used for informational purposes only. It is very important to do your own analysis before making any investment.\n\nQuestions or Comments about the article? Write to editor@tipranks.com",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-18": {
        "0": {
            "title": "3 Top AI Stocks to Consider Right Now (Hint: Nvidia's Not One)",
            "link": "https://www.fool.com/investing/2025/01/18/3-top-ai-stocks-to-consider-right-now-hint-nvidias/",
            "snippet": "Here are three top AI stocks (not named Nvidia) that investors can buy today. Their respective growth prospects and valuations offer compelling value.",
            "score": 0.9256435036659241,
            "sentiment": null,
            "probability": null,
            "content": "AI could become a $826 billion market over the next five years. Nvidia is an obvious choice, but don't overlook these other AI power players.\n\nThe significant interest in artificial intelligence (AI) stocks has shined a bright spotlight on Nvidia, the leading AI chip company.\n\nNvidia has been an incredible stock for investors, but AI stretches far beyond one company. You can be sure there are many winners in a global AI industry that experts believe could swell to more than $826 billion by 2030. And despite the broader market trading within shouting distance of all-time highs and a remarkable run by technology stocks over the past two years, there are still deals out there.\n\nHere are three top AI stocks (not named Nvidia) that investors can buy today. Their respective growth prospects and valuations offer compelling value that should translate to stellar investment returns as AI technology advances.\n\n1. Lam Research\n\nMany people may not realize how much goes into making AI chips; it goes beyond designers like Nvidia. Lam Research (LRCX 4.56%) designs and builds equipment used in chip fabrication (manufacturing). To sum up Lam Research's value to the chip industry, its equipment helps companies build smaller and more efficient chip designs.\n\nNvidia's H100 AI chip has 80 billion transistors. Its successor, built on Blackwell architecture, has 208 billion. Remember, you can hold these chips in your hand. Cutting-edge equipment is needed to make such small circuits.\n\nAs AI technology improves and requires smaller and more powerful chips, Lam Research will be among the companies making these advancements possible. The company's reputation, intellectual property, and long-standing relationships with chip fabricators make its equipment hard to displace.\n\nThe stock currently trades at a price-to-earnings ratio of about 24. Meanwhile, analysts estimate the company will grow earnings by an average of 16% annually over the long term. The resulting 1.5 PEG ratio makes Lam Research a smart buy at these prices.\n\n2. Taiwan Semiconductor Manufacturing\n\nLet's explore the AI chip space a bit longer. Taiwan Semiconductor Manufacturing (TSM 1.46%) is the leading chip fabricator. As you might guess, this makes the company a pillar of the AI industry, as chips are the building blocks on which AI models train and operate.\n\nJust how dominant is Taiwan Semiconductor? In the third quarter 2024, the company manufactured approximately 64% of the world's chips. That includes Nvidia's AI chips, which Taiwan Semiconductor builds with a custom manufacturing process. The company could have years of rampant growth ahead as Nvidia and other companies design increasingly complex chips and seek Taiwan Semiconductor's expertise and capacity to build them.\n\nAnalysts estimate Taiwan Semiconductor will grow earnings by an average of 31% annually over the long term. That makes the stock a compelling value at just 32 times earnings, a PEG ratio of just 1.\n\nPart of the reason why the stock isn't more expensive is the geopolitical dispute between Taiwan and China. The latter has long claimed Taiwan (where Taiwan Semiconductor operates) belongs under its control, even threatening force to make it happen. The dispute adds some uncertainty to owning the stock, though the value becomes too compelling to pass up at some point. Taiwan Semiconductor has begun expanding into the United States, Japan, and Germany to mitigate its geopolitical risks, so this may become less of an issue.\n\n3. Alphabet\n\nLastly, let's shift from AI chips to almost everything else AI. Alphabet (GOOG 1.75%) (GOOGL 1.68%), Google's parent company, might be the closest thing to a do-it-all AI company. Alphabet still counts on its search engine for much of its profits, but it has branched out into various other industries, including a heavy tilt into AI. Alphabet has the third-largest cloud computing platform, Google Cloud. It has developed and deployed an AI model (Gemini) to enhance its existing products and services. It's also among the leaders in autonomous vehicle technology and is making breakthroughs in quantum computing. I mean, what doesn't Alphabet do?\n\nWhen people think of AI today, most think of generative AI, software that creates content. However, over the coming decades, AI should become much more, including self-driving cars and robotics -- whatever technology makes possible.\n\nAlphabet's financial resources, innovative background, and leadership in existing fields give it a great shot at competing in almost any market AI creates in the future. Analysts expect the business to grow earnings by 16% annually. At a P/E of only 25 today, Alphabet is one of the most intelligent AI stock buys you can make.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "NVIDIA Corporation (NVDA): Why Should You Buy This Growth Stock For The Next 5 Years?",
            "link": "https://finance.yahoo.com/news/nvidia-corporation-nvda-why-buy-000903558.html",
            "snippet": "We recently compiled a list of the 15 Best Growth Stocks to Buy for the Next 5 Years. In this article, we are going to take a look at where NVIDIA...",
            "score": 0.9382563829421997,
            "sentiment": null,
            "probability": null,
            "content": "We recently compiled a list of the 15 Best Growth Stocks to Buy for the Next 5 Years. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against the other growth stocks.\n\nKevin Mahn, President & CIO at Hennion & Walsh Asset Management, recently appeared on CNBC on January 6 to discuss the current market momentum and emphasize the need for investors to be selective in 2025 to find growth opportunities. He highlighted that while the MAG7 have led the market recently, their leadership may not continue. Mahn referenced historical data, noting that since 1950, there have been nine instances where the market rallied by 20% or more, with the market rising in eight of those cases. However, he pointed out that gains in the following year averaged only 3.6%, indicating a need for careful selection. He also acknowledged recent market trends, including a decline in the S&P 500's performance and a potential shift in investor sentiment following events like the Santa Claus Rally.\n\nHe predicted a path of lower interest rates, expecting 50 basis points of cuts this year instead of the previously anticipated 100 basis points. Mahn suggested that this environment would create favorable conditions for stocks and bonds but urged investors to diversify beyond mega-cap tech stocks into sectors like biotech and aerospace. Earlier on January 3 as well, Mahn noted that after two consecutive years of gains, a third year of strong performance appears unlikely. He remarked that it seems the Grinch got in the way of the Santa Claus rally this year.\n\nHe also addressed concerns from investors tempted to time the market or sell their holdings. He warned against trying to time the market, describing it as often futile. Instead, he advocated for rebalancing portfolios to align with long-term goals and risk tolerance. He suggested that the economic landscape is changing, with lower interest rates and stagnant economic growth expected moving forward. Mahn advised investors to take profits from sectors that previously led the market and consider reallocating those funds into different areas poised for future growth. He highlighted biotech as a promising sector, noting bipartisan agreement on the need to lower drug prices. This shift could lead large-cap pharmaceutical companies to seek new revenue sources, making smaller biotech firms attractive.\n\nMethodology\n\nWe first sifted through online rankings, and internet lists to compile a list of the top growth stocks to buy for the next 5 years. We then selected the stocks with high 5-year revenue growth and high analysts' upside potential. From those we picked 15 stocks that were the most popular among elite hedge funds and that analysts were bullish on. The stocks are ranked in ascending order of the number of hedge funds that have stakes in them, as of Q3 2024.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Interested in quantum computing investments? Hear what Nvidia's CEO just said about it",
            "link": "https://www.usatoday.com/story/money/2025/01/18/quantum-computing-investing-nvidia/77742922007/",
            "snippet": "The Nvidia executive appears bullish on quantum computing's capabilities, he asserted that the technology could be 20 years away from becoming very useful.",
            "score": 0.9278672933578491,
            "sentiment": null,
            "probability": null,
            "content": "Interested in quantum computing investments? Hear what Nvidia's CEO just said about it\n\nShow Caption Hide Caption Nvidia's stock drops as US tightens its grip on AI chip flows Shares of Nvidia fell as much as five percent Monday morning after the U.S. government said it would further restrict exports of artificial intelligence chips.\n\nRecently, Nvidia (NASDAQ: NVDA) CEO Jensen Huang made a head-turning, market-moving comment regarding his thoughts on quantum computing. Stocks in this space sold off in response. Even so, I predict that in 2025 quantum computing will emerge as one of the next big waves for investors interested in artificial intelligence (AI).\n\nWhat did Huang just say about quantum computing?\n\nHuang provided his thoughts on quantum computing applications during a panel discussion at this year's CES conference in Las Vegas. While the Nvidia executive appears bullish on quantum computing's capabilities, he asserted that the technology could be 20 years away from becoming \"very useful.\"\n\nWhy his comments matter\n\nThe chart below illustrates the price movements of several high-profile quantum computing stocks. Over the last three months, the stock prices for Rigetti Computing, Quantum Computing, D-Wave Quantum and IonQ have each experienced considerable momentum.\n\nHowever, all four stocks started witnessing considerable sell-offs beginning right around the same time earlier this month. I personally do not find the timing of Huang's public comments and the cratering share prices of quantum computing stocks to be coincidental.\n\nThe dynamics illustrated above underscore a couple of important ideas. First, just because quantum computing stocks rallied in the last few months doesn't necessarily mean they are wise investments. The reality is that all of the companies in the chart above are very early-stage enterprises \u2014 they're barely generating revenue and will remain cash-burning operations for some time.\n\nThis leads to my second point. Influential Wall Street personalities and corporate executives can at times move markets with their rhetoric. While Huang's comments regarding quantum computing weren't negative by any means, his views caused many investors to recognize the sobering reality that quantum computing is going to be a long-term opportunity.\n\nAs such, those who bought quantum computing stocks throughout November and December have likely been left as unfortunate bag holders.\n\nMore from The Motley Fool: Meta Platforms is ending its fact-checking program. That could be risky for the stock\n\nShould you invest in quantum computing?\n\nConsidering that Huang acknowledged that this technology has the potential to help solve increasingly important and sophisticated problems eventually, the overarching question remains: Does quantum computing deserve an allocation in your AI portfolio? In my opinion, there are a few ways to invest in it. The first and most direct way is to buy specific quantum computing stocks.\n\nHowever, a more insulated approach would be to invest in a quantum computing-themed exchange-traded fund (ETF), such as the Defiance Quantum ETF. ETFs provide some degree of cushioning from volatility since they are comprised of an array of individual stocks. With that said, thematic investing can be a higher-risk option than buying broader market funds, as you'll still be getting exposure to one specific focus area.\n\nIn my view, the best and most prudent way to invest in quantum computing would be through the shares of diversified megacap technology companies for which quantum computing is just one area of interest. Companies such as IBM, Alphabet and Microsoft are all exploring quantum computing as part of their AI road maps, but are not betting their entire futures on the technology. In addition, adjacent businesses that will be integral in the development of quantum chips \u2014 Nvidia or Advanced Micro Devices, for example \u2014 could also be suitable picks.\n\nAt the end of the day, I agree with Huang that quantum computing has meaningful potential. However, identifying which specific companies will emerge at the forefront of the trend is nearly impossible to do at this stage, given how far away the technology is from becoming commercialized.\n\nSuzanne Frey, an executive at Alphabet, is a member of The Motley Fool's board of directors. Adam Spatacco has positions in Alphabet, Microsoft and Nvidia. The Motley Fool has positions in and recommends Alphabet, International Business Machines, Microsoft and Nvidia. The Motley Fool recommends the following options: long January 2026 $395 calls on Microsoft and short January 2026 $405 calls on Microsoft. The Motley Fool has a disclosure policy.\n\nThe Motley Fool is a USA TODAY content partner offering financial news, analysis and commentary designed to help people take control of their financial lives. Its content is produced independently of USA TODAY.\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nOffer from the Motley Fool: Ever feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia:if you invested $1,000 when we doubled down in 2009,you\u2019d have $353,272!*\n\nApple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $45,049!*\n\nNetflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $457,459!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nSee 3 \u201cDouble Down\u201d stocks \u00bb",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia teams up with TSMC in silicon photonics development",
            "link": "https://focustaiwan.tw/business/202501180004",
            "snippet": "U.S.-based artificial intelligence chip designer Nvidia Corp. and Taiwan Semiconductor Manufacturing Co. (TSMC) have partnered with each other in silicon...",
            "score": 0.6027594208717346,
            "sentiment": null,
            "probability": null,
            "content": "To activate the text-to-speech service, please first agree to the privacy policy below.\n\nTaipei, Jan. 18 (CNA) U.S.-based artificial intelligence chip designer Nvidia Corp. and Taiwan Semiconductor Manufacturing Co. (TSMC) have partnered with each other in silicon photonics development, according to Jensen Huang (\u9ec3\u4ec1\u52f3), founder and CEO of the American AI giant.\n\nSpeaking with reporters after he met with TSMC Chairman C.C. Wei (\u9b4f\u54f2\u5bb6) in Taipei on Friday, Huang said his company was working with the world's largest contract chipmaker in silicon photonics but admitted it was unlikely for the cooperation to yield results anytime soon and both sides still needed several years to achieve concrete outcomes.\n\nTo have a stake in the silicon photonics supply chain, TSMC and Taiwan-based IC packaging and testing services provider ASE Technology Holding Co. have organized the Silicon Photonics Industry Alliance (SiPhIA), which is comprised of more than 30 Taiwanese companies.\n\nSilicon photonics is known as the applications of photonic systems using silicon as a medium for optical transmission, which has some promising features, such as low power consumption, extensive transmission distance, and lower costs.\n\nAt a time when AI applications are booming, finding ways to cut energy consumption has become a focus.\n\nHuang expressed gratitude to all of TSMC's employees for their support for Nvidia which he said has enabled his company to make a significant improvement in production as AI applications are in great demand.\n\nIn addition to TSMC, Huang also visited several of Nvidia's business partners in Taiwan.\n\nDuring the visits, Huang said global demand for Nvidia's advanced Grace and Blackwell graphics processing units (GPUs) was strong so he also thanked these Taiwanese partners for providing about 45 factories to help his company.\n\nHe named Nvidia's partners in expressing his gratitude, including AI server makers Hon Hai Precision Industry Co., Quanta Computer Inc. and Wistron Corp., cloud-enabled services provider Wiwynn Corp., AI graphics card vendor Giga-Byte Technology Co., IC packaging and testing services provider, Siliconware Precision Industries Co. (SPIL), and PC component maker Cooler Master Co.\n\nHuang said Nvidia has created GPUs and AI applications and laid a new foundation for future computer development, and he believed the AI industry will become the mainstream of the global industrial development as AI will allow machines to learn 24 hours a day to help human beings.\n\nHe added that now is just a beginning of the AI era, and he expected several trillion U.S. dollars worth of business opportunities to come.\n\nHe added that Nvidia has to grow rapidly to meet the trend but without these Taiwanese partners, which are \"working around the clock,\" it was impossible for his company to make it.\n\nOn Thursday, Huang attended the opening ceremony of a new plant of SPIL in Taichung Tanzi Technology Industrial Park, in central Taiwan.\n\nHuang said SPIL, a wholly owned subsidiary of the world's largest IC packaging and testing services provider ASE Technology, was Nvidia's important long-term back-end IC packaging and testing partner in AI-related GPUs and robot development.\n\nHe said Nvidia's cooperation scale with SPIL has grown 10 fold from a decade ago, and risen 200 percent from 2024 as both sides have forged close ties in their 27-year cooperation in high-end IC packaging and testing development, referring to 3D Chip-on-Wafer-on-Substrate (CoWoS) IC packaging services used in AI chip manufacturing.\n\nWhile Nvidia is shifting from single-die CoWoS technology (CoWoS-S) to more sophisticated duel-die CoWoS technology (CoWoS-L) in Blackwell CPU production, his company will continue to use CoWoS-S in Hopper GPU manufacturing, Huang said.\n\nHuang added that he expected orders from his company for CoWoS services as a whole will not be cut but will increase significantly this year.\n\nIndustrial sources said Huang's visit to SPIL was aimed at securing CoWoS production. As its CoWoS services have secured certification, SPIL is expected to raise production gradually starting from the second quarter of this year, the sources added.\n\nHuang said he is expected to meet with representatives of other Taiwanese suppliers on Saturday but he did not elaborate.\n\nMeanwhile, Huang said Nvidia was still looking for a location in Taiwan to build its headquarters outside the United States as his company was growing and needed a larger venue for its operations.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia stock bleeds, Bitcoin bounces back, and Jamie Dimon's view: Markets news roundup",
            "link": "https://qz.com/nvidia-stock-ai-bitcoin-price-jamie-dimon-jpmorgan-1851742259",
            "snippet": "The stock market is in the red, with the tech-heavy Nasdaq plunging more than 1% on Monday morning as new regulatory measures introduced by President Biden...",
            "score": 0.8536227345466614,
            "sentiment": null,
            "probability": null,
            "content": "The S&P 500 is close to erasing all the gains it notched since Donald Trump\u2019s election victory.\n\nAdvertisement\n\nAs of market close on Monday, the S&P 500 was up just 0.92% since Nov. 5 \u2014 the day Trump clinched a second term in the White House and set off a stock market rally. S&P 500 futures were up 0.27% early Tuesday, pointing to a potential bounce back after sinking 0.77% so far in January.\n\nRead More",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA\u2019s GeForce RTX 5090 & RTX 5090D Tested At Blender; Revealing Up To 30% Difference From GeForce RTX 4090",
            "link": "https://wccftech.com/nvidia-geforce-rtx-5090-rtx-5090d-tested-at-blender/",
            "snippet": "NVIDIA's GeForce RTX 5090 & RTX 5090D Tested At Blender; Revealing Up To 30% Difference From GeForce RTX 4090 ... NVIDIA's GeForce RTX 5090 and RTX 5090D have...",
            "score": 0.9223164319992065,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's GeForce RTX 5090 and RTX 5090D have finally been tested at Blender, revealing the capabilities of flagship Blackwell GPUs in rendering scenarios.\n\nNVIDIA's Flagship RTX Blackwell GPUs To Debut With Considerable Difference In Rendering Performance From Previous Generation\n\nWell, as the official launch of Team Green's RTX 50 series SKUs approaches, we have started to see leaks surfaced online, revealing the performance of specific models, including the high-end variants. This time, fortunately, we now have an idea of what to expect with the GeForce RTX 5090 and RTX 5090D in terms of professional workloads, notably rendering, as Blender benchmarks have revealed the performance of these SKUs, and by the looks of it, there are some serious generational gains.\n\nStarting off with the GeForce RTX 5090, the GPU has managed to obtain a median score of 17822.17, which almost marks a 30% difference from the previous-gen flagship model, the GeForce RTX 4090. Moreover, with the RTX 5090D, you are looking at around a 28% increase in performance from the GeForce RTX 4090, which is massive. Interestingly, this also reveals that the GeForce RTX 5090 and the RTX 5090D may not be completely equal in performance, or at least what the Blender benchmarks conclude.\n\nWell, just recently, the GeForce RTX 5090 was tested within the Geekbench OpenCL & Vulkan APIs, offering up to 37% faster performance over the RTX 4090, which indeed validates the fact that in terms of performance difference within flagship models, we are looking at an average of 30%-40% difference, depending upon workloads and environments. Yet again, this is just an estimate for now, and real-time figures might change once we get a hold of the SKUs and the review embargo lifts.\n\nThe NVIDIA GeForce RTX 5090 GPU will be launched in just a few weeks and will be the fastest card in the RTX 50 \"Blackwell\" Gaming lineup, coming in with an MSRP of $1,999. Along with this, the GeForce RTX 5090D, which is the \"China-compliant\" alternative, will launch too as well, hoping to capitalize on the Chinese market.\n\nNVIDIA GeForce RTX 50 GPU Specs (Preliminary):",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Newegg Sets The Stage on Nvidia RTX 5000 series GPUs",
            "link": "https://gamerant.com/nvidias-rtx-5090-rtx-5080-skus-on-newegg/",
            "snippet": "Set to be released on January 31st, the new Nvidia RTX 5080 and RTX 5090 are nearly ready to be scooped up, but will you be able to get one?",
            "score": 0.9194821119308472,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia leads, Tesla trails among Mag 7 in earnings prospects",
            "link": "https://seekingalpha.com/news/4396384-nvidia-stock-leads-tesla-stock-earnings",
            "snippet": "Nvidia (NVDA) stock leads Tesla (TSLA) stock in earnings growth prospects among Magnificent Seven stocks. Read more here.",
            "score": 0.4982847571372986,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "NVIDIA faces new AI export control rules",
            "link": "https://www.investing.com/news/sec-filings/nvidia-faces-new-ai-export-control-rules-93CH-3819996",
            "snippet": "A new US government rule, \"Export Control Framework for Artificial Intelligence Diffusion\" (IFR), will impose a global licensing requirement on products.",
            "score": 0.5894291996955872,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Nvidia CEO to miss Trump inauguration, unlike peers",
            "link": "https://www.taipeitimes.com/News/biz/archives/2025/01/18/2003830398",
            "snippet": "Nvidia Corp CEO Jensen Huang (\u9ec3\u4ec1\u52f3) is expected to miss the inauguration of US president-elect Donald Trump on Monday, bucking a trend among high-profile US...",
            "score": 0.8973405957221985,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO to miss Trump inauguration, unlike peers\n\nBloomberg\n\n\n\n\n\nNvidia Corp CEO Jensen Huang (\u9ec3\u4ec1\u52f3) is expected to miss the inauguration of US president-elect Donald Trump on Monday, bucking a trend among high-profile US technology leaders.\n\nHuang is visiting East Asia this week, as he typically does around the time of the Lunar New Year, a person familiar with the situation said.\n\nHe has never previously attended a US presidential inauguration, said the person, who asked not to be identified, because the plans have not been announced.\n\nNvidia CEO Jensen Huang, right, and Wistron Corp chairman Simon Lin, center, stand on stage during Wistron\u2019s year-end party in Taipei yesterday. Photo, Ann Wang, Reuters\n\nThat makes Nvidia an exception among the most valuable technology companies, most of which are sending cofounders or CEOs to the event. That includes Apple Inc CEO Tim Cook, Amazon.com Inc cofounder Jeff Bezos and Tesla Inc head Elon Musk, a close Trump ally.\n\nA representative for Santa Clara, California-based Nvidia declined to comment whether there has been communication with the incoming administration.\n\nHuang earlier this month said that he would be delighted to meet Trump and \u201cdo whatever we can to make this administration succeed.\u201d However, he said he had not yet been invited to the president-elect\u2019s home base at Mar-a-Lago in Florida.\n\nHuang also said he expected Trump to bring less regulation.\n\n\u201cI think that\u2019s a good thing,\u201d he said. \u201cAs an industry, we want to move fast.\u201d\n\nNvidia is facing a barrage of new rules and regulations in the final days of US President Joe Biden\u2019s administration. The measures include steps to tighten access to advanced technology, such as Nvidia\u2019s industry-leading artificial intelligence (AI) chips, to keep them out of China\u2019s hands.\n\nNvidia has chafed at new export curbs unveiled this week, calling them an \u201coverreach\u201d and warning that they could be catastrophic for the tech industry.\n\n\u201cAs the first Trump administration demonstrated, the US wins through innovation, competition and by sharing our technologies with the world \u2014 not by retreating behind a wall of government overreach,\u201d Nvidia government affairs vice president Ned Finkle said earlier this week.\n\nNvidia and Huang have become the biggest stars of the chip world over the past two years. As the company\u2019s sales have surged, Huang has crisscrossed the globe promoting AI technology, often in his signature leather jacket. Nvidia is the world\u2019s most valuable chipmaker and only ranks behind Apple among all tech companies.\n\nThe chip industry would have a presence at the inauguration. Arm Holdings PLC CEO Rene Haas is one executive who plans to attend, a person familiar with Haas\u2019 plans said.\n\nHuang yesterday said that he had met Taiwan Semiconductor Manufacturing Co (\u53f0\u7a4d\u96fb) chairman C.C. Wei (\u9b4f\u54f2\u5bb6) for lunch, during which they talked about ramping up production of Nvidia\u2019s most advanced AI graphics processing units, Blackwell.\n\nAdditional reporting by Reuters",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-17": {
        "0": {
            "title": "How This NVIDIA Kaggle Grandmaster Merges Innovation and Play",
            "link": "https://blogs.nvidia.com/blog/nvidia-life-kazuki-onodera/",
            "snippet": "Onodera, now a senior deep learning scientist at NVIDIA, first discovered Kaggle in 2015 while building predictive models of Japanese banks as a financial...",
            "score": 0.9060349464416504,
            "sentiment": null,
            "probability": null,
            "content": "Growing up, Kazuki Onodera spent his free time in video game arcades strategizing gameplay tactics. He never imagined that his passion for gaming would one day help him land a spot competing with the Kaggle Grandmasters of NVIDIA, or KGMoN.\n\nKaggle is the world championship for machine learning, a virtual stage for data scientists to share their knowledge and techniques through competitions, courses and discussion forums.\n\nOnodera, now a senior deep learning scientist at NVIDIA, first discovered Kaggle in 2015 while building predictive models of Japanese banks as a financial consultant. He entered a competition recreationally to practice data modeling and found it to be a terrific outlet for creative problem-solving.\n\nFour years and five gold medals later, he became a grandmaster \u2014 a title held by just 300 of the community\u2019s 19 million learners.\n\nOnodera, based in Tokyo, has been part of the KGMoN team since its founding in 2020. The nine data scientists and engineers, who hail from France to Brazil, compete in challenges to broaden access to and test the capabilities of the latest accelerating computing methods.\n\n\u201cWhen I heard NVIDIA was forming this team, I was surprised my hobby could make a real impact,\u201d Onodera said. \u201cSince joining, I\u2019ve worked on models simulating climate patterns, predicting RNA molecular structures and interpreting MRI images of the brain.\u201d\n\nDuring the pandemic, Onodera built a model predicting the degradation rate of the COVID-19 mRNA vaccine. His gold-medal-winning solution, which calculated the heat sensitivity of the RNA molecules, helped ensure an effective global distribution of the vaccine.\n\nFor Amazon\u2019s KDD Cup 2023 competition, Onodera teamed with KGMoN members across four continents to build multilingual recommender systems \u2014 winning all three tracks of the prestigious challenge. The team worked seamlessly across time zones, exchanging their global perspectives. They also swept all five tracks in the Amazon KDD Cup 2024.\n\n\u201cAt NVIDIA, I constantly have the opportunity to build cutting-edge solutions while learning from talented colleagues around the world,\u201d Onodera said. \u201cI can use my passion for knowledge-sharing to advance technology.\u201d\n\nIn addition to participating in competitions, KGMoN members publish open-source code, engage in discussions and create tutorials for developers. Their insights help guide engineering projects such as RAPIDS and NVIDIA NeMo, which help advance generative AI and data science.\n\n\u201cProviding valuable feedback that empowers data scientists globally has been my greatest pride,\u201d Onodera said. \u201cMachine learning competitions used to only be a hobby, but now, it\u2019s become a part of my life\u2019s work at NVIDIA.\u201d\n\nFollow @nvidialife on Instagram and learn more about NVIDIA life, culture and careers.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia CEO says not attending Trump's inauguration",
            "link": "https://www.reuters.com/world/us/nvidia-ceo-says-not-attending-trumps-inauguration-2025-01-17/",
            "snippet": "Nvidia's chief executive Jensen Huang said on Friday he will not be attending U.S. President-elect Donald Trump's inauguration, but will instead be \"on the...",
            "score": 0.6158632636070251,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia CEO says its advanced packaging technology needs are changing",
            "link": "https://www.taipeitimes.com/News/biz/archives/2025/01/17/2003830329",
            "snippet": "\u201cThe technology that we're working on is becoming more sophisticated. Chips are getting more and more complex, and the packaging technology will need to evolve...",
            "score": 0.5224250555038452,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO says its advanced packaging technology needs are changing\n\nReuters, TAICHUNG\n\n\n\n\n\nNvidia Corp\u2019s demand for advanced packaging from Taiwan Semiconductor Manufacturing Co (TSMC, \u53f0\u7a4d\u96fb) remains strong though the kind of technology it needs is changing, Nvidia CEO Jensen Huang (\u9ec3\u4ec1\u52f3) said yesterday, after he was asked whether the company was cutting orders.\n\nNvidia\u2019s most advanced artificial intelligence (AI) chip, Blackwell, consists of multiple chips glued together using a complex chip-on-wafer-on-substrate (CoWoS) advanced packaging technology offered by TSMC, Nvidia\u2019s main contract chipmaker.\n\n\u201cAs we move into Blackwell, we will use largely CoWoS-L. Of course, we\u2019re still manufacturing Hopper, and Hopper will use CowoS-S. We will also transition the CoWoS-S capacity to CoWos-L,\u201d Huang said on the sidelines of an event by chip testing and packaging company Siliconware Precision Industries Co (SPIL, \u77fd\u54c1\u7cbe\u5bc6) in Taichung.\n\nNvidia Corp CEO and cofounder Jensen Huang, left, and Siliconware Precision Industries Co (SPIL) chairman Tsai Chi-wen take part in the opening ceremony of the SPIL Tan Ke Plant in Taichung yesterday. Nvidia is working on silicon photonics together with SPIL, a subsidiary of ASE Technology Holding Co, Huang said. Photo: An Rong Xu, Bloomberg\n\n\u201cSo it\u2019s not about reducing capacity. It\u2019s actually increasing capacity into CoWoS-L,\u201d he added.\n\nHopper refers to Nvidia\u2019s graphics processing unit (GPU) architecture platform before the company announced Blackwell in March last year.\n\nNvidia has so far relied mainly on one type of CoWoS technology, CoWoS-S, to combine its AI chips.\n\nTF International Securities Group Co (\u5929\u98a8\u570b\u969b\u8b49\u5238) analyst Kuo Ming-chi (\u90ed\u660e\u9324) on Wednesday said Nvidia was shifting its focus to a newer type of technology, CoWoS-L, and that suppliers would be affected.\n\nIn addition, local media reported that Nvidia was cutting CoWoS-S orders from TSMC in a potential hit to the Taiwanese chip foundry\u2019s revenue.\n\nNvidia has been selling its Blackwell chips as quickly as TSMC can make them, but packaging has remained a bottleneck due to capacity constraints.\n\nStill, Huang said that the amount of advanced chip packaging capacity was \u201cprobably four times\u201d the amount available less than two years ago.\n\nHe declined to answer questions on the new US export restrictions that limit AI chip exports to most countries except for a select group of close US allies including Taiwan.\n\nHuang was in Taichung yesterday to attend the opening ceremony of a new SPIL plant in the city\u2019s Tanzi Technology Industrial Park (\u6f6d\u5b50\u79d1\u6280\u7522\u696d\u5712\u5340).\n\n\u201cThe technology that we\u2019re working on is becoming more sophisticated. Chips are getting more and more complex, and the packaging technology will need to evolve as well. What\u2019s even more exciting is the integration of silicon photonics, enabling us to connect multiple packages into one massive system,\u201d Huang said.\n\nHe said Nvidia\u2019s partnership with SPIL would be instrumental in pushing the boundaries of innovation in coming years.\n\nHuang also addressed the broader implications of AI development, saying that AI combined with robotics will bring tremendous benefits to Taiwan\u2019s world-leading electronics industry.\n\nSPIL expressed enthusiasm for the collaboration, stating that Huang\u2019s visit highlights the strong relationship between the two companies.\n\nIndustrial sources told CNA that Huang\u2019s visit to SPIL aims to secure production of 3D CoWoS packaging services used in AI chip manufacturing.\n\nTSMC, which has entered advanced CoWoS development, has outsourced the services to SPIL to meet strong global demand, the sources said.\n\nAs its CoWoS services have secured certification, SPIL is expected to raise production gradually starting from the second quarter of this year, the sources added.\n\nIt was the first public event for Huang in Taiwan this year. He is also expected to attend Nvidia Taiwan\u2019s annual Lunar New Year party this week in Taipei.\n\nIn addition, Huang is to meet with Taiwanese tech executives from Hon Hai Precision Industry Co (\u9d3b\u6d77), Quanta Computer Inc (\u5ee3\u9054) and Inventec Corp (\u82f1\u696d\u9054), which all roll out AI servers powered by Nvidia\u2019s chips, as well as PC brands Asustek Computer Inc (\u83ef\u78a9) and Micro-Star International Co (\u5fae\u661f).\n\nHuang, who was born in Tainan, Taiwan\u2019s historic capital, before emigrating to the US at the age of nine, is hugely popular in the country.\n\nAdditional reporting by CNA and AP",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia Blackwell and GeForce RTX 50-Series GPUs: Specifications, release dates, pricing, and everything we know (updated)",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-blackwell-rtx-50-series-gpus-everything-we-know",
            "snippet": "The top-tier RTX 5090 and 5080 will arrive first, in January 2025. The RTX 5070 Ti and RTX 5070 will come next, probably in February. The 5060-class hardware...",
            "score": 0.9455826878547668,
            "sentiment": null,
            "probability": null,
            "content": "The next-generation Nvidia Blackwell GPU architecture and RTX 50-series GPUs are coming, basically on schedule. Nvidia officially detailed the first four cards in the Blackwell RTX 50-series family at CES 2025, during CEO Jensen Huang's keynote on January 6. We expect the various Blackwell GPUs will join the ranks of the best graphics cards, replacing their soon-to-be-prior-generation counterparts.\n\n\n\nWhen we spoke with some people in early 2024, the expectation was that we'd see at least the RTX 5090 and RTX 5080 by the time the 2024 holiday season rolled around. But then came the delay of Blackwell B200 along with packaging problems, and that appears to have pushed things back. Now, we're looking at a January 2025 announcement with at least one or two models coming before the end of the month, and perhaps as many as four different desktop cards \u2014 and the possibility of laptop RTX 50-series also exists.\n\n\n\nNvidia already provided many of the core details for its data center Blackwell B200 GPU. The AI and data center variants will inevitably differ from consumer parts, but there are some shared aspects between past consumer and data center Nvidia GPUs, and that should continue. That gives some good indications of certain aspects of the future RTX 50-series GPUs.\n\n\n\nThings are starting to clear up now, with hard specifications and pricing details for the first four GPUs. There's still no official word on the 5060-class GPUs, but those should arrive at some point in the coming months.\n\n\n\nLet's talk about specifications, technology, pricing, and other details. We've been updating this article for a while now, as information became available, and we're now in the home stretch. Here's everything we know about Nvidia Blackwell and the RTX 50-series GPUs.\n\nDespite what we personally heard in early 2024, the RTX 50-series didn't make it out the door in 2024, but the first models will launch in January 2025. There were some delays, but not directly related to the consumer GPUs.\n\n\n\nNvidia's data center Blackwell B100/B200 GPUs encountered packaging problems and were also delayed. Given how much money the data center segment raked in over the past year (see Nvidia's latest earnings), putting more money and wafers into getting B200 ready and available makes sense. Gamers? Yeah, we're no longer Nvidia's top priority.\n\n\n\nThe consumer Blackwell GPUs are \"late,\" based on historical precedent. The Ada Lovelace RTX 40-series GPUs first appeared in October 2022. The Ampere RTX 30-series GPUs first appeared in September 2020. Prior to that, RTX 20-series launched two years earlier in September 2018, and the GTX 10-series was in May/June 2016, with the GTX 900-series arriving in September 2014. That's a full decade of new Nvidia GPU architectures arriving approximately every two years. Even so, we're still only a few months beyond the normal cadence.\n\n\n\nAnd now we're into 2025, and Nvidia spilled the beans on the RTX 5090, 5080, 5070 Ti, and 5070 \u2014 along with mobile variants \u2014 during the CES 2025 keynote. The top-tier RTX 5090 and 5080 will arrive first, in January 2025. The RTX 5070 Ti and RTX 5070 will come next, probably in February. The 5060-class hardware could come any time within the next six months after the first 50-series GPUs. As usual, we expect Blackwell GPUs to follow the typical staggered release schedule.\n\nStill TSMC 4N, 4nm Nvidia\n\nNvidia's B200 chips will use TSMC 4NP (Image credit: Nvidia)\n\nOne of the surprising announcements at GTC 2024 was that Blackwell B200 will use the TSMC 4NP node \u2014 \"4nm Nvidia Performance,\" or basically a tuned/tweaked variation of the N4 node used on the RTX 40-series. Blackwell B200 also uses a dual-chip solution, with the two identical chips linked via a 10 TB/s NV-HBI (Nvidia High Bandwidth Interface) connection.\n\n\n\nWhile it's certainly true that process names have largely become detached from physical characteristics, many expected Nvidia to move to a variant of TSMC's cutting-edge N3 process technology. Instead, it opted for a refinement of the existing 4N node that has already been used with Hopper and Ada Lovelace GPUs for the past two years.\n\n\n\nFor the consumer Blackwell chips, though, Nvidia will stick with TSMC 4N. Yes, the same node and process as Ada, with no changes that we're aware of. Going this route certainly offers some cost savings, though TSMC doesn't disclose the contract pricing agreements with its various partners. Perhaps Nvidia just didn't think it needed to move to a 3nm-class node for this generation.\n\n\n\nAMD will be moving to TSMC N4, while Intel will use TSMC N5 for Battlemage. So, even though Nvidia didn't choose to pursue 3nm or 2nm this round, it's still equal to or better than the competition. And the Ada architecture was already well ahead in terms of efficiency, performance, and features in many areas.\n\nNext generation GDDR7 memory\n\nGDDR7 chips were shown at GTC 2024 (Image credit: Tom's Hardware)\n\nBlackwell GPUs will move to GDDR7 memory, at least for the RTX 5070 and above. We don't know for certain what the 5060-class will use, and we don't have official clock speeds, but we do know that the 5070 through 5090 are all using GDDR7 memory.\n\n\n\nThe current generation RTX 40-series GPUs use GDDR6X and GDDR6 memory, clocked at anywhere from 17Gbps to 23Gbps. GDDR7 has target speeds of up to 36Gbps, 50% higher than GDDR6X and 80% higher than vanilla GDDR6. SK hynix says it will even have 40Gbps chips, though the exact timeline for when those might be available hasn't been given. Regardless, GDDR7 will provide a much-needed boost to memory bandwidth at all levels.\n\n\n\nNvidia won't actually ship cards with memory clocked at 36Gbps, though. In the past, it used 24Gbps GDDR6X chips but clocked them at 22.4Gbps or 23Gbps \u2014 and some 24Gbps Micron chips were apparently down-binned to 21Gbps in the various RTX 4090 graphics cards that we tested. The RTX 5090, 5070 Ti, and 5070 will clock their GDDR7 at 28Gbps, while the RTX 5080 opts for a higher memory speed of 30Gbps. Either way, that's still a healthy bump to bandwidth.\n\n\n\nAt 28Gbps, GDDR7 memory provides a solid 33% increase in memory bandwidth compared to the 21Gbps GDDR6X used on the RTX 4090. The RTX 5080 opts for 30Gbps GDDR7, a 30% increase in bandwidth relative to the RTX 4080 Super, and 34% more bandwidth than the original 4080's 22.4Gbps memory. As with so many other aspects of Blackwell, it remains to be seen just how far Nvidia and its partners will push things.\n\n\n\nNvidia will keep using a large L2 cache with Blackwell. This will provide even more effective memory bandwidth \u2014 every cache hit means a memory access that doesn't need to happen. With a 50% cache hit rate as an example, that would double the effective memory bandwidth, though note that hit rates vary by game and settings, with higher resolutions in particular reducing the hit rate.\n\n\n\nGDDR7 also potentially addresses the issue of memory capacity versus interface width. At GTC, we were told that 16Gb chips (2GB) are in production, with 24Gb (3GB) chips also coming. Are the larger chips with non-power-of-two capacity ready for upcoming Blackwell GPUs? Apparently not for the initial salvo, or at least not for the initial desktop cards.\n\n\n\nThere's been at least one rumor suggesting Nvidia might have 16GB (2GB chips) and 24GB (3GB chips) variants of the RTX 5080. As long as the price difference isn't too onerous and the other specs remain the same, that wouldn't be a bad approach. The base models announced so far all come with 2GB chips, while upgraded variants could have 50% more VRAM capacity courtesy of the 3GB chips.\n\n\n\nWe've already seen this with the RTX 5090 Laptop GPU, which uses the same GB203 chip as the desktop RTX 5080/5070 Ti, with a 256-bit memory interface. But then it uses 3GB GDDR7 chips so that the resulting graphics chip has 24GB of VRAM. If Nvidia can do this for the mobile 5090, why isn't it already doing it for the desktop 5080? Probably because it doesn't feel that it needs to \u2014 and to leave room for a mid-cycle refresh using those chips in the future.\n\n\n\nRight now, there's no pressing need for consumer graphics cards to have more than 24GB of memory. But RTX 5090 has a 512-bit interface, meaning it will come with a default 32GB configuration and could offer a 48GB variant in the future. The higher capacity GDDR7 chips could be particularly beneficial for professional and AI focused graphics cards, where large 3D models and LLMs are becoming increasingly common. A 512-bit interface with 3GB chips on both sides of the PCB could yield a professional RTX 6000 Blackwell Generation as an example with 96GB of memory.\n\n\n\nMore importantly, the availability of 24Gb chips means Nvidia (along with AMD and Intel) could put 18GB of VRAM on a 192-bit interface, 12GB on a 128-bit interface, and 9GB on a 96-bit interface, all with the VRAM on one side of the PCB. We could also see 24GB cards with a 256-bit interface, and 36GB on a 384-bit interface \u2014 and double that capacity for professional cards. Pricing will certainly be a factor for VRAM capacity, but it's more likely a case of \"when\" rather than \"if\" we'll see 24Gb GDDR7 memory chips on consumer GPUs.\n\n\n\nBut there's more going on than just raw VRAM capacity. Shown during the CES 2025 keynote, RTX Neural Materials could cut the VRAM requirements of textures by about one third. If that needs to be implemented on a per-game basis, it won't help 8GB cards in all situations, but if it's a driver-side enhancement, 8GB could actually be \"enough\" for most games again.\n\nThe Blackwell architecture will have various updates and enhancements over the previous generation Ada Lovelace architecture. Nvidia hasn't gone into a lot of detail, but one thing is clear: AI is a big part of Nvidia's plans for Blackwell. We know that AI TOPS (teraops) performance per tensor core has been doubled, but Nvidia is pulling a fast one there. Technically, that's for FP4 workloads (which should be classified as FP4 TFLOPS, not AI TOPS, but we digress).\n\n\n\nThe native support for FP4 and FP6 formats boosts raw compute with lower precision formats, but the per-tensor-core compute has remained the same otherwise. Every generation of Nvidia GPUs has contained other architectural upgrades as well, and we can expect the same to occur this round.\n\n\n\nNvidia has increased the potential ray tracing performance in every RTX generation, and Blackwell continues that trend, doubling the ray/triangle intersection rates for the RT cores. With more games like Alan Wake 2 and Cyberpunk 2077 pushing full path tracing \u2014 not to mention the potential for modders to use RTX Remix to enhance older DX10-era games with full path tracing \u2014 there's even more need for higher ray tracing throughput.\n\n\n\nWhat other architectural changes might Blackwell bring? Considering Nvidia is sticking with TSMC 4N for the consumer parts, we don't anticipate massive alterations. There will still be a large L2 cache, and the tensor cores are now in charge of handling the optical flow calculations for frame generation and multi frame generation. DLSS 4 \"neural rendering\" is coming with various other enhancements including multi-frame generation on the RTX 50-series.\n\n\n\nRaw compute, for both graphics and more general workloads, sees a modest bump on the RTX 5090 compared to the RTX 4090, though other GPUs don't seem to be getting as much of an increase. Again, AI might make up for that, but a lot remains to be seen. The 5070 for example offers up 31 TFLOPS of compute, compared to 29 TFLOPS on the 4070. The 5090 has 107 TFLOPS compared to 83 TFLOPS on the 4090.\n\nRTX 50-Series Pricing\n\n(Image credit: Shutterstock)\n\nHow much will the RTX 50-series GPUs cost? Frankly, considering the current market conditions, we're pleasantly surprised so far. A lot of people were understandably angry during the 40-series launches at the generational increase in prices, and with the 50-series Nvidia is mostly holding steady or even stepping back a bit. That doesn't apply to the top-end RTX 5090, though, which will cost $1,999 at launch \u2014 $400 more than the 4090.\n\n\n\nFor dedicated desktop graphics cards we're now living in a world where \"budget\" means around $300, \"mainstream\" means $400\u2013$600, \"high-end\" is for GPUs costing $800 to $1,000, and the \"enthusiast\" segment targets $1,500 or more. Or at least, that appears to be Nvidia's take on the situation. Other than the increase on the 5090, so far the other GPUs are at the same price or $50 lower than their direct predecessors.\n\nBlackwell specifications\n\nWith the official reveal out of the way, we have the specifications for the RTX 5070 and above. The 5060 Ti and 5060 are more speculative, with various unknowns and question marks. But here's our updated specifications table (and we'll update the table as other 50-series GPUs are officially revealed).\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nSwipe to scroll horizontally Graphics Card RTX 5090 RTX 5080 RTX 5070 Ti RTX 5070 RTX 5060 Ti RTX 5060 Architecture GB202 GB203 GB203 GB205 GB207 GB207 Process Node TSMC 4N TSMC 4N TSMC 4N TSMC 4N TSMC 4N TSMC 4N Transistors (Billion) 92.2 45.6 45.6 31.0 ? ? Die size (mm^2) 750 378 378 263 ? ? SMs 170 84 70 48 36? 24? GPU Shaders (ALUs) 21760 10752 8960 6144 4608? 3072? Tensor / AI Units 680 336 280 192 144? 96? Ray Tracing Units 170 84 70 48 36? 24? Boost Clock (MHz) 2407 2617 2452 2512 2500? 2500? VRAM Speed (Gbps) 28 30 28 28 30? 28? VRAM (GB) 32 16 16 12 8? 8? VRAM Bus Width 512 256 256 192 128? 128? L2 Cache 96 64 48 48 32? 24? Render Output Units 176 112 96 80 48? 32? Texture Mapping Units 680 336 280 192 144 96 TFLOPS FP32 (Boost) 104.8 56.3 43.9 30.9 23.0? 15.4? TFLOPS FP16 (INT8 TOPS) 838 (3352) 450 (1801) 352 (1406) 247 (988) 199? (737?) 133? (492?) Bandwidth (GB/s) 1792 960 896 672 480? 448? TBP (watts) 575 360 300 250 200? 150? Launch Date Jan 2025 Jan 2025 Feb 2025 Feb 2025 May 2025? Jun 2025? Launch Price $1,999 $999 $749 $549 $399? $299?\n\nGPU boost clocks are slightly lower than the 40-series on several GPUs, but note that the 40-series cards tended to exceed the stated boost clocks by around 200 MHz on average. The rest of the specs all follow from the SMs (Streaming Multiprocessors), which gives the CUDA, RT, and tensor core counts based on the usual 128 CUDA, 1 RT, and 4 tensor cores per SM. There are also four TMUs (Texture Mapping Units) per SM.\n\n\n\nGPC (graphics processing cluster) counts aren't listed, but the 5090 has 11 (out of a potential 12 for the full GB202 chip), 5080 has seven, 5070 Ti has six, and 5070 has five. The ROPS are tied to the GPC counts, with 16 ROPS per GPC. The 5060 class GPUs are probably still a few months out, but the RTX 5070 Laptop GPU has been announced and it still has a 128-bit memory interface and 8GB of VRAM. <sigh>\n\n\n\nWe'll continue to update this table with other official numbers once those become available. Eventually, everything that's unknown or guesswork will get replaced with concrete information. There will almost certainly be far more than the four announced GPUs plus our two placeholders, just as there are ten different RTX 40-series desktop GPUs and twelve different RTX 30-series desktop variants.\n\n16-Power Connectors, Take Three\n\n(Image credit: Tom's Hardware)\n\nAfter the 16-pin meltdown fiasco that plagued the first wave of RTX 4090 cards, many people probably want Nvidia to abandon the new PCI-SIG standard. That's not going to happen, though the change to the modified ATX 12V-2x6 connector has hopefully put any potential problems to rest.\n\n\n\nWhat's interesting is that the RTX 40-series wasn't the first generation of GPUs to come with a 16-pin connector. The RTX 30-series used 12-pin adapters (without the extra four sense pins of 12VHPWR) starting clear back in 2020. We didn't hear a bunch of stories about melting 3090 and 3080 adapters, but then most of those cards had TGPs well under 400W. The RTX 3090 Ti GPUs were the first to use the newer 16-pin connector, but again with no rash of reported meltdowns. With RTX 40-series making widespread use of 16-pin, that means Blackwell will be the third generation of Nvidia GPUs to at least partially adopt the standard.\n\n\n\nOne of the key elements with the 4090 melting problems seems to be pulling 450W or more through a single relatively compact connector. With the 5090 set to have a stock power level of 575W, it's a big step up from the 4090. Let's hope everyone involved has learned a few lessons from the 4090 meltdowns and builds the rising generation to be more robust.\n\n\n\nAlso an interesting point here is that Nvidia's Founders Edition cards (5090, 5080, and 5070) have returned to an angled 16-pin connector rather than the vertical orientation of the 40-series. That's good, we think, as the 30-series was a bit easier to fit into cases since it didn't have the 8-pin to 16-pin adapters sticking straight up. The adapters aren't necessary if you have an ATX 3.0 power supply, but if you have an older PSU the adapters become necessary. The cables on the new adapters are more flexible as well, which should help with system builders.\n\nThe future GPU landscape\n\n(Image credit: Shutterstock)\n\nNvidia won't be the only game in town for next-generation graphics cards. Intel's Battlemage has already launched, at least with the Arc B580, with Arc B570 now available. AMD RDNA 4 will also arrive at some point \u2014 the RX 9070 XT and RX 9070 should launch by March, with the 9060 cards coming by mid 2025.\n\n\n\nBut while there will certainly be competition, Nvidia has dominated the GPU landscape for the past decade. At present, the Steam Hardware Survey indicates Nvidia has 75.8% of the graphics card market, AMD sits at 16.2%, and Intel accounts for just 7.7% (with 0.3% \"other\"). That doesn't even tell the full story, however.\n\n\n\nBoth AMD and Intel make integrated graphics, and it's a safe bet that a large percentage of their respective market shares comes from laptops and desktops that lack a dedicated GPU. AMD's highest market share for what is clearly a dedicated GPU comes from the RX 6600, sitting at 0.99%. Intel doesn't even have a dedicated GPU listed in the survey \u2014 integrated Arc does show up with 0.24%, though. For the past three generations of AMD and Nvidia dedicated GPUs, the Steam survey suggests Nvidia has 92.1% of the market compared to 7.9% for AMD.\n\n\n\nGranted, the details of how Valve collects data are obtuse, at best, and AMD may be doing better than the survey suggests. Still, it's a green wave of Nvidia cards at the top of the charts. Recent reports from JPR say that Nvidia controlled 88% of the add-in GPU market compared to 12% for AMD, as another example of the domination currently going on.\n\n\n\nIntel apparently wants Battlemage to compete more in the budget to mainstream segment of the graphics space. We'll have to see if there's a higher spec Battlemage GPU, and how high it reaches, but the B580 targets $249 while the upcoming B570 will start at $219. AMD competes better with Nvidia for the time being, both in performance and drivers and efficiency, but we're still waiting for its GPUs to experience their \"Ryzen moment\" \u2014 GPU chiplets so far haven't proven an amazing success. And AMD isn't going after anything above the \"5070\" level it seems with RDNA 4.\n\n\n\nCurrently, Nvidia offers higher overall performance at the top of the GPU totem pole, and much higher ray tracing performance. It also dominates in the AI space, with related technologies like DLSS \u2014 including DLSS 3.5 Ray Reconstruction \u2014 Broadcast, and other features. It's Nvidia's race to lose, and it will take a lot of effort for AMD and Intel to close the gap and gain significant market share, at least outside of the integrated graphics arena. On the other hand, high Nvidia prices and a heavier focus on AI for the non-gaming market could leave room for its competitors. We'll see where the chips land soon enough.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Applied Digital CEO Wes Cummins Talks Nvidia, Liquid Cooling, And Finding Capacity Amid 'This Big Infrastructure Revolution'",
            "link": "https://www.crn.com/news/data-center/2025/applied-digital-ceo-wes-cummins-talks-nvidia-liquid-cooling-and-finding-capacity-amid-this-big-infrastructure-revolution",
            "snippet": "Applied Digital CEO Wes Cummins talks about the demand signals in the data center space, the progress of the company's massive North Dakota build-out and...",
            "score": 0.7038396000862122,
            "sentiment": null,
            "probability": null,
            "content": "Applied Digital CEO Wes Cummins Talks Nvidia, Liquid Cooling, And Finding Capacity Amid 'This Big Infrastructure Revolution' 'This is the internet build-out in the late \u201890s and early 2000s on steroids,' Applied Digital CEO Wes Cummins tells CRN. \u2018It\u2019s just happening much, much faster \u2026 That was laying cables, and this is building the structures that the cabling connects. So it\u2019s power-driven because using compute equals using power.'\n\nAs the AI gold rush takes off, billions of dollars are being poured into the construction of data centers across the country and with early investments in land, power \u2014 including a two-gigawatt wind farm in North Dakota \u2014 Applied Digital CEO Wes Cummins has set the table for a feast in 2025. \u201cThis is the internet build-out in the late \u201890s and early 2000s on steroids,\u201d Cummins told CRN. \u201cIt\u2019s just happening much, much faster \u2026 That was laying cables, and this is building the structures that the cabling connects. So it\u2019s power-driven because using compute equals using power.\u201d During its most recent quarter reported this week, the Dallas-based Nvidia Elite partner said revenue grew 51 percent year over year to $63.9 million. That growth was driven by the expansion of the company\u2019s cloud services business. The company reported a net loss for the second quarter of $139.4 million or $0.66 per share as Applied Digital finished construction of the first of three facilities in North Dakota. The firm\u2019s stock is up 10 percent since the Jan. 14 earnings announcement and nearly 40 percent in the last six months. [RELATED: Nvidia Elite Partner Applied Digital Hires Data Center Veteran To Make Operations \u2018Predictable, Repeatable And Scalable\u2019] The company also won $5 billion in funding from Macquarie Asset Management, with up to $900 million being directed to fund Applied Digital\u2019s Ellendale HPC data center campus, its high-performance computing facility that is currently under construction in North Dakota. Another $4.1 billion of those funds is reserved for Applied Digital data center pipeline. Cummins said the investment took about seven months to come together. \u201cI\u2019m very happy with the terms of this structure with Macquarie,\u201d Cummins said when asked about the deal during the company\u2019s earnings call on Tuesday. \u201cBut we did a lot of diligence to make sure we were getting the best partner, and the best terms.\u201d Cummins said the investment is part of the \u201clast piece of the puzzle\u201d for Applied Digital to completing its Ellendale, N.D. facility, which turned on the site\u2019s main substation transformer for the first time last quarter, a milestone to opening the 100-megawatt facility and eventually increasing its capacity to 400 megawatts. Cummins told investors on the company\u2019s earnings call is in late-stage negotiations with multiple hyperscale customers for a leasing agreement at that facility. \u201cI don\u2019t know of anything of this scale available in 2025,\u201d Cummins said during the call. \u201cI\u2019m really optimistic about signing a lease on that facility and moving on to the next one.\u201d Applied Digital said its cloud services business generated $27.7 million during the quarter, about $23.2 million more than a year ago as demand for high performance computing power took off. Meanwhile, its data center hosting business is running at capacity across two facilities powering 286 megawatts. CRN spoke with Cummins ahead of Applied Digital\u2019s earnings about the demand signals in the data center space, the progress of the company\u2019s massive North Dakota build-out and the opportunities he sees in 2025.\n\nDemand for data center seems to be reaching another high this year. There\u2019s a bunch of investment coming to the data center space, including the DAMAC announcement. We talked last year about some of the supply chain constraints. It would be great to hear what\u2019s top of mind for you this year. I would say when you watch demand from my seat in real time, you get these ebbs and flows. The demand is definitely up and to the right. It goes from strong demand to there feels like there\u2019s panic buying. That happened last year multiple times, where all of a sudden just within four days of each other, we get pinged by three very potentially large customers that are looking for X amount of capacity, and they needed it by this date. Microsoft was out with a blog post saying they plan $80 billion in data center spend in 2025. I saw an interview with someone from AWS, they\u2019re going to spend $60 billion in 2025. So just those two by themselves, that\u2019s probably as big as the data center market was, four years ago, the entire market. And so that\u2019s just them. You had all their traditional data center infrastructure investors that have been around the hoop for a decade or more. And now you\u2019re seeing everyone trying to get into the space. So I\u2019ve spent a lot of my time educating people about this space, because they just know that there's a lot of money being put to work and it\u2019s this big infrastructure revolution. This is the internet build-out in the late \u201890s and early 2000s on steroids. It\u2019s just happening much, much faster. If you remember, it was all about connectivity. It was connectivity build. There was a fight over whether DSL or cable modems were going to win but either way, there was a huge amount of infrastructure built. That was laying cables, and this is building the structures that the cabling connects. So it\u2019s power-driven, because using compute equals using power. So power is still the big bottleneck, and I think it just keeps getting worse. And then after that, you have supply chain constraints. In my opinion, we\u2019ve moved on from in 2023 and the early part of 2024 the supply chain constraint was primarily GPUs, and now you've moved more to where do I put those GPUs? That\u2019s the piece that was so clear to me in 2023, when we started going hard at building these types of data centers. Look, they\u2019ll fix wafer throughput. They\u2019ll fix advanced packaging, like co-ops packaging. They\u2019ll fix those types of bottlenecks much faster than we\u2019ll fix, \u2018How do we get more power generation?\u2019 \u2018How do we get permitting, land, buildings built, all the equipment that needs to go inside of data centers built?\u2019 I thought that would become the bottleneck, and that\u2019s really what we\u2019ve seen.\n\nHow are you approaching some of the constraints? How are you finding success there, whether it\u2019s power, whether it\u2019s supply or whether it\u2019s real estate? What we\u2019ve done is we have focused on stranded power in North America. And so stranded power is created generally in two ways in North America. One, is a really power-hungry industry goes out of business. So think of an aluminum smelter, steel mill goes out of business, and leaves all this power infrastructure behind and doesn\u2019t really have any use for it. So if you look at the big bitcoin campus in the center of Texas, I believe that was an old Alcoa Aluminum plant that had all this power-gen and infrastructure that went away. The other way is, typically through renewable energy. The incentive programs in the U.S., especially around renewables, is you\u2019re generating renewable energy credits, RECs. And so if you\u2019re a wind farm developer, ours happens to be mostly wind. So I use that example, if you\u2019re a wind farm developer, you want to go for the most bang for the buck on generating renewable energy credits. You want to go where the wind blows the most, and the land is the cheapest, and so the center of the country has a lot of this. North Dakota where we\u2019re building our big campus now. That substation is surrounded by about two gigawatts of wind power, and it\u2019s much faster to permit and build generation versus transmission. So you can permit and build (power) generation in 18 to 36 months. New transmission takes 12 to 15 years to build. So you end up with locations like ours, where there's more power generated than can be used. And so hence the term stranded power. And so we have several of those locations, and that\u2019s the route we have went. Now, I think that\u2019s a short-term solution, right? We\u2019ll build our capacity up. We\u2019ll lease it out. But really, when you look into the future, to feed this demand, you need new generation. And, I think the gap in between is mostly going to be filled by a lot of natural gas. So if we compare natural gas with more renewables. Natural gas, you can take up and down pretty easily to pair nicely with renewables, because renewables generate either when the wind is blowing or the sun is shining, and so if you can match those together. But, over a long period of time, you've seen a lot around nuclear power. You\u2019ve seen contracts with Microsoft. You\u2019ve seen announcements from Amazon. You\u2019ve seen a lot with the hyperscalers around nuclear power. It's just that it's going to be slow and take a while. But I think that\u2019s kind of the solution, if you\u2019re looking seven-plus years out.\n\nWhen you hear that $20 billion is going to be spent on data center \u2014 between Texas, Louisiana, Indiana, Illinois \u2014 does that affect kind of one way or the other what Applied Digital does or your strategy for the year? For us, our strategy is pretty locked in for the year. We have our power that we have locked in. We\u2019re building our facility at Ellendale. We expect to start building more in 2025. But when you look at the market at large, the data center capacity available for 2025 is largely baked in at this point. It's really 2026 planning, and then out into 2027 and beyond planning at this point. So our strategy for this year is staying on time and on budget for what we're building for \u201825 capacity, and then starting to lock in \u201826 capacity from a customer perspective. We've done a good job of locking in supply chain as well. Supply chain is really difficult, as you can imagine. So backup generators\u2019 really long, lead times, switch gear, UPS, step-down transformers, chillers \u2014 all of these things that you need in a data center have extraordinarily long lead times, because of the unprecedented demand we\u2019re seeing.\n\nDell and Lenovo have both introduced liquid-cooled servers in the last year that promise to remove much of, if not nearly all of, the heat from the compute. With the power constraints in the data center, are you seeing demand for those? I think that\u2019s going to be one of the biggest stories of \u201925: liquid cool at scale. Because liquid cool has been done for a long time, but not at the scale of the deployments that we\u2019re looking at for with AI. So we\u2019re going to see a lot of stories about it this year, good and bad. There's going to be some snafus with deploying at scale. There\u2019s going to be some great success stories. So our Ellendale facility \u2013 a 100-megawatt, critical, IT load building that you can see videos of in our social media -- is liquid to liquid. We call waterless liquid to liquid. We\u2019re able to do that in North Dakota because of the climate. I think if you look at some new stories around Microsoft specifically, as far as hyperscalers go, they\u2019re trying to move to a waterless liquid-to-liquid solution. But I think you\u2019re going to see a lot of that go out this year. There\u2019s all of the plumbing that we do building the liquid cool data center that has to happen. And then there\u2019s a decision point for our customers where they actually choose which type of rack they're going to use for their liquid cooling. And so there\u2019s a lot of different vendors for those solutions. We won't actually choose that. That'll be our customers that do that, but it\u2019s our job to get basically all the way down to the CPU, and then deploying whatever solution that they choose, but you\u2019re gonna see a lot of that this year.\n\nOn the supply chain side, do you foresee products like the GB200 easing the availability of Nvidia\u2019s last generation of chip, or is there always going to be a flight to the highest quality chip? We're in this part of the market. Everyone's really focused on Blackwell right now and getting Blackwell, and there\u2019s not a huge amount of supply that I\u2019ve seen. You don\u2019t have to take it from me, you can just read plenty of news articles about the supply of Blackwell. Nvidia is a great company, so they\u2019ll get there on the supply but you\u2019re seeing a lot of focus on that from the market as well. And the reason is while the chips use more power, you\u2019re actually getting much more compute. I think it's like 3x the amount of compute on a per-watt basis from the chip. So it lowers the cost overall to run. As these chips get more and more efficient, even though you're going to use more power in a dense area, you\u2019re getting much more compute than the extra power. You're getting, like I said, I think I saw recently, it\u2019s about 3x the amount of flops that you're getting out of those on a watt-for-watt basis. People are going to move towards that because it lowers costs, right? If you're getting 3x the amount of compute for a similar price, and it uses a third of the power, even though the power density is going up, I think there's going to be a push to move towards those newer chips. Because it's expensive, right? The AI infrastructure piece is expensive. The chips are expensive. The data centers are expensive. And the more you can push down, or the more efficiency you can get out of power usage, the more efficiently you can operate the data center.\n\nIn terms of chip rivalries, Broadcom gets brought up a lot these days. How serious a threat is Broadcom to the big GPU makers? There\u2019s a lot of great chip companies out there, Broadcom being one of the best. And so you\u2019re going to see this. You\u2019ve seen it in many markets before, right? It typically takes longer than you think for people to get displaced. There was a time when you thought the BlackBerry would never get replaced, right? And it lasted for a long time. But when markets get big enough, and enough capital flows in, eventually there's going to be competing solutions. What I will always say with Nvidia is that I think people focus way too much on the GPU with Nvidia, and what Nvidia has done, in a spectacular way, is create a system. That\u2019s the harder part, I think, to replicate, versus just, \u2018Can someone else build a chip?\u2019 So Nvidia has the GPU, but they have the networking, which is super important. Like when they announced Blackwell, I thought one of the biggest advancements there, obviously, you're getting many more flops per kilowatt or per watt going in on those GPUs. But the way it works is a system with the networking and then the software layered on top. It creates a really powerful mode for Nvidia, and that\u2019s why you see them, I think, get the valuation. That\u2019s why you see them just continue to grow rapidly. There absolutely are already competitors today, and are there going to be more and more? Absolutely. But there's just this really big moat around the system, not just the chip. So I wouldn't just focus on a chipmaker, and then what the market is going to try to do overall. There's a lot of work around Ethernet to try to mimic what InfiniBand does to capture the networking portion. But then you still have the software piece that layers on top, right? CUDA is a very powerful platform.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "NVIDIA\u2019s GeForce RTX 50 GPUs Expected to Have Limited Launch Availability; Team Green To \u201cNitpick\u201d Retailer Distribution of SKUs",
            "link": "https://wccftech.com/nvidia-geforce-rtx-50-gpus-expected-to-have-limited-launch-availability/",
            "snippet": "NVIDIA's GeForce RTX 50 GPUs Expected to Have Limited Launch Availability; Team Green To \u201cNitpick\u201d Retailer Distribution of SKUs ... It seems like NVIDIA has a...",
            "score": 0.8724252581596375,
            "sentiment": null,
            "probability": null,
            "content": "It seems like NVIDIA has a new strategy for distributing NVIDIA's RTX 50 series SKUs among retailers, as Team Green may \"nitpick\" stores where the GPUs will be available.\n\nNVIDIA's GeForce RTX 5090 & RTX 5080 GPUs Might Be Impossible To Get At MSRP Around Launch Days\n\nGetting NVIDIA's newest Blackwell consumer GPUs, especially the GeForce RTX 5090 and GeForce RTX 5080, might be pretty tricky this time, at least around the launch period, since, according to Pokerclock, a prominent leaker at PC Games Hardware forum, Team Green might itself decide how to distribute stock among B2B retailers. Along with this, not only will the first batch of RTX 50 SKUs have \"limited availability,\" but this time, resellers and holders looking to get their hands on the initial batch of GPUs might be disappointed since NVIDIA has plans to combat them.\n\nIt is claimed that NVIDIA will, this time, decide which retailer will have the larger share of NVIDIA's RTX 50 GPUs. This could potentially mean that many famous stores out there might not have stock available on day one or even weeks after that, since availability at this time will be pretty confined. This would affect not only the average consumer, but also individuals who look to get their hands on new GPUs on launch dates to later sell at inflated prices and might need to find something else. The only way to access the RTX 50 GPUs might be through \"digital-queue\" systems, which yet again are influenced by automated bots.\n\nThe market launch of Nvidia's new graphics cards is imminent and, as I was able to learn from well-informed dealer circles, the available contingent of graphics cards will be extremely limited! This applies in particular to the RTX 5090. Accordingly, Nvidia determines where and who exactly will offer graphics cards at the market launch. B2B dealers and the entire local wholesale trade, which primarily also works with business customers, will most likely come away empty-handed. - PC Games Hardware Forum\n\nThe reason behind NVIDIA's distribution approach isn't certain for now. Still, the general notion this time is that Team Green will have a confined stock altogether compared to previous releases. Especially for flagship SKUs like the GeForce RTX 5090, getting them at MSRPs would be almost impossible, given that, this time, availability is pretty low. While this move would undoubtedly help NVIDIA prevent resellers from getting most of the stock, it will still affect the end user tremendously.\n\nImage Credits: PC Games Hardware\n\nIt is claimed that retailers like MindFactory and Alternate will have a higher portion of stock in Germany, so that's something to keep in mind. The availability situation may favor Team Red with their RDNA 4 release if they manage to provide better inventory levels and significantly superior performance from the RTX 50 series SKUs, but yet again, that's something for the future.\n\nNews Source: Videocardz",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia CEO Skips Trump Inauguration, AI Export Battle Looms Amid Biden's Crackdown",
            "link": "https://finance.yahoo.com/news/nvidia-ceo-skips-trump-inauguration-120712368.html",
            "snippet": "Huang skips inauguration for Lunar New Year as Nvidia grapples with new U.S. AI export restrictions under Biden.",
            "score": 0.840270459651947,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NASDAQ:NVDA) CEO Jensen Huang said Friday he will not attend U.S. President-elect Donald Trump's inauguration on Jan. 20, opting instead to travel for Lunar New Year celebrations with employees and their families.\n\nSpeaking to reporters outside Nvidia's New Year party in Taipei, Huang said he has yet to discuss with Trump's incoming administration the new artificial intelligence export control rules announced by outgoing President Joe Biden's administration earlier this week. \"But I'll look forward to congratulating the Trump administration when they take office,\" Huang said.\n\nThe executive also confirmed a recent meeting with C.C. Wei, chairman of Nvidia's key supplier Taiwan Semiconductor Manufacturing Co. (TSMC), to discuss ramping up production of Nvidia's latest AI chips, known as Blackwell. Santa Clara, California-based Nvidia, now among the world's most valuable companies with a market capitalization exceeding $3 trillion, faces challenges from new U.S. government restrictions on AI chip exports.\n\nThe Biden administration's regulations, unveiled Jan. 13, limit exports to most countries except select U.S. allies, including Taiwan, while maintaining restrictions on shipments to China. On Monday, Nvidia criticized the new controls, warning that tighter regulations could threaten U.S. leadership in AI.\n\nIt remains unclear how the Trump administration will enforce the measures, but both administrations have taken a similar stance on China's access to advanced semiconductor technology.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "This Week in AI: Biden Signs Executive Orders; Nvidia Targets Healthcare",
            "link": "https://www.pymnts.com/news/artificial-intelligence/2025/this-week-in-ai-biden-signs-executive-orders-nvidia-targets-healthcare/",
            "snippet": "President Joe Biden issued executive orders related to AI leadership and safety while Nvidia unveiled partnerships with healthcare organizations to accelerate...",
            "score": 0.8798349499702454,
            "sentiment": null,
            "probability": null,
            "content": "This week in artificial intelligence, President Joe Biden issued executive orders related to AI leadership and safety while Nvidia unveiled partnerships with healthcare organizations to accelerate drug discovery. OpenAI, Amazon and Elon Musk were also in the news.\n\nBiden\u2019s Executive Orders Focus on Governance, Safety\n\nPresident Joe Biden issued executive orders related to maintaining leadership of AI in the United States and policies to protect against potential AI harms.\n\nHe signed an order Tuesday (Jan. 14) to set aside federal lands for the building of AI data centers, with the full cost borne by the builders of these foundation AI models. Companies building data centers such as Google, Meta, Amazon and Microsoft must ensure that there is a clean energy source for these data centers since AI workloads are famous energy guzzlers.\n\nBiden also signed an order Thursday (Jan. 16) designed to improve the nation\u2019s cybersecurity. The steps he outlined include accelerating the development and deployment of AI.\n\nThe order also requires the federal government to adopt secure software acquisition practices and ensure software providers use secure software development practices; adopt proven security practices in terms of identity and access management; and implement strong authentication and encryption to improve the security of communications.\n\nNvidia Targets Healthcare Industry\n\nNvidia unveiled partnerships with healthcare organizations Mayo Clinic, Illumina, IQVIA and Arc Institute. They will use Nvidia\u2019s technology to accelerate drug discovery, enhance genomic research and develop advanced services using agentic and generative AI.\n\nThey plan to use AI agents that can help accelerate clinical trials, AI models that can help discover new drugs faster and physical AI robots for surgery.\n\nThe Mayo Clinic will work with Nvidia to develop pathology foundation models to accelerate diagnosis and treatment of disease. IQVIA, which provides clinical research services, is using Nvidia\u2019s AI foundry service to build custom foundation models trained on its data.\n\nIllumina, a DNA sequencing company, is using Nvidia\u2019s accelerated computing and AI toolsets for drug discovery. Arc Institute, a biology and machine learning research group, will work with Nvidia to develop AI models and tools to advance biomedical discovery.\n\nScheduling Comes to ChatGPT\n\nOpenAI\u2019s ChatGPT is now letting users schedule tasks for a later, or recurring, date. Scheduled tasks is rolling out in beta for users of ChatGPT Plus, Pro and Teams. Enterprise users will get access soon; free users will also eventually get it. OpenAI did not set specific dates.\n\nWith tasks, users can tell ChatGPT to do things like set up a daily 15-minute workout that focuses on lifting weights. It also can give them recurring reminders, like taking out the trash on Sundays at 6 p.m. Users can edit or cancel the task.\n\nScheduled Tasks is available on the ChatGPT website, iOS, Android and macOS. Integration into the Windows app is coming in the first quarter. Tasks use OpenAI\u2019s GPT-4o model. Users are limited to 10 tasks at a time. In addition, each plan\u2019s messaging limits also apply.\n\nAmazon Hits Snags in Transforming Alexa With GenAI\n\nAmazon\u2019s revamp of Alexa to put generative AI at its core is hitting technical snags such as hallucinations.\n\nRohit Prasad, who leads the artificial general intelligence (AGI) team at Amazon, told the Financial Times that hallucinations, in which AI models make things up, have to be \u201cclose to zero.\u201d Since large language models are probabilistic, they can hallucinate when asked questions or encounter scenarios outside their training data.\n\nThat means when a customer orders from Alexa, the agentic AI assistant might purchase another product or invent the number of orders, for example. Alexa\u2019s global scale makes it a sensitive issue. With over half a billion devices, it must get requests and orders right.\n\nFTC, DOJ Signal Support for Elon Musk in OpenAI Lawsuit\n\nThe Federal Trade Commission and the Department of Justice offered legal analysis supporting Elon Musk\u2019s claim that OpenAI and Microsoft engaged in anticompetitive practices. However, the government was not officially expressing an opinion on the case.\n\nMusk\u2019s lawyer said the participation of the FTC and DOJ is a \u201csign of how seriously regulators take OpenAI and Microsoft\u2019s misconduct.\u201d\n\nMusk is suing OpenAI to prevent it from becoming a for-profit company from a capped-profit firm with a nonprofit parent. He also alleges that OpenAI and Microsoft are acting anticompetitively by making investors agree not to invest in rival AI firms and by sharing board members.\n\nTreasury Department Hack Committed by Silk Typhoon\n\nChinese hacking group Silk Typhoon was reportedly behind the December hack of the Department of the Treasury.\n\nThe hackers stole a digital key from a third-party service provider and used it to access documents stored on laptops and desktop computers. There is no evidence that the group still has access to Treasury systems or information.\n\nCyber officials in the U.S. have said that by hacking America\u2019s critical infrastructure, Chinese cybercriminals aim to disrupt critical services to hinder the U.S. military in times of crisis.\n\nFor all PYMNTS AI coverage, subscribe to the daily AI Newsletter.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "The Future of Copilots and AI Agents: Takes from Microsoft, Salesforce, & NVIDIA",
            "link": "https://www.cxtoday.com/conversational-ai/the-future-of-copilots-and-ai-agents-takes-from-microsoft-salesforce-nvidia/",
            "snippet": "CX Today covers Conversational AI news including Agent Assist, AI Agents, Artificial Intelligence, Generative AI, Virtual Assistant and more.",
            "score": 0.9361955523490906,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia GeForce RTX 5090 only 30% faster than RTX 4090 without DLSS 4",
            "link": "https://www.pcgamesn.com/nvidia/geforce-rtx-5090-30-faster-4090-dlss-4",
            "snippet": "Nvidia has revealed that its new flagship gaming GPU, the RTX 5090, is only 30% faster than the RTX 4090 in ray-traced game performance if you don't engage its...",
            "score": 0.6554005742073059,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has revealed that its new flagship gaming GPU, the RTX 5090, is only 30% faster than the RTX 4090 in ray-traced game performance if you don\u2019t engage its multi-frame generation DLSS 4 tech. That corresponds roughly with the 33% increase in CUDA cores on the new card, but points to its architecture not offering any extra hidden performance gains.\n\nAs we found in our Nvidia GeForce RTX 5090 review, this new Nvidia GPU is now the fastest and ultimately best graphics card you\u2019ll be able to buy this year, making it a hugely tempting option for anyone seeking the latest and greatest. However, those hoping a 33% CUDA core count increase and corresponding price increase would result in an even larger performance improvement might be a bit disappointed.\n\nThese figures fly in the face of the 2x performance increase Nvidia claimed for the card at its launch. Nvidia was clear that those figures referred to performance with DLSS 4 Multi Frame Generation, but potential buyers might have been hoping the unsaid raw performance boost was higher.\n\nDLSS 4 Multi Frame Generation is the latest addition to the company\u2019s DLSS suite that boosts game frame rates by generating new frames using AI. This was available already with DLSS 3 Frame Generation, but DLSS 4 extends its capability further by offering improved image quality and the ability to generate up to three frames per pair using AI, rather than just the extra one frame with DLSS 3.\n\nOur DLSS 4 hands on experience had us very impressed with how much it can boost frame rates and how good it can look, but its performance and quality aren\u2019t guaranteed across all games. Plus, based on our past experience of DLSS 2 and 3, there will be games where it isn\u2019t a desirable choice to leave the feature turned on due to its impact on image quality, during particularly fast motion.\n\nMeanwhile, when taken in isolation, a 30% increase in performance for a new flagship gaming GPU isn\u2019t a bad return. We\u2019ve seen new card releases before when the increase was far lower. However, it\u2019s the fact that Nvidia is bumping up the price of this card by 33% that\u2019s potentially the kicker. If the RTX 5090 were the same price as the RTX 4090 \u2013 or, say, only 10-20% more expensive \u2013 it would feel like a better value return with this performance, but as it stands, it doesn\u2019t look set to be a particularly great value proposition.\n\nAll that and Nvidia has been notably quiet about the RTX 5090 rasterization performance, its gaming performance without ray tracing. Nvidia has noted the improved capabilities and performance of its ray tracing cores, and its much more capable AI processing power in its latest architecture \u2013 the latter being the reason the company now has the highest value in the world \u2013 but next to nothing about the rendering method still used by the vast majority of modern games. Time and our testing will tell when it comes to that side of the story.\n\nFor now, it\u2019s clear the RTX 5090 will remain the go-to choice for those looking to simply have the best card available. AMD\u2019s upcoming RX 9070 XT isn\u2019t set to challenge Nvidia at the top end of the market and the RTX 5090 is set to offer more performance across the board than the RTX 4090. However, there a few warning signs that, for some gamers, the RTX 5090 might not be the complete upgrade for which they were hoping.\n\nOne final reason you might want to consider the RTX 5090, though, is that Nvidia\u2019s own RTX 5090 FE card is actually surprisingly small and conforms to the company\u2019s SFF-Ready small form factor case scheme. That means it can fit in compact cases such as the tiny Fractal Mood, that you can buy here. Meanwhile, you can read more about SFF-Ready here.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-16": {
        "0": {
            "title": "NVIDIA Releases NIM Microservices to Safeguard Applications for Agentic AI",
            "link": "https://blogs.nvidia.com/blog/nemo-guardrails-nim-microservices/",
            "snippet": "NVIDIA NeMo Guardrails includes new NVIDIA NIM microservices to enhance accuracy, security and control for enterprises building AI across industries.",
            "score": 0.9389822483062744,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA NeMo Guardrails includes new NVIDIA NIM microservices to enhance accuracy, security and control for enterprises building AI across industries.\n\nAI agents are poised to transform productivity for the world\u2019s billion knowledge workers with \u201cknowledge robots\u201d that can accomplish a variety of tasks. To develop AI agents, enterprises need to address critical concerns like trust, safety, security and compliance.\n\nNew NVIDIA NIM microservices for AI guardrails \u2014 part of the NVIDIA NeMo Guardrails collection of software tools \u2014 are portable, optimized inference microservices that help companies improve the safety, precision and scalability of their generative AI applications.\n\nCentral to the orchestration of the microservices is NeMo Guardrails, part of the NVIDIA NeMo platform for curating, customizing and guardrailing AI. NeMo Guardrails helps developers integrate and manage AI guardrails in large language model (LLM) applications. Industry leaders Amdocs, Cerence AI and Lowe\u2019s are among those using NeMo Guardrails to safeguard AI applications.\n\nDevelopers can use the NIM microservices to build more secure, trustworthy AI agents that provide safe, appropriate responses within context-specific guidelines and are bolstered against jailbreak attempts. Deployed in customer service across industries like automotive, finance, healthcare, manufacturing and retail, the agents can boost customer satisfaction and trust.\n\nOne of the new microservices, built for moderating content safety, was trained using the Aegis Content Safety Dataset \u2014 one of the highest-quality, human-annotated data sources in its category. Curated and owned by NVIDIA, the dataset is publicly available on Hugging Face and includes over 35,000 human-annotated data samples flagged for AI safety and jailbreak attempts to bypass system restrictions.\n\nNVIDIA NeMo Guardrails Keeps AI Agents on Track\n\n\n\nAI is rapidly boosting productivity for a broad range of business processes. In customer service, it\u2019s helping resolve customer issues up to 40% faster. However, scaling AI for customer service and other AI agents requires secure models that prevent harmful or inappropriate outputs and ensure the AI application behaves within defined parameters.\n\nNVIDIA has introduced three new NIM microservices for NeMo Guardrails that help AI agents operate at scale while maintaining controlled behavior:\n\nContent safety NIM microservice that safeguards AI against generating biased or harmful outputs, ensuring responses align with ethical standards.\n\nthat safeguards AI against generating biased or harmful outputs, ensuring responses align with ethical standards. Topic control NIM microservice that keeps conversations focused on approved topics, avoiding digression or inappropriate content.\n\nthat keeps conversations focused on approved topics, avoiding digression or inappropriate content. Jailbreak detection NIM microservice that adds protection against jailbreak attempts, helping maintain AI integrity in adversarial scenarios.\n\nBy applying multiple lightweight, specialized models as guardrails, developers can cover gaps that may occur when only more general global policies and protections exist \u2014 as a one-size-fits-all approach doesn\u2019t properly secure and control complex agentic AI workflows.\n\nSmall language models, like those in the NeMo Guardrails collection, offer lower latency and are designed to run efficiently, even in resource-constrained or distributed environments. This makes them ideal for scaling AI applications in industries such as healthcare, automotive and manufacturing, in locations like hospitals or warehouses.\n\nIndustry Leaders and Partners Safeguard AI With NeMo Guardrails\n\n\n\nNeMo Guardrails, available to the open-source community, helps developers orchestrate multiple AI software policies \u2014 called rails \u2014 to enhance LLM application security and control. It works with NVIDIA NIM microservices to offer a robust framework for building AI systems that can be deployed at scale without compromising on safety or performance.\n\nAmdocs, a leading global provider of software and services to communications and media companies, is harnessing NeMo Guardrails to enhance AI-driven customer interactions by delivering safer, more accurate and contextually appropriate responses.\n\n\u201cTechnologies like NeMo Guardrails are essential for safeguarding generative AI applications, helping make sure they operate securely and ethically,\u201d said Anthony Goonetilleke, group president of technology and head of strategy at Amdocs. \u201cBy integrating NVIDIA NeMo Guardrails into our amAIz platform, we are enhancing the platform\u2019s \u2018Trusted AI\u2019 capabilities to deliver agentic experiences that are safe, reliable and scalable. This empowers service providers to deploy AI solutions safely and with confidence, setting new standards for AI innovation and operational excellence.\u201d\n\nCerence AI, a company specializing in AI solutions for the automotive industry, is using NVIDIA NeMo Guardrails to help ensure its in-car assistants deliver contextually appropriate, safe interactions powered by its CaLLM family of large and small language models.\n\n\u201cCerence AI relies on high-performing, secure solutions from NVIDIA to power our in-car assistant technologies,\u201d said Nils Schanz, executive vice president of product and technology at Cerence AI. \u201cUsing NeMo Guardrails helps us deliver trusted, context-aware solutions to our automaker customers and provide sensible, mindful and hallucination-free responses. In addition, NeMo Guardrails is customizable for our automaker customers and helps us filter harmful or unpleasant requests, securing our CaLLM family of language models from unintended or inappropriate content delivery to end users.\u201d\n\nLowe\u2019s, a leading home improvement retailer, is leveraging generative AI to build on the deep expertise of its store associates. By providing enhanced access to comprehensive product knowledge, these tools empower associates to answer customer questions, helping them find the right products to complete their projects and setting a new standard for retail innovation and customer satisfaction.\n\n\u201cWe\u2019re always looking for ways to help associates go above and beyond for our customers,\u201d said Chandhu Nair, senior vice president of data, AI and innovation at Lowe\u2019s. \u201cWith our recent deployments of NVIDIA NeMo Guardrails, we ensure AI-generated responses are safe, secure and reliable, enforcing conversational boundaries to deliver only relevant and appropriate content.\u201d\n\nTo further accelerate AI safeguards adoption in AI application development and deployment in retail, NVIDIA recently announced at the NRF show that its NVIDIA AI Blueprint for retail shopping assistants incorporates NeMo Guardrails microservices for creating more reliable and controlled customer interactions during digital shopping experiences.\n\nConsulting leaders Taskus, Tech Mahindra and Wipro are also integrating NeMo Guardrails into their solutions to provide their enterprise clients safer, more reliable and controlled generative AI applications.\n\nNeMo Guardrails is open and extensible, offering integration with a robust ecosystem of leading AI safety model and guardrail providers, as well as AI observability and development tools. It supports integration with ActiveFence\u2019s ActiveScore, which filters harmful or inappropriate content in conversational AI applications, and provides visibility, analytics and monitoring.\n\nHive, which provides its AI-generated content detection models for images, video and audio content as NIM microservices, can be easily integrated and orchestrated in AI applications using NeMo Guardrails.\n\nThe Fiddler AI Observability platform easily integrates with NeMo Guardrails to enhance AI guardrail monitoring capabilities. And Weights & Biases, an end-to-end AI developer platform, is expanding the capabilities of W&B Weave by adding integrations with NeMo Guardrails microservices. This enhancement builds on Weights & Biases\u2019 existing portfolio of NIM integrations for optimized AI inferencing in production.\n\nNeMo Guardrails Offers Open-Source Tools for AI Safety Testing\n\nDevelopers ready to test the effectiveness of applying safeguard models and other rails can use NVIDIA Garak \u2014 an open-source toolkit for LLM and application vulnerability scanning developed by the NVIDIA Research team.\n\nWith Garak, developers can identify vulnerabilities in systems using LLMs by assessing them for issues such as data leaks, prompt injections, code hallucination and jailbreak scenarios. By generating test cases involving inappropriate or incorrect outputs, Garak helps developers detect and address potential weaknesses in AI models to enhance their robustness and safety.\n\nAvailability\n\n\n\nNVIDIA NeMo Guardrails microservices, as well as NeMo Guardrails for rail orchestration and the NVIDIA Garak toolkit, are now available for developers and enterprises. Developers can get started building AI safeguards into AI agents for customer service using NeMo Guardrails with this tutorial.\n\nSee notice regarding software product information.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Is Nvidia Stock a Buy Now?",
            "link": "https://www.fool.com/investing/2025/01/16/is-nvidia-stock-a-buy-now/",
            "snippet": "Should I buy Nvidia stock? Under current conditions, investors should treat Nvidia as a hold. Considering its valuation metrics and the chip industry's...",
            "score": 0.9148306250572205,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) is one of the most widely followed stocks today, and it's easy to see why. Its lead in the artificial intelligence (AI) accelerator market supercharged its revenue growth and made it the largest semiconductor stock, as measured by market cap, next to Apple.\n\nUnfortunately for investors who want to buy Nvidia shares, its success has made determining whether it is a buy now a more difficult problem. Do its technical lead and continuous improvement make it a no-brainer buy, or has its valuation made it too expensive to touch at current levels?\n\nNvidia's dominance\n\nAs most tech investors know, the company's AI accelerators and the revenue gains caused most of the growth in the stock price. With that, the data center segment that designs AI chips went from the company's second-largest revenue source to producing 88% of company revenue in just three years.\n\nThis is fortuitous when considering the state of the growing AI chip industry. Grand View Research forecasts a compound annual growth rate of 29% through 2030. Between the predicted growth and shortages in accelerators, Nvidia is also the company best positioned to serve this market.\n\nThe innovation does not stop with accelerators or the CUDA software platform that cements its AI chip dominance. Nvidia has continued to develop numerous new products, some of which it just announced at CES.\n\nThis includes a graphics card built on Blackwell architecture and other AI-driven advancements designed to power humanoid robots and self-driving cars. Even though time will tell how these products perform in the marketplace, this innovation increases the likelihood that Nvidia will play an even more essential role in the tech industry in the foreseeable future.\n\nDangers in its financials\n\nTo that end, Nvidia generated $35 billion in revenue in the third quarter of fiscal 2024 (ended Oct. 27, 2024), a yearly increase of 94%. Amid that improvement, it earned $19 billion in net income in fiscal Q3, a 109% rise from year-ago levels.\n\nIndeed, investors should be thrilled to experience such growth, and experienced investors know that the triple-digit revenue increases of past quarters are not sustainable.\n\nUnfortunately, investors tend to punish stocks for slowing revenue growth, and the company's valuation may leave it vulnerable, at least if digging beyond the surface.\n\nOn the surface, its price-to-earnings ratio of 53 is above S&P 500 averages, though many slower-growing tech stocks have a higher earnings multiple. The price-to-sales (P/S) ratio of 30 better highlights how expensive the stock has become, but that may not deter investors who want to benefit from Nvidia's rapid growth.\n\nNonetheless, the price-to-book value ratio of 51 takes its valuation into nosebleed territory. In comparison, AMD trades at just over 3 times book value.\n\nAdditionally, Nvidia's success makes it more subject to the cyclicality that has always defined the semiconductor industry. Indeed, the stock prices may hold, and it is possible that Nvidia stock will avoid a significant downturn in the near term.\n\nStill, its situation points to a key vulnerability. New AI accelerators sell for more than $30,000. However, if demand falls, prices will likely follow, a factor that could reduce or even reverse the company's revenue growth.\n\nNvidia stock has also experienced pullbacks of more than 50% twice in the last seven years. Such a reversal could bring about another significant decline.\n\nThankfully, cycles also move upward at some point, so Nvidia stock should eventually recover from any downturn due to its new technology. Still, considering that a down cycle will inevitably occur at some point, investors may want to think twice about making huge purchases at this time.\n\nShould I buy Nvidia stock?\n\nUnder current conditions, investors should treat Nvidia as a hold. Considering its valuation metrics and the chip industry's cyclicality, investors are likely overpaying for Nvidia at its current price.\n\nNonetheless, the company benefits from clear dominance in the AI chip industry, and that is unlikely to change soon. Moreover, its continued innovation will likely cement that dominance and foster more long-term growth.\n\nHence, when balancing Nvidia's attributes with its challenges, staying the course is probably the best action for its shareholders.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia CEO says its advanced packaging technology needs are changing",
            "link": "https://www.reuters.com/technology/nvidia-ceo-says-its-advanced-packaging-technology-needs-are-changing-2025-01-16/",
            "snippet": "Nvidia's demand for advanced packaging from TSMC remains strong though the kind of technology it needs is changing, the U.S. AI chip giant's CEO Jensen...",
            "score": 0.5224250555038452,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "2025 could be a peak year for Nvidia stock: analyst",
            "link": "https://www.investing.com/news/stock-market-news/2025-could-be-a-peak-year-for-nvidia-stock-analyst-3816677",
            "snippet": "A note from DA Davidson on Thursday suggests that 2025 could mark the peak year for Nvidia (NASDAQ:NVDA) stock as the firm continues to express caution.",
            "score": 0.7346363067626953,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Unreleased Nvidia TITAN RTX components displayed at CES",
            "link": "https://www.dlcompare.com/gaming-news/unreleased-nvidia-titan-rtx-components-displayed-at-ces-49937",
            "snippet": "Unreleased Nvidia TITAN RTX components displayed at CES ... During CES 2025, YouTuber \u201cToro Tocho Reviews\u201d found components belonging to Nvidia's upcoming RTX...",
            "score": 0.9438185095787048,
            "sentiment": null,
            "probability": null,
            "content": "During CES 2025, YouTuber \u201cToro Tocho Reviews\u201d found components belonging to Nvidia\u2019s upcoming RTX TITAN GPU. According to the video below, this GPU was designed to occupy 4 PCIe slots and would have the same color scheme as the RTX 40 series graphics cards.\n\nA company reportedly hired to produce the cooler unintentionally showcased the unreleased TITAN RTX cooler frame. Creating such an accurate mockup without precise measurements would be nearly impossible, making its CES appearance likely something NVIDIA didn\u2019t intend. Given that this GPU uses Nvidia\u2019s contemporary GeForce font, it is likely that the heatsink is intended for an RTX 40 series \u201cAda\u201d model. Unfortunately, Nvidia never launched the anticipated Ada TITAN GPU, making Nvidia\u2019s RTX 4090 their official flagship product for Ada.\n\nIf Nvidia had released an Ada-based TITAN model, it probably would have featured 48GB of GDDR6 VRAM. The GPU might have also made use of previously unused parts from Nvidia\u2019s AD102 silicon, providing it with additional CUDA cores, Tensor Cores, and RT Cores. This would have positioned Nvidia\u2019s new TITAN GPU as their most powerful ADA-based offering.\n\nThe presence of the Nvidia TITAN heatsink indicates that Nvidia did indeed plan to release Ada-based TITAN products at some time. It\u2019s evident that Nvidia ultimately opted for a different direction, and it remains uncertain whether they will develop a new Blackwell-based TITAN product. We are already aware that Nvidia could produce a GPU superior to their RTX 5090, a fact that Nvidia confirmed at CES 2025.\n\nAnd as always, while waiting for more exciting news, remember to use our comparison tool to find the best deal for your next favorite game.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia CEO Will Miss Trump Inauguration, Unlike Many Tech Peers",
            "link": "https://finance.yahoo.com/news/nvidia-ceo-miss-trump-inauguration-195652545.html",
            "snippet": "(Bloomberg) -- Nvidia Corp. Chief Executive Officer Jensen Huang is expected to miss the inauguration of President-elect Donald Trump on Jan.",
            "score": 0.8819074034690857,
            "sentiment": null,
            "probability": null,
            "content": "(Bloomberg) -- Nvidia Corp. Chief Executive Officer Jensen Huang is expected to miss the inauguration of President-elect Donald Trump on Jan. 20, bucking a trend among high-profile US technology leaders.\n\nMost Read from Bloomberg\n\nHuang is visiting East Asia this week, as he typically does around the time of the Lunar New Year, according to a person familiar with the situation. He\u2019s never previously attended a US presidential inauguration, said the person, who asked not to be identified because the plans haven\u2019t been announced.\n\nThat makes Nvidia an exception among the most valuable technology companies, most of which are sending co-founders or CEOs to the event. That includes Apple Inc. CEO Tim Cook and Amazon.com Inc. co-founder Jeff Bezos, as well as Tesla Inc. head Elon Musk, a close Trump ally.\n\nA representative for Santa Clara, California-based Nvidia declined to comment on Huang\u2019s whereabouts or whether there\u2019s been communication with the incoming administration.\n\nHuang said earlier this month that he\u2019d be delighted to meet Trump and \u201cdo whatever we can to make this administration succeed.\u201d But he hadn\u2019t yet been invited to the president-elect\u2019s home base at Mar-a-Lago in Florida, Huang said at the time.\n\nHuang also said he expected Trump to bring less regulation. \u201cI think that\u2019s a good thing,\u201d he said. \u201cAs an industry, we want to move fast.\u201d\n\nNvidia is facing a barrage of new rules and regulations in the final days of the Biden administration. The measures include steps to tighten access to advanced technology, such as Nvidia\u2019s industry-leading AI chips, to keep them out of China\u2019s hands.\n\nNvidia has chafed at new export curbs unveiled this week, calling them an \u201coverreach\u201d and warning that they could be catastrophic for the tech industry.\n\n\u201cAs the first Trump administration demonstrated, America wins through innovation, competition, and by sharing our technologies with the world \u2014 not by retreating behind a wall of government overreach,\u201d Ned Finkle, Nvidia\u2019s vice president of government affairs, said earlier this week.\n\nNvidia and Huang himself have become the biggest stars of the chip world over the past two years. As the company\u2019s sales have surged, Huang has crisscrossed the globe promoting artificial intelligence technology, often in his signature leather jacket. Nvidia is the world\u2019s most valuable chipmaker and only ranks behind Apple among all tech companies.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia releases microservices to safeguard AI agents",
            "link": "https://siliconangle.com/2025/01/16/nvidia-releases-microservices-safeguard-ai-agents/",
            "snippet": "Nvidia NIM is a set of containerized microservices designed to speed up the deployment of generative AI models and today's announcement builds on NeMo...",
            "score": 0.9351962208747864,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia\u2019s Rev Lebaredian Talks Training AI-Powered Robots",
            "link": "https://time.com/7204663/nvidia-rev-lebaredian-ai-robotics/",
            "snippet": "Nvidia's Rev Lebaredian Talks Training AI-Powered Robots \u00b7 There's lots of discussion about human-centered AI, which augments the human experience rather than...",
            "score": 0.7769027948379517,
            "sentiment": null,
            "probability": null,
            "content": "There\u2019s lots of discussion about human-centered AI, which augments the human experience rather than replaces it. But aren\u2019t humanoid robots a displacement technology?\n\nIt\u2019s a touchy subject for many. The fact is we\u2019ve been building a pyramid scheme throughout mankind\u2019s existence. Every generation banks on the next generation paying for the people that exist today. And now we\u2019re at a potential crisis, where the next generation in many places is actually going to be smaller. We just don\u2019t have enough labor. Robots are the only hope we have of producing the services we need to sustain our society. How are we going to do all of that stuff if we don\u2019t have advanced manufacturing, advanced transportation, that are using agents that are superhuman in many of those capabilities?\n\nYour work involves creating a digital twin of the real world, called Omniverse, in which AI robots can learn how to act. How does that work?\n\nIt\u2019s just like a video game. Omniverse is a kind of video game but different in that they\u2019re not fantasy worlds. They\u2019re constrained to the laws of physics of the real world as accurately as possible, so we can run a bunch of simulations and test things. The AI, the robot, thinks it\u2019s inside the real world and can go try things much faster, because we can just throw more computing at it. In one hour of the real world, it drives millions of hours in the virtual world, and if it makes mistakes, it\u2019s not going to harm anyone.\n\nWere you tempted to call it the Matrix?\n\nIt\u2019s maybe like the Matrix in that it\u2019s indistinguishable from the real world. That\u2019s the key. Because at some point you want to transfer the robot brain into the actual robot to operate in the real world. And if what it\u2019s trained on is a cartoony, non-realistic version, then it\u2019s not going to operate well.\n\nWhat do you see as the first uses for AI robotics?\n\nIt\u2019s always a difficult thing to predict. There\u2019s $100 trillion of stuff that could be helped by robots with general capabilities. I\u2019m particularly focusing on the industrial world, manufacturing, warehousing, supply chains, and logistics. Construction eventually. There aren\u2019t enough people to work in our factories. Some robots will be better working in areas where humans can\u2019t go, like operating in a nuclear reactor, or dangerous mining.\n\nSome people fear this technology could have military applications.\n\nI don\u2019t know if it\u2019s a worry of mine. I feel like we\u2019ve already created many technologies that are really frightening but somehow we found a way, despite all the geopolitical conflicts, to come to some understanding of what\u2019s OK and what isn\u2019t. All technology can be used for good or bad, but I think we can find some way to agree.\n\nNvidia\u2019s CEO Jensen Huang has been outspoken about the harmful effect of U.S. export controls. Your most advanced chips can\u2019t be sold in China, and it feels like new restrictions are unveiled constantly. Are you concerned about the trajectory of global trade?\n\nWe comply with all laws in the countries where we work. We\u2019re a U.S.-based company and, of course, we not only abide by our rules but respect the fact that our democratically elected government has to be stewards of our country\u2019s security. But every person on earth deserves to benefit from these new capabilities, not just the United States. And so we hope that whatever regulations and export controls they put into place also take that into account.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia developing 30MW data center in Israel",
            "link": "https://www.datacenterdynamics.com/en/news/nvidia-developing-30mw-data-center-in-israel/",
            "snippet": "Nvidia is developing a 30MW data center - making it one of the largest in the country. Local press report the facility is close to the Elyakim Interchange near...",
            "score": 0.8969843983650208,
            "sentiment": null,
            "probability": null,
            "content": "The company reportedly aims to invest $500 million in the facility, which will host Blackwell GPUs to serve the company\u2019s local R&D teams. It will be used by the company\u2019s dozens of engineering and product teams, developing data center technologies including networking hardware and software technologies, CPU design, AI software, and more.\n\nLocal press report the facility is close to the Elyakim Interchange near Yokne\u2019am, in the north of the country.\n\nAs reported by Globes and confirmed to DCD , Nvidia is developing a 30MW data center - making it one of the largest in the country.\n\nThe company said the 10,000 sqm (107,640 sq ft) facility will host hudnreds of liquid cooled systems. Operations at the site are due to start in the first half of this year.\n\nReportedly located in the Mevo Carmel Science and Industry Park between Elyakim and Kibbutz Ramat Hashofet, the facility is set to be housed in a logistics building belonging to real estate company Shoval Investment and Management that has been converted into a data center.\n\n\"Our employees are our most important resource, and we are committed to continued investment in them,\" Amit Krig, SVP networking software, and Nvidia-Israel site leader, told the publication. \"The new research, development, and engineering site will enable Nvidia\u2019s thousands of employees in Israel, and the many who will join later, to continue developing the technologies that drive artificial intelligence, the most important technological power of our time.\"\n\nNvidia has a large R&D presence in Israel, with thousands of employees and seven labs in the country. It has acquired several Israel-based companies, including Mellanox, Deci, and Run:ai. The company launched an Israeli supercomputer, known as Israel-1, in 2023. The system ranks 34th on the Top500 list of most powerful supercomputers, capable of 53.68 peak petaflops (Linpack).\n\nShoval Investment and Management is an Israeli real estate company specializing in office buildings.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NVIDIA Partnerships to Boost AI in the Healthcare Industry",
            "link": "https://www.channelinsider.com/news-and-trends/nvidia-ai-partnerships-healthcare-innovation/",
            "snippet": "NVIDIA strengthens healthcare with AI partnerships in clinical trials, genomics, and drug discovery, driving innovation and improving patient care.",
            "score": 0.9055781960487366,
            "sentiment": null,
            "probability": null,
            "content": "Channel Insider content and product recommendations are editorially independent. We may make money when you click on links to our partners. View our editorial policy here.\n\nMultinational tech giant NVIDIA made a handful of recent announcements that highlight its participation in the AI revolution permeating the healthcare industry.\n\nDuring a press briefing, Kimberly Powell, Vice President of Healthcare at NVIDIA, broke down NVIDIA\u2019s recent moves in the AI and healthcare space, including collaborating with other entities to enable new products and services with AI.\n\nNVIDIA is fostering partnerships with several organizations as the convergence of AI, accelerated computing, and biological data turns healthcare into the largest technology industry. These partnerships include collaborations with IQVIA for clinical trials, Illumina for genomics insights, the Mayo Clinic for pathology models, and the Arc Institute for biology AI models.\n\nCurrently, NVIDIA\u2019s AI enterprise platform supports over 1,000 healthcare startups developing AI agents.\n\nNVIDIA\u2019s newest collaborations\n\nIQVIA partnership to boost drug and medical device deployment\n\nAmong the announcements include clinical research leader IQVIA partnering with NVIDIA in their AI foundry and factory platforms and services to accelerate their development and deployment of AI agents for their over 10,000 customers across healthcare and life sciences.\n\n\u201cIQVIA and the NVIDIA AI Foundry is going to really help IQVIA streamline custom model development using NVIDIA NIMs, our newly announced Llama Nemotron models, our AI blueprints as reference workflows, and the NEMO platform on dedicated capacity of NVIDIA DGX cloud,\u201d said Powell. \u201cNVIDIA\u2019s collection of models, AI agents, and reference workflows will be made available through their IQVIA healthcare-grade AI solutions to its global pharmaceutical, biotech, and med device customers.\u201d\n\nPowell says that the partnership is going to focus initially on clinical trials. The partnership will accelerate trial execution while significantly reducing the administrative burden through the deployment of agents of all kinds.\n\nNVIDIA will also be releasing its latest generation of molecular generation NIM called GenMol, a goal-directed molecular generation NIM that is going to be utilized for virtual screening.\n\nArc Institute collaboration to scale biology AI models\n\nArc Institute, a California-based research organization, will be collaborating with NVIDIA to develop and share powerful AI models and tools to enhance biomedical discovery.\n\n\u201cIf we want to scale science, we must use machine learning, and if we need machine learning for science, we need the AI factories to do it,\u201d Powell said. \u201cArc is pioneering a new model in scientific acceleration. They have very non-traditional funding approaches, they have long-term thinking, and they are all about cutting-edge technology centers, which we\u2019re partnering with them on and building at their computational technology center.\u201d\n\nAs part of this collaboration, NVIDIA is providing Arc Institute with expertise in large-scale model development, the NVIDIA BioNeMo platform running on NVIDIA DGX Cloud for easy-to-use, optimized training, and NVIDIA NIM microservices and blueprints.\n\nMayo Clinic collaboration to develop NextGen pathology foundation models\n\nNVIDIA will also collaborate with Mayo Clinic to accelerate the development of next-generation pathology foundation models to push the frontiers in personalized health experiences, as well as predictive and efficient treatment strategies.\n\n\u201cMayo Clinic will deploy NVIDIA DGX Blackwell, featuring 1.4 terabytes of GPU memory. This is ideal for handling those large digital pathology whole slide images and these systems will integrate with NVIDIA\u2019s MONAI,\u201d said Powell.\n\nPowell also states that the company\u2019s ultimate goal is to create a human digital twin, a dynamic digital representation that includes medical imaging, pathology, health records, and wearables. To achieve this, Mayo Clinic and NVIDIA will leverage the latest AI models and vision language models like NVIDIA Cosmos Nemotron and NIM microservices.\n\nIllumina partnership to boost genomic breakthroughs\n\nLastly, NVIDIA will be working with Ilumina, a global leader in DNA sequencing and informatics technologies, to unlock next-generation genomics for drug discovery and human health.\n\nThrough this partnership, Illumina will be enabled to use NVIDIA accelerated computing and AI toolsets for multiomics analysis software and workflows. Illumina will offer DRAGEN analysis software on NVIDIA accelerated computing within the Illumina Connected Analytics platform. The integration will expand DRAGEN accessibility globally to wherever NVIDIA\u2019s computing platform exists.\n\n\u201cThis collaboration will combine Illumina\u2019s world-leading sequencing technologies and their connected analytics platform with all of NVIDIA\u2019s Clara AI tools to develop and deploy foundation models that unlock the next generation of genomics insights, truly expanding the opportunities of genomics into more areas of drug discovery and precision health,\u201d said Powell. \u201cOur partnership will revolutionize the analysis and interpretation of multiomics data.\u201d\n\nNVIDIA\u2019s AI enterprise and healthcare startups\n\nNVIDIA has over 1,000 digital healthcare startups in their program developing thousands of AI agents. These agents are built with NVIDIA AI enterprise, which provides them all with the essential building blocks for AI creators from pre-trained models to state-of-the-art retrieval, augmented, generation, and guardrails for agentic AI.\n\n\u201cThese AI agents are like knowledge robots,\u201d Powell said. \u201cThey\u2019re designed to assist healthcare staff or enhance patient experiences and improve operations. For example, Abridge uses AI agents to automate clinical documentation in real-time, converting these patient clinic and clinician conversations into structured clinical notes. This is saving doctors up to three hours of clerical work every single day.\u201d\n\nAmong other AI agents partnered with NVIDIA, include Intrivo\u2019s AI agent called Ray, a natural and emotionally intelligent tool that acts as a medical concierge that meets user needs while helping navigate healthcare solutions, and Hippocratic AI which made some recent announcements concerning their agents, including how their agents are helping healthcare providers during natural disasters.\n\nEvolution of AI and its applications in healthcare\n\nNVIDIA sees AI agents as digital employees and they\u2019re becoming the new enterprise applications. The next phase of AI is poised to be physical AI, which includes having physical robots and a physical environment all operating and embodying AI in the physical world.\n\n\u201cEvery single nation is struggling to meet the demands of aging populations and chronic disease management,\u201d said Powell. \u201cAI agents, AI instruments, and AI robots will be powered by AI factories, running large language models, running biology foundation models, running world foundation models, and assembling these models into agents that perform tasks.\u201d\n\nAI of all kinds is going to be required to meet the growing demand of healthcare and 2025 will be a critical year along this journey.\n\nConclusion and vision for AI in healthcare\n\nThe healthcare industry is a $10 trillion industry, with over 30 percent of operating expenses being dedicated to meeting the growing demand.\n\n\u201cNo single company can achieve this alone,\u201d said Powell. \u201cThe incredible ecosystem and partnerships we\u2019ve just announced will demonstrate this rapid adoption of NVIDIA AI by the world\u2019s leading companies in the industry. We\u2019re on a mission to improve patient care, increase the accessibility, and, together, we\u2019re going to write the next chapter in medicine.\u201d\n\nNVIDIA AI tools aim to transform healthcare, from drug discovery to physical AI in hospitals, and to develop solutions that help advance human health.\n\nNVIDIA continues to make moves in the AI space both inside and outside of the healthcare space. Read more about NVIDIA\u2019s recent AI collaboration with Accenture and KION on AI-powered digital twins.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-15": {
        "0": {
            "title": "These Nvidia AI NPCs Are Just as Obnoxious as Real Players",
            "link": "https://gizmodo.com/these-nvidia-ai-npcs-are-just-as-obnoxious-as-real-players-2000550516",
            "snippet": "Nvidia and Krafton's PUBG Ally is a bad friend and worse co-op buddy, but I prefer it to AI which doesn't know the difference between orange and purple.",
            "score": 0.7570739984512329,
            "sentiment": null,
            "probability": null,
            "content": "The RTX 50-series GPUs may have been the stars of the show this CES, but AI is Nvidia\u2019s real bread and butter. In Nvidia\u2019s perfect world, the PCs and games would all be subsumed by AI that runs directly on your PC without cloud processing. If there were anything I wish AI could do, it would be to stop talking so much.\n\nNvidia covered my flight and lodgings to check out a suite of demos showing off an early look at its AI concepts. Some of its planned models could make sense with more fine-tuning. The company is working on a text-to-body motion framework for developers, though we have to see how professional animators react to the tool. Additional AI lip-syncing and \u201cautonomous enemies\u201d are supposed to transform the typical boss fight\u2014where you learn then exploit a predicable pattern\u2014into something more spontaneous for both enemies and players.\n\nThe generative AI made to renovate your classic NPC or enemy AI in games doesn\u2019t quite stick the landing. Take, for instance, the AI \u201cAlly\u201d Nvidia helped create for the battle royale game PUBG: Battlegrounds. It was a silly trailer for what was essentially your usual co-op AI, but one you can issue orders to with your voice.\n\nI asked it to talk like a pirate. The AI responded, \u201cAh, the pirate life. Let\u2019s hope we don\u2019t end up walking the plank. Stick with me, and we\u2019ll find a buggy soon.\u201d It\u2019s worse to try and ascribe some personality to the AI that will talk back to you rather than the unblinking NPC buddy you can yell at for getting in your way every few seconds. I found the AI was pretty slow at helping me acquire a weapon. When I inevitably got shot and crawled toward him, pleading with him to pick me up, the bot shot absently at the enemy, then ran past into a neighboring house. I only survived because the developers offered invincibility if you got shot during the demo.\n\nThe PUBG Ally is the same asinine experience of playing co-op with a bot, except this one always talks back to you in complete sentences. Krafton also uses Nvidia\u2019s tech to craft inZOI, a Sims-type life simulator that puts chatbots in characters\u2019 heads. Despite the AI supposedly planning and making life choices for the in-game characters, it still looked like a dull version of The Sims, lacking most of the charm of those games. I see the vision; it\u2019s just not there yet.\n\nWe\u2019ve previously tried Nvidia\u2019s ACE, or its autonomous game NPCs, at the last CES and later in 2024. Based on our previous demos, the generated voice lines are incredibly stiff and sound like they\u2019re being read from a parody book of cyberpunk and detective genre cliches. Nvidia didn\u2019t have an update for its AI NPCs this time; instead, the company showed off another demo in a demo of an upcoming game called ZooPunk. It\u2019s supposedly about a rebel bunny with two laser swords on his back. How can anybody make that uncool? By adding scratchy AI voice lines to its characters.\n\nOur demo saw Nvidia ask a character in-game to change the color of his spaceship from beige to purple. \u201cNo problem. Let\u2019s get started,\u201d the AI replied. Instead of purple, it made the ship orange. On the second try, the AI managed to find the right end of the color spectrum. Then, Nvidia asked it to change the decal on the ship to a narwhal fighting a unicorn. Instead, the AI-generated an image that seemed to show a narwhal doing a Lady and the Tramp spaghetti scene with a ball-shaped horned gelding.\n\nThe real showcase of Nvidia\u2019s AI was in its desktop companion apps. First was its G-Assist, a chatbot you can use to automatically manipulate settings in Nvidia\u2019s apps and calculate the best in-game settings for your PC\u2019s specs. The chatbot can even give you a graph of your GPU and CPU performance levels over time, and it should run directly from your PC rather than the cloud. I would prefer it if it could adjust your settings for you rather than having to turn the knobs yourself. Still, at the very least, it is an intriguing use for AI.\n\nBut AI can\u2019t just be intriguing. It needs to be wild. Otherwise, who cares? There were demos for an AI head made for streamers that were surprisingly rude to users. However, the real star was a talking mannequin head sitting like a gargoyle on your home desktop screen. it\u2019s using the company\u2019s neural faces, lip-syncing, text-to-voice, and skin models to try and make something \u201crealistic.\u201d In my estimation, it still lands on the wrong side of the uncanny valley.\n\nThe idea is you could talk to it like you would Copilot on Windows, but you can also drop files onto its forehead for the AI to read and regurgitate information from. Nvidia demoed this with an old PDF of the booklet that came with Doom 3 (as if to remind us when playing games on PC was as simple as buying and installing a disc). Based on the booklet, it could offer a rundown of the game\u2019s story.\n\nYou can add a face to AI, but it will still just be a text-generation machine that doesn\u2019t comprehend what it says. Nvidia mentioned it could add G-Assist to its animated assistant, but that wouldn\u2019t make it any speedier, faster, or more reliable. It would still be a big black hole of AI taking up the bottom of your desktop, staring at you with lifeless eyes and an empty smile.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "Illumina, Nvidia Launch AI-Based Genomics Partnership",
            "link": "https://www.genengnews.com/gen-edge/illumina-nvidia-launch-ai-based-genomics-partnership/",
            "snippet": "Nvidia and Illumina have also agreed to collaborate on advancing multi-omic data analysis using Illumina Connected Analytics, in addition to developing new...",
            "score": 0.8686747550964355,
            "sentiment": null,
            "probability": null,
            "content": "SAN FRANCISCO\u2014Nvidia will partner with Illumina to apply genomics and AI technologies to analyze and interpret multi-omic data in drug discovery, clinical research, and human health, the companies said in a collaboration of technology leaders announced early during the 43rd Annual J.P. Morgan Healthcare Conference.\n\nIllumina will offer its DRAGEN analysis software on Nvidia\u2019s accelerated computing platforms and make Nvidia\u2019s BioNeMo tools available on the next-gen sequencing giant\u2019s Illumina Connected Analytics (ICA) platform . By integrating their technologies, the companies aim to expand the reach of DRAGEN and Illumina multi-omics analysis globally to wherever users access Nvidia\u2019s computing platform.\n\n\u201cNvidia has had great success in building GPUs as an acceleration platform, and they have a wide distribution of that asset. They have GPUs everywhere driven by the AI demand that has risen. We\u2019ll work together to make sure that we can also run DRAGEN on GPUs,\u201d Rami Mehio, Illumina\u2019s head of global software and informatics, told GEN Edge.\n\nThat work, Mehio said, has been in progress for a year and is very far along, but will take some additional time to complete.\n\n\u201cOur code is quite big, and to basically port it all in an efficient way on GPU takes time,\u201d Mehio explained. \u201cBoth teams, Nvidia and us, have been working together on it. We do have prototypes running today, but they\u2019re not production quality yet. We\u2019re still adding functionality and removing bugs.\u201d\n\n\u201cI would expect that we will have something certainly before the end of this year that\u2019s available,\u201d Mehio added.\n\nIntegrating NVIDIA with DRAGEN and Illumina Connected Analytics helps Nvidia at a time when it is working with several undisclosed countries as they establish national sovereign AI strategies.\n\n\u201cReally unique opportunity\u201d\n\n\u201cThey want to protect their population data, but they also want to create the conditions that will enable them to have a world-class health system, all at the same time. So, we have a really unique opportunity by bringing DRAGEN in the Illumina platform on our GPUs,\u201d Kimberly Powell, Nvidia\u2019s vice president of healthcare and life sciences, told GEN Edge. \u201cWe\u2019re going to have the ability to really bring all of this amazing genomics capability into these sovereign AI efforts and allow them to have state-of-the-art genomics capabilities in these nations.\u201d\n\nNvidia and Illumina have also agreed to collaborate on advancing multi-omic data analysis using Illumina Connected Analytics, in addition to developing new biology foundation models, with plans to incorporate Nvidia\u2019s image processing and single-cell tertiary analysis tools onto the Illumina Connected Software.\n\nThe companies said their researchers will also work to integrate through Illumina Connected Analytics an array of Nvidia technologies ranging from Nvidia RAPIDS\u2122 accelerated data science software included in the Nvidia AI Enterprise software platform; to the Nvidia BioNeMo\u2122 generative AI platform for drug discovery, including generative AI models and fine-tuning capabilities for proprietary datasets; and the Nvidia MONAI framework for spatial cell imaging workflows.\n\n\u201cBy bringing them into the [Illumina] Connected Analytics platform, we\u2019re going to essentially be able to broaden and grow the genomics ecosystem substantially into places like the research community, into places like drug discovery, into hopefully every clinic on the planet, because we\u2019re moving away from just reading the human genome into what kind of insights can we gather from this human genome,\u201d Powell said.\n\n\u201cThere\u2019s a deep desire by more and more parts of the industry that would like to discover more about biology, about disease, to use these tools in the efforts of finding new medicines or coming up with better treatment strategies,\u201d Powell elaborated. \u201cThe challenge is, that the whole human genome is not yet human readable. It has to be machine-readable. Then, you want to continue to gain insights from that data. We\u2019re really at the point that we have to start applying machine learning and artificial intelligence in a lot more serious ways.\u201d\n\nVisiting Jensen Huang\n\nIllumina\u2019s Mehio said the collaboration began exactly one year ago during last year\u2019s J.P. Morgan Healthcare Conference, when he joined Illumina\u2019s newly appointed CEO Jacob Thaysen, PhD, in meeting Nvidia\u2019s founder and CEO Jensen Huang at his house.\n\n\u201cThat kicked off our initial work on porting DRAGEN, but also with an eye on these bio-foundational models that we want to explore together, which really hold the promise of the future,\u201d Mehio recalled.\n\nThe companies pursued two tracks as they began to collaborate. One was how to leverage the availability of GPUs with DRAGEN. \u201cAnd the second part we basically discussed is how Nvidia and the world are in the midst of a revolution in AI, and genomics is something that would benefit from it,\u201d Mehio said.\n\n\u201cOur first foray into that is to basically bring Nvidia\u2019s AI and compute capability into our data platform, Illumina Connected Analytics, and complement it with our own AI efforts and analysis that we\u2019re doing to drive that new space of foundational models for biology and so on,\u201d Mehio summarized. \u201cSo, two pieces to the collaboration. We\u2019ve progressed far on the first one, and we\u2019re in the initial steps on the second one.\u201d\n\nThe multimodal AI models expected to result from the companies\u2019 efforts can uncover insights and streamline processes to boost the capabilities of human experts, Illumina CEO Thaysen told Huang during an informal \u201cFireside Chat\u201d hosted by the Nvidia CEO Monday evening at the Fairmont San Francisco hotel, about a hilly half mile north of where the J.P. Morgan conference was taking place.\n\n\u201cCombining other information, other modalities, other \u2018omics,\u2019\u201d Thaysen observed, \u201cis going to give us much deeper insight into biology. But while DNA was very difficult itself, when you then combine all the omics, it becomes exponentially more challenging. It\u2019s getting so complicated that we do need huge computing power and AI to really understand and process it.\u201d\n\nThe companies said their partnership was intended to further expand the global genomics market\u2014projected at $37.94 billion last year by Precedence Research in a report released December 31\u2014and help enable breakthroughs in identifying targets, developing drugs in clinical phases, and discovering biomarkers.\n\nContinuing to push\n\n\u201cThere are a lot of reasons to continue to push the ability to reduce the cost and increase the speed, to introduce new approaches like artificial intelligence, and to really expand the market,\u201d Powell said.\n\nAddressing the 43rd Annual J.P. Morgan Healthcare Conference on Monday, Powell offered additional details of the collaboration and potential customers for the combined offerings.\n\n\u201cWe\u2019re going to work together to build new applications in Illumina Connected Analytics, bringing genomics foundation models, bringing single cell applications that are essentially real-time at very, very large scale, and expand the genomic accessibility and utility into new markets like drug discovery,\u201d Powell said.\n\n\u201cWe still have so much to learn about the foundational building blocks of biology. It\u2019s in the amazing data that Illumina is generating, and now we\u2019re going to be working together to build those tools, build those capabilities, and expand the genomics market into the next generation, into generative AI genomics, and all the way to making it more accessible to the research community, to the biopharma and biotech community, and even expanding it into clinical and research-clinical applications,\u201d Powell added.\n\nSteven Barnard, PhD, Illumina\u2019s CTO, told GEN Edge that the Nvidia collaboration meshes with his company\u2019s three-pillar approach to its technology strategy going forward. Pillar one entails continuing to offer market-leading instrumentation and sequencing. Pillar two, expanding Illumina\u2019s content offerings into multi-omics, as articulated by CEO Jacob Thaysen, PhD, in a recent GEN interview.\n\nPillar three, Barnard elaborated, \u201cis the ability to be essentially the de facto insight and interpretation engine for all this biological data.\u201d\n\n\u201cBesides the software that sits on top of the hardware, having the best tools, having the best technology to support that third pillar is extremely important,\u201d added Barnard, who joined Illumina in 1998 as the company\u2019s first scientist and fourth employee.\n\nIQVIA, Mayo Clinic, and Arc Institute\n\nThe Illumina collaboration was one of four partnerships announced today by Nvidia. The other three are:\n\nIQVIA\u2014The global provider of clinical research services, commercial insights, and healthcare intelligence for life sciences and healthcare companies plans to use the Nvidia AI Foundry service to build custom foundation models on its more than 64 petabytes of information, coupled with its deep domain expertise. IQVIA is also developing agentic AI solutions\u2014outfitted with Nvidia NIM\u2122 microservices and Nvidia Blueprints\u2014designed to speed research, clinical development, and access to new treatments.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "I'll say it: The best thing I saw from Nvidia at CES wasn't its sweet new GPUs, but some tasty AI every RTX gamer can enjoy",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/ill-say-it-the-best-thing-i-saw-from-nvidia-at-ces-wasnt-its-sweet-new-gpus-but-some-tasty-ai-every-rtx-gamer-can-enjoy/",
            "snippet": "DLSS will be powered by the transformer model going forward, and will deliver far better image quality, being far more accurate and stable.",
            "score": 0.7007842659950256,
            "sentiment": null,
            "probability": null,
            "content": "AI gets a bad wrap. To be clear, that's generally for good reason\u2014it's either being put to nefarious use, is coming fer our jobs, or is being used as some marketing gimmick with no actual relation to genuine artificial intelligence. But the most impressive thing I saw in my time at CES 2025, and throughout the Nvidia RTX Blackwell Editor's Day was AI-based. And it's a tangible advancement on the AI us gamers likely use every day: DLSS.\n\nDLSS\u2014or Deep Learning Super Sampling, to give it its full title\u2014is Nvidia's upscaling technology used to deliver higher frame rates for our PC games (mostly because ray tracing sucked them down), and single-handedly kicked off the upscaling revolution that is now demanded of every single GPU and games console maker across the planet.\n\nAnd its fourth iteration offers \"by far the most ambitious and most powerful DLSS yet.\" So says Nvidia's deep learning guru, Brian Catanzaro. And the reason it's such a massive deal is that it's moving from using a US news network for its AI architecture to the technology behind Optimus Prime.\n\nYes, DLSS 4 is now powered by energon cubes. Wait, no, I think I've fundamentally misunderstood something vital. Hang on.\n\nFrom the introduction of DLSS 2 in 2020, Nvidia's upscaling has been using a technology called CNN, which actually stands for convolutional neural network (not Cable News Network, my apologies). CNN has gone hand-in-hand with GPUs since the early 2000s when it was shown that the parallel processing power of a graphics chip could hugely accelerate the performance of a convolutional neural network compared with a CPU. But the big breakthrough event was in 2012 when the AlexNet CNN architecture became the instant standard for AI image recognition.\n\nBack then it was trained on a pair of GTX 580s, how times have changed\u2026\n\nAt its most basic form, a CNN is designed to locally bunch pixels together in an image and analyse them in a branching structure, going from a low level to a higher level. This allows a CNN to summarise an image in a very computationally efficient way and, in the case of DLSS, display what it 'thinks' an image should look like based on all its aggregated pixels.\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nConvolutional neural networks, however, are no longer the cutting edge of artificial intelligence and deep learning. That is now something called the transformer. And this is the big switcheroo that has come out of CES 2025\u2014DLSS will be powered by the transformer model going forward, and will deliver far better image quality, being far more accurate and stable... though that does come with a hit on gaming performance.\n\nThe transformer architecture was developed by the smart bods at Google, and is essentially the power behind the latest AI boom as it forms the heart of large language models, such as ChatGPT. In fact, the GPT part there stands for generative pre-trained transformer.\n\nImage 1 of 3 (Image credit: Nvidia) (Image credit: Nvidia) (Image credit: Nvidia)\n\nWhile CNN is more closely related to images, transformers are far more generalised. Their power is about where computational attention is directed.\n\n\"The idea behind transformer models,\" Catanzaro tells us, \"is that attention\u2014how you spend your compute and how you analyse data\u2014should be driven by the data itself. And so the neural network should learn how to direct its attention in order to look at the parts of the data that are most interesting or most useful to make decisions.\n\n\"And, when you think about DLSS, you can imagine that there are a lot of opportunities to use attention to make a neural graphics model smarter, because some parts of the image are inherently more challenging.\"\n\nTransformer models are also computationally efficient, and that has allowed Nvidia to increase the size of the models it uses for DLSS 4 because they can be trained on much larger data sets now, and \"remember many more examples of things that they saw during training.\"\n\nNvidia has also \"dramatically increased the amount of compute that we put into our DLSS model,\" says Catanzaro.\" Our DLSS 4 models use four times as much compute as our previous DLSS models did.\"\n\nAnd it can make a huge difference in games that support DLSS 4. I think maybe the most obvious impact is in games which use Ray Reconstruction to improve the ray tracing and the denoising of a gameworld. I was hugely impressed with Ray Reconstruction when I first used it, and it still anchors characters in a world far better than previous solutions to ray traced lighting.\n\nBut you do get a noticeable smearing effect sometimes in games such as Cyberpunk 2077 and Alan Wake 2. With the smarts of the transformer model, however, that's all gone. The improved neural network perceives scene elements more accurately than the CNN model, most especially those trickier scene elements, and most especially in darker environments.\n\nAlan Wake 2 is the epitome of that, with its always brooding level design, and the example we were given on stage highlights a ceiling fan spinning around, leaving the familiar Ray Reconstruction smearing effect in the wake of its fan blades using the CNN model. With DLSS 4 and the transformer model, it just looks like a regular fan, with the dark ceiling behind the spinning fan now given clear detail.\n\nYou can also see how much more stable fine details are from this example of a chain-link fence, and you get the same effect with trailing overhead cables, etc.\n\n(Image credit: Nvidia)\n\nI've also seen it used to great effect in Cyberpunk 2077, and in the RTX Remix version of Half-Life 2\u2014which is still yet to be released but I got to have a play with it last week. I got to check out the demo of HL2 where we could toggle between the two DLSS models at will and the level of detail that suddenly pops out of the darkness with the transformer architecture in play is pretty astounding.\n\nNot in a strange, 'why is it highlighting that?' kinda way, but more naturally. The way that even in the flickering gloom outside the ring of light around a flaming touch, set just outside of Ravenholm, you can make out a trail of leaves where before there were just muddy pixels.\n\nAnd that extra level of visual fidelity is coming to all games that support DLSS 4, which, thankfully, is every game which currently supports DLSS. There are going to be 75 games and apps at the launch of the RTX 50-series cards that support DLSS 4 out of the box, and they will either give you the option to switch between CNN and transformer models\u2014this is what the build of Cyberpunk 2077 I've played with does\u2014or it just switches over wholesale for DLSS 4.\n\nImage 1 of 2 (Image credit: Nvidia) (Image credit: Nvidia)\n\nBut, via the Nvidia App, you're also going to be able to override the DLSS version any game currently uses to essentially drop in the new transformer model to take advantage of the improved image quality. You will also be able to add Multi Frame Generation (MFG) into any game that currently supports Nvidia's existing Frame Generation feature via this method, too. So long as you have an RTX Blackwell card, anyways\u2026\n\nIt is worth noting, however, that even though MFG is hardware-locked to Blackwell, the standard two-times Frame Generation benefits from an enhanced model, too. Nvidia says its new AI model is 40% faster and uses 30% less VRAM. It also no longer uses the optical flow hardware baked into the Ada architecture, as it's been replaced by a more efficient AI model to do the same job. RTX 40-series cards can also now take advantage of the Flip Metering frame pacing tech\u2014the same thing which locks MFG to RTX Blackwell\u2014just without enhanced hardware in the display engine.\n\nImage 1 of 3 (Image credit: Nvidia) (Image credit: Nvidia) (Image credit: Nvidia)\n\nBut DLSS 4's transformer model for Ray Reconstruction and Super Resolution isn't restricted to RTX 50-series or RTX 40-series cards, however, and is available to all RTX GPUs from Turing upwards. Which is a hell of a sell.\n\n\"One of the reasons this is possible,\" says Catanzaro, \"is because the way that we built DLSS 4 was to be as compatible as possible with DLSS 3. So, games that have integrated DLSS 3 and DLSS 3.5 can just fit right in.\n\n\"DLSS 4 has something for all RTX gamers.\"\n\nBut there always has to be some sort of caveat. While it is supported by all RTX GPUs, the transformer model is more computationally intensive than the previous CNN incarnation. From speaking to Nvidia folk at the show they estimated potentially around 10% more expensive in terms of frame rates. So, if you just look at the raw frame rate data you will see DLSS 4 performing worse at the same DLSS levels compared with previous iterations.\n\nWhat you won't see there is the extra visual fidelity.\n\nYou could potentially offset that performance hit, however, by leaning on that extra fidelity to drop down a DLSS tier, from Quality to Balanced, for example, and that might well give you better performance and maybe even slightly improved visuals. But we will, however, have to check that out for ourselves when DLSS 4 launches out of beta in full.\n\nI for one am almost looking forward more to the DLSS 4 launch than I am to getting my hands on the svelte new RTX 5090. Though I will say, its Multi Frame Generation does have to be seen to be believed.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "there's 'a big AI supercomputer at Nvidia\u2026 running 24/7, 365 days a year improving DLSS. And it's been doing that for six years'",
            "link": "https://hardforum.com/threads/theres-a-big-ai-supercomputer-at-nvidia-running-24-7-365-days-a-year-improving-dlss-and-its-been-doing-that-for-six-years.2039173/",
            "snippet": "Brian Catanzaro, Nvidia's VP of applied deep learning research, took to the stage to talk through DLSS 4 and the many changes and challenges it brings with it.",
            "score": 0.8475673794746399,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "NVIDIA Announces First-Ever Quantum Day At GTC 2025, Days After Jensen Huang Said Quantum Computing Is 20 Years Away",
            "link": "https://finance.yahoo.com/news/nvidia-announces-first-ever-quantum-213014221.html",
            "snippet": "NVIDIA Corp. (NASDAQ:NVDA) has announced its inaugural Quantum Day, set to take place on Mar. 20 during the GTC 2025 event. The announcement comes just days...",
            "score": 0.922508716583252,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA Corp. (NASDAQ:NVDA) has announced its inaugural Quantum Day, set to take place on Mar. 20 during the GTC 2025 event. The announcement comes just days after CEO Jensen Huang stated that practical quantum computing may still be two decades away.\n\nWhat Happened: The event is set to spotlight the swift advancements in quantum computing, a technology expected to transform sectors like drug discovery, materials science, and financial forecasting. Huang will spearhead discussions with leaders from key industry players, including Alice & Bob, Atom Computing, IONQ Inc (NYSE:IONQ) and D-Wave (NYSE:QBTS), the company said in a blog post on Wednesday.\n\nDon't Miss:\n\nQuantum Day will offer sessions exploring both the current and future potentials of quantum technology. These sessions will be led by Huang and other experts from the quantum field. Additionally, a developer day session will emphasize collaborations between NVIDIA and its partners to advance quantum computing technologies.\n\nWhy It Matters: Last week, during Nvidia's Analyst Day, CEO Huang shared his optimism about the future of quantum computing but cautioned that practical applications could still be 15 to 30 years away.\n\n\"If you kind of said 15 years for very useful quantum computers, that would probably be on the early side. If you said 30, it's probably on the late side,\" he said.\n\nHuang\u2019s comments were backed by Meta Platforms Inc. CEO Mark Zuckerberg. During his appearance on Joe Rogan's podcast Saturday, Zuckerberg admitted his limited expertise on the subject but aligned with the industry consensus, suggesting a decade-long timeline for the widespread adoption of quantum computing.\n\nSee Also: The Biggest Disruption to IP since Disney \u2014 Get in now as they monetize a $2 Trillion market by building content around the most profitable Character IP in history and combining it with the Patented Technology IP of the future.\n\nMeanwhile, despite Huang stating that practical quantum computing might be two decades away, NVIDIA is investing heavily in its future. The company is strategically hiring to strengthen its quantum computing capabilities. This indicates NVIDIA\u2019s long-term commitment to integrating quantum and classical computing systems.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia Shares Fall on New AI Chip Export Rules. Is This a Golden Opportunity to Buy the Stock?",
            "link": "https://www.fool.com/investing/2025/01/15/nvidia-shares-falls-on-new-ai-chip-export-rules-is/",
            "snippet": "Nvidia Shares Fall on New AI Chip Export Rules. Is This a Golden Opportunity to Buy the Stock? ... The Biden Adminstration placed tighter export restriction on...",
            "score": 0.8178924918174744,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) shares came under pressure recently after the Biden administration imposed tougher guidelines regarding the export of artificial intelligence (AI) chips. Under the new rules, most countries would be capped on the amount of advanced AI chips, such as Nvidia's graphic processing units (GPUs), they can purchase.\n\nBased on this Interim Final Rule on Artificial Intelligence Diffusion, 18 countries face no restrictions and will not be required to get a license to buy GPUs, but 24 countries are banned outright. Most countries will need to get a license to import more than 1,700 GPUs. They could then purchase up to 50,000 GPUs, or up to 100,000 if certain requirements are met. Some nations, meanwhile, will be allowed to purchase up to 320,000 GPUs over a two-year period.\n\nNvidia blasted the rule, saying it was done in secret under the guise of an \"anti-China\" measure, and the company argued it would only weaken U.S. global competitiveness, threaten innovation, and harm economic growth. At the same time, the company appealed to the incoming Trump administration, saying the first Trump administration helped lay the foundation that led to the strength the U.S. sees in AI today.\n\nImpact on Nvidia\n\nRight now, there is a 120-day commentary period before the rule goes into effect, and the incoming Trump administration could make changes. However, even some Republicans are in favor of tighter restrictions on advanced chips.\n\nThe 18 countries with no restrictions are some of the largest in the world, and Nvidia's sales into those European and Asian countries would not be impacted. However, there are data centers spread around the globe, and there are places in the Middle East that are emerging as AI data center hubs. Meanwhile, a company in India recently completed its first AI data center with 60,000 GPUs.\n\nLimiting entire countries to 50,000 or even 100,000 GPUs would certainly hurt AI data center development in those regions. Large language models (LLMs) from Meta Platforms and Elon Musk's xAI are already being trained on more GPUs than these export limits, with Llama 4 using 160,000 GPUs. And there is talk of companies using GPU clusters made up of 1 million AI chips in the near future to train the next generation of AI models.\n\nWhen looking at a breakdown of Nvidia's revenue so far in 2024, 45% came from the U.S. and 17% from Taiwan, which is not on the export control list. Nearly 13% of its revenue has come from China, where it was already prohibited from selling its most advanced chips. The rest comes from other countries, some of which the rule likely affects.\n\nThat said, much of Nvidia's business comes from large hyperscaler customers (companies that own huge data center complexes), and that is where much of its growth will come from in the future. These companies, including big cloud computing companies like Amazon, Microsoft, and Alphabet, will be able to seek approval to bypass the licenses for establishing AI data centers in impacted countries. Nvidia's top three customers accounted for about 34% of its revenue through the first nine months of fiscal 2025.\n\nIs it time to buy Nvidia stock?\n\nGiven the current demand for GPUs, a lot of the impact from a stricter export policy is likely to be absorbed elsewhere in the near term. The big hyperscalers were always going to be the biggest source of growth and demand for Nvidia's chips, and that is now more likely than ever.\n\nCountries that need more than the allotted amount of chips could look to partner with the big cloud computing companies. Microsoft has already partnered with the United Arab Emirates, and in response to the announcement, Microsoft said it can continue to meet the technology needs of countries and customers around the globe.\n\nAs such, it sounds like many countries will be able to get the amount of chips they need as long as they have a U.S. tech gatekeeper. That will allow countries to move forward with their AI plans while making sure chips don't get directed toward places like China or Russia.\n\nFrom a valuation perspective, the stock currently trades at a forward price-to-earnings (P/E) ratio of just below 30 based on analysts' fiscal 2026 estimates. Its price/earnings-to-growth ratio (PEG) is below 1, which typically indicates a stock is undervalued, although growth stocks often carry PEGs much higher.\n\nWhile the new export curbs add some additional potential risk for the company, Nvidia's valuation and overall opportunity remain attractive enough to make the stock a solid buy for investors.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia RTX 5090 shows 32% boost over RTX 4090 without DLSS but there\u2019s a caveat",
            "link": "https://www.club386.com/nvidia-rtx-5090-shows-32-boost-over-rtx-4090-without-dlss/",
            "snippet": "These benchmarks show that RTX 5090 is roughly 32% faster than the RTX 4090 when put on equal footing. This makes it less impressive when factoring in its 25%...",
            "score": 0.94688481092453,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has shared some new benchmarks of its upcoming RTX 50 Series GPUs showing the rasterised uplift compared to last generation. Now, we can see the real 15% to 32% improvement through the AI fog.\n\nThe brand has released a range of slides and performance metrics among which we can spot two benchmarks starring Resident Evil 4 and Horizon Forbidden West. The special thing about these tests is their lack of frame generation and DLSS 4. This should give us a more apples-to-apples comparison between RTX 40 Series and RTX 50 Series.\n\nWithout true metrics along the Y axis, Nvidia makes it particularly difficult to see the exact difference. We\u2019re sticklers for details, though, so we\u2019ve measured the pixel count (Shift + M in Gimp) to determine what you can expect, at least in the eyes of Team Green with its cherry-picked internal numbers.\n\nRTX 5090 vs RTX 4090.\n\nUntil our review lands with a larger sample size and clearer comparisons (keep your eyes peeled), these benchmarks show that RTX 5090 is roughly 32% faster than the RTX 4090 when put on equal footing. This makes it less impressive when factoring in its 25% higher price tag \u2013 $1,999 vs $1,599 for its predecessor. And we are not even talking about the expensive custom models.\n\nRTX 5080 vs RTX 4080.\n\nRTX 5080 on the other hand is only 15% faster than the RTX 4080, marking the least uplift in this list. That said, this time the price difference is a net positive, as RTX 5080 comes 16% cheaper at $999 than the 4080\u2019s original $1,199 MSRP. Let\u2019s call it acceptable.\n\nRTX 5070 Ti vs RTX 4070 Ti.\n\nAs for RTX 5070 Ti against RTX 4070 Ti, things depend on the game, ranging from 19% in RE4 to 22% in Horizon. Again, adding the $749 MSRP into the equation shows 6% reduction compared to the 4070 Ti\u2019s $799. Just keep in mind that these are launch prices and you may find better deals on RTX 40 Series cards.\n\nRTX 5070 vs RTX 4070.\n\nLastly, RTX 5070 is 20% faster in RE4 and 22% in Horizon compared to RTX 4070. While not as head-turning as Jensen\u2019s claim of RTX 4090 performance, this one offers the best value out of the bunch when factoring in price. At $549 it is 8% cheaper than the 4070\u2019s $499 tag. And with DLSS and frame generation backing its raw power, this mid-ranger shapes up to be a winner. No wonder Nvidia chose to open the RTX 50 unveiling with it.\n\nWith all this in mind, the RTX 50 Series real weapon is DLSS 4. Improved upscaling, x3 frame generation, better handling of ray-traced lighting, and the possibility to upgrade games to the latest DLSS version are all important aspects beyond the base frame rate. We shall cover these in detail on future occasions.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "NVIDIA Announces First-Ever Quantum Day At GTC 2025, Days After Jensen Huang Said Quantum Computing Is 20 Years Away",
            "link": "https://www.benzinga.com/news/25/01/43001201/nvidia-announces-first-ever-quantum-day-at-gtc-2025-days-after-jensen-huang-said-quantum-computing-is-20-years-away",
            "snippet": "Comments ... NVIDIA Corp. ... has announced its inaugural Quantum Day, set to take place on Mar. 20 during the GTC 2025 event. The announcement comes just days...",
            "score": 0.922508716583252,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA Corp. NVDA has announced its inaugural Quantum Day, set to take place on Mar. 20 during the GTC 2025 event. The announcement comes just days after CEO Jensen Huang stated that practical quantum computing may still be two decades away.\n\nWhat Happened: The event is set to spotlight the swift advancements in quantum computing, a technology expected to transform sectors like drug discovery, materials science, and financial forecasting. Huang will spearhead discussions with leaders from key industry players, including Alice & Bob, Atom Computing, IONQ Inc IONQ and D-Wave QBTS, the company said in a blog post on Wednesday.\n\nQuantum Day will offer sessions exploring both the current and future potentials of quantum technology. These sessions will be led by Huang and other experts from the quantum field. Additionally, a developer day session will emphasize collaborations between NVIDIA and its partners to advance quantum computing technologies.\n\nWhy It Matters: Last week, during Nvidia's Analyst Day, CEO Huang shared his optimism about the future of quantum computing but cautioned that practical applications could still be 15 to 30 years away.\n\n\"If you kind of said 15 years for very useful quantum computers, that would probably be on the early side. If you said 30, it's probably on the late side,\" he said.\n\nHuang\u2019s comments were backed by Meta Platforms Inc. CEO Mark Zuckerberg. During his appearance on Joe Rogan's podcast Saturday, Zuckerberg admitted his limited expertise on the subject but aligned with the industry consensus, suggesting a decade-long timeline for the widespread adoption of quantum computing.\n\nMeanwhile, despite Huang stating that practical quantum computing might be two decades away, NVIDIA is investing heavily in its future. The company is strategically hiring to strengthen its quantum computing capabilities. This indicates NVIDIA\u2019s long-term commitment to integrating quantum and classical computing systems.\n\nMoreover, the announcement comes at a time when quantum computing stocks are gaining momentum during Wednesday premarket. Following Microsoft Corp.\u2018s MSFT declaration of making its business clients \u201cquantum-ready\u201d by 2025, stocks like Quantum Computing Inc. QUBT and D-Wave Quantum have seen significant gains. This surge underscores the growing anticipation and investment in quantum computing technologies.\n\nRead Also:\n\nDisclaimer: This content was partially produced with the help of Benzinga Neuro and was reviewed and published by Benzinga editors.\n\nImage via Shutterstock",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "NVIDIA RTX 5090 at 575 Watts, RTX 5080, 5070 Ti, & 5070 Specs",
            "link": "https://gamersnexus.net/gpus/nvidia-rtx-5090-575-watts-rtx-5080-5070-ti-5070-specs",
            "snippet": "GPUs NVIDIA RTX 5090 at 575 Watts, RTX 5080, 5070 Ti, & 5070 Specs January 15, 2025 Last Updated: 2025-01-15 NVIDIA announces 4 Blackwell GPUs and claims...",
            "score": 0.9492496252059937,
            "sentiment": null,
            "probability": null,
            "content": "RTX Blackwell\n\nNVIDIA spent a lot of time talking about AI, and we\u2019ll get into some of that briefly at the end.\n\nWe\u2019ll start with the hard information that we have.\n\nNVIDIA announced its Blackwell GPU as it\u2019ll arrive to the consumer market in January. The GPU, presumably the full Blackwell die, was noted as a 92 billion transistor solution (as compared to the RTX 4090\u2019s approximate 76 billion).\n\nNotably, the card CEO Jensen Huang showed off at the event appeared to be a 2-slot card, which he noted has 2 fans. This is a stark contrast to the huge prototype NVIDIA cooler we just tore down. This is a big swing from the 3-slot and 4-slot cards we\u2019ve become used to, which were necessary for thermal and power management.\n\nHard Specs\n\nNVIDIA also posted some of the hard specs to its website.\n\nWe can start with the power, which is listed as 575W total graphics power for the 5090.\n\nThat\u2019s a huge amount of load to put on a single 16-pin connector and we\u2019re concerned about the strain on it. We also wonder if this will be coupled with a redesign of coolers to try and actively cool the area of the power connector.\n\nThe RTX 5090 Blackwell GPU is listed as having 21,760 CUDA cores, memory at 32 GB of GDDR7, clocks at 2.41 GHz boost and 2.01 GHz base, and a large 512-bit memory interface width. NVIDIA has iterated its Tensor core generation to 5 and its RT core generation to 4 for Blackwell, though we don\u2019t yet have architectural details on what that actually means on the consumer side. We expect those details soon.\n\nThe FE card is listed as 304mm by 137mm for dimensions, and 2 slots thick.\n\nJumping over to the prior RTX 4090 (watch our review) for reference: The 4090 has 16,384 CUDA cores, down notably from the 21,760 of the 5090 -- but cores can\u2019t be linearly compared, especially cross generation, so the real-world impact likely won\u2019t track linearly. The 4090 also ran 24 GB of GDDR6X rather than the 32GB GDDR7 on the 5090. The 4090\u2019s clocks are higher as advertised, though, at 2.52 GHz boost and 2.23 GHz base -- but clocks, like core count, aren\u2019t everything.\n\nZooming out to look at memory capacity, we see the 5090 at 32 GB, the 5080 at 16 GB, the 5070 Ti at 16 GB, and the 5070 at 12 GB. For perspective, the RTX 4070 (watch our review) is also 12 GB, the 4070 Ti is 12GB, the 4070 Ti Super (read our review) is 16GB, and the 4080 is 16GB.\n\nGoing to a fuller look at the specs, the 5080 is listed at 10,752 CUDA Cores, which is slightly more than the RTX 4080\u2019s 9,728 CUDA cores and not the same huge change we see with the 4090 to 5090. The 5070 Ti lists 8960, against 7680 on the 4070 Ti (watch our review), and the 5070 lists 6144, up from 5888 on the 4070 configuration.\n\nAgain, these aren\u2019t directly comparable as they\u2019re different generations, but are useful for establishing how NVIDIA is positioning the cards.\n\nThe full specs page shows a 2.62 GHz max boost on the 5080, 2.45 GHz on the 5070 Ti, and 2.51 GHz on the 5070. We already said the 5090 has a larger memory bus. The 5080 and 5070 Ti both run a 256-bit bus, with the 5070 at 192 bits.\n\nOther than the 575W of the 5090, NVIDIA lists the other cards at 360W, 300W, and 250W for total board power. Broadly speaking, NVIDIA\u2019s power consumption appears to be increasing.\n\nThis is good timing with our efficiency testing: It\u2019s possible efficiency is up despite power draw also going up, but that\u2019s what we\u2019ll find out.\n\nFirst-Party Marketing Claims\n\nGrab a GN15 Large Anti-Static Modmat to celebrate our 15th Anniversary and for a high-quality PC building work surface. The Modmat features useful PC building diagrams and is anti-static conductive. Purchases directly fund our work! (or consider a direct donation or a Patreon contribution!)\n\nWe\u2019ll have our own benchmarks soon enough and you should rely on those or the independent benchmarks of other trusted reviewers. We can still reference NVIDIA\u2019s first-party claims to get an idea for where they stand.\n\n\n\n\n\n\n\n\n\nNVIDIA\u2019s webpage has a relative performance chart that\u2019s pretty hard to actually read, but we can get an idea. NVIDIA claims the 5090 outperforms the 4090 by over 2x in some situations, such as with Cyberpunk and Wukong, among others. In these tests, they list \u201cDLSS+Full RT\u201d as the settings. The footnote that\u2019s nearly the same color as the page background says that the 40-series used frame generation in this testing, but the 50 series used MFG 4x mode. This makes the comparison not like-for-like.\n\nWe consider this approach flawed, but we\u2019ll look at their other claims for full perspective. Switching to the RTX 5080, NVIDIA shows it as outperforming the RTX 4080 (watch our review) by, again, sometimes 2x -- this is tough to filter through differing settings tested, unfortunately.\n\nThe 5070 Ti shows beyond 2x gains against the 4070 Ti in the same way and with the same settings difference, with the 5070 and 4070 showing the same in NVIDIA\u2019s first-party claims.\n\nOf all of these, the Plague Tale comparison is maybe the most fair since NVIDIA notes that it only has DLSS 3.\n\nAs for what DLSS 4 actually is, NVIDIA ran this new blog post to introduce it. Of MFG, or Multi-Frame Generation, the post reads, \u201cDLSS Multi Frame Generation generates up to three additional frames per traditionally rendered frame, working in unison with the complete suite of DLSS technologies to multiply frame rates by up to 8X over traditional brute-force rendering.\u201d\n\nThe page continues to say, \u201cOur new frame generation AI model is 40% faster, uses 30% less VRAM, and only needs to run once per rendered frame to generate multiple frames. For example, in Warhammer 40,000: Darktide, this model provided a 10% faster frame rate, while using 400MB less memory at 4K, max settings, using DLSS Frame Generation.\u201d\n\nThis section of the article also indicates that you\u2019ll be able to override the DLSS model used in games that don\u2019t get updates from devs, which is already possible to be done manually by some users who replace .dll files. This new approach looks like it will be more user-friendly.\n\nNVIDIA stated that its Blackwell GPUs will have 2x the memory bandwidth of Ada, at 1.8TB/s for the cited spec, it claimed 2x the the RT TFLOPS, and 1.5x Ada shader performance.\n\nThe PCB showcased was a relatively small square -- like a further cut-down version of the 4090 FE PCB if you were to chop the wings off -- that appears to be sandwiched between two full flow-through fans.\n\nThe design is shown in this explosion diagram where NVIDIA illustrates the PCB centrally with densely populated components on both sides of the board. The cooler also utilizes a vapor chamber cooling solution with what appears to be 5 heatpipes, assuming the render is accurate. The GPU directly contacts the vapor chamber as you would expect, with the flow through area handling the flanking heatsinks.\n\nUniquely, that appears to take some learnings NVIDIA had from this cooler, which is a full flow-through design.\n\nAI on GeForce\n\nBuy a GN 4-Pack of PC-themed 3D Coasters! These high-quality, durable, flexible coasters ship in a pack of 4, each with a fully custom design made by GN's team. You'll get a motherboard-themed coaster with debug display & reset buttons, a SATA SSD with to-scale connectors, RAM sticks, and a GN logo. These fund our web work! Buy here.\n\nNVIDIA also spent limited time talking about other AI features for gaming. Although it is publishing materials to its site, we\u2019ll wait for the full architectural briefing to get into more depth. For now, the company highlighted these:\n\nRTX Neural Material, wherein it showed an example material cut from 47 MB to 16 MB with this solution.\n\nDLSS improvements by moving to a Transformer model rather than the previous CNN model.\n\nNVIDIA\u2019s keynote was lengthy and covered topics outside of our typical coverage scope. For now, we\u2019re just focusing on getting these basics out to you and will revisit the other announcements in more depth as we have time to adequately read through all the released materials.\n\nAs for release dates: NVIDIA cited January in its keynote, but its website specifically lists January 30th for the RTX 5090 and 5080. It says the 5070 Ti and 5070 will arrive in February.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia plots Quantum Day at GTC 2025 \u2013 for tech it called a distant dream",
            "link": "https://www.theregister.com/2025/01/15/nvidia_quantum_day_gtc/",
            "snippet": "Nvidia announced it will be holding a Quantum Day at its own GTC 2025 conference in March, describing quantum computing as one of the most exciting areas in...",
            "score": 0.8283953666687012,
            "sentiment": null,
            "probability": null,
            "content": "A week after Nvidia chief Jensen Huang demolished the market valuations of listed quantum computing brands by saying the technology is 20 years away from being useful, the GPU maker has confirmed it is hosting a quantum computing day.\n\nSpeaking at the CES technology trade show, Nvidia's rock star CEO claimed quantum systems are probably five to six orders of magnitude short of the number of qubits needed to make them practical. He speculated it would take more than 15 years and probably more like 20 years to fix this.\n\nIn response, the share prices of quantum businesses such as Rigetti, D-Wave, and IonQ slumped by nearly 50 percent when they weren't exactly riding high to begin with.\n\nYet this week, Nvidia announced it will be holding a Quantum Day at its own GTC 2025 conference in March, describing quantum computing as \"one of the most exciting areas in computer science, promising progress in accelerated computing beyond what's considered possible today.\"\n\nNever one to be outdone in the marketing hype stakes, Microsoft also proclaimed 2025 as \"the year to become quantum ready,\" claiming the industry is on the cusp of an era of reliable quantum computing, making this \"a critical and catalyzing time for business leaders to act.\"\n\nThe issue with current systems is that the quantum bits \u2013 qubits \u2013 they are based on are generally too susceptible to environmental noise and prone to errors, making them largely impractical for real-world applications. They also need many more qubits than exisitng quantum processors, though many companies are working to scale up.\n\nThis isn't stopping Microsoft from peddling its Quantum Ready program, through which it aims to provide businesses with the insight and tools to build hybrid applications and invest in skills and access to reliable quantum systems.\n\nThe grievance leveled at proponents of quantum is that they have talked up the technology for decades and swallowed investment dollars, yet practical applications seem as far off as ever, as Huang articulated.\n\nThose in the quantum industry were quick to hit back last week. D-Wave CEO Dr Alan Baratz told The Register that Huang's remarks were \"just the latest in a string of misinformation\" driving a negative market reaction to quantum computing, and claimed the Nvidia CEO has a misunderstanding of quantum tech.\n\nD-Wave built its business around a type of processing called quantum annealing, which is handy for solving some optimization problems. However, D-Wave has also begun work on its own version of the quantum gate technology favored by most other companies in the field.\n\nIonQ chairman and CEO Peter Chapman also defended quantum technology:\n\n\"Today's classical computing hardware is limited by computational capacity and power requirements in ways that will likely prohibit society from ever being able to solve some of its most pressing problems,\" he said.\n\nChapman believes his company will eventually be profitable, predicting sales approaching $1 billion by 2030. However, during IonQ's last set of results - for the nine months ended September 2024 - the business reported revenue of $31.4 million and a net loss of $130 million.\n\nAnother proponent, Quantum Circuits, reckons Nvidia's decision to host an event validates the advice to organizations to start planning their quantum strategies now rather than waiting years.\n\n\"It's time for enterprises to explore how quantum applications can drive their businesses. We're making great progress, and we expect significant advances in 2025,\" claimed CEO Ray Smets.\n\nThis echoes sentiment from cloud biz OVH, whose former CTO, Thierry Souche, once told us in an interview that businesses should be prepared to invest to build quantum skills in the workforce sooner rather than later.\n\nHeather West, research manager for Quantum Computing, Infrastructure Systems, Platforms, and Technology at IDC, told The Register that quantum computing is still a nascent technology.\n\n\"Most systems under development are digital gate-based models, which are the most difficult to build. These are the systems that currently cannot be used for any real-world applications, which I believe Jensen was referring to in his original comment. For these systems to be useful, they will need to scale to a large number of high-quality qubits that will be used for quantum computation and error correction,\" she noted.\n\n\"Some quantum hardware vendors predict that this feat will be accomplished in the next five to seven years. As the technology advances and the systems can be used to solve problems of value, we can expect customer spend to increase as well.\"\n\nQuantum annealers and neutral atom systems use the quantum properties of qubits differently, and enterprises can already operate these systems to solve real-world optimization problems, she noted.\n\nIn response to Huang's remark that practical quantum computing is 20 years away, Forrester VP and Principal Analyst Charlie Dai told us that it depends how you define \"practical\" and \"quantum computing.\"\n\n\"On one hand, commercial quantum computing is already making an impact in niche markets, mostly in the quantum annealing domain. On the other hand, universal quantum computing has been facing a range of critical challenges since day one, such as error correction for fault tolerance, gate fidelity and switching speed, qubit connectivity, coherence time, and infrastructure scalability,\" Dai said.\n\n\"Scientists and vendors are making promising progress, but universal quantum computing for broad commercial adoption may still be some years away, as Jensen mentioned.\" \u00ae",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-14": {
        "0": {
            "title": "NVIDIA GTC 2025: Quantum Day to Illuminate the Future of Quantum Computing",
            "link": "https://blogs.nvidia.com/blog/gtc-2025-quantum-day/",
            "snippet": "NVIDIA is celebrating and exploring remarkable progress in quantum computing by announcing its first Quantum Day at GTC 2025 on March 20.",
            "score": 0.8984760046005249,
            "sentiment": null,
            "probability": null,
            "content": "At the first-of-its-kind event, NVIDIA founder and CEO Jensen Huang will host industry leaders to discuss where quantum computing is headed.\n\nQuantum computing is one of the most exciting areas in computer science, promising progress in accelerated computing beyond what\u2019s considered possible today.\n\nIt\u2019s expected that the technology will tackle myriad problems that were once deemed impractical or even impossible to solve. Quantum computing promises huge leaps forward for fields spanning drug discovery and materials development to financial forecasting.\n\nBut just as exciting as quantum computing\u2019s future are the breakthroughs already being made today in quantum hardware, error correction and algorithms.\n\nNVIDIA is celebrating and exploring this remarkable progress in quantum computing by announcing its first Quantum Day at GTC 2025 on Thursday, March 20. This new focus area brings together leading experts for a comprehensive and balanced perspective on what businesses should expect from quantum computing in the coming decades \u2014 mapping the path toward useful quantum applications.\n\nDiscussing the state of the art in quantum computing, NVIDIA founder and CEO Jensen Huang will share the stage with executives from industry leaders, including:\n\nAlice & Bob\n\nAtom Computing\n\nD-Wave\n\nInfleqtion\n\nIonQ\n\nPasqal\n\nPsiQuantum\n\nQuantinuum\n\nQuantum Circuits\n\nQuEra Computing\n\nRigetti\n\nSEEQC\n\nLearn About Quantum Computing at NVIDIA GTC\n\nQuantum Day will feature:\n\nSessions exploring what\u2019s possible and available now in quantum computing, and where quantum technologies are headed, hosted by Huang and representatives from across the quantum community.\n\nA developer day session outlining how partners are working with NVIDIA to advance quantum computing.\n\nEducational sessions providing attendees with hands-on training on how to use the most advanced tools to explore and develop quantum hardware and applications.\n\nA Quantum Day special address, unveiling the latest NVIDIA quantum computing news and advances shortening the timeline to useful applications.\n\nQuantum Day at GTC 2025 is the destination for leaders and experts seeking to chart a course into the future of quantum computing.\n\nRegister for GTC.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia\u2019s CES Announcements Resulted in a Six Percent Stock Drop. Why?",
            "link": "https://www.prnewsonline.com/nvidias-ces-announcements-resulted-in-a-six-percent-stock-drop-why/",
            "snippet": "At the recent Consumer Electronics Show (CES), Nvidia made two significant missteps: alienating its core gaming audience with a pricey, power-hungry new GPU...",
            "score": 0.9299545884132385,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has seen tremendous growth in its stock value over the past year, driven by its advancements in AI chips and its dominant role in supplying GPUs for gaming. However, at the recent Consumer Electronics Show (CES) in Las Vegas, the company made two significant missteps: alienating its core gaming audience with a pricey, power-hungry new GPU, and frustrating investors by delivering vague, overly ambitious updates on its AI initiatives without providing concrete timelines or measurable progress.\n\nThese missteps\u2014one rooted in disappointing product expectations, the other in a lack of clarity about the company\u2019s near-term goals\u2014contributed to a six percent drop in Nvidia\u2019s stock. What went wrong, and what can other companies learn from Nvidia\u2019s miscalculations at CES?\n\nToo Much Money and Energy\n\nNvidia\u2019s gaming audience had been eagerly awaiting the development of the RTX 5090 GPU, which would purportedly offer significant improvements in gameplay graphics. But gamers were displeased to hear that the GPU would cost them $2000, a significant price increase over the previous generation. Gamers seemed to agree that the promised performance enhancements didn\u2019t justify the price increase. Not only that, the GPU would require a lot more energy than other options, up to 28 percent more. Many gamers\u2019 hardware would be unable to handle these new energy requirements.\n\nThe lesson for other companies is clear: know your audience, or risk losing them. Companies rolling out high-price products with marginal improvements need to prepare for scrutiny. Think about how Apple has long been criticized for releasing new \u201cgenerations\u201d of expensive phones with only incremental improvements instead of significant upgrades. Companies can\u2019t just slap a price tag on a shiny new feature without being transparent about what customers are paying for. If the value isn\u2019t obvious, or if the messaging misses the mark, trust evaporates fast.\n\nInstead of relying on outdated assumptions about what customers want, businesses can lean on AI tools for real-time sentiment analysis and social listening. These technologies can dig into customer feedback, track emerging trends, and even predict audience reactions based on historical data. Companies can then adjust messaging\u2014or even the product and pricing\u2014to avoid tone-deaf launches that alienate their core demographics.\n\nHazy AI Aspirations\n\nAs mentioned earlier, the other critical misstep contributing to Nvidia\u2019s stock drop was its handling of AI announcements at CES. The company failed to back up its ambitious claims with specific, near-term timelines. Instead, the presentations left investors grappling with unclear projections and uncertainty about when these innovations might yield tangible results.\n\nThe announcements ultimately came across as overly ambitious and frustratingly vague. CEO Jensen Huang\u2019s remarks on quantum computing, describing breakthroughs as \u201ctwo decades away,\u201d only reinforced concerns that tangible results were far off. Such unclear communication erodes trust and raises doubts about whether Nvidia is prioritizing steady, sustainable growth or simply chasing flashy, speculative projects with no immediate return.\n\nFor companies facing similar scrutiny, clarity and credibility must take center stage. Big visions should always come with the specific communication of milestones and realistic timelines. Equally important is setting appropriate expectations\u2014companies must ensure that messaging aligns with what their audience values most; whether that\u2019s innovation, financial returns or practical usability. Engaging directly with key stakeholders, including investors and customers, can provide valuable insights into how to frame announcements. Failing to do so risks creating a perception of overpromising and underdelivering.\n\nEvents like CES don\u2019t just showcase cutting-edge innovations: they also provide masterclasses in effective\u2014or ineffective\u2014communication. For PR professionals and companies alike, these events offer chances to analyze what resonates and what falls flat. Nvidia\u2019s missteps serve as a stark reminder that knowing your audience isn\u2019t just a best practice\u2014it\u2019s a business imperative.\n\nStamatis Astra is Co-Founder and Chief Business Officer of Intelligent Relations.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia partners with Mayo Clinic, others on healthcare AI",
            "link": "https://www.techtarget.com/searchenterpriseai/news/366617966/Nvidia-partners-with-Mayo-Clinic-others-on-healthcare-AI",
            "snippet": "Nvidia has partnered with Mayo Clinic and Illumina to enhance AI in healthcare and the life sciences.",
            "score": 0.684368371963501,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Cosmos Marks Another Masterful Stroke For Nvidia In AI Robotics",
            "link": "https://www.forbes.com/sites/davealtavilla/2025/01/13/cosmos-marks-another-masterful-stroke-for-nvidia-in-ai-robotics/",
            "snippet": "A new Nvidia generative AI technology dubbed Cosmos, that some folks might have glossed over due to its complexity was, in my opinion, another star of the show.",
            "score": 0.8696492314338684,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia stock falls after Biden administration releases updated export rule for AI chips",
            "link": "https://finance.yahoo.com/news/nvidia-stock-falls-after-biden-administration-releases-updated-export-rule-for-ai-chips-151755666.html",
            "snippet": "Nvidia stock fell Monday after the Biden administration released new rules aimed at controlling the flow of artificial intelligence to China.",
            "score": 0.922271192073822,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock (NVDA) fell nearly 2% Monday after the Biden administration released an updated export rule aimed at controlling the flow of artificial intelligence chips to \"adversaries\" such as China.\n\nThe White House said the rule would cap the number of AI chips called GPUs (graphics processing units) that can be ordered by most countries without a special license. Smaller orders of 1,700 or fewer GPUs would not count toward the export cap.\n\n\"Artificial intelligence is quickly becoming central to both security and economic strength,\" the White House said in a statement Monday. \"The United States must act decisively to lead this transition by ensuring that U.S. technology undergirds global AI use and that adversaries cannot easily abuse advanced AI.\"\n\nSome 18 \"key\" US allies, including the UK, Netherlands, and Taiwan, will face no restrictions on shipments of AI chips, and 24 countries that are subject to arms controls \u2014 such as China, North Korea, and Russia \u2014 still face an outright ban on receiving exports of the latest AI chips.\n\nThe primary significance of the updated restrictions is in their cap on the amount of compute capacity in a given group of AI chips that can be shipped to the remaining countries in the world.\n\nUnder the rule, US companies could ship AI chips with a total processing power equal to roughly 50,000 Nvidia Hopper chips or 20,000 of its latest Blackwell chips, Bernstein analyst Stacy Rasgon said. The countries subject to this cap include US allies like Switzerland and Israel.\n\nFor reference, Microsoft (MSFT) alone reportedly purchased 485,000 of Nvidia\u2019s Hopper GPUs in 2024, while Meta (META) purchased 224,000 of the AI chips, according to the Financial Times.\n\nThe rule aims to close loopholes in prior export restrictions on AI chips in 2022 and 2023 \u201cby thwarting smuggling\u201d and \u201craising AI security standards,\" the White House said.\n\n\"[These restrictions] will make it even more difficult for Chinese entities to purchase the most advanced NVIDIA chips,\" DA Davidson analyst Gil Luria told Yahoo Finance in an email Monday.\n\n\u201cWhile there have been some restrictions on chip sales already, there have been reports of advanced NVIDIA chips making it to China, likely due to the fact that NVIDIA has limited control over its resellers,\u201d Luria explained in an earlier email last week.\n\nIn addition to Nvidia's advanced chips sold through resellers, Nvidia makes specific versions of chips that comply with current US trade restrictions on China. Sales of Nvidia's H20 chips \u2014 its Hopper chips for China \u2014 \"should be unaffected by the controls,\" Rasgon wrote in the note.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "5": {
            "title": "JPM dispatches: Nvidia, Recursion, Dexcom and more",
            "link": "https://www.statnews.com/2025/01/14/jpm-dispatches-nvidia-recursion-dexcom-health-tech/",
            "snippet": "At the JP Morgan Health Care conference, Nvidia announced a slew of new deals focused on the use of AI in life sciences.",
            "score": 0.9450095891952515,
            "sentiment": null,
            "probability": null,
            "content": "You\u2019re reading the web edition of STAT\u2019s Health Tech newsletter, our guide to how technology is transforming the life sciences. Sign up to get it delivered in your inbox every Tuesday and Thursday.\n\nHospitals rush to check algorithms before deadline\n\nOver the last four years, health systems across the United States have phased out the use of several clinical tools that use race to predict patient outcomes, replacing them with race-free versions that carry less risk of perpetuating inequitable care.\n\nadvertisement\n\nBut as STAT\u2019s Katie Palmer reports, there\u2019s a wide world of other calculators and algorithms used to make decisions about patients every day \u2014 many of which use race, sex, and other traits protected by federal anti-discrimination laws. As a deadline for federally funded health systems to vet those tools for discrimination approaches in May, it\u2019s still unclear how they\u2019ll tackle the challenge.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Is Nvidia\u2019s Jensen Huang Right About Quantum Computing?",
            "link": "https://observer.com/2025/01/is-nvidias-jensen-huang-right-about-quantum-computing/",
            "snippet": "\u201cVery useful quantum computers are still a few decades away,\u201d Nvidia (NVDA) CEO Jensen Huang said during a keynote presentation at the 2025 Consumer...",
            "score": 0.8978380560874939,
            "sentiment": null,
            "probability": null,
            "content": "\u201cVery useful quantum computers are still a few decades away,\u201d Nvidia (NVDA) CEO Jensen Huang said during a keynote presentation at the 2025 Consumer Electronics Show (CES) in Las Vegas last week. His comment sent shockwaves through the quantum computing industry, with shares of leading companies like Rigetti Computing, D-Wave Quantum and IonQ plummeting between 30 and 50 percent. These companies\u2019 CEOs were quick to defend their industry in a bid to win back investor confidence.\n\nSign Up For Our Daily Newsletter Sign Up Thank you for signing up! By clicking submit, you agree to our <a href=\"http://observermedia.com/terms\">terms of service</a> and acknowledge we may use your information to send you emails, product samples, and promotions on this website and other properties. You can opt out anytime. See all of our newsletters\n\n\u201cJensen Huang has a misunderstanding of quantum. He is \u2018dead wrong\u2019 about D-Wave,\u201d Alan Baratz, CEO of D-Wave Quantum, which develops quantum computing systems, told Observer. \u201cThere is more than one approach to building a quantum computer. Our (D-Wave) systems are performing scientific computations on important problems that are not solvable by even massively parallel GPU systems.\u201d\n\n\n\nD-Wave uses quantum annealing, an approach that excels in solving specific computational optimization problems. This method is particularly useful for materials simulation, scheduling and logistics applications. \u201cCommercial quantum computing is already here,\u201d Baratz added. For example, Canada-based Pattison Food Group reduced an 80-hour scheduling task to 15 hours using D-Wave\u2019s technology. Another client, NTT DOCOMO, Japan\u2019s largest telecom provider, slashed the time to optimize network resources from 27 hours to just 40 seconds.\n\nQuantum computing CEOs are also talking up their companies\u2019 revenue and profit prospects. D-Wave estimated its 2024 sales jumped 120 percent from the previous year. IonQ, which makes quantum computers, predicts it will turn a profit on more than $1 billion in sales by 2030, CEO Peter Chapman wrote in a blog post on Jan. 10.\n\nWhat is quantum computing?\n\nQuantum computing relies on qubits\u2014units of data that can exist in multiple states at once\u2014and holds the potential to quickly solve calculations that would take even supercomputers a thousand years. Exciting recent developments include Google\u2019s Willow chip, which solved a random circuit sampling (RCS) benchmark problem in just five minutes. RCS, one of the most challenging benchmarks for quantum computers, would take today\u2019s fastest supercomputer 10 septillion years (1 followed by 24 zeros) to solve, Google claims.\n\nHowever, showcasing potential is different than consistently solving problems. \u201cQuantum systems are highly susceptible to environmental noise, which can disrupt quantum computations. Nick Harris, CEO and co-founder of Lightmatter, an Alphabet-backed photonic computing startup, told Observer. \u201cThe breakthrough of Google\u2019s Willow chip does not apply to all quantum computing hardware platforms, and the reality today is that scaling up is nowhere near a given.\u201d\n\nOther major challenges include scaling qubit systems while preserving their coherence and connectivity, as well as improving qubit fidelity. \u201cEven with error correction, individual qubit operations still carry a non-zero error rate. Reducing these errors further is essential for achieving reliable computations,\u201d Harris explained.\n\nThe quantum computing industry is projected to reach $2 trillion by 2035, according to McKinsey & Company. However, Huang\u2019s remarks brought that lofty vision back down to earth. Huang predicted it would be 15 to 30 years before quantum computing is commercially viable. Daniela Herrmann, co-founder of Dynex, a quantum-as-a-service company, partially agrees with that assessment. \u201cHuang\u2019s extended timeline reflects the broader understanding of the challenges in achieving fully developed quantum systems, but it doesn\u2019t fully acknowledge the progress being made by existing quantum-inspired applications,\u201d Herrmann told Observer. She noted that Huang\u2019s remarks seem to keep the spotlight on initiatives like Nvidia\u2019s CUDA-Q, which is yet to solve real-world problems at scale.\n\nThe quantum computing industry is making meaningful strides\n\nQuantum computing companies, including Phasecraft, Zapata Computing and Algorithmiq, are making notable strides in developing algorithms that can leverage the limited capabilities of existing quantum hardware. Moreover, advancements in quantum computation optimization are also being spearheaded by the University of Chicago and Stanford University, with recent initiatives focusing on optical computing and exploring its applications in quantum technologies.\n\n\u201cAdvanced language model A.I. tasks that previously required extensive resources are now being completed with 90 percent less computational overhead,\u201d Herrmann said.\n\n\u201cMany tangible business cases don\u2019t necessarily require a universal (gate-model) quantum computer to provide advantages,\u201d Anders Indset, chairman of Njordis Group, a VC firm investing in quantum technology companies, told Observer. \u201cRecent sharp dip seems more like a near-term market correction than a final verdict on quantum\u2019s future. \u201d\n\nIndset believes that Huang\u2019s own economic interests in the field could influence his desire to slow down potential competitors. \u201cWhile Jensen Huang\u2019s caution may reflect the tough realities ahead, history shows that breakthroughs can come faster than expected,\u201d he said. \u201cA.I.\u2019s voracious need for compute resources is pushing the industry to explore new hardware frontiers\u2014quantum included. Once quantum machines achieve a certain threshold, they could supercharge A.I. by making specific training or inference tasks dramatically faster.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "JPM25, Day 2: News from Nvidia, Tempus AI and Health Catalyst; A look at Children's Health Dallas' expansion plans",
            "link": "https://www.fiercehealthcare.com/providers/jpm25-day-2-healthcare-conference-draws-protesters-news-nvidia-tempus-ai-and-health",
            "snippet": "This work includes an expansion of its campus in Plano, Texas as well as a new pediatric campus in Dallas, CEO Chris Durovich told investors on Tuesday morning.",
            "score": 0.8610879182815552,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "3 Reasons Nvidia Stock Is a Forever Buy and Hold",
            "link": "https://www.fool.com/investing/2025/01/14/3-reasons-nvidia-stock-is-a-forever-buy-and-hold/",
            "snippet": "With a forward P/E of 31.7, Nvidia's premium valuation reflects its market dominance and growth potential in physical AI and robotics.",
            "score": 0.9412879943847656,
            "sentiment": null,
            "probability": null,
            "content": "Artificial intelligence (AI) has unleashed a technological revolution, and one company sits at its very core. Nvidia (NVDA 5.27%) powers nearly every major AI breakthrough, from autonomous vehicles to drug discovery to advanced robotics. The company's technological dominance in this revolutionary field has driven its stock price up an astounding 2,127% over the past five years.\n\nAt first glance, Nvidia's valuation might cause investors to pause. The company's shares trade at an eye-catching 31.7 times forward earnings, well above the S&P 500's 23.5 multiple, which also happens to be on the high side, historically speaking. Yet this premium reflects something extraordinary: Nvidia's commanding position in computing's next great transformation. As we enter the age of intelligent machines, here are three compelling reasons why Nvidia deserves a permanent place in long-term portfolios.\n\nReason No. 1: Data center dominance\n\nAccording to a report from Markets and Markets, the global data center chip market is set to reach a mind-boggling $390.65 billion by 2030, growing at 13.5% annually from 2025. According to industry analysts, Nvidia controls around 80% of the AI chip market, with its chief rival in the space, Advanced Micro Devices, holding approximately 11% of the segment.\n\nThis market dominance stems from Nvidia's early lead in parallel computing architecture. The company's graphics processing units (GPUs) process multiple calculations simultaneously, making them vastly more efficient than traditional processors for AI workloads.\n\nFurthermore, Nvidia's Compute Unified Device Architecture software platform has become the industry standard for AI development across financial services, healthcare, and retail. This entrenched ecosystem creates high customer switching costs, making it increasingly difficult for competitors to gain meaningful traction.\n\nWith an insurmountable lead in key hardware and software segments, Nvidia's grip on the AI chip market appears unshakeable for years to come.\n\nReason No. 2: Innovation beyond graphics\n\nNvidia's latest chip designs demonstrate its expanding technological capabilities beyond traditional graphics processing. The company continues to set new performance benchmarks while making AI computing more accessible to developers and businesses.\n\nThe expansion into developer-focused products opens new revenue streams without cannibalizing high-end data center sales. This multitiered strategy allows Nvidia to capture value across the entire AI computing spectrum.\n\nThe company's innovation pipeline extends into AI-powered gaming, self-driving vehicles, and advanced robotics. These initiatives showcase Nvidia's ability to leverage its core technology into entirely new markets.\n\nIts technological leadership has created multiple growth engines that reduce reliance on any single market. With breakthroughs spanning multiple industries, Nvidia's impact on computing extends far beyond its gaming origins.\n\nReason No. 3: Strategic AI investments\n\nNvidia's investment portfolio targets companies at the forefront of AI applications. Recent investments include Applied Digital (NASDAQ: APLD), a data center provider specializing in AI infrastructure; Serve Robotics (NASDAQ: SERV), a leader in autonomous delivery robots for last-mile delivery; and Recursion Pharmaceuticals (NASDAQ: RXRX), which uses AI to revolutionize drug discovery.\n\nThese strategic stakes give Nvidia early insight into emerging AI applications. Serve Robotics demonstrates AI's potential in automation, while Recursion shows how AI can accelerate pharmaceutical research. Applied Digital's focus on AI-optimized data centers strengthens Nvidia's position in computing infrastructure.\n\nEach investment targets a distinct market where AI promises disruption. Recursion aims to reduce drug development timelines from decades to perhaps weeks or even days, while Serve Robotics seeks to transform last-mile delivery through autonomous robots and drones.\n\nThese forward-looking investments position Nvidia to capitalize on AI adoption across multiple industries. By backing innovators in robotics, drug discovery, and computing infrastructure, Nvidia gains valuable insights into how these nascent markets are developing while simultaneously expanding its technological moat.\n\nA generational investment opportunity\n\nNvidia's dominance in AI computing extends far beyond its current core area of expertise in chip design. The company has built an ecosystem of hardware, software, and development tools that makes it essential to AI advancement.\n\nWhile the tech stock's valuation reflects high expectations, Nvidia's expanding competitive advantages and massive growth opportunities justify its premium. For investors seeking exposure to this ongoing technological revolution reshaping modern society, Nvidia represents a foundational holding built to compound value for decades to come.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Mayo Clinic, Nvidia and Aignostics partner on digital pathology AI tools",
            "link": "https://www.medicaldesignandoutsourcing.com/mayo-clinic-nvidia-aignostics-partner-on-digital-pathology-ai-tools/",
            "snippet": "Mayo Clinic has partnered with Aignostics and Nvidia to create Mayo Clinic Digital Pathology, which is meant to unleash the power of artificial intelligence on...",
            "score": 0.8651009798049927,
            "sentiment": null,
            "probability": null,
            "content": "Mayo Clinic has partnered with Aignostics and Nvidia to create Mayo Clinic Digital Pathology, which is meant to unleash the power of artificial intelligence on the top-tier health provider\u2019s archive of 20 million digital slides.\n\nRochester, Minnesota\u2013based Mayo Clinic said the collaboration has already resulted in promising early achievements.\n\n\u201cMayo Clinic is reimagining what is possible in disease detection and prediction, both within its own system and globally. We are doing this by using large, diverse datasets to build powerful artificial intelligence models in pathology. This will make diagnoses faster, more accurate, and more efficient, improving treatment approaches and speeding new cures to patients,\u201d Jim Rogers, CEO of Mayo Clinic Digital Pathology, said in a news release.\n\nAccording to some estimates, only about a tenth of pathology labs in the U.S. have digitized at this point. In many cases, cancer patients looking for a second opinion or alternative treatment option at a different health system need to have physical slides mailed to the new location.\n\nHowever, where digitization has already taken place, there are signs that digitization combined with artificial intelligence could take oncology to the next level. For example, at our DeviceTalks West show in October 2024, UCSF\u2019s Dr. Zoltan G. Laszik and Philips\u2019 digital pathology business leader Kevin Coady demonstrated how AI tools could enable pathologists to assess prostate cancer more accurately. (Discover more medtech insights at DeviceTalks Boston, April 30\u2013May 1, and DeviceTalks Minnesota, June 11.)\n\nMayo Clinic, with its 20 million digital slides linked to 10 million patient records, seeks to show even more what is possible when artificial intelligence gets combined with digital pathology.\n\n\u201cAI-driven insights can accelerate diagnostics, enhance precision medicine and revolutionize patient care,\u201d said Nvidia VP of Healthcare and Life Sciences Kimberly Powell. \u201cBy digitizing and harnessing the power of vast datasets through its Digital Pathology platform, powered by Nvidia\u2019s accelerated computing, Mayo Clinic is helping pave the way for a future with faster medical breakthroughs, better treatments and improved outcomes for patients across the globe.\u201d\n\n\u201cMerging Mayo Clinic\u2019s data and expertise with our advanced machine learning capabilities will produce breakthrough foundation models and AI products that advance the field of precision medicine and meaningfully improve patient care,\u201d said Viktor Matayas, CEO of Aignostics.\n\nOver less than two months, Mayo Clinic and Berlin-based Aignostics created a novel foundation model called Atlas. They trained it on 1.2 million unidentified histopathology whole slide images from Mayo Clinic and Charit\u00e9 \u2013 Universit\u00e4tsmedizin Berlin. They published their findings on Jan. 9, reporting that Atlas achieved state-of-the-art performance across 21 public benchmark datasets, even though it was neither the largest model by parameter count nor training dataset size. They\u2019re presently developing and deploying new features enabled by the model and plan to build new models, including one being trained on 5 million slides.\n\nThe Nvidia collaboration is focused on creating infrastructure to build and deploy foundation models to accelerate generative AI advances in pathology and beyond. Nvidia has a healthcare-specific full-stack computing architecture for artificial intelligence called Nvidia Clara. Mayo Clinic is using it to build models that its officials think will open new frontiers in medicine.\n\n\u201cThese new capabilities using digital pathology data will unlock this critically important clinical information for building AI solutions for advanced diagnosis and care of patients, and that will improve the lives of patients globally,\u201d said Dr. Matthew Callstrom, chair of Mayo Clinic Radiology in the Midwest and medical director for Generative AI and Strategy.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-13": {
        "0": {
            "title": "NVIDIA Statement on the Biden Administration\u2019s Misguided \u2018AI Diffusion\u2019 Rule",
            "link": "https://blogs.nvidia.com/blog/ai-policy/",
            "snippet": "The Biden Administration now seeks to restrict access to mainstream computing applications with its unprecedented and misguided \u201cAI Diffusion\u201d rule.",
            "score": 0.5315893292427063,
            "sentiment": null,
            "probability": null,
            "content": "For decades, leadership in computing and software ecosystems has been a cornerstone of American strength and influence worldwide. The federal government has wisely refrained from dictating the design, marketing and sale of mainstream computers and software \u2014 key drivers of innovation and economic growth.\n\nThe first Trump Administration laid the foundation for America\u2019s current strength and success in AI, fostering an environment where U.S. industry could compete and win on merit without compromising national security. As a result, mainstream AI has become an integral part of every new application, driving economic growth, promoting U.S. interests and ensuring American leadership in cutting-edge technology.\n\nToday, companies, startups and universities around the world are tapping mainstream AI to advance healthcare, agriculture, manufacturing, education and countless other fields, driving economic growth and unlocking the potential of nations. Built on American technology, the adoption of AI around the world fuels growth and opportunity for industries at home and abroad.\n\nThat global progress is now in jeopardy. The Biden Administration now seeks to restrict access to mainstream computing applications with its unprecedented and misguided \u201cAI Diffusion\u201d rule, which threatens to derail innovation and economic growth worldwide.\n\nIn its last days in office, the Biden Administration seeks to undermine America\u2019s leadership with a 200+ page regulatory morass, drafted in secret and without proper legislative review. This sweeping overreach would impose bureaucratic control over how America\u2019s leading semiconductors, computers, systems and even software are designed and marketed globally. And by attempting to rig market outcomes and stifle competition \u2014 the lifeblood of innovation \u2014 the Biden Administration\u2019s new rule threatens to squander America\u2019s hard-won technological advantage.\n\nWhile cloaked in the guise of an \u201canti-China\u201d measure, these rules would do nothing to enhance U.S. security. The new rules would control technology worldwide, including technology that is already widely available in mainstream gaming PCs and consumer hardware. Rather than mitigate any threat, the new Biden rules would only weaken America\u2019s global competitiveness, undermining the innovation that has kept the U.S. ahead.\n\nAlthough the rule is not enforceable for 120 days, it is already undercutting U.S. interests. As the first Trump Administration demonstrated, America wins through innovation, competition and by sharing our technologies with the world \u2014 not by retreating behind a wall of government overreach. We look forward to a return to policies that strengthen American leadership, bolster our economy and preserve our competitive edge in AI and beyond.\n\nNed Finkle is vice president of government affairs at NVIDIA.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "1": {
            "title": "Nvidia Announces Partnerships with Mayo Clinic, Illumina & More at JPM25",
            "link": "https://medcitynews.com/2025/01/nvidia-ai-healthcare-jpm-biopharma/",
            "snippet": "Nvidia announced four new partnerships focused on scaling AI models across the healthcare industry. The company is teaming up with Mayo Clinic, Illumina, IQVIA...",
            "score": 0.6703284382820129,
            "sentiment": null,
            "probability": null,
            "content": "On Monday, AI development powerhouse Nvidia kicked off this year\u2019s J.P. Morgan Healthcare Conference in San Francisco by announcing new partnerships focused on scaling AI models across healthcare.\n\nOne of Nvidia\u2019s new partnerships is with Mayo Clinic \u2014 the collaboration seeks to accelerate the development of pathology foundation models.\n\nMayo Clinic\u2019s digital pathology platform houses 20 million whole-slide images, as well as 10 million associated patient records. To quicken the pace of foundation model development on this platform, the health system will deploy Nvidia\u2019s DGX Blackwell AI systems, as well as Monai, the company\u2019s healthcare imaging platform.\n\n\u201cOur ultimate goal is to create a human digital twin. This is a dynamic digital representation, including medical imaging, pathology, health records and wearables. To achieve this, Mayo Clinic and Nvidia will leverage some of the latest AI models and vision language models like Cosmos Nemotron and our microservices, and this collaboration will be a cornerstone for new applications in drug discovery and diagnostic medicine,\u201d said Kimberly Powell, vice president and general manager of healthcare at Nvidia, during a press briefing.\n\nAnother one of Nvidia\u2019s partnerships is with IQVIA, a life sciences analytics and clinical research firm.\n\nThe firm will leverage Nvidia\u2019s AI foundry service to create foundation models \u2014 with the ultimate goal of deploying AI agents to IQVIA\u2019s more than 10,000 customers across healthcare and life sciences, Powell noted.\n\n\u201cIQVIA has a really rich and unique dataset of 64 petabytes of proprietary data that\u2019s representing over a billion unique and anonymized patients from 100 different countries,\u201d she pointed out.\n\nNvidia\u2019s AI foundry service includes models, software and expert services to help organizations create and deploy AI tools. By using Nvidia\u2019s AI foundry, IQVIA will be able to develop AI agents more quickly, Powell remarked.\n\nShe highlighted AI agents \u2014 which are AI models that complete specific tasks without human intervention \u2014 as an important area taking off within the greater AI field.\n\n\u201cAgent AI cannot only perceive, but it can reason, plan and even take action,\u201d Powell declared.\n\nOn the biopharma side of things, Nvidia is teaming up with DNA sequencing powerhouse Illumina.\n\nIllumina will use Nvidia\u2019s computing and AI toolsets to augment its multiomics analysis technology \u2014 which analyzes data from the genome, transcriptome, proteome, epigenome, metabolome microbiome to advance drug discovery and precision medicine.\n\nThis will enable Illumina to create foundation models that will \u201cunlock the next generation of genomics insights,\u201d Powell remarked.\n\n\u201cWe\u2019re going to unlock new markets for genomics by making genomics insights \u2014 not just the data, but the insights \u2014 much more accessible and impactful, driving significant advancements in disease research and drug discovery,\u201d she stated.\n\nThe final partnership Nvidia announced on Monday was also in the biopharma space. The company is collaborating with Arc Institute, a research organization focusing on biology and machine learning, to develop AI tools to advance biomedical discovery.\n\nArc\u2019s researchers are working with Nvidia\u2019s engineers to build and scale foundation models that can be generalized across various modalities within biology, such as DNA, RNA and proteins.\n\n\u201cOur partnership is going to focus on developing true foundation models for biology using Nvidia BioNeMo and DGX Cloud. All of our resulting work is going to be shared and contributed back to the open source in BioNeMo so we can truly democratize large scale biomedical research,\u201d Powell declared.\n\nShe said Nvidia hopes to be \u201cactively announcing the outputs\u201d of these collaborations throughout the rest of the year.\n\nCredit: MR.Cole_Photographer, Getty Images",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Illumina and NVIDIA collaborate to decode biology and propel precision health",
            "link": "https://www.prnewswire.com/news-releases/illumina-and-nvidia-collaborate-to-decode-biology-and-propel-precision-health-302348920.html",
            "snippet": "Illumina, Inc. (NASDAQ: ILMN), a global leader in genomic sequencing and analysis, today announced it is collaborating with NVIDIA to advance technology...",
            "score": 0.6499632596969604,
            "sentiment": null,
            "probability": null,
            "content": "Collaboration combines Illumina's sequencing technologies and Illumina Connected Software with NVIDIA tools to develop biological foundation models\n\nSAN DIEGO, Jan. 13, 2025 /PRNewswire/ -- Illumina, Inc. (NASDAQ: ILMN), a global leader in genomic sequencing and analysis, today announced it is collaborating with NVIDIA to advance technology platforms for the analysis and interpretation of multiomic data, accelerating progress in clinical research, genomics AI development, and drug discovery.\n\nTo optimize analysis of the vast amounts of data involved in multiomic research, Illumina and NVIDIA will combine advancements in AI with multiomic data at scale. Leveraging Illumina's leading sequencing technology and informatics tools alongside NVIDIA's leading AI technology, the partnership aims to accelerate drug discovery and clinical development, delivering powerful tools to help pharmaceutical companies identify new and better drug targets.\n\nIllumina's leading sequencing technology\u2014as well as its menu of DRAGEN-powered multiomics offerings, genomics AI tools, and Illumina Connected Analytics platform\u2014has streamlined genomic data generation and analysis. Illumina has invested in AI for genomic interpretation, developing the leading SpliceAI, PrimateAI-3D, and Emedgene xAI algorithms.\n\nNow, in addition to its own efforts, Illumina will look to expand its customer offerings with models developed by the NVIDIA Biology Foundation Model Research Team and partners. Customers will also be able to leverage these models with their own proprietary datasets to improve the performance for biologically relevant tasks of interest, such as cell state or gene transcription prediction.\n\nWith this new partnership, the global R&D community can tap into rich genomic data by integrating NVIDIA RAPIDS\u2122 accelerated data science software with the NVIDIA BioNeMo\u2122 platform's generative AI models and fine-tuning capabilities for proprietary datasets, as well as MONAI for spatial cell imaging workflows. Illumina and NVIDIA will work to make these tools accessible on the Illumina Connected Analytics platform.\n\n\"Over the past 20 years, Illumina has democratized sequencing, and with the progress in AI and multiomic analysis, we are enabling customers to derive novel insights for their applications,\" said Rami Mehio, head of global software and informatics at Illumina. \"This collaboration with NVIDIA moves us closer to that vision. This is also part of our commitment to continue to enrich our analysis and interpretation tools that will enable deeper biological insights, delivering total workflow solutions for our customers.\"\n\n\"AI and data science will find their most profound application in genomics,\" said Rory Kelleher, senior director, global head of Business Development, Healthcare and Life Sciences at NVIDIA. \"Combining Illumina's world-leading sequencing and analytics platforms with NVIDIA's accelerated computing and AI, we will drive the next generation of genomics interpretation and democratize genomics for drug discovery through AI-powered insights.\"\n\nIn the first phase of the collaboration, Illumina and NVIDIA will work toward enabling DRAGEN algorithms on NVIDIA GPUs. Bringing Illumina DRAGEN to NVIDIA accelerated computing will expand the accessibility of Illumina multiomics analysis. The two companies will also work to incorporate NVIDIA's image processing and single-cell tertiary analysis tools onto the Illumina Connected Software multiomics module.\n\nUse of forward-looking statements\n\nThis release may contain forward-looking statements that involve risks and uncertainties. Among the important factors to which our business is subject that could cause actual results to differ materially from those in any forward-looking statements are: (i) challenges inherent in developing and launching new products and services, including modifying and scaling manufacturing operations, and reliance on third-party suppliers for critical components; (ii) our ability to manufacture robust instrumentation and consumables; and (iii) the acceptance by customers of our newly launched products, which may or may not meet our and their expectations, together with other factors detailed in our filings with the Securities and Exchange Commission, including our most recent filings on Forms 10-K and 10-Q, or in information disclosed in public conference calls, the date and time of which are released beforehand. We undertake no obligation, and do not intend, to update these forward-looking statements, to review or confirm analysts' expectations, or to provide interim reports or updates on the progress of the current quarter.\n\nAbout Illumina\n\nIllumina is improving human health by unlocking the power of the genome. Our focus on innovation has established us as a global leader in DNA sequencing and array-based technologies, serving customers in the research, clinical, and applied markets. Our products are used for applications in the life sciences, oncology, reproductive health, agriculture, and other emerging segments. To learn more, visit illumina.com and connect with us on X, Facebook, LinkedIn, Instagram, TikTok, and YouTube.\n\nContacts\n\nInvestors:\n\nSalli Schwartz\n\n858-291-6421\n\n[email protected]\n\nMedia:\n\nChristine Douglass\n\n[email protected]\n\nSOURCE Illumina, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "NVIDIA GeForce RTX 5090 reviews go live January 24, RTX 5080 on January 30",
            "link": "https://hardforum.com/threads/nvidia-geforce-rtx-5090-reviews-go-live-january-24-rtx-5080-on-january-30.2039131/",
            "snippet": "Nvidia wants people to see 5090 reviews a week before their launch. 5080s however they want no one seeing reviews for till they are already for sale.",
            "score": 0.951833963394165,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Illumina, Nvidia team to use AI in foundation models",
            "link": "https://www.medtechdive.com/news/Illumina-Nvidia-collaboration-AI-biological-foundation-models/737174/",
            "snippet": "Illumina aims to provide researchers with advanced tools that apply generative AI to analyze the vast amounts of data involved in multiomic research.",
            "score": 0.8108538389205933,
            "sentiment": null,
            "probability": null,
            "content": "Listen to the article 2 min This audio is auto-generated. Please let us know if you have feedback\n\nDive Brief:\n\nIllumina said Monday it will collaborate with computer chip maker Nvidia on technology platforms to analyze multiomic data for clinical research, genomics artificial intelligence development and drug discovery.\n\nThe collaboration combines Illumina\u2019s sequencing technologies and software with Nvidia\u2019s computing and AI capabilities to build biological foundation models.\n\nThe partnership is one of several healthcare collaborations Nvidia unveiled Monday focused on expanding the adoption of AI in the life sciences industry.\n\nDive Insight:\n\nIn partnering with Nvidia, Illumina aims to provide researchers with advanced tools that apply generative AI to analyze the vast amounts of data involved in multiomic research, an approach combining data from multiple \u201comic\u201d groups including genomics, transcriptomics and epigenetics.\n\nThe collaboration will allow Illumina to use Nvidia accelerated computing and AI tools for its multiomics analysis software and workflows, making analysis of the human genome more accessible to researchers, pharmaceutical companies and other life sciences customers, Nvidia said.\n\nNvidia and Illumina plan to collaborate for multiomics data analysis on Illumina\u2019s connected analytics platform, in addition to developing new biology foundation models. Illumina will integrate its Dragen analysis software and connected analytics with Nvidia\u2019s accelerated computing.\n\n\u201cOur ability to combine the power of AI with multiomics data is revolutionizing how we can understand disease,\u201d Steve Barnard, Illumina\u2019s chief technology officer, said in Nvidia\u2019s statement. \u201cWe aim to enable pharma and biotech companies to unlock their own multiomics data to uncover transformative insights and improve success rates in developing lifesaving therapies.\u201d\n\nNvidia said it is also partnering with Iqvia, Mayo Clinic and Arc Institute to use its technologies to advance drug discovery and digital pathology.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Synchron to Advance Implantable Brain-Computer Interface Technology with NVIDIA Holoscan",
            "link": "https://www.businesswire.com/news/home/20250113376337/en/Synchron-to-Advance-Implantable-Brain-Computer-Interface-Technology-with-NVIDIA-Holoscan",
            "snippet": "Synchron to Advance Implantable Brain-Computer Interface Technology with NVIDIA Holoscan ... NEW YORK--(BUSINESS WIRE)--Synchron, a category-defining brain-...",
            "score": 0.5206682682037354,
            "sentiment": null,
            "probability": null,
            "content": "NEW YORK--(BUSINESS WIRE)--Synchron, a category-defining brain-computer interface (BCI) company, announced today a step forward in implantable BCI technology to drive the future of neurotechnology. Synchron\u2019s BCI technology, in combination with the NVIDIA Holoscan platform, is poised to redefine the possibilities of real-time neural interaction and intelligent edge processing.\n\nSynchron will leverage NVIDIA Holoscan to advance a next-generation implantable BCI in two key domains. First, Synchron will enhance real-time edge AI capabilities for on-device neural processing, improving signal processing and multi-AI inference technology. This will reduce system latency, bolster privacy, and provide users with a more responsive and intuitive BCI experience. NVIDIA Holoscan provides Synchron with: (i) a unified framework supporting diverse AI models and data modalities; (ii) an optimized application framework, from seamless sensor I/O integration, GPU-direct data ingestion, to accelerated computing and real-time AI.\n\nSecond, Synchron will explore the development of a groundbreaking foundation model for brain inference. By processing Synchron\u2019s neural data on an unprecedented scale, this initiative will create scalable, interpretable brain-language models with the potential to transform neuroprosthetics, cognitive expression, and seamless interaction with digital devices.\n\n\u201cSynchron\u2019s vision is to scale neurotechnology to empower humans to connect to the world, and the NVIDIA Holoscan platform provides the ideal foundation,\u201d said Tom Oxley, M.D., Ph.D., CEO & Founder, Synchron. \u201cThrough this work, we\u2019re setting a new benchmark for what BCIs can achieve.\u201d\n\nAbout Synchron\n\nSynchron is the leading implantable brain-computer interface company, pioneering neurotechnology designed to restore functionality for individuals with paralysis by expanding autonomy. Since 2019, Synchron has completed two human clinical trials and is now preparing for a larger-scale clinical study with a device optimized for scalable manufacturing. The company has an exclusive partnership with flexible MEMs manufacturer Aquandas, ensuring access to cutting-edge technology for their innovative devices. The BCI market represents a $400 billion opportunity (Morgan Stanley report), and Synchron is at the forefront, defining the category and pushing the boundaries of what\u2019s possible in neuroprosthetics and digital autonomy. Headquartered in New York City, Synchron is shaping the future of neurotechnology. For more information, visit www.synchron.com and follow us @synchroninc.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Although Quantum is 'Decades' Away For NVIDIA, The Company is Investing in Quantum Talent Today",
            "link": "https://thequantuminsider.com/2025/01/13/although-quantum-is-decades-away-for-nvidia-the-company-is-investing-in-quantum-talent-today/",
            "snippet": "NVIDIA seems to be investing heavily in quantum technologies and laying the groundwork for a hybrid quantum-classical future \u2014 perhaps even near future \u2014 by...",
            "score": 0.5924071073532104,
            "sentiment": null,
            "probability": null,
            "content": "Insider Brief\n\nNVIDIA CEO Jensen Huang estimates that practical quantum computing is still 20 years away, yet the company is strategically investing in quantum technologies and workforce development to prepare for a hybrid quantum-classical future.\n\nNVIDIA\u2019s job postings, including roles in quantum error correction and algorithm engineering, suggests a focus on foundational quantum research and immediate use cases.\n\nHuang emphasizes the complementary nature of quantum and classical computing, framing NVIDIA\u2019s investments as groundwork for integrating quantum accelerators with classical supercomputers to solve complex problems.\n\nNVIDIA CEO Jensen Huang has recently been candid about the timeline for practical quantum computing, stating in multiple forums that \u201cvery useful\u201d quantum computers are likely 20 years away.\n\n\n\nIn a conversation with analysts, Huang suggested that practical quantum computers might emerge in a timeline of 15 to 30 years, with 20 years being a reasonable estimate that most of his colleagues in NVIDIA would agree on.\n\nHowever, despite this long horizon, NVIDIA seems to be investing heavily in quantum technologies and laying the groundwork for a hybrid quantum-classical future \u2014 perhaps even near future \u2014 by building their own quantum workforce. The company currently has approximately 10 jobs and internships that appear directly related to quantum technology, or positions with job descriptions that suggest they will involve quantum at some point. Most of the positions are centered in NVIDIA\u2019s quantum computing unit, described as \u201ca small, strong, and visible group both inside and outside of NVIDIA.\u201d\n\nJobs That Signal NVIDIA\u2019s Quantum Priorities And Timeline\n\nNVIDIA\u2019s recent job postings might help illustrate how it is positioning itself for this quantum future.\n\nSeveral roles focus directly on foundational aspects of quantum computing. The Senior Quantum Error Correction Research Scientist and Quantum Algorithm Engineer roles aim to address challenges like error correction and algorithm development, critical for advancing quantum systems.\n\nNVIDIA\u2019s jobs also suggest the company is connecting its strengths in machine learning and artificial intelligence with the emerging quantum field: \u201cWe are seeking experts in quantum error correction (QEC) and fault-tolerant quantum computing (FTQC) with a desire to scale current state-of-the-art methods with AI and ML.\u201d\n\nThe quantum algorithm engineer, specifically, will investigate quantum applications in chemistry, machine learning, error correction and related areas with a desire to enable researchers across a range of disciplines.\n\nAnother position, the Director, Quantum Computing Applied Research, suggests that NVIDIA is actively exploring applications where quantum can complement its classical capabilities.\n\nQuantum Computing as a Complementary Technology\n\nHuang outlined NVIDIA\u2019s vision for quantum\u2019s role alongside classical computing during his keynote at GTC 2024. He described quantum as a \u201cquantum accelerator,\u201d emphasizing the necessity of classical computing infrastructure to drive quantum systems.\n\n\u201cYou can\u2019t just program a quantum computer all by itself. You need to have classical computing sitting next to it,\u201d Huang said, explaining how NVIDIA\u2019s expertise in high-performance computing positions the company as an ideal partner for the quantum ecosystem.\n\nThis perspective aligns with NVIDIA\u2019s broader strategy of preparing for a hybrid quantum-classical paradigm. The company envisions quantum processors integrated with classical supercomputers to solve complex problems that neither can tackle alone. This symbiotic relationship might suggest why NVIDIA is investing in quantum now \u2014 rather than 20 years from now.\n\nBased on how NVIDIA is investing in a quantum workforce, Huang\u2019s measured timeline for quantum computing doesn\u2019t contradict NVIDIA\u2019s current investments. Instead, it might underscore a pragmatic approach: investing today to secure a foothold in the ecosystem and build the infrastructure for future quantum adoption. The move would not be different from NVIDIA\u2019s work with accelerators for AI. Further, the focus on hybrid applications, such as weather forecasting, further justifies these investments.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "JP Morgan 2025: NVIDIA partners with IQVIA, Illumina and Mayo Clinic",
            "link": "https://www.pharmaceutical-technology.com/news/jp-morgan-2025-nvidia-partners-with-iqvia-illumina-and-mayo-clinic/",
            "snippet": "JP Morgan 2025: NVIDIA partners with IQVIA, Illumina and Mayo Clinic. NVIDIA starts 2025 by announcing a trio of partnerships and a research collaboration to...",
            "score": 0.5096405148506165,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA is the world\u2019s most valuable company with a market cap of $3.33tn. Image credit: Shutterstock / JHVEPhoto.\n\nIn moves that continue to strengthen its affiliation with the healthcare industry, NVIDIA has added three new companies to its growing list of AI-based partnerships.\n\nThe tech company has partnered with clinical research services company IQVIA, genomics specialist Illumina, and Mayo Clinic. Along with a collaboration with research organisation Arc Institute, the collaborations were announced during the JP Morgan Healthcare Conference on 13 January.\n\nNvidia, the world\u2019s most valuable company with a market cap of $3.33tn, has expanded its presence in the healthcare sector over the past few years. The company states that AI, accelerated computing and biological data coming together is \u201cturning healthcare into the largest technology industry\u201d.\n\nA 2023 report by GlobalData predicts that global revenue for AI platforms across healthcare will reach $18.8bn by 2027, with Nvidia\u2019s technological advancements in the field a key reason for the market\u2019s growth. The newly announced partnerships are the first of 2025 as the tech company looks ahead to a year when it could see its share price soar even higher.\n\nIQVIA will use Nvidia\u2019s AI Foundry \u2013 a service that allows companies to build custom generative AI models \u2013 to create new solutions in clinical research workflows. IQVIA is also developing agentic AI solutions kitted out with Nvidia\u2019s NIM and Blueprints software that will expedite research and clinical development. Agentic AI perceive, reason, and act within complex and multi-step tasks, and have been referred to as \u201cdigital employees\u201d.\n\nDuring a press briefing ahead of the conference, Nvidia\u2019s healthcare vice president Kimberly Powell said: \u201cInitially, we\u2019re going to focus on clinical trials. This is the longest, most expensive workflow and drug and medical device development, our partnership will accelerate trial execution while significantly reducing administrative version burden through the deployment of agents of all kinds.\u201d\n\nThe partnership with Illumina aims to harness next-generation genomics to enhance drug discovery and human health. As part of the deal, Illumina will use Nvidia\u2019s accelerated computing and AI toolsets for multiomics analysis and workflows. Illumina expanded its reach in the multiomics space with the acquisition of Fluent BioSciences and its single-cell research technology in July 2024. Nvidia said that its technology on Illumina\u2019s platforms will help make analysis and insights of the human genome more accessible to life science researchers and pharmaceutical companies.\n\nIllumina\u2019s chief technology officer Steve Barnard said: \u201cBy combining Illumina\u2019s expertise in genomics data and analysis with Nvidia\u2019s powerful AI platforms, we aim to enable pharma and biotech companies to unlock their own multiomics data to uncover transformative insights and improve success rates in developing life-saving therapies.\u201d\n\nThe collaboration with the Mayo Clinic aims to progress digital pathology \u2013 a field which has transformed disease diagnosis by speeding up and improving histological slide analysis. Mayo Clinic has built a platform using robotic scanning that has a created dataset of 20 million whole-slide images with ten million patient-associated records. The medical centre will deploy Nvidia\u2019s Blackwell chip and imaging platform MONAI to handle the datasets.\n\nPowell said in the briefing: \u201cOur ultimate goal is to create a human digital twin \u2013 this is a dynamic digital representation, including medical imaging, pathology, health records, wearables\u2026and this collaboration will be a cornerstone for new applications in drug discovery and diagnostic medicine.\u201d\n\nNvidia now has over 3,500 healthcare members in its Inception programme, a free framework offered by the company to nurture startups and support co-marketing opportunities. Members of the ecosystem come from market segments such as digital health, surgical robotics, imaging, and biopharma, among others.\n\n\u201cWe\u2019ve added over 1,300 [members] just last year,\u201d Powell said. \u201c[Healthcare] is the singular, largest industry in the inception programme. Healthcare is a $10trn industry with over 30% of operating expenses dedicated to meeting the growing demand. To address this challenge, we need AI solutions of all kinds.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia Forges Partnerships to Tackle $10 Trillion Health Sector",
            "link": "https://www.pymnts.com/healthcare/2025/nvidia-forges-partnerships-to-tackle-10-trillion-health-sector/",
            "snippet": "Chipmaker Nvidia has launched a series of partnerships designed to boost the healthcare sector via artificial intelligence (AI).",
            "score": 0.8901471495628357,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has launched a series of partnerships designed to boost the healthcare sector via AI.\n\nThe collaborations include work with a number of major figures in the $10 trillion healthcare field, the chipmaker announced Monday (Jan. 13) at the J.P. Morgan Healthcare Conference in San Francisco.\n\n\u201cThe convergence of AI, accelerated computing and biological data is turning healthcare into the largest technology industry,\u201d Nvidia said in a news release. \u201cHealthcare leaders IQVIA, Illumina and Mayo Clinic, as well as Arc Institute, are using the latest Nvidia technologies to develop solutions that will help advance human health.\u201d\n\nThese solutions include artificial intelligence (AI) agents that can speed clinical trials by reducing administrative burden, AI models that learn from biology instruments to advance drug discovery and digital pathology and physical AI robots for surgery, patient monitoring and operations.\n\n\u201cAI agents, AI instruments and AI robots will help address the $3 trillion of operations dedicated to supporting industry growth and create an AI factory opportunity in the hundreds of billions of dollars,\u201d the company added.\n\nIn the case of the Mayo Clinic, the medical center will use Nvidia\u2019s tech to accelerate the development of new pathology foundation models. DNA sequencing and informatics technology provider Illumina, meanwhile, will employ Nvidia accelerated computing and AI toolsets for its multiomics analysis software and workflows.\n\n\u201cThis will help make analysis of \u2014 and insights from \u2014 the human genome more accessible to researchers, pharmaceutical companies and other life sciences customers,\u201d the release said.\n\nThe announcement comes one week after Nvidia CEO Jensen Huang introduced several new developments from the company, including next-generation chips, new large language models and a mini AI supercomputer, while also announcing a partnership with Toyota.\n\n\u201cIt\u2019s been an extraordinary journey, extraordinary year,\u201d Huang said to a packed crowd during a keynote address at the CES trade show in Las Vegas.\n\nLast year saw Nvidia become one of the world\u2019s most valuable companies in terms of market value. It currently occupies the No. 2 spot, at $3.2 trillion \u2014 right below $3.4 trillion Apple.\n\nIt was a year, PYMNTS wrote earlier this month, that saw AI capabilities move from simple automation to tackle complex tasks in fields like healthcare but also finance and manufacturing.\n\n\u201cBy year\u2019s end, tech\u2019s landscape had fundamentally shifted. AI wasn\u2019t just another feature or stock catalyst \u2014 it had become the lens through which companies viewed their futures,\u201d that report said. \u201cThe question shifted from whether AI would transform business to how companies would harness its potential.\u201d\n\nFor all PYMNTS AI coverage, subscribe to the daily AI Newsletter.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "NVIDIA Collaborates with IQVIA, Illumina, and Mayo Clinic to Drive Drug Discovery",
            "link": "https://hitconsultant.net/2025/01/13/nvidia-collaborates-with-iqvia-illumina-and-mayo-clinic/",
            "snippet": "NVIDIA announced collaborations with key players like IQVIA, Illumina, Mayo Clinic, and Arc Institute to accelerate drug discovery.",
            "score": 0.5946385860443115,
            "sentiment": null,
            "probability": null,
            "content": "What You Should Know:\n\n\u2013 NVIDIA announced collaborations with key players like IQVIA, Illumina, Mayo Clinic, and Arc Institute to accelerate drug discovery, enhance genomic research, and pioneer advanced healthcare services using agentic and generative AI.\n\n\u2013 These strategic partnerships and technologies mark a significant step forward in the application of AI in healthcare.\n\nAI is Transforming Healthcare\n\nThe convergence of AI, accelerated computing, and biological data is rapidly turning healthcare into a major technology industry. NVIDIA\u2019s latest technologies are empowering healthcare leaders to develop innovative solutions that will advance human health.\n\nKey Partnerships and Initiatives\n\nIQVIA: Leveraging NVIDIA AI Foundry services to build custom foundation models on its vast dataset and develop agentic AI solutions to speed research and clinical development.\n\nLeveraging NVIDIA AI Foundry services to build custom foundation models on its vast dataset and develop agentic AI solutions to speed research and clinical development. Illumina: Collaborating with NVIDIA to unlock the next generation of genomics for drug discovery and human health, making genomic analysis more accessible to researchers and life sciences companies.\n\nCollaborating with NVIDIA to unlock the next generation of genomics for drug discovery and human health, making genomic analysis more accessible to researchers and life sciences companies. Mayo Clinic: Partnering with NVIDIA to accelerate the development of next-generation pathology foundation models using NVIDIA DGX\u2122 Blackwell systems and MONAI, paving the way for AI applications in drug discovery, personalized diagnostics, and treatments.\n\nPartnering with NVIDIA to accelerate the development of next-generation pathology foundation models using NVIDIA DGX\u2122 Blackwell systems and MONAI, paving the way for AI applications in drug discovery, personalized diagnostics, and treatments. Arc Institute: Working with NVIDIA to develop and share powerful AI models and tools for biomedical discovery, scaling the potential of foundation models for biology and advancing applications in drug discovery, synthetic biology, and disease research.\n\n\u201cAI offers an exceptional opportunity to advance healthcare and life sciences with tools that help providers detect diseases earlier and discover new treatments faster,\u201d said Kimberly Powell, vice president of healthcare at NVIDIA. \u201cThe combination of NVIDIA\u2019s AI and accelerated computing capabilities with the expertise of industry leaders is poised to usher in a new era of medical and biological innovation and improve patient outcomes worldwide.\u201c",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-12": {
        "0": {
            "title": "Jim Cramer Reveals the Next Best Time to Buy NVIDIA (NVDA) Stock",
            "link": "https://finance.yahoo.com/news/jim-cramer-reveals-next-best-200657458.html",
            "snippet": "We recently published a list of Jim Cramer's Latest Lightning Round: Top 10 Stocks. In this article, we are going to take a look at where NVIDIA Corporation...",
            "score": 0.9145035147666931,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of Jim Cramer\u2019s Latest Lightning Round: Top 10 Stocks. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against other top stocks from Jim Cramer\u2019s Latest Lightning Round.\n\nJim Cramer in a recent program on CNBC recommended investors to avoid worrying when others are anxious or get too excited about something when others are also doing the same. Cramer said by the time an idea is common among investors, its price already reflects its potential.\n\n\u201cStocks don\u2019t quite travel at the speed of thought but they come pretty close. So the moment a preponderance of hedge fund and mutual fund managers decide that the economy is slowing, speeding up, or flatlining, stocks start trading like that\u2019s already the case. Usually, it takes some time to build that kind of consensus, which is why you rarely see these moves happening instantaneously. But once the big institutional portfolio managers are on the same page about something, you can be pretty darn confident that it\u2019s baked into the averages. This is some basic economics 101 stuff.\u201d\n\nIf every piece of news is priced in, does that mean it\u2019s meaningless to invest in stocks and you are better off putting your money in broader market funds? Cramer calls this idea \u201cbogus\u201d and says the market is pretty irrational at times and stocks are incorrectly priced, giving investors an opportunity.\n\n\u201cThe simple truth is that markets are not perfectly efficient. In fact, frankly, they\u2019re often irrational. They ignore things, make mistakes, misvalue information every day. And that\u2019s a major reason why anyone can make money picking individual stocks. These anomalies are everywhere, and they can be great for your portfolio,\u201d Cramer added.\n\nREAD ALSO: 7 Best Stocks to Buy For Long-Term and 8 Cheap Jim Cramer Stocks to Invest In\n\nFor this article, we watched the latest programs of Jim Cramer and picked 10 stocks he is talking about these days. With each stock, we have mentioned the number of hedge fund investors. Why are we interested in the stocks that hedge funds pile into? The reason is simple: our research has shown that we can outperform the market by imitating the top stock picks of the best hedge funds. Our quarterly newsletter\u2019s strategy selects 14 small-cap and large-cap stocks every quarter and has returned 275% since May 2014, beating its benchmark by 150 percentage points (see more details here).\n\nJim Cramer Reveals the Next Best Time to Buy NVIDIA (NVDA) Stock\n\nNVIDIA Corporation (NASDAQ:NVDA)\n\nNumber of Hedge Fund Investors: 193\n\nJim Cramer in a recent program said that there\u2019s \u201cnothing\u201d wrong with NVIDIA Corporation (NASDAQ:NVDA) and the company remains the market leader in the AI chips space. He also told investors when to buy NVIDIA Corp (NASDAQ:NVDA) shares.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia is preparing for the post-GPU AI era as it is reportedly recruits ASIC engineers to fend off competition from Broadcom and Marvell",
            "link": "https://www.techradar.com/pro/security/nvidia-is-preparing-for-the-post-gpu-ai-era-as-it-is-reportedly-recruiting-asic-engineers-to-fend-off-competition-from-broadcom-and-marvell",
            "snippet": "Nvidia CEO Jensen Huang confirmed in 2024 the company will recruit 1000 engineers in Taiwan. Now, as reported by Taiwan's Commercial Times.",
            "score": 0.9060184359550476,
            "sentiment": null,
            "probability": null,
            "content": "ASICs are far more efficient than GPUs for inference, not unlike mining cryptocurrency\n\nThe Inference AI chip market is expected to grow exponentially by the end of this decade\n\nHyperscalers like Google have already jumped on the bandwagon\n\nNvidia, already a leader in AI and GPU technologies, is moving into the Application-Specific Integrated Circuit (ASIC) market to address growing competition and shifting trends in AI semiconductor design.\n\nThe global rise of generative AI and large language models (LLMs) has significantly increased the demand for GPUs, and Nvidia CEO Jensen Huang confirmed in 2024 the company will recruit 1000 engineers in Taiwan.\n\nNow, as reported by Taiwan's Commercial Times (originally published in Chinese), the company has now established a new ASIC department and is actively recruiting talent.\n\nThe rise of inference chips\n\nNvidia\u2019s H series GPUs optimized for AI learning tasks have been widely adopted for training AI models. However, the AI semiconductor market is undergoing a shift toward inference chips, or ASICs.\n\nThis surge is driven by the demand for chips optimized for real-world AI applications, such as large language models and generative AI. Unlike GPUs, ASICs offer superior efficiency for inference tasks, as well as cryptocurrency mining.\n\nAccording to Verified Market Research, the inference AI chip market is projected to rise from a 2023 valuation of $15.8 billion to $90.6 billion by 2030.\n\nMajor tech players including Google have already embraced custom ASIC designs in its AI chip \"Trillium\", made generally available in December 2024.\n\nAre you a pro? Subscribe to our newsletter Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nThe shift toward custom AI chips has intensified competition among semiconductor giants. Companies such as Broadcom and Marvell have surged in relevance and stock value as they collaborate with cloud service providers to develop specialized chips for data centers.\n\nTo stay ahead, Nvidia\u2019s new ASIC department focuses on leveraging local expertise by recruiting from leading companies like MediaTek.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Think Nvidia Stock Is Expensive? This Chart Might Change Your Mind.",
            "link": "https://www.fool.com/investing/2025/01/12/think-nvidia-stock-is-expensive-this-chart-might-c/",
            "snippet": "Nvidia ... Nvidia (NVDA 2.63%) was one of the best-performing stocks of 2024. It even dwarfed the returns of the Nasdaq Composite index in a year the index itself...",
            "score": 0.8009443879127502,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) was one of the best-performing stocks of 2024. It even dwarfed the returns of the Nasdaq Composite index in a year the index itself returned an amazing 28.6%. Its stock price soared as the market became enthused by consistent triple-digit revenue gains that led to triple-digit net income gains. That led to a 171% increase in Nvidia's stock price in 2024. It also pushed the stock's price-to-sales ratio to 30.\n\nThose who might think that means the stock is now too expensive should look at the opportunity still ahead. Enough opportunity can trump a high P/S. One chart might help explain.\n\nThe market opportunity tells the story\n\nBig tech companies continue to ramp up spending to build artificial intelligence (AI) infrastructure. Microsoft and Amazon each recently announced commitments to invest $80 billion and $11 billion, respectively, in the coming months. The investments are for data centers to train AI models, and distribute AI and cloud-based applications. That's because the market for generative AI is expected to grow tenfold over the remainder of the decade.\n\nGenerative AI has a wide array of applications. It is already being used to generate content and enhance creativity. Businesses use it to enhance customer experiences by adapting content and services to individual preferences. It can enhance existing data, improve cost efficiency, foster innovation, and aid in simulations and planning. Investors themselves are using generative AI for multiple reasons.\n\nNvidia will be a major beneficiary of the investments to build out the needed infrastructure. Its Blackwell architecture is already in high demand, and the company will have its new Rubin platform available next year to succeed Blackwell.\n\nCEO Jensen Huang also introduced a new desktop supercomputer at the recent CES conference in Las Vegas. The $3,000 unit can be used by researchers, data scientists, and even students. It shows that Nvidia is broadening the market for its advanced chips beyond just hyperscaler data centers.\n\nWith a quickly growing market for generative AI and innovative products, Nvidia has enough business opportunity ahead to justify the elevated valuation.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "I wasn\u2019t planning on building a new gaming PC but the RTX 5080 just changed my mind",
            "link": "https://www.tomsguide.com/computing/hardware/i-wasnt-planning-on-building-a-new-gaming-pc-but-the-rtx-5080-just-changed-my-mind",
            "snippet": "Nvidia's upcoming GeForce RTX 50-series cards could give us a good excuse to upgrade or build an all-new PC. Here's why the RTX 5080 seems so compelling.",
            "score": 0.8738433718681335,
            "sentiment": null,
            "probability": null,
            "content": "I wasn\u2019t surprised when Nvidia finally unveiled the GeForce RTX 50-series GPUs during CES 2025. Not only had I seen several upcoming PCs with \u201cnext-gen Nvidia\u201d graphics cards at events before CES, but there had also been months of rumors and leaks\u2014many of which were accurate. Yes, I knew RTX 50 cards were coming, but I didn\u2019t know the reveal would convince me to upgrade my gaming PC.\n\nI bought a gaming PC with an Nvidia RTX 3080 Ti almost three years ago, and it has done an excellent job of running the best PC games at 4K resolution and over 60 frames per second with settings set to High or even Max. I was so satisfied with the experience that I skipped the Nvidia RTX 40 series. However, I\u2019ve had to lower graphical settings more frequently as games have become more graphically demanding. That\u2019s a sign that it\u2019s time for me to upgrade.\n\nNvidia GeForce RTX 5090, RTX 5080, RTX 5070 Ti and RTX 5070 graphics cards will be available soon. Of those four, I have my eyes on the Nvidia GeForce RTX 5080. I\u2019ll explain why below.\n\nMy PC is (kind of) ancient\n\n(Image credit: Tom's Guide)\n\nMy current gaming PC might not have impressive specs now but it was kind of a monster back when I had NZXT build it for me. It has an 11th Gen Intel Core i7 11700KF CPU, an Nvidia GeForce RTX 3080 Ti, 32GB of GDDR4 RAM, 1TB of SSD storage and an ROG Strix Z590-E motherboard.\n\nIn all fairness to my rig, it\u2019s still quite capable of handling most games so long as I properly tweak the settings or let the Nvidia Experience app configure them. But with games becoming more demanding, my PC is showing its age. Given that and what the RTX 50-series promises to deliver, I think now is the best time for me to upgrade my gaming rig.\n\nBalance of price and performance\n\n(Image credit: Nvidia)\n\nI\u2019m sure you\u2019re wondering why I\u2019m not eyeing the Nvidia RTX 5090 i.e. the king of the RTX 50-series lineup. Don\u2019t get me wrong, the RTX 5090 is very tempting with its monstrous 32GB of VRAM and enhanced AI capabilities. However, if you\u2019ve researched the RTX 5090 then I\u2019m sure you already know why I\u2019m opting for the RTX 5080. The RTX 5090 is one heck of an expensive GPU!\n\nSwipe to scroll horizontally Nvidia GeForce RTX 50-series desktop GPUs GPU name Starting price Graphics memory RTX 5090 $1.999 32GB GDDR7 RTX 5080 $999 16GB GDDR7 RTX 5070 Ti $749 16GB GDDR7 RTX 5070 $549 12GB GDDR7\n\nAt launch, the Nvidia RTX 5080 will cost an eye-watering $1,999. That\u2019s almost half what I paid for my current rig back in 2022. Toss in other components I want, such as a 14th Gen Intel Core i9 14900K CPU, 64GB DDR5 RAM, 2TB of SSD storage \u2014 along with a motherboard, power supply, and PC case (among other things), and I\u2019m likely to spend a small fortune on a new gaming rig.\n\nIf I can lower the cost of my next gaming PC by $1,000 by opting for an RTX 5080 instead of a 5090, that seems like the smarter play.\n\nOutlook\n\nWhile I\u2019m excited about the new Nvidia GeForce RTX 50-series cards, I won\u2019t make a final purchasing decision until we\u2019ve tested the new GPUs ourselves. Not only do I want raw performance metrics, but I also want to see how graphically demanding games like Cyberpunk 2077 benefit from DLSS4 \u2014 especially at high graphical settings.\n\nThe Nvidia RTX 5080 and RTX 5090 arrive on January 30. If the former card lives up to the hype, then it\u2019ll be time for me to build an all-new rig. If not, perhaps I\u2019ll save some extra money and buy its more powerful sibling. Either way, 2025 should be the year when I upgrade to a new PC \u2014 and it\u2019s all due to Nvidia and its impressive (on paper) graphics cards.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "NVIDIA takes on physical AI for automotive, industrial and robotics",
            "link": "https://www.rcrwireless.com/20250112/ai-ml/nvidia-takes-on-physical-ai-for-automotive-industrial-robotics",
            "snippet": "Simulation, prediction, control\u2014NVIDIA has a new world model meant to lend autonomous mobile systems the intuition they need to operate in the real world.",
            "score": 0.9089046120643616,
            "sentiment": null,
            "probability": null,
            "content": "At CES NVIDIA CEO Jensen Huang proposes a three-computer solution to the figurative three-body problem of physical AI\u2014using a digital twin to connect and refine physical AI training and deployment\n\nWhether it\u2019s an autonomous vehicle (AV), highly-digitized, lights-out manufacturing environments or use case involving humanoid robotics, NVIDIA CEO Jensen Huang, speaking in a keynote during the Consumer Electronics Show (CES) earlier this week in Las Vegas, Nevada, sees it as a three-body problem with a three-computer solution.\n\nFirst things first, the Three-Body Problem, from 2008, is the first book in a trilogy by Chinese author Liu Cixin. The titular \u201cproblem\u201d is a physics classic\u2014how do you calculate the trajectories of three co-orbiting celestial bodies at a point in time using Newtonian mathematics. In the novels, an alien race\u2019s approach to solving the three-body problem sets off a multi-generational thriller that\u2019s well worth the read. In Huang\u2019s keynote, the three-body problem of training, deploying and continuously optimizing objects with autonomous mobility, is addressed by a three-computer solution.\n\n\u201cEvery robotics company will ultimately have to build three computers,\u201d Huang said. \u201cThe robotics system could be a factory, the robotics system could be a car, it could be a robot. You need three fundamental computers. One computer, of course, to train the AI\u2026Another, of course, when you\u2019re done, to deploy the AI\u2026that\u2019s inside the car, in the robot, or in an [autonomous mobile robot]\u2026These computers are at the edge and they\u2019re autonomous. To connect the two, you need a digital twin\u2026The digital twin is where the AI that has been trained goes to practice, to be refined, to do its synthetic data generation, reinforcement learning, AI feedback and such and such. And so it\u2019s the digital twin of the AI.\u201d\n\nSo, he continued, \u201cThese three computers are going to be working interactively. NVIDIA\u2019s strategy for the industrial world, and we\u2019ve been talking about this for some time, is this three-computer system. Instead of a three-body problem, we have a three-computer solution.\u201d\n\nAnd those three computers are: the NVIDIA DGX platform for AI training, including hardware, software and services; the NVIDIA AGX platform, essentially a computer to support computationally-intensive edge AI inferencing; and then a digital twin to connect the training and inferencing which is NVIDIA Omniverse, a simulation platform made up of APIs, SDKs and services.\n\nHere\u2019s what\u2019s new. At CES, Huang announced NVIDIA Cosmos, a world foundation model trained on 20 million hours of \u201cdynamic physical things,\u201d as the CEO put it. \u201cCosmos models ingest text, image or video prompts and generate virtual world states as videos. Cosmos generations prioritize the unique requirements of AV and robotics use cases, like real-world environments, lighting and object permanence.\u201d\n\nHuang continued: \u201cDevelopers use NVIDIA Omniverse to build physics-based, geospatially accurate scenarios, then output Omniverse renders into Cosmos, which generates photoreal, physically-based synethic data.\u201d So AGX trains the physical AI, DGX runs edge inferencing for the physical AI, and the combo of Cosmos and Omniverse creates a loop between a digital twin and a physical AI model that devs \u201ccould have\u2026generate multiple physically-based, physically-plausible scenarios of the future\u2026Because this model understands the physical world\u2026you could use this foundation model to train robots\u2026The platform has an autoregressive model for real time applications, has diffusion model for a very high quality image generation\u2026And a data pipeline so that if you would like to take all of this and then train it on your own data, this data pipeline, because there\u2019s so much data involved, we\u2019ve accelerated everything end to end for you.\u201d\n\nThis idea of using a world foundation model, and other computing platforms, to give autonomous mobile systems the ability to operate effectively and naturally in the real world reminds me of a section from the book Out of Control by Kevin Kelly where he examines \u201cprediction machinery.\u201d One bit is based on a conversation with Doyne Farmer who, when the book was published, was focused on making and monetizing short-term financial market predictions.\n\nFrom the book: \u201cFarmer contends you have a model in your head of how baseballs fly. You could predict the trajectory of a high-fly using Newton\u2019s classic equation of f=ma, but your brain doesn\u2019t stock up on elementary physics equations. Rather, it builds a model directly from experiential data. A baseball player watches a thousand baseballs come off a bat, and a thousands times lifts his gloved hand, and a thousand times adjusts his guess with his mitt. Without knowing how, his brain gradually compiles a model of where the ball lands\u2014a model almost as good as f=ma, but not as generalized.\u201d\n\nKelly continues to equate \u201cprediction machinery\u201d with \u201ctheory-making machinery\u2014devices for generating abstractions and generalizations. Prediction machinery chews on themes of seemingly random chicken-scratched data produced by complex and living things. If there is a sufficiently large stream of data over time, the device can discern a small bit of a pattern. Slowly the technology shapes an internal ad-hoc model of how the data might be produced\u2026Once it has a general fit\u2014a theory\u2014it can make a prediction. In fact prediction is the whole point of theories.\u201d\n\nIt appears NVIDIA is combining cutting-edge, high-performance compute, AI, the new world foundation model and other bits of tech, to essentially give robots the type of intuition that humans rely on. And systematizing intuition (simulations and predictions) and making it reliably available at scale to the worlds of AVs, heavy industry and robotics could prove to be a breakthrough in the control of our physical world.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Everything You Want To Know About Nvidia Project Digits AI Supercomputer",
            "link": "https://www.forbes.com/sites/janakirammsv/2025/01/12/everything-you-want-to-know-about-nvidia-project-digits-ai-supercomputer/",
            "snippet": "Nvidia Digits represents a significant leap forward in AI technology. By combining powerful hardware, a comprehensive software stack and a compact,...",
            "score": 0.9441378116607666,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "NRF | AI in the real world: Standing by to make you smarter, more productive",
            "link": "https://nrf.com/blog/ai-in-the-real-world-standing-by-to-make-you-smarter-more-productive",
            "snippet": "Walmart U.S. President and CEO John Furner speaks with NVIDIA Vice President and General Manager, Retail & CPG Azita Martin at NRF 2025: Retail's Big Show.",
            "score": 0.8565648198127747,
            "sentiment": null,
            "probability": null,
            "content": "With so many options for incorporating artificial intelligence \u2014 back office, inventory, customer experience, personalization \u2014 it can be tough to know where to start. That doesn\u2019t matter, according to the opening session at NRF 2025: Retail\u2019s Big Show. Just start.\n\nJohn Furner, president and CEO, Walmart U.S., joined Azita Martin, vice president and general manager, Retail & CPG, NVIDIA, on stage for a conversation that ran the gamut from current examples of AI use in retail to the importance of risk-taking and ongoing learning.\n\nMartin recalled a quote from NVIDIA CEO Jensen Huang. \u201cSomone asked, \u2018Is ChatGPT going to take our job?\u2019 He made a point that has always stood out with me,\u201d she said. \u201cI always use it on his behalf. And that is, \u2018No, but someone using generative AI may take your job.\u2019 So, embrace it. It\u2019s a tool that just makes you more productive, and it\u2019s going to make you so much smarter.\u201d\n\nNVIDIA, a leader in artificial intelligence computing, recently announced its AI Blueprint for retail shopping assistants, a generative AI reference workflow designed to transform shopping experiences online and in stores. It helps developers create AI-powered digital assistants that work with and support human workers.\n\nThe company also unveiled Mega, an Omniverse Blueprint for developing, testing and optimizing physical AI and robot fleets at scale in a digital twin before deployment into real-world facilities. Then there\u2019s Nemotron, a family of models that have both language skills and perception, helping developers create and deploy AI agents across a range of applications. \u201cIt\u2019s to the physical world what ChatGPT is to text and language,\u201d Martin said.\n\nMartin spoke of L\u2019Or\u00e9al combining digital twins of assets with generative AI models, fostering faster \u2014 and more engaging \u2014 creation of ads and marketing campaigns. Walmart has also worked with NVIDIA\u2019s data science acceleration libraries to focus on forecasting. \u201cFor a company of Walmart\u2019s scale, we know that even one percent improvement in forecast accuracy represents a significant amount of business benefits,\u201d she said.\n\nBoth Furner and Martin expressed excitement for the use of artificial intelligence in the supply chain. The ability to create a physically accurate digital twin of a store or distribution center, for example, allows the simulation of different layouts and the ability to observe how people and objects interact before capital investment is made.\n\nLowe\u2019s has created a digital twin of 1,700 of its stores, Martin said, updating several times a day with operational and inventory data. \u201cAs a result, it will simulate different layouts, to really optimize how customers are shopping in the stores,\u201d she said, \u201chow to change their layouts, and ultimately improve their sales and revenue.\u201d The company is now trying 3D digital twins with its planograms to optimize merchandising, she said.\n\nFurner likes to think of digital twins as akin to a video game. \u201cIt\u2019s an idea in your head, and there\u2019s a reality you\u2019ve created, you simulate, and play in it, and results happen,\u201d he said. \u201cYou can do the same thing in business.\u201d\n\nFurner urged show attendees to take full advantage of the AI exhibits, sessions and discussions throughout the event. \u201cListen, learn, ask questions,\u201d he said. \u201cWe put this together for all of you, and we want you to benefit from it.\u201d",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NRF 2025: Retail's Big Show | January 12-14",
            "link": "https://www.nvidia.com/en-sg/events/nrf-retails-big-show/",
            "snippet": "Join NVIDIA at NRF 2025 to learn how AI and generative AI are helping retailers personalize consumer recommendations, optimize store layout, and more.",
            "score": 0.9365193247795105,
            "sentiment": null,
            "probability": null,
            "content": "This site requires Javascript in order to view all its content. Please enable Javascript in order to access all the functionality of this web site. Here are the instructions how to enable JavaScript in your web browser.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Massive exit: NVIDIA acquires Israeli company",
            "link": "https://www.jpost.com/consumerism/consumer-news/article-835656",
            "snippet": "The U.S. Department of Justice has approved the acquisition of the Israeli company Run:ai by NVIDIA, and the deal is now set to proceed.",
            "score": 0.6604097485542297,
            "sentiment": null,
            "probability": null,
            "content": "After facing regulatory hurdles, NVIDIA has now received approval from the U.S. regulator to acquire the Israeli company Run:ai. The deal, initially announced in April but delayed since, is finally moving forward. While the exact amount remains undisclosed, estimates suggest it to be around $700 million.\n\nThe deal underwent scrutiny by the U.S. Department of Justice\u2019s antitrust division and the European Union's competition authority. Recently, the U.S. regulator gave its approval, following earlier approval by the European Commission, the EU's executive branch, earlier this month.\n\nEven before the acquisition, Run:ai had a partnership with NVIDIA, developing an operating system for the new AI processors, based on advanced virtualization technologies. This technology, developed by Run:ai, allows clients to maximize output from the chips, a particularly sought-after service given the growing demand for chips that now surpasses supply.\n\nRun:ai founders Omri Geller and Ronen Dar with Aya Peterburg, a partner at S Capital which is Run:ai's largest investor (credit: Sharon Levin)\n\nRun:ai was founded in 2018 by Omri Geller, the company\u2019s CEO, who previously worked in a technological unit of the Prime Minister\u2019s Office, and Dr. Ronen Dar, a former researcher at Bell Labs and an algorithm engineer at Anobit, which was sold to Apple.\n\nSince its inception, the company has raised $118 million. Its largest investor is S Capital, a fund led by Aya Peterburg and Haim Sadger, which invested in Run:ai from its early stages. Other early-stage investors include the TLV Partners fund, with later-stage investments from Tiger Global and Insight Partners.\n\nNVIDIA, a tech giant producing AI chips, became the world's most valuable company in November, surpassing Apple following a 890% increase in value over two years, reaching a market cap of $3.36 trillion. NVIDIA also joined the Dow Jones Index in November, replacing Intel. The index includes the stocks of 30 selected large-cap companies.\n\nAya Peterburg, a partner at S Capital, stated: \"Israel has not only embraced the AI revolution but is uniquely positioned to leverage the opportunities it brings, particularly in technological infrastructure. AI is still in its early stages of development.\n\n\"NVIDIA, founded in 1993, is a clear example\u2014its phenomenal success only came decades later with the breakthrough of advanced graphical processing applications for AI. Similarly, with Run:ai, we observed a comparable pattern: when we invested in the company, the market was unclear and directions were uncertain, but we identified an extraordinary founding team that recognized opportunities in infrastructure\u2014a critical area that forms the foundation of the AI revolution.\n\n\"Now, as the AI ecosystem is still taking shape, this is a pivotal period allowing Israeli entrepreneurs to leverage their capabilities and stand out on the global stage. The current opportunities lie not only in creating applications but also in laying the groundwork for a new generation of AI technologies.\"",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "How to Switch Primary GPU to NVIDIA on Wayland for KDE Plasma and GNOME",
            "link": "https://9to5linux.com/how-to-switch-primary-gpu-to-nvidia-on-wayland-for-kde-plasma-and-gnome",
            "snippet": "Here's a quick tutorial on how to switch the primary GPU to NVIDIA on Wayland. I've tested this tutorial on the GNOME and KDE Plasma desktop environments using...",
            "score": 0.9296455979347229,
            "sentiment": null,
            "probability": null,
            "content": "Most distros default to the integrated GPU (e.g. AMD or Intel) of your computer, whether it\u2019s a laptop or a desktop workstation, so here\u2019s a quick tutorial on how to switch the primary GPU to NVIDIA on Wayland.\n\nI\u2019ve tested this tutorial on the GNOME and KDE Plasma desktop environments using Wayland since they\u2019re the most popular amongst Linux users, but it should work on other desktops if they use either Plasma\u2019s KWin or GNOME\u2019s Mutter window and composite managers.\n\nI must emphasize that this tutorial is for those using Wayland by default. If you\u2019re using an X11/Xorg session, you should check out the instructions I provided in my How to Connect Your Linux Laptop to an External Monitor (Fix for HDMI \u201cNo Signal\u201d Issue) tutorial on how to switch the primary GPU to NVIDIA.\n\nSo basically, all we have to do is pass an environment variable or set an udev rule. But first, check what GPU your system is using by default by running the following command in your favorite terminal emulator.\n\nglxinfo | egrep \"OpenGL vendor|OpenGL renderer\"\n\nSwitch the primary GPU to NVIDIA on GNOME\n\nFirst, for the GNOME desktop environment, we must create a new file called 61-mutter-primary-gpu.rules in the /etc/udev/rules.d/ directory. To do that, run the following command in your favorite Terminal app, assuming you have the GNU nano editor installed (or whatever command-line editor you prefer to use).\n\nsudo nano /etc/udev/rules.d/61-mutter-primary-gpu.rules\n\nThis command will create the 61-mutter-primary-gpu.rules file and open GNU nano so you can paste the following udev rule to switch the primary GPU to NVIDIA. Please note that not all distros have the rules.d directory in the /usr/lib/udev/ path, so you must find it and adjust the command accordingly.\n\nENV{DEVNAME}==\"/dev/dri/card0\", TAG+=\"mutter-device-preferred-primary\"\n\nIn the udev rule above, which we set in the 61-mutter-primary-gpu.rules file, /dev/dri/card0 is the NVIDIA GPU on my system. To check the location of your GPUs, run the commands below in the terminal emulator. The first command will show you the PCI number for each GPU (e.g. 01:00.0 for card0 and 05:00.0 for card 1).\n\nls -l /dev/dri/by-path/\n\nWith the second command, you will identify which GPU is card0 based on the PCI number assigned. So, with these two commands, we\u2019ve identified that the NVIDIA GPU is card0 with the 01:00.0 PCI number.\n\nlspci -k | grep -EA3 'VGA|3D|Display'\n\nSwitch the primary GPU to NVIDIA on KDE Plasma\n\nFor the KDE Plasma desktop environment, we need to create a file called environment.d/90-nvidia.conf in the ~/.config/environment.d/ directory that you also need to create. Run the commands below, the first one to create the directory, and the second one to create the configuration file.\n\nmkdir ~/.config/environment.d/ nano ~/.config/environment.d/90-nvidia.conf\n\nNow, paste the following lines in the environment.d/90-nvidia.conf file, save it, and exit nano.\n\n__NV_PRIME_RENDER_OFFLOAD=1 __GLX_VENDOR_LIBRARY_NAME=nvidia __VK_LAYER_NV_optimus=NVIDIA_only\n\nThat\u2019s it, now reboot your computer to use your NVIDIA graphics card on Wayland as the primary GPU. Verify if NVIDIA is the default graphics card via the command line or graphically in the System Settings app.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-11": {
        "0": {
            "title": "How Nvidia is creating a $1.4T data center market in a decade of AI",
            "link": "https://siliconangle.com/2025/01/11/nvidia-creating-1-4t-data-center-market-decade-ai/",
            "snippet": "A trillion-dollar-plus data center business is poised for transformation, powered by what we refer to as extreme parallel computing, or EPC.",
            "score": 0.7906681299209595,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia Nemotron Models Aim to Accelerate AI Agent Development",
            "link": "https://www.infoq.com/news/2025/01/nvidia-nemotron-agents/",
            "snippet": "Nvidia has launched Llama Nemotron large language models (LLMs) and Cosmos Nemotron vision language models (VLMs) with a special emphasis on workflows...",
            "score": 0.703709602355957,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia introduced Llama Nemotron large language models (LLMs) and Cosmos Nemotron vision language models (VLMs) with a special emphasis on workflows powered by AI agents such as customer support, fraud detection, product supply chain optimization, and more. Models in the Nemotron family come in Nano, Super, and Ultra sizes to better fit the requirements of diverse systems.\n\nAI agents are a new frontier of generative AI evolution, says Nvidia, aiming to create systems able to act autonomously to carry complex tasks through. This requires combining language skills, as displayed by LLMs, with the ability to perceive and interact with the environment.\n\nTo be effective, many AI agents need both language skills and the ability to perceive the world and respond with the appropriate action.\n\nThat explains why the Nemotron Model family includes models derived from Meta's LLaMA models as well as new Cosmos Nemotron VLMs that enable analyzing and responding to images and video captured in the user environment.\n\nThe availability of agents with vision capabilities, says Nvidia, could make it feasible to analyze videos from industrial cameras in a multitude of environments in real-time to help detect incidents, reduce defects, or guide humans through some course of action. Currently, according to the company, less than 1% of video from industrial cameras is watched live by humans.\n\nAccording to Nvidia, they trained Llama Nemotron models to efficiently execute a number of common agentic tasks so you can use just one single model whereas you would normally use multiple specialized models.\n\nThe models are pruned to reduce latency and improve compute efficiency, then retrained using a hiqh-quality dataset with distillation and alignment methods to increase accuracy across tasks. This results in smaller models with high accuracy and throughput.\n\nNemotron models are optimized for distinct compute requirements, including Nano for PC application developers, Super to provide high performance on a single GPU, and Ultra, designed for data-center-scale applications.\n\nThe Nvidia Nemotron ecosystem also includes Nvidia NeMo to customize models with proprietary data, and NeMo Aligner to better align a model to follow instruction and generate human preferred responses. Additionally, Nvidia provides Nvidia AI Blueprints as a tool to quickly create AI agents by using NIM microservices as building blocks to serve Nemotron models.\n\nOn a related note, Nvidia also announced its Cosmos world foundation models which are specially tailored to generate physics-aware videos for robotics and autonomous vehicles.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Uber (UBER) and NVIDIA Partner to Accelerate AI-Driven Autonomous Driving Technology",
            "link": "https://finance.yahoo.com/news/uber-uber-nvidia-partner-accelerate-175915801.html",
            "snippet": "We recently published a list of 13 AI News and Ratings You Probably Missed. In this article, we are going to take a look at where Uber Technologies,...",
            "score": 0.8515881299972534,
            "sentiment": null,
            "probability": null,
            "content": "We recently published a list of 13 AI News and Ratings You Probably Missed. In this article, we are going to take a look at where Uber Technologies, Inc. (NYSE:UBER) stands against other AI stocks.\n\nAs per a CNBC report, in 2025, significant changes are expected in global AI regulation, especially in the U.S., the EU, and the UK. President-elect Donald Trump, who is supported by business leaders like Elon Musk, may influence AI policies, with Musk\u2019s experience in AI raising expectations for U.S. advancements. In Europe, the EU\u2019s AI Act has introduced comprehensive regulations, but tensions with U.S. tech companies are still there over its strict measures.\n\nMeanwhile, the U.K. is considering a more flexible, principles-based approach, especially regarding the use of copyrighted content for AI training. Additionally, U.S. and China relations could affect global AI development, with both countries competing for AI dominance and potentially creating safety regulations to prevent uncontrolled AI advancements.\n\nREAD ALSO: 15 AI Stocks That Skyrocketed in Q4 and 14 AI Stocks Making Waves on Wall Street.\n\nNavigating the AI Investment Landscape\n\nAI is driving a major transformation in technology as significant investments are being funneled into infrastructure, data centers, and the AI value chain. Companies are rapidly building out AI infrastructure, with an increasing focus on supply chain components beyond just semiconductors, which highlight potential profits in sectors like memory, cooling, and fiber optics.\n\nJon McNeill, co-founder of Vistashares, discussed the impact of AI infrastructure on investment opportunities in an interview with Romaine Bostick and Scarlet Fu of Bloomberg. He explained that while AI\u2019s impact on data centers is often discussed in terms of semiconductors, the actual supply chain involves different elements, such as memory, cooling, and fiber optics. McNeill emphasized the potential for profits in the AI supply chain, especially for investors looking beyond the major producers to lesser-known companies.\n\nHe also addressed the risks of international investments and noted that while the U.S. dominates the AI sector, Asia is rapidly building its own AI infrastructure. McNeill highlighted the growing value of AI applications in autonomous driving and robotics, which could drive significant economic growth. He pointed to large investments in data centers, such as AWS\u2019 $11 billion investment in Georgia, as signs that this growth will continue, with the major tech companies leading the charge in the AI race.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Jensen Huang Just Delivered Trillion-Dollar News to Nvidia Investors",
            "link": "https://www.fool.com/investing/2025/01/11/jensen-huang-just-delivered-trillion-dollar-news/",
            "snippet": "CEO Jensen Huang recently delivered a trillion-dollar message to Nvidia investors that could signal more gains over the long term.",
            "score": 0.9108118414878845,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) has offered investors good news on many occasions in recent years. From reports of record revenue and high demand for its new products to an invitation last year to join the Dow Jones Industrial Average, the chipmaker has been on fire. This is because Nvidia has built an empire in today's hottest growth area: artificial intelligence (AI).\n\nThe tech giant is the source of the world's most sought-after chips as well as a wide variety of related products and services -- and the world's biggest companies, from Amazon to Tesla, are spending massive sums on them. These companies use Nvidia's tools to power their AI programs or in the case of cloud infrastructure providers like Amazon Web Services (AWS), to offer their processing power to their customers. All of this helped lift Nvidia's stock price by 171% last year -- the best performance of any Dow component.\n\nBut Nvidia's exciting news is far from over. CEO Jensen Huang recently delivered a trillion-dollar message to Nvidia investors that could signal more gains over the long term.\n\nAn entire ecosystem of AI products\n\nLet's start by taking a quick look at Nvidia's story so far. The company sells the world's top graphic processing units (GPUs), chips capable of providing the rapid parallel-processing power required for tasks such as the training and inferencing of AI models. It has also built an entire ecosystem of AI products, including an enterprise software platform and tools that cater to specific industries such as automobiles or healthcare.\n\nAll of this has helped Nvidia's revenue take off: It reached a record of more than $35 billion in the company's most recent fiscal quarter, Q3 2025. To offer some perspective, its top line for the entirety of its fiscal 2023 was just $27 billion.\n\nNow, let's consider the recent commentary from Huang. Speaking at CES this past week in Las Vegas, he offered many updates on Nvidia's platforms, announced new products, and even gave investors some insight into the potential of AI over time. He also spoke of specific industries and how they would use AI, which brings us to his trillion-dollar comment.\n\nA multi-trillion-dollar industry\n\nIn reference to the autonomous vehicle market, Huang said, \"I predict that this will likely be the first multitrillion-dollar robotics industry.\"\n\nNoting that there are more than a billion vehicles on the road globally that drive more than a trillion total miles every year, and forecasting that in the future, many of these vehicles will be heavily autonomous, he concluded, \"this is going to be a very, very large industry.\"\n\nIn autonomous vehicles, Nvidia already has reached a $5 billion annual revenue run rate, and it's working with every major car company around the world. It also announced that its next-generation car computer, Thor, is in full production. Thor takes data from sensors, processes it, and turns it into driving information. This generation of Thor has 20 times the processing capacity of its predecessor, representing gains in speed and quality.\n\nNvidia offers its customers three computers for the construction and use of autonomous vehicles: one to train AI models, another to test drive and produce synthetic data, and a third that serves as a supercomputer in the car. So, Nvidia could help automakers through every stage of autonomous vehicle development, and remain a key supplier of systems for the final products. This makes it a one-stop shop for any company aiming to add autonomous features to its vehicles or go all in on self-driving cars.\n\nAt CES, Huang announced a big win: Nvidia just signed an agreement to help Toyota create its next-generation autonomous vehicle.\n\nWhat does this mean for investors?\n\nSo, what does all of this mean for investors? We often think of Nvidia as a designer of AI chips -- and it is. But the company has become so much more, offering entire platforms for high-growth industries such as autonomous vehicles. And this particular industry represents a massive long-term growth opportunity.\n\nAll of this suggests that Nvidia's high-flying days are far from over. Even if the stock takes a pause at some point, over time, this AI player has what it takes to keep soaring.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia\u2019s AI empire: A look at its top startup investments",
            "link": "https://techcrunch.com/2025/01/11/nvidias-ai-empire-a-look-at-its-top-startup-investments/",
            "snippet": "Below is a list of startups that raised rounds exceeding $100 million over the past two years where Nvidia is a named participant.",
            "score": 0.8928889036178589,
            "sentiment": null,
            "probability": null,
            "content": "No company has capitalized on the AI revolution more dramatically than Nvidia. Its revenue, profitability, and cash reserves have skyrocketed since the introduction of ChatGPT over two years ago \u2014 and the many competitive generative AI services that have launched since. And its stock price soared.\n\nDuring that period, the world\u2019s leading high-performance GPU maker has used its ballooning fortunes to significantly increase investments in all sorts of startups but particularly in AI startups.\n\nThe chip giant ramped up its venture capital activity in 2024, participating in 49 funding rounds for AI companies, a sharp increase from 34 in 2023, according to PitchBook data. It\u2019s a dramatic surge in investment compared to the previous four years combined, during which Nvidia funded only 38 AI deals. Note that these investments exclude those made by its formal corporate VC fund, NVentures, which also significantly ramped up its investing in the last two years. (PitchBook says NVentures engaged in 24 deals in 2024, compared to just 2 in 2022.)\n\nIn 2025, Nvidia has already participated in seven rounds.\n\nNvidia has stated that the goal of its corporate investing is to expand the AI ecosystem by backing startups it considers to be \u201cgame changers and market makers.\u201d\n\nBelow is a list of startups that raised rounds exceeding $100 million where Nvidia is a named participant since 2023, including new ones it has backed so far in 2025, organized from the highest amount to lowest raised in the round.\n\n\n\nThe billion-dollar-round club\n\nOpenAI: Nvidia backed the ChatGPT maker for the first time in October, reportedly writing a $100 million check toward a colossal $6.6 billion round that valued the company at $157 billion. The chipmaker\u2019s investment was dwarfed by OpenAI\u2019s other backers, notably Thrive, which according to the New York Times invested $1.3 billion.\n\nxAI: Nvidia participated in the $6 billion round of Elon Musk\u2019s xAI. The deal revealed that not all of OpenAI\u2019s investors followed its request to refrain from backing any of its direct competitors. After investing in the ChatGPT maker in October, Nvidia joined xAI\u2019s cap table a few months later.\n\nInflection: One of Nvidia\u2019s first significant AI investments also had one of the most unusual outcomes. In June 2023, Nvidia was one of several lead investors in Inflection\u2019s $1.3 billion round, a company founded by Mustafa Suleyman, who earlier founded DeepMind. Less than a year later, Microsoft hired Inflection AI\u2019s founders, paying $620 million for a non-exclusive technology license, leaving the company with a significantly diminished workforce and a less defined future.\n\nWayve: In May, Nvidia participated in a $1.05 billion round for the U.K.-based startup, which is developing a self-learning system for autonomous driving. The company is testing its vehicles in the U.K. and the San Francisco Bay Area.\n\nScale AI: In May 2024, Nvidia joined Accel and other tech giants Amazon and Meta to invest $1 billion in Scale AI, which provides data-labeling services to companies for training AI models. The round valued the San Francisco-based company at nearly $14 billion.\n\nThe many-hundreds-of-millions-of-dollars club\n\nCrusoe: A startup building data centers reportedly to be leased to Oracle, Microsoft, and OpenAI raised $686 million in late November, according to an SEC filing. The investment was led by Founders Fund, and the long list of other investors included Nvidia.\n\nFigure AI: In February 2024, AI robotics startup Figure raised a $675 million Series B from Nvidia, OpenAI Startup Fund, Microsoft, and others. The round valued the company at $2.6 billion.\n\nMistral AI: Nvidia invested in Mistral for the second time when the French-based large language model developer raised a $640 million Series B at a $6 billion valuation in June.\n\nLambda: AI cloud provider Lambda, which provides services for model training, raised a $480 million Series D at a reported $2.5 billion valuation in February. The round was co-led by SGW and Andra Capital Lambda, and joined by Nvidia, ARK Invest and others. A significant part of Lambda\u2019s business involves renting servers powered by Nvidia\u2019s GPUs.\n\nCohere: In June, Nvidia invested in Cohere\u2019s $500 million round, a large language model provider serving enterprises. The chipmaker first backed the Toronto-based startup in 2023.\n\nPerplexity: Nvidia first invested in Perplexity in November of 2023 and has participated in every subsequent round of the AI search engine startup, including the $500 million round in December, which values the company at $9 billion, according to PitchBook data.\n\nPoolside: In October, the AI coding assistant startup Poolside announced it raised $500 million led by Bain Capital Ventures. Nvidia participated in the round, which valued the AI startup at $3 billion.\n\nCoreWeave: Nvidia invested in the AI cloud computing provider in April 2023, when CoreWeave raised $221 million in funding. Since then, CoreWeave\u2019s valuation has jumped from about $2 billion to $19 billion, and the company has filed for an IPO. CoreWeave allows its customers to rent Nvidia GPUs on an hourly basis.\n\nTogether AI: In February, Nvidia participated in the $305 million Series B of this company, which offers cloud-based infrastructure for building AI models. The round valued TogetherAi at $3.3 billion, and was co-led by Prosperity7, a Saudi Arabian venture firm, and General Catalyst. Nvidia backed the company for the first time in 2023.\n\nSakana AI: In September, Nvidia invested in the Japan-based startup, which trains low-cost generative AI models using small datasets. The startup raised a massive Series A round of about $214 million at a valuation of $1.5 billion.\n\nImbue: The AI research lab that claims to be developing AI systems that can reason and code raised a $200 million round in September 2023 from investors, including Nvidia, Astera Institute, and former Cruise CEO Kyle Vogt.\n\nWaabi: In June, the autonomous trucking startup raised a $200 million Series B round co-led by existing investors Uber and Khosla Ventures. Other investors included Nvidia, Volvo Group Venture Capital, and Porsche Automobil Holding SE.\n\nDeals of over a $100 million\n\nAyar Labs: In December, Nvidia invested in the $155 million round of Ayar Labs, a company developing optical interconnects to improve AI compute and power efficiency. This was the third time Nvidia backed the startup.\n\nKore.ai: The startup developing enterprise-focused AI chatbots raised $150 million in December of 2023. In addition to Nvidia, investors participating in the funding included FTV Capital, Vistara Growth, and Sweetwater Private Equity.\n\nHippocratic AI: This startup, which is developing large language models for healthcare, announced in January that it raised a $141 million Series B at a valuation of $1.64 billion led by Kleiner Perkins. Nvidia participated in the round, along with returning investors Andreessen Horowitz, General Catalyst and others. The company claims that its AI solutions can handle non-diagnostic patient-facing tasks such as pre-operating procedures, remote patient monitoring, and appointment preparation.\n\nWeka: In May, Nvidia invested in a $140 million round for AI-native data management platform Weka. The round valued the Silicon Valley company at $1.6 billion.\n\nRunway: In June of 2023, Runway, a startup building generative AI tools for multimedia content creators, raised a $141 million Series C extension from investors, including Nvidia, Google, and Salesforce.\n\nBright Machines: In June 2024, Nvidia participated in a $126 million Series C of Bright Machines, a smart robotics and AI-driven software startup.\n\nEnfabrica: In September 2023, Nvidia invested in networking chips designer Enfabrica\u2019s $125 million Series B. Although the startup raised another $115 million in November, Nvidia didn\u2019t participate in the round.\n\nEditor\u2019s note: Previous version of this story incorrectly stated that Nvidia is a backer of Safe Superintelligence and an investor in Vast Data\u2019s Series E round. Nvidia hasn\u2019t invested in Vast Data since the company\u2019s Series D.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Should You Forget Nvidia and Buy This Tech Stock Instead?",
            "link": "https://www.nasdaq.com/articles/should-you-forget-nvidia-and-buy-tech-stock-instead-1",
            "snippet": "Semiconductor designer Nvidia (NASDAQ: NVDA) has been crushing the stock market in recent years. As a leading hardware provider for the artificial...",
            "score": 0.905400812625885,
            "sentiment": null,
            "probability": null,
            "content": "Semiconductor designer Nvidia (NASDAQ: NVDA) has been crushing the stock market in recent years. As a leading hardware provider for the artificial intelligence (AI) boom, Nvidia's sales and profits are soaring. The stock rose 171% in 2024 and more than 720% since OpenAI released the Nvidia-powered ChatGPT system in 2021.\n\nNvidia is the talk of the town and many investors expect its business to keep skyrocketing. But the enthusiasm has its limits. Some would say that Nvidia's soaring stock price already accounts for many years of high-octane growth. This market darling might be due for a price correction, or maybe it's just primed for lower price gains over the next couple of years.\n\nStart Your Mornings Smarter! Wake up with Breakfast news in your inbox every market day. Sign Up For Free \u00bb\n\nI'm not saying you should forget about Nvidia. Even if the stock is poised for a price correction, I'm still talking about a dominant technology leader in a thriving AI market. Holding the stock should work out in the long run.\n\nStill, I don't think Nvidia is the best tech stock to buy right now. Have you taken a close look at Intel (NASDAQ: INTC) lately? You might like what you find in the undervalued chip giant.\n\nIntel's recent struggles\n\nIntel has seen its share of issues in recent years. There's been turnover in the CEO office, longtime rival Advanced Micro Devices (NASDAQ: AMD) is stealing market share from Intel in a couple of key segments, and the company formerly known as \"Chipzilla\" has not found a footing in the surging AI opportunity.\n\nAs such, the stock has fallen 71% in the last five years to trade at just 1.5 times trailing sales and 20 times next-year earnings estimates. These valuation measures are cheap from a historical perspective and extremely affordable next to AMD's and Nvidia's lofty ratios.\n\nIn short, the market seems to be pricing Nvidia's stock for perfection while pricing Intel's stock for absolute disaster. The latter's market value is currently below its book value, as though suggesting that investors might be better off if Intel shut down its business, sold its chip factories and other assets, and found a tax-free way to return the resulting cash to shareholders.\n\nIntel's ambitious long-term plan\n\nAnd I think that's a shortsighted view of Intel.\n\nYou see, the semiconductor pioneer is making significant shifts to its business model. It's a costly plan that involves spending about $100 billion on domestic chip-making facilities over the next five years.\n\nIt's also an ambitious business idea that should turn Intel into a world-class third-party chip builder with most of its factories on American soil. That's an important detail, because the global tech industry's reliance on Taiwan-based manufacturing plants may not be sustainable in the long run.\n\nFurthermore, Intel's third-party chip foundry business has already attracted orders from global giants such as Microsoft (NASDAQ: MSFT). Even Nvidia is thinking about sending chip orders to Intel, taking advantage of the company's American footprint and a supply chain that's almost entirely separate from the large but still limited resources available to global leader Taiwan Semiconductor (NYSE: TSM).\n\nSo the new Intel is not the chip maker you're used to, but the revamped business plan could be quite valuable in the long run.\n\nIntel's temporary struggles created a fantastic buying window\n\nI don't expect a huge Intel turnaround in 2025 or even 2026. The foundry-focused strategy shift will take time, and the company is under an interim management team right now.\n\nBut I don't mind taking the long view. Intel's stock may have been due for a price correction amid the turmoil seen in recent years, but it didn't deserve a 71% price cut. This isn't even a turnaround story, but an investor overreaction to Intel's drastic strategy shift. I'd much rather buy into this misunderstood ambition than pick up more Nvidia shares at sky-high share prices. You might reach the same conclusion after doing your own research.\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nEver feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia: if you invested $1,000 when we doubled down in 2009, you\u2019d have $352,417 !*\n\nif you invested $1,000 when we doubled down in 2009, !* Apple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $44,855 !*\n\nif you invested $1,000 when we doubled down in 2008, !* Netflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $451,759!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nSee 3 \u201cDouble Down\u201d stocks \u00bb\n\n*Stock Advisor returns as of January 6, 2025\n\nAnders Bylund has positions in Intel and Nvidia. The Motley Fool has positions in and recommends Advanced Micro Devices, Intel, Microsoft, Nvidia, and Taiwan Semiconductor Manufacturing. The Motley Fool recommends the following options: long January 2026 $395 calls on Microsoft, short February 2025 $27 calls on Intel, and short January 2026 $405 calls on Microsoft. The Motley Fool has a disclosure policy.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "I am thrilled by Nvidia\u2019s cute petaflop mini PC wonder, and it\u2019s time for Jensen\u2019s law: it takes 100 months to get equal AI performance for 1/25th of the cost",
            "link": "https://www.techradar.com/pro/i-am-thrilled-by-nvidias-cute-petaflop-mini-pc-wonder-and-its-time-for-jensens-law-youll-get-the-same-ai-performance-for-1-25th-of-the-price-in-100-months",
            "snippet": "It may be time to introduce Jensen's law to complement Moore's law: At equal AI performance, it takes about 100 months to cut the price per FLOP by 25.",
            "score": 0.5192500948905945,
            "sentiment": null,
            "probability": null,
            "content": "Nobody really expected Nvidia to release something like the GB10. After all, why would a tech company that transformed itself into the most valuable firm ever by selling parts that cost hundreds of thousands of dollars, suddenly decide to sell an entire system for a fraction of the price?\n\nI believe that Nvidia wants to revolutionize computing the way IBM did it almost 45 years ago with the original IBM PC.\n\nIt may be time to introduce Jensen\u2019s law to complement Moore\u2019s law: At equal AI performance, it takes about 100 months to cut the price per FLOP by 25. D. Athow\n\nProject DIGITS, as a reminder, is a fully formed, off-the-shelf super computer built into something the size of a mini PC. It is essentially a smaller version of the DGX-1, the first of its kind launched almost a decade ago, back in April 2016. Then, it sold for $129,000 with a 16-core Intel Xeon CPU and eight P100 GPGPU cards; Digits costs $3,000.\n\nNvidia confirmed it has an AI performance of 1,000 Teraflops at FP4 precision (dense/sparse?). Although there\u2019s no direct comparison, one can estimate that the diminutive super computer has roughly half the processing power of a fully loaded 8-card Pascal-based DGX-1.\n\nAt the heart of Digits is the GB10 SoC, which has 20 Arm Cores (10 Arm Cortex-X925 and 10 Cortex-A725). Other than the confirmed presence of a Blackwell GPU (a lite version of the B100), one can only infer the power consumption (100W) and the bandwidth (825GB/s according to The Register).\n\nYou should be able to connect two of these devices (but not more) via Nvidia\u2019s proprietary ConnectX technology to tackle larger LLMs such as Meta's Llama 3.1 405B. Shoving these tiny mini PCs in a 42U rack seems to be a near impossibility for now as it would encroach on Nvidia\u2019s far more lucrative DGX GB200 systems.\n\nAll about the moat\n\nWhy did Nvidia embark on Project DIGITS? I think it is all about reinforcing its moat. Making your products so sticky that it becomes near impossible to move to the competition is something that worked very well for others: Microsoft and Windows, Google and Gmail, Apple and the iPhone.\n\nThe same happened with Nvidia and CUDA - being in the driving seat allowed Nvidia to do things such as shuffling the goal posts and wrongfooting the competition.\n\nThe move to FP4 for inference allowed Nvidia to deliver impressive benchmark claims such as \u201cBlackwell delivers 2.5x its predecessor\u2019s performance in FP8 for training, per chip, and 5x with FP4 for inference\u201d. Of course, AMD doesn\u2019t offer FP4 computation in the MI300X/325X series and we will have to wait till later this year for it to roll out in the Instinct MI350X/355X.\n\nNvidia is therefore laying the ground for future incursions, for lack of a better word or analogy, from existing and future competitors, including its own customers (think Microsoft and Google). Nvidia CEO Jensen Huang\u2019s ambition is clear; he wants to expand the company\u2019s domination beyond the realm of the hyperscalers.\n\n\u201cAI will be mainstream in every application for every industry. With Project DIGITS, the Grace Blackwell Superchip comes to millions of developers, placing an AI supercomputer on the desks of every data scientist, AI researcher and student empowers them to engage and shape the age of AI,\u201d Huang recently commented.\n\nShort of renaming Nvidia as Nvid-ai, this is as close as it gets to Huang acknowledging his ambitions to make his company\u2019s name synonymous with AI, just like Tarmac and Hoover before them (albeit in more niche verticals).\n\n(Image credit: Storagereview.com)\n\nWhy Mediatek?\n\nI was also, like many, perplexed by the Mediatek link and the rationale for this tie-up can be found in the Mediatek press release. The Taiwanese company \u201cbrings its design expertise in Arm-based SoC performance and power efficiency to [a] groundbreaking device for AI researchers and developers\u201d it noted.\n\nThe partnership, I believe, benefits Mediatek more than Nvidia and in the short run, I can see Nvidia quietly going solo. Reuters reported Huang dismissed the idea of Nvidia going after AMD and Intel, saying, \u201cNow they [Mediatek] could provide that to us, and they could keep that for themselves and serve the market. And so it was a great win-win\u201d.\n\nThis doesn\u2019t mean Nvidia will not deliver more mainstream products though, just they would be aimed at businesses and professionals, not consumers where cut throat competition makes things more challenging (and margins wafer thin).\n\nReuters article quotes Huang saying, \"We're going to make that a mainstream product, we'll support it with all the things that we do to support professional and high-quality software, and the PC (manufacturers) will make it available to end users.\"\n\nSwipe to scroll horizontally Header Cell - Column 0 DIGITS DIGITS 2.4X DGX-1 v1 Variance (DGX vs DIGITS) Depth (est.) in mm 89 89 866 9.73x Width (est.) in mm 135 324 444 1.37x Height (est.) in mm 40 40 131 3.28x Weight in Kg ~1 ~2.4 60.8 25.35x Price USD (Nov 2024 adjusted) 3000 7200 170100 23.63x Performance GPU FP16 (TF) 0 0 170 Row 5 - Cell 4 Performance GPU FP16 Dense (TF) ~282 676.8 680 1.00x Performance GPU FP4 Dense (TF) 1000 Row 7 - Cell 2 0 Row 7 - Cell 4 GPU memory (GB) 128 307.2 128 0.42x Max Power consumption (W) ~150 ~300 3200 10.67x Storage (TB) 4 9.6 7.68 0.80x GPU Family Blackwell Blackwell Pascal Row 11 - Cell 4 GPU power comsumption (W) x8 ~100 ~240 2400 10x GPU transistor count (bn) x8 ~30 ~72 120 1.67x Memory Bandwidth (GB/sec) x ~850 ~850 720 0.85x\n\nGazing in my crystal ball\n\nOne theory I came across while researching this feature is that more data scientists are embracing Apple\u2019s Mac platform because it offers a balanced approach. Good enough performance - thanks to its unified memory architecture - at a \u2018reasonable\u2019 price. The Mac Studio with 128GB unified memory and 4TB SSD currently retails for $5,799.\n\nSo where does Nvidia go from there? An obvious move would be to integrate the memory on the SoC, similar to what Apple has done with its M series SoC (and AMD with its HBM-fuelled Epyc). This would not only save on costs but would improve performance, something that its bigger sibling, the GB200 already does.\n\nThen it will depend on whether Nvidia wants to offer more at the same price or the same performance at a lower price point (or a bit of both). Nvidia could go Intel\u2019s way and use the GB10 as a prototype to encourage other key partners (PNY, Gigabyte, Asus) to launch similar projects (Intel did that with the Next Unit of Computing or NUC).\n\nI am also particularly interested to know what will happen to the Jetson Orin family; the NX 16GB version was upgraded just a few weeks ago to offer 157 TOPS in INT8 performance. This platform is destined to fulfill more DIY/edge use cases rather than pure training/inference tasks but I can\u2019t help but think about \u201cWhat If\u201d scenarios.\n\nNvidia is clearly disrupting itself before others attempt to do so; the question is how far will it go.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia's AI agents, the best of CES, and TikTok's day in court: Tech news roundup",
            "link": "https://qz.com/nvidia-ai-agents-ces-2025-tiktok-biden-apple-siri-1851737118",
            "snippet": "Nvidia goes all in on AI agents and humanoid robots at CES ... As the AI world races toward next-generation breakthroughs, Nvidia (NVDA) fortified its position...",
            "score": 0.9274075627326965,
            "sentiment": null,
            "probability": null,
            "content": "As the artificial intelligence race heats up, so does the global competition for computing power \u2014 something the boom would be impossible without.\n\nAdvertisement\n\nSupercomputing, which is a type of high-performance computing, involves multiple central processing units, or CPUs, grouped into compute nodes that communicate to solve problems. This technology has been used to discover new materials for battery and chip development, in disease research, and increasingly to run AI workloads.\n\nRead More",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "NVIDIA Just Revealed Another Game Changer",
            "link": "https://247wallst.com/investing/2025/01/11/nvidia-just-revealed-another-game-changer/",
            "snippet": "Enter Nvidia Digits. An AI supercomputer meant for the home. Though there was more than one thing Nvidia unleashed to get euphoric over, I do think there was...",
            "score": 0.8714900612831116,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NVIDIA's next-gen GB206 GPU inside of desktop GeForce RTX 5060 leaked: N22Y-ES-A1 GPU spotted",
            "link": "https://www.tweaktown.com/news/102546/nvidias-next-gen-gb206-gpu-inside-of-desktop-geforce-rtx-5060-leaked-n22y-es-a1-spotted/index.html",
            "snippet": "NVIDIA is reportedly set to use the higher-end GB206 die for its upcoming GeForce RTX 5060 desktop graphics card, with new leaks of the GB206 GPU smiling for...",
            "score": 0.879967212677002,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-10": {
        "0": {
            "title": "NVIDIA Announces Blueprint for AI Retail Shopping Assistants",
            "link": "https://nvidianews.nvidia.com/news/nvidia-announces-blueprint-for-ai-retail-shopping-assistants",
            "snippet": "NVIDIA today announced the NVIDIA AI Blueprint for retail shopping assistants, a generative AI reference workflow designed to transform shopping experiences...",
            "score": 0.6840043663978577,
            "sentiment": null,
            "probability": null,
            "content": "New Workflow Provides Developers Generative AI and 3D Visualization Technologies to Elevate Shopping Experiences Online and In Stores\n\nNRF\u2014NVIDIA today announced the NVIDIA AI Blueprint for retail shopping assistants, a generative AI reference workflow designed to transform shopping experiences online and in stores.\n\nThe blueprint \u2014 built on the NVIDIA AI Enterprise and NVIDIA Omniverse\u2122 platforms \u2014 helps developers create AI-powered digital assistants that work alongside and support human workers. These digital assistants can deliver the expertise of a company\u2019s best sales associate, stylist or designer, elevating customer satisfaction and workforce efficiency.\n\nUsing NVIDIA NeMo\u2122 microservices provided within the blueprint, these highly skilled AI shopping assistants can understand text- and image-based prompts, search for multiple items simultaneously, complete complicated tasks such as creating a travel wardrobe, and answer contextual questions like whether a product is waterproof.\n\nDevelopers can use the Omniverse platform in conjunction with a spatial-scanning solution to enable AI agents to present products in physically accurate virtual environments. For example, customers looking to buy a couch could preview how the furniture would look in their own living room.\n\nAI agents with advanced capabilities like these are designed to enhance customer experiences, drive higher conversion rates, lower product return rates and increase the average size of orders through highly intelligent, personalized suggestions of complementary products or upgrades.\n\n\u201cAI agents can elevate shopping experiences, turning what can be impersonal transactions into smarter, more enjoyable interactions,\u201d said Azita Martin, vice president of AI for retail, consumer packaged goods and quick-service restaurants at NVIDIA. \u201cShoppers everywhere want intelligent product recommendations, personalized interactions and lightning-fast response times. AI agents built using the NVIDIA AI Blueprint for retail shopping assistants can deliver this kind of exceptional, always-on service to every customer.\u201d\n\nAccelerating Retail AI Assistant Development\n\nSoftServe, a leading IT advisor to some of the most recognizable brands worldwide, today announced its SoftServe Gen AI Shopping Assistant, developed using NVIDIA\u2019s shopping assistants blueprint in early access.\n\nThe assistant can help browse product catalogs, search for items and access detailed product information quickly. A standout feature is its virtual try-on capability, allowing customers to visualize how products look on them directly through an online chat before making a purchase.\n\n\u201cWhether it\u2019s online, offline or hybrid shopping, retailers face challenges like purchase hesitation, costly returns and the need for personalized customer experiences,\u201d said Ivan Leshko, executive vice president North America at SoftServe. \u201cThe NVIDIA AI Blueprint for retail shopping assistants enabled us to build a turnkey solution quickly so our retail customers can use AI to keep consumers engaged and drive higher conversion rates.\u201d\n\nNVIDIA AI Drives Business Impact, Enhanced Customer Experience\n\nWith AI shopping assistants, retailers can deliver more engaging customer interactions, around the clock and across the world.\n\nNVIDIA\u2019s retail shopping assistant blueprint features NVIDIA AI Enterprise software, including NVIDIA NIM\u2122 microservices for Meta Llama 3.3 70B and NVIDIA NeMo Retriever embedding and reranking microservices, to deliver AI performance at scale. It also includes NVIDIA NeMo Guardrails safety features, as well as NVIDIA Omniverse for design and visualization capabilities.\n\nKey features of the blueprint include:\n\nMultimodal and multi-query capabilities, enabling consumers to use text and images in queries.\n\nIntegration with large language models (LLMs) that bring reasoning capabilities to AI shopping assistants for natural, humanlike interactions. This includes NVIDIA Llama Nemotron LLMs, announced this week at the CES trade show, which will be available later this year.\n\nThe ability to ingest retailers\u2019 product catalog text and image data for accurate, context-aware responses.\n\nGuardrails that help ensure customer conversations with the shopping assistant remain safe and on-topic, protecting brand values.\n\nState-of-the-art simulation tools that can help customers visualize products in their own physically accurate spaces.\n\nThe shopping assistant blueprint is the latest addition to a growing repository of NVIDIA AI Blueprints, many announced at CES earlier this week. This includes a new version of the NVIDIA AI Blueprint for video search and summarization that helps retailers build video analytics AI agents that can analyze large volumes of live and archived content to boost operational efficiency and safety.\n\nBringing Retail Shopping Assistants to Retailers Worldwide\n\nNVIDIA\u2019s global network of partners is leveraging the fundamental technologies of the shopping assistant blueprint to develop and deploy innovative solutions for the $30 trillion retail industry. According to eMarketer, the industry is projected to grow to $35 trillion by 2028, representing significant opportunities for transformation and growth.\n\nIn addition to SoftServe, NVIDIA partners Dell Technologies and World Wide Technology (WWT) will use the blueprint in early access to make it easier for retailers to adopt AI.\n\n\u201cRetail leaders harnessing AI-powered personalization are creating unprecedented competitive advantages in today\u2019s digital-first marketplace,\u201d said Adam Dumey, global vice president of retail at WWT. \u201cWe\u2019re excited to leverage this new retail shopping assistant blueprint from NVIDIA to bring the full force of our nearly $1 billion investment in our Advanced Technology Center and AI Proving Ground \u2014 along with our expert team of data scientists, retail consultants and AI architects \u2014 to help our retail customers accelerate their AI transformation.\u201d\n\nAvailability\n\nThe NVIDIA AI Blueprint for retail shopping assistants is now available in early access. Developers can sign up to be notified when the blueprint is generally available.\n\nJoin NVIDIA at NRF: Retail\u2019s Big Show, running Jan. 12-14, to learn more about how retailers are using AI shopping assistants to advance the industry. Visit Dell Technologies\u2019 (5721) and Supermicro\u2019s (3165) booths on level three to meet with NVIDIA AI experts and explore live shopping assistant demos.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "CES 2025 NVIDIA Storage And Memory Product Requirements",
            "link": "https://www.forbes.com/sites/tomcoughlin/2025/01/10/ces-2025-nvidia-storage-and-memory-product-requirements/",
            "snippet": "At the 2025 CES NVIDIA's memory and storage play an important role in AI training and inference and his presentation showed how they enable modern AI...",
            "score": 0.9482065439224243,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia year in review",
            "link": "https://www.networkworld.com/article/3800823/nvidia-year-in-review.html",
            "snippet": "2024 was Nvidia's year. Its command of mindshare and market share was unequaled among tech vendors. Here's a recap of some of the key events of 2024.",
            "score": 0.8368628621101379,
            "sentiment": null,
            "probability": null,
            "content": "Elon Musk\u2019s xAI taps Blackwell GPUs for its supercomputer\n\nNot to be outdone by Oracle, Elon Musk and his team behind the xAI company took delivery and deployed 100,000 H200 Blackwell GPUs in an incredible 19 days. Huang had said that such a deployment should take four years complete. The cluster is in a data center outside of Memphis, Tennessee, and will be used for all things GenAI.\n\nHeat rumors start up again, just in time for earnings\n\nFor the second time, reports of heat problems with Nvidia\u2019s newest processors came out right before the release of earnings. In this case, the report stated that due to heat from the Blackwell chips, servers needed to be redesigned to accommodate the heat. This was dismissed by Nvidia on its earnings call \u2013 and at the same time, Nvidia reported revenue gains of 17% compared to the prior quarter and 94% compared to the prior year.\n\nAI factory designs\n\nNvidia released reference designs for what it calls AI factories, or data centers dedicated to AI processing. The company had been talking about AI factories for some time, but this was finally a blueprint telling people how to build one.\n\nEdge computing platform for AI processing\n\nNvidia launched a new platform called EGX Platform designed to bring real-time artificial intelligence to edge networks. The idea is to put AI computing closer to where sensors collect data before it is sent to larger data centers. This serves as a buffer to whittle down the data sent to data centers so only relevant data is sent down the wire for processing. This can be up to 90% or more of data collected is discarded.\n\nVerizon, Nvidia team up for enterprise AI networking\n\nNvidia announced a partnership with Verizon to build AI services for enterprises that run workloads over Verizon\u2019s 5G private network. Dubbed 5G Private Network with Enterprise AI, it will run a range of AI applications and workloads over Verizon\u2019s private 5G network with Mobile Edge Compute (MEC), a colocated infrastructure that is a part of Verizon\u2019s public wireless network.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "It's time for me to admit that AI-accelerated frame generation might actually be the way of the future and that's a good thing",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/its-time-for-me-to-admit-that-ai-accelerated-frame-generation-might-actually-be-the-way-of-the-future-and-thats-a-good-thing/",
            "snippet": "AI-accelerated frame generation is just another way of utilising GPU hardware to generate frames. It's no less local than CUDA Cores or Stream Processors.",
            "score": 0.6986772418022156,
            "sentiment": null,
            "probability": null,
            "content": "Jacob Fox, hardware writer (Image credit: Future) This week: I've been trying to make heads or tails of the million-and-one new and exciting products announced at CES, all while soldiering bravely on through a cold. A true martyr.\n\nCES had a lot to offer this year, but the main announcement for us PC gamers has without a doubt been the Nvidia RTX 50-series. It feels like it's been forever and a day since RTX 40-series cards became the best graphics cards, but the RTX 50-series is finally officially here\u2014well, just as soon as the cards actually launch at the end of January and through February, that is.\n\nApart from the RTX 5070 seeming to have a shockingly reasonable price tag and the RTX 5090 having a downright painful one, the main thing that's struck the heart of many a PC gamer has been Nvidia's claim that the RTX 5070 will deliver \"twice the performance of the 4090\". And while some have been delighted by that prospect, others have responded with cynicism, pointing out that Nvidia's claim will only be true if DLSS 4 is enabled.\n\nApart from the urge to express an obvious response to such cynics\u2014\"duh, of course that's only with DLSS 4 enabled, Nvidia's been pretty up-front about that\"\u2014I think this is the first time I've realised that I don't actually care whether my frames are made by traditional rendering or by some AI-accelerated frame generation magic.\n\nAnd trust me, that actually kind of pains me to say. For years now, I'd considered myself a staunch enemy of fake frames. Only those sweet real ones for me, thank you\u2014ones borne of the blood and sweat of traditional shader cores.\n\nWhy was I so anti-frame gen? Well, after waving through the smokescreen reasons I only ever actually half cared about\u2014latency, artifacts, and so on\u2014the real reason, I must admit, was that something just rubbed me the wrong way about not owning my own GPU power. I thought: \"Hey, if I'm paying hundreds for a piece of hardware, I don't want that performance to be reliant on Nvidia's machine learning and the beneficent game devs who decide to implement it. I want raw horsepower.\"\n\nThe future is now, old man Me, to me\n\nBut now, I'm starting to realise that this argument's not quite right. After all, what performance would I actually own if a GPU was just packed with CUDA cores? Those cores wouldn't mean a damn thing without (at minimum) good drivers and game devs making proper use of them. The GPU cores are nothing in themselves. I'd been reliant on software all along, I just didn't realise it.\n\nWhat I've come to realise is that AI-accelerated frame generation is just another way of utilising GPU hardware to generate frames. It's no less \"local\" than CUDA Cores or Stream Processors unless I arbitrarily pick \"does not rely on machine learning\" as the criterion for \"local\". But what reason do I have for picking that criterion, given CUDA Cores/SPs also rely on much on the software level, too?\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nThe only real reason for me to pick that criterion is that traditional rendering is what I'm used to. But the future is now, old man. That's what I find myself telling myself when I see Nvidia's RTX 50-series and DLSS 4 performance claims. If AI-accelerated rendering works, maybe it's time I get with the program, especially if the results are as dramatic as Nvidia's claiming.\n\nMaybe those who sneer \"the RTX 5070 will only offer double the RTX 4090's performance if it uses DLSS 4\" are akin to the luddite saying \"the car will only go faster than the horse if it uses wheels.\" Maybe we need to accept that wheels are the future, and that that's okay.\n\nOf course, all of this depends on whether new frame gen tech can deliver on the quality front. I was sceptical of DLSS 3's frame gen for a very long time, but most of the wrinkles have been smoothed out now. And if initial hands-on reports of FSR 4 are anything to go by, AMD's upcoming frame gen tech seems very impressive.\n\nYouTube Watch On\n\nAh, but then there's latency. That circle, unfortunately, is harder to square. As our resident cynic Jeremy Laird reminded me earlier today, only a \"real\" frame can help with latency. AI-generated frames can never improve it, which means at best you're stuck with whatever latency you would have been getting before the extra frames were generated.\n\nOne initial response to this is to say that the games where latency matters the most\u2014esports titles\u2014tend to be easier to traditionally render, meaning we might not need to worry too much about them, anyway. But that's a bit of a cop-out, I suppose, because we do also want low latency in non-esports titles.\n\nSo, I'll hold my hands up and say we definitely need to keep some of the old as we hurry in with the new. We're always going to need traditional rendering\u2014even the AI king himself, Nvidia CEO Jen-Hsun Huang, says so\u2014not least because these are the frames that can actually adjust to your input. The frames between are essentially just padding. (Though I do wonder whether there could be a way to change that in the future. For instance, perhaps there'll someday be a way to interject input into the frame generation pipeline, ie, take a control input to guide the next frame's generation.)\n\nThankfully, it does seem like AMD and Nvidia are keeping the old with the new. We do still see improvements in traditional rendering performance, after all. The problem is that these improvements might be starting to plateau, perhaps as a simple result of Moore's law. (Jeremy the cynic chimes in again here to point out that Nvidia and AMD could be exaggerating the extent to which Moore's law is limiting core density.)\n\nIn which case, would we rather GPU companies don't try to give us heaps of extra performance in other ways? Yeah, no. I think I'm finally ready to admit that I like frame gen. Frame gen improvements are perfectly reasonable replacements for traditional rendering improvements, especially given the latter seems like an increasingly low-return proposition compared to the might of AI acceleration.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Accenture and NVIDIA to Team with KION to Optimize Supply Chains with AI",
            "link": "https://www.channelinsider.com/tech-companies/accenture-nvidia-kion-ai-digital-twins-warehouses/",
            "snippet": "Accenture, NVIDIA, and KION unveil AI-powered digital twins to optimize warehouse operations, boosting efficiency, safety, and supply chain resilience.",
            "score": 0.8087563514709473,
            "sentiment": null,
            "probability": null,
            "content": "Channel Insider content and product recommendations are editorially independent. We may make money when you click on links to our partners. View our editorial policy here.\n\nKION, Accenture, and NVIDIA jointly showcased how clients can define ideal set-ups for new warehouses and continuously enhance existing facilities with Mega, an NVIDIA Omniverse blueprint for large-scale industrial digital twins at the Consumer Electronics Show (CES) 2025.\n\nThree companies will bring \u2018digital twins\u2019 to warehouses\n\nThe companies are building physical AI-powered digital twins with NVIDIA software to improve productivity and the functional design of intelligent warehouses, which operate with automated forklifts, smart cameras, and the latest automation and robotics solutions.\n\nBy utilizing both NVIDIA Omniverse and Mega, KION will provide digital twins of warehouses to allow facility operators to design the most efficient and safe warehouse configuration without interrupting operations for testing. This will include optimizing the number of robots, workers, and automation equipment.\n\nThe digital twin will provide a testing ground for all aspects of warehouse operations, including facility layouts, robot fleet behavior, and the optimal number of workers and intelligent vehicles.\n\n\u201cFuture warehouses will function like massive autonomous robots, orchestrating fleets of robots within them,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cBy integrating Omniverse and Mega into their solutions, KION and Accenture can dramatically accelerate the development of industrial AI and autonomy for the world\u2019s distribution and logistics ecosystem.\u201d\n\nFurther, the digital twin will simulate and test configurations, while also training the warehouse robots to handle changing conditions like demand, inventory fluctuations, and layout changes.\n\nThe digital twin will be integrated with KION\u2019s warehouse management software, enabling companies to assign tasks like moving goods from buffer zones to storage locations to virtual robots. Advanced AI will power the virtual robots, which can plan, execute, and refine these tasks in a continuous loop that simulates and optimizes real-world operations with infinite scenarios.\n\n\u201cModernizing supply chains to make them more resilient and agile, with real-time flexibility, is the next digital frontier,\u201d said Julie Sweet, chair and CEO of Accenture. \u201cThis collaboration with our long-term client KION and partner NVIDIA will break exciting new ground in not only reinventing the warehouse, but also in raising their performance standards with technology, data, and AI, helping our clients operate autonomous, safe supply chains that better serve their customers and consumers, enhance productivity and efficiency, and create new value.\u201d\n\nGoing forward, the three partners will work to integrate the digital twin with a fine-tuned vision language model to capture real-time insights from warehouses, reducing the risk of bottlenecks, accidents, and other unforeseen events, pairing cameras, robots, and NVIDIA NIM, a set of services for deployment of foundation models to edge devices in the warehouse.\n\nFurther announcements\n\nThis partnership between the three companies was one of three announcements Accenture made this week to coincide with CES 2025. The company also released its 2025 Technology Vision report, which publicizes research on how AI will drive new levels of autonomy for businesses and unlock innovation opportunities.\n\nThe company also announced Accenture AI Refinery for Industry, a solution containing a collection of 12 industry-specific agent solutions to assist organizations with building and deploying custom AI agents.\n\nAccenture and NVIDIA have made various moves over the past few months to help enterprises scale AI adoption. Read more about how the two companies launched an AI platform to help clients lay the foundation for agentic AI functionality.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Nvidia Stock Posts Sharp Weekly Loss Despite CES Glow",
            "link": "https://www.investors.com/news/technology/nvidia-stock-nvda-ces-huang-artificial-intelligence-ai/",
            "snippet": "Nvidia Stock Posts Sharp Weekly Loss Despite CES Glow ... Nvidia stock posted a weekly loss Friday despite rave reviews for CEO Jensen Huang's CES appearance.",
            "score": 0.9720309972763062,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock posted a weekly loss Friday despite rave reviews for CEO Jensen Huang's CES appearance.\n\nNvidia (NVDA) stock shed 3% to close at 135.91, skidding below the 50-day line, down 6% for the week. It was the stock's first weekly slump in four weeks.\n\nOn Monday, Nvidia stock cleared a 146.54 buy point, then briefly hit a record 153.13 Tuesday morning before reversing sharply lower.\n\n\u2191 X AI Chipmaker Hovers In Buy Zone As Earnings Approach See All Videos NOW PLAYING AI Chipmaker Hovers In Buy Zone As Earnings Approach\n\nThe chipmaker's shares joined Friday's broad market sell-off triggered by December employment numbers that came in stronger than expected.\n\nNvidia stock also fell on concerns about the impact of new AI-related export curbs that could also impact rival AMD (AMD).\n\nNVDA stock is slumping despite Nvidia's appearance at the CES electronics industry event in Las Vegas where Huang raised the bar in the battle for AI dominance.\n\nHuang announced that Nvidia's latest generation AI chip series, Blackwell, is in full production. He also touted initiatives in automotive and \"physical AI.\"\n\nNvidia Stock: 'Boss Of AI'\n\nBernstein Research analyst Stacy Rasgon described Huang's keynote as \"compelling enough,\" adding in a note, \"Jensen always puts on a good show.\"\n\n\"There was not much incremental (news) on the data center business,\" he wrote. \"But the announcements made, spanning across gaming, automotive, robotics and physical AI, and even new compact AI developer workstation were enough for us at this point.\"\n\nNvidia stock rallied Monday, but then retreated Tuesday in what appeared to be a sell-the-news move.\n\nThat was the view of Melius Research analyst Ben Reitzes.\n\n\"Nvidia CEO Jensen Huang is clearly 'The Boss' in AI,\" he wrote in a client note. \"While the stock sold off Tuesday with the broader semis group and some 'sell on the news' \u2014 we are buyers.\"\n\nNvidia stock's Relative Strength rating did dip to 93, down from 94 last week and from a perfect 99 six months ago.\n\nBut Reitzes said: \"We think AI is in the early innings as Nvidia makes the market and directs all the traffic.\"\n\nYOU MAY ALSO LIKE:\n\nBiotech Stocks Prepare For Action In 2025. Weight-Loss Drugs, AI And Trump 2.0 Are The Catalysts.\n\nTikTok Ban Heads For Supreme Court Today. What It Means For Meta, Snap, Oracle Stock.\n\nIBD Live: A Useful Tool For Daily Stock Market Analysis\n\nNvidia Raises Bar At CES 2025. But Stock Slides.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Why Advanced Micro Devices (AMD) Could Outperform Nvidia in 2025",
            "link": "https://finance.yahoo.com/news/why-advanced-micro-devices-amd-221047821.html",
            "snippet": "Nvidia (NVDA) was the big winner in the AI space during 2024 as it solidified its lead in GPUs, which led to a 185% stock rally. Meanwhile, Advanced Micro...",
            "score": 0.5581706166267395,
            "sentiment": null,
            "probability": null,
            "content": "One of the main reasons I\u2019m bullish on Nvidia is the safety we\u2019ve historically seen in investing in the company based on its solid fundamentals and projected earnings. There have been times when Nvidia might have seemed expensive based on trailing 12-month earnings, but I\u2019ve often found reassurance in the fact that the stock remained relatively cheap when looking at forward two-year guidance. This is largely due to the predictable demand for its products, which has kept its valuation multiples in the mid-20s.\n\nLooking ahead to 2026, we\u2019ll see a bit of a slowdown. Analysts expect Nvidia\u2019s EPS to grow by 26%, with revenues up by 21%. That said, these projections have already been revised upward by about 18% in the past few months as analysts have adjusted their forecasts to reflect Nvidia\u2019s ongoing success. A big part of this optimism comes from the continued progress in Nvidia\u2019s profitability, especially with the upcoming launch of its next-generation Blackwell technology.\n\nFirst off, Nvidia\u2019s guidance suggests that growth should remain strong for a while. Analysts expect Nvidia to grow its bottom line (earnings) by around 50% year-over-year and its top line (revenue) by 51% for the current fiscal year, which ends in January 2026. Keep in mind that these projections come after a truly massive 2025 Fiscal Year, where EPS is expected to soar by 128%, and revenues are set to jump by 113%.\n\nAlthough I believe in the idea that \u201cthe trees cannot climb to the skies forever,\u201d I still think it\u2019s unlikely that Nvidia will experience a major decline in 2025. So, for now, I wouldn\u2019t suggest that bulls take their finger off the \u201cbuy\u201d button just yet.\n\nIn this article, I\u2019ll dive deeper into why I think the performance gap between Nvidia and AMD could look very different in 2025 compared to the past year.\n\nNvidia (NVDA) was the big winner in the AI space during 2024 as it solidified its lead in GPUs, which led to a 185% stock rally . Meanwhile, Advanced Micro Devices (AMD) lagged behind with a more limited AI presence that caused its stock to drop 11%. However, looking ahead to 2025, Nvidia is expected to see a moderate slowdown in growth, which could allow AMD to gain more ground over the next couple of years. While I\u2019m bullish on both companies for the long term, I believe AMD is better positioned for a bigger upside in 2025.\n\nStory Continues\n\nWhile some bears argue that most of Nvidia\u2019s growth expectations are already priced into the stock, I believe there\u2019s still significant potential. In particular, I expect strong demand for Blackwell from hyperscalers and large-cap tech companies to continue beyond Fiscal 2026. Based on earnings projections for Fiscal 2027 (which corresponds to 2026), Nvidia\u2019s forward P/E multiple would be around 26.7x. That\u2019s much more reasonable compared to its current five-year average P/E of 63.6x.\n\nIt\u2019s worth mentioning, though, that Sequoia Capital, a well-known venture capital firm, has projected that AI hardware spending will stabilize in 2025 and beyond. If that happens, it could justify applying a more cautious P/E multiple\u2014possibly below 20x. Even so, considering Nvidia\u2019s competitive advantages and market leadership, I don\u2019t think this means the stock is trading at overly stretched or irrational multiples that would rule out a buying opportunity right now.\n\nIs NVDA A Buy, According to Wall Street Analysts?\n\nAt TipRanks, the consensus on Nvidia is overwhelmingly positive. Out of 40 analysts, 37 recommend buying the stock, while the other three are on the fence. The average price target for Nvidia is $177.03, which, based on its current price, suggests an upside of about 18.5%.\n\nThe Case for AMD In 2025\n\nI\u2019m currently bullish on AMD and think 2025 could be a big year for the company\u2019s shares. After a rough 2024, when AMD\u2019s stock got sidelined, it\u2019s looking for redemption. Even though its stock has been down in recent quarters, AMD\u2019s fundamentals are actually improving. Take its most recent quarter (Q3) as an example\u2014revenues hit a record $6.82 billion, up 17.5% year-over-year, and gross margins set a new high at 53.56%. Operating income also saw a nice bump, reaching $724 million for Q3 after being negative for four straight quarters between the second half of 2022 and the first half of 2023.\n\nSo, what does this mean for 2025? Based on estimates, AMD is expected to end 2024 with EPS and revenue growth of 25.4% and 13.4%, respectively\u2014much more modest growth compared to Nvidia. However, for 2025, AMD\u2019s projections show EPS growing by 53.2% and revenues by 26.8%. Even more interesting, AMD is projected to grow by 38% in EPS and 23.2% in revenue in 2026, which are higher growth rates than Nvidia\u2019s projections for the same period.\n\nWhy is this important? Well, AMD is likely becoming better positioned in the AI space. The company has a strong, growing AI server CPU business that\u2019s less concentrated than Nvidia\u2019s and an AI server GPU segment with the potential to challenge Nvidia\u2019s 90% market share. Specifically, AMD\u2019s upcoming MI350 series is expected to ramp up in 2025, which should provide stiff competition to Nvidia\u2019s Blackwell.\n\nAMD\u2019s Valuation Multiples Overview\n\nAnother reason I\u2019m optimistic about AMD in 2025 is that it\u2019s in a great position for investors to take advantage of a fairly priced stock based on its future growth potential.\n\nAMD has had strong growth in the last couple of quarters, hitting record numbers in both revenue growth and margins. Yet, I feel like the company\u2019s fundamentals are somewhat overlooked. Looking ahead to 2026, with projections showing EPS growth of 25.4% in 2024, 53.2% in 2025, and another 38% in 2026, AMD would trade at a P/E ratio of just 18.4x, adjusted for growth. That\u2019s much more reasonable than Nvidia\u2019s valuation.\n\nGiven this, I think AMD presents an attractive investment opportunity at more de-risked multiples. We could see more investors start to favor AMD, viewing it as a strong contender for the \u201cAI darling\u201d of 2025.\n\nIs AMD A Buy, According to Wall Street Analysts?\n\nSimilar to Nvidia, analysts are quite bullish on AMD as well. Out of 32 analysts, 24 have a Buy recommendation, while eight are neutral, giving the stock a Strong Buy rating. The upside potential looks even more promising than NVDA\u2019s, with an average price target of $184.37, which suggests a 42.3% upside from its current price.\n\nConclusion\n\nFor this year, I don\u2019t expect AMD to grow its top and bottom lines at the same pace as Nvidia. However, given that 2025 performance will likely reflect projections for 2026 and beyond, the trend seems to be that Nvidia will gradually lose some of its massive market share to AMD.\n\nAs a result, while I see both stocks as a Buy, AMD might be more appealing right now. It\u2019s less risky in terms of valuation adjusted for growth, though it\u2019s uncertain whether the market will reward AI and data center players that come in second place. Still, if I had to pick the better stock for 2025, I\u2019d go with AMD this time.\n\nDisclosure",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia wins, Intel loses, and xAI builds a Colossus: A year in compute",
            "link": "https://www.datacenterdynamics.com/en/analysis/nvidia-wins-intel-loses-and-xai-becomes-a-colossus-a-year-in-compute/",
            "snippet": "An incomplete list of the compute, storage, and networking stories everyone's been talking about in 2024",
            "score": 0.786039412021637,
            "sentiment": null,
            "probability": null,
            "content": "An incomplete list of the compute, storage, and networking stories everyone\u2019s been talking about in 2024\n\nAs the AI bubble continues to inflate, there\u2019s been no shortage of news to report on in the world of compute, storage, and networking in 2024. From startup funding announcements to quantum computing deployments, government sanctions, new hardware releases, and AI compute clusters on a scale we\u2019ve never seen before, trying to round up all the headlines from the last 12 months is a feat that may challenge even the most powerful supercomputer. That being said, there have been a few stand-out stories from the year that we can expect to continue well into 2025 and perhaps beyond. So, in case you missed them the first time around, here are some selected highlights (or lowlights) from the year.\n\n\u2013 Intel\n\nIntel\u2019s annus horribilis Of the many companies that were covered by DCD this year, arguably none have had a drama-filled 12 months quite like Intel. For the chipmaker, 2024 started with the announcement of an ambitious plan to expand its global chip fabrication footprint and carve out its Product and Foundry lines into two separate businesses, and ended with the chipmaker laying off 15,000 employees, stalling construction on its proposed fabs, and announcing the \u2019retirement\u2019 of its CEO, Pat Gelsinger. Following successive quarters where the company reported billions of dollars in losses, Gelsinger\u2019s departure was perhaps an inevitability but has left many in the industry wondering what\u2019s next for Intel. Gelsinger first joined the company as a teenage technician back in 1979, leaving in 2009 for two executive stints at EMC and VMware, before rejoining Intel as CEO in 2021. Upon his appointment as Intel\u2019s chief executive, he promised to return the company to its former glories with a five-year strategy he dubbed IDM (integrated device manufacturing), which would see the company manufacturing its own cutting-edge chips as well as supplying components to third parties. However, while the ambition was admirable, the execution did not match. Intel has been criticized for missing the boat regarding the AI boom, while its Gaudi 3 accelerator has been described as difficult to use. Software problems also meant the company failed to reach its target of $500 million in Gaudi 3 sales for the year. For now, Intel\u2019s future remains uncertain - although construction at many of its fabs outside the US remains stalled, the company was able to successfully negotiate an (albeit reduced) funding agreement under the US CHIPS and Science Act. That being said, speaking at the Barclays technology conference in San Francisco on December 12, the company\u2019s interim co-CEO Dave Zinsner said no decision had yet been made regarding a formal separation of the company\u2019s factory and product development divisions, while its other leader, MJ Holthaus, told attendees that moving forward, Intel would be focused on developing more generic accelerator offerings that would make the company more competitive. Whatever happens, Holthaus said Intel will remain focused on \u201c\u2026building world-class products and a world-class foundry, we\u2019re still highly invested in doing that and those two things together will help differentiate us in the marketplace.\u201d She said: \u201cAt the end of the day, if we build products that allow our customers to win, we\u2019ll win.\u201d\n\nNvidia CEO Jensen Huang \u2013 Nvidia\n\nWill anyone be able to usurp Nvidia? Where Intel has struggled, Nvidia has continued to soar, unveiling Blackwell, its most powerful GPU offering to date, in March before reaching a $3 trillion market cap in June and twice overtaking Apple to become the world\u2019s most valuable company. However, while Nvidia continues to have a stranglehold on the market - the waiting list for Blackwell hardware is now pushing 12 months, with new orders unlikely to be filled until late 2025 - one might start to wonder if the company that has driven much of the AI boom might have started to fly a bit too close to the sun. At launch, the news that its B100, B200, and GB200 offerings would be liquid-cooled and operate at between 700W and 1,200W certainly got people talking. However, Blackwell has been plagued with issues. First, an unexpected production error reported in August saw Nvidia announce it would be pushing deliveries back to early 2025, then reports started to emerge that the AI processors were overheating when linked together in 72-chip data center racks - the GB200 NVL72 configuration is capable of running 72 GB200 GPUs, 36 Grace CPUs, and nine NVLink switch trays, each of which has two NVLink switches. Despite the challenges with Blackwell, Nvidia has already announced the product\u2019s successor - Rubin - and earlier this year, Nvidia CEO Jensen Huang said that the company\u2019s updated roadmap will see it launch a new product family every year. While a shiny new product launch might be what\u2019s keeping Nvidia occupied right now, it should also be noted that hot GB200s might not be the only fires the company is fighting. In August, the US Department of Justice (DOJ) launched two separate antitrust probes into the GPU giant, evaluating whether the company has abused its market dominance and forced companies to buy additional products to receive GPUs while penalizing those that buy rival chips. The DOJ is also reportedly looking into the company's $700 million acquisition of Run:ai in April 2024 and its 2022 purchase of software firm Bright Computing. Nvidia\u2019s French offices were raided in 2023 and it\u2019s believed the French Autorit\u00e9 de la concurrence (Competition Authority) is considering raising anti-competition charges against the company. The UK and EU are looking into AI competition risks more broadly. That being said, it\u2019s unlikely Nvidia will be toppled any time soon however, attention should be paid to its competitors in 2025. AMD has had a very successful 2024, posting record data center revenues of $3.5 billion for Q3 2024. CEO Dr. Lisa Su told analysts on a revenue call that the company\u2019s MI325X will \u201ccompete very well with H200 and the MI350 series will compete very well with Blackwell,\u201d adding that it should also be assumed that the company was \u201cworking with all the large customers out there.\u201d And who knows, maybe Intel will make that comeback after all\u2026\n\nThe xAI supercomputer, in Memphis, Tennessee \u2013 Michael Dell\n\nxAI brings whole new meaning to supercomputer This year saw Elon Musk, the world\u2019s richest man, became the keeper of the world\u2019s largest GPU cluster - the 100,000-strong Colossus supercomputer that has made its home amongst many a disgruntled local in Memphis, Tennessee. And the good news for all those unhappy campers is that in October, Musk announced plans to first double its compute capacity, a figure that was then superseded in December when the city\u2019s Chamber of Commerce claimed xAI intends to expand the size of its Colossus supercomputer to one million GPUs. Currently, the 750,000 sq ft (69,677 sqm) former Electrolux plant houses 100,000 liquid-cooled Nvidia H100 GPUs and relies on the Nvidia Spectrum-X Ethernet networking platform for its Remote Direct Memory Access (RDMA) network. Servers have been provided by Supermicro and Dell. xAI claims the Colossus supercomputer, which is being used to train and run the company\u2019s AI chatbot Grok, was assembled in 122 days, with Musk announcing the cluster had gone live on July 22, 2024. While the arrival of xAI has been hailed by business leaders in Memphis as the largest multi-billion dollar investment in the city\u2019s history, campaigners have voiced concerns about the amount of power granted to the facility by grid operator Tennessee Valley Authority, as well as its impact on air quality in the city.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia Could Conquer PCs Next. Chip Makers Should Be Nervous.",
            "link": "https://www.barrons.com/articles/nvidia-ces-ai-chips-project-digits-85486af1",
            "snippet": "Project Digits, the first ever desktop-sized computer made by Nvidia, will be available in May, starting at $3000.",
            "score": 0.850321888923645,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Jensen Huang Says Nvidia Is a \u2018Technology Company,\u2019 but It\u2019s Really an AI Company",
            "link": "https://gizmodo.com/jensen-huang-says-nvidia-is-a-technology-company-but-its-really-an-ai-company-2000548869",
            "snippet": "Jensen Huang Says Nvidia Is a 'Technology Company,' but It's Really an AI Company ... Nvidia Ceo Jensen Huang holding up a Rtx 50 Series Gpu and a 50-series...",
            "score": 0.9361479878425598,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia is a \u201ctechnology company,\u201d not a \u201cconsumer\u201d or \u201centerprise\u201d company, as emphasized by CEO Jensen Huang. What does he mean, exactly? Doesn\u2019t Nvidia want consumers to spend hundreds or thousands of dollars on the new, expensive RTX 50-series GPUs? Don\u2019t they want more companies to buy their AI training chips? Nvidia is the kind of company with a lot of fingers in a lot of pies. To hear Huang tell it, if the crust of those pies is the company\u2019s chips, then AI is the filling.\n\n\u201dOur technology influence is going to impact the future of consumer platforms,\u201d Huang\u2014clad in his typical black jacket and the warm bosom of AI hype\u2014said in a Q&A with reporters a day after his blowout opening CES keynote. But how does a company like Nvidia fund all those epic AI experiments? The H100 AI training chips made Nvidia such a tech powerhouse over the past two years, with a few stumbles along the way. But Amazon and other companies are trying to create alternatives to cut out Nvidia\u2019s monopoly. What should happen if competition cuts the spree short?\n\n\u201cWe\u2019re going to respond to customers wherever they are,\u201d Huang said. Part of that is helping companies build \u201cagentic AI,\u201d AKA multiple AI models able to complete complex tasks. That includes several AI toolkits made to throw a bone to businesses. While the H100 has made Nvidia big, and RTX keeps gamers coming back, it wants its new $3,000 \u201cProject Digits\u201d AI processing hub to open up \u201ca whole new universe\u201d for those who can use it. Who will use it? Nvidia said it\u2019s a tool for researchers, scientists, and maybe students\u2014or at least those who stumble across $3,000 in their cup of $1.50 instant ramen they\u2019re eating for dinner for the fifth night in a row.\n\nNvidia made sure you knew about the RTX 5090\u2019s 3,352 TOPS of AI performance. Then, Huang\u2019s company dropped details on several software initiatives\u2014both gaming and non-gaming related. None of his declarations were more confusing than its \u201cworld foundation\u201d AI models. These models should be able to train on real-life environments, which could be used for helping autonomous vehicles or robots navigate their environment. It\u2019s a lot of future tech, and Huang admitted he failed to better articulate it to a crowd who had mostly come to see cool new GPUs.\n\n\u201c[The world foundation model] understands things like friction, inertia, grabbing, object presence, and elements, geometric and spatial understanding,\u201d he said. \u201cYou know, the things that children know. They understand the physical world in a way that language models didn\u2019t know.\u201d\n\nHuang opened up CES 2025 on Jan. 6 with a keynote that packed the Michelob Ultra arena in Las Vegas\u2019 Mandalay Bay casino. There was certainly a huge portion of gamers who\u2019d come to see the latest RTX 50-series cards in the flesh, but more were there to see how a company as lucrative as Nvidia moves forward. RTX and Project Digits drew hollers and shouts from the crowd. Spending half his time talking about his world foundation model, the audience didn\u2019t seem nearly as enthused.\n\nIt points to how awkward AI messaging can be, especially for a company that bears much of its popularity to the attentive population of PC gamers. There has been so much talk about AI that it\u2019s easy to forget Nvidia was in this game years before ChatGPT came on the scene. Nvidia\u2019s in-game AI upscaling tech, DLSS, has been around for close to six years, improving all the time, and it\u2019s now one of the best AI-upscalers in games, though limited by its exclusivity to Nvidia\u2019s cards. It was good before the advent of generative AI. Now, Nvidia promises Transformer models will further enhance upscaling and ray reconstruction.\n\nTo top it off, the touted multi-frame gen could possibly grant four times the performance for 50-series GPUs, at least if the game supports it. That is a boon for those who can afford the new RTX 50-series. The RTX 5090 tops off at $2,000. The gamers who would most benefit from frame gen are those who may only afford a lower-end GPU. Huang declined to offer any hints about an RTX 5050 or 5060, joking \u201cWe announced four cards, and you want more?\u201d\n\nThe world foundation model is just a prototype, just like much of Nvidia\u2019s new AI software on display to the public. The real questions are, when will it be ready for primetime, and who will end up using it? Nvidia showed off oddball AI NPCs, in-game chatbots, AI nurses, and an audio generator last year. This year, it wants to bloom with its world foundation model, plus a host of AI \u201cmicroservices,\u201d including a weird animated talking head that\u2019s supposed to serve as your PC\u2019s always-on assistant. Perhaps, some of these will stick. In the cases where Nvidia hopes AI replaces nurses or audio engineers, we hope that doesn\u2019t happen.\n\nHuang considers Nvidia \u201ca small company\u201d with 32,000 worldwide employees. Yes, that\u2019s less than half of the staff Meta has, but you can\u2019t think of it as small in terms of the market influence for AI training chips. Because of its market position, it holds an outsized influence on the tech industry. The more people using AI, the more people will need to buy its AI-specific GPUs, plus any of its other AI software. If everybody buys their own at-home AI processing chip, they don\u2019t have to rely on outside data centers and external chatbots. Nvidia, just like every tech company, just needs to find a use for AI beyond replacing all our jobs.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-09": {
        "0": {
            "title": "GeForce NOW at CES: Bring PC RTX Gaming Everywhere With the Power of GeForce NOW",
            "link": "https://blogs.nvidia.com/blog/geforce-now-thursday-ces-2025-recap/",
            "snippet": "Avowed from Obsidian Entertainment, known for iconic titles such as Fallout: New Vegas, will join GeForce NOW. The cloud gaming platform will also bring DOOM:...",
            "score": 0.7140761613845825,
            "sentiment": null,
            "probability": null,
            "content": "Cloud support coming to Steam Deck, Apple Vision Pro spatial computers, Meta Quest 3 and 3S, and Pico mixed-reality devices; plus, six new games join the cloud this week, with \u2018DOOM: The Dark Ages\u2019 and \u2018Avowed\u2019 at launch.\n\nThis GFN Thursday recaps the latest cloud announcements from the CES trade show, including GeForce RTX gaming expansion across popular devices such as Steam Deck, Apple Vision Pro spatial computers, Meta Quest 3 and 3S, and Pico mixed-reality devices.\n\nGamers in India will also be able to access their PC gaming library at GeForce RTX 4080 quallity with an Ultimate membership for the first time in the region. This follows expansion in Chile and Columbia with GeForce NOW Alliance partner Digevo.\n\nMore AAA gaming is on the way, with highly anticipated titles DOOM: The Dark Ages and Avowed joining GeForce NOW\u2019s extensive library of over 2,100 supported titles when they launch on PC later this year.\n\nPlus, no GFN Thursday is complete without new games. Get ready for six new titles joining the cloud this week.\n\nHead in the Clouds\n\nCES 2025 is coming to a close, but GeForce NOW members still have lots to look forward to.\n\nMembers will be able to play over 2,100 titles from the GeForce NOW cloud library at GeForce RTX quality on Valve\u2019s popular Steam Deck device with the launch of a native GeForce NOW app, coming later this year. Steam Deck gamers can gain access to all the same benefits as GeForce RTX 4080 GPU owners with a GeForce NOW Ultimate membership, including NVIDIA DLSS 3 technology for the highest frame rates and NVIDIA Reflex for ultra-low latency.\n\nGeForce NOW delivers a stunning streaming experience, no matter how Steam Deck users choose to play, whether in handheld mode for high dynamic range (HDR)-quality graphics, connected to a monitor for up to 1440p 120 frames per second HDR, or hooked up to a TV for big-screen streaming at up to 4K 60 fps.\n\nGeForce NOW members can take advantage of RTX ON with the Steam Deck for photorealistic gameplay on supported titles, as well as HDR10 and SDR10 when connected to a compatible display for richer, more accurate color gradients.\n\nGet immersed in a new dimension of big-screen gaming. In collaboration with Apple, Meta and ByteDance, NVIDIA is expanding GeForce NOW cloud gaming to Apple Vision Pro spatial computers, Meta Quest 3 and 3S, and Pico virtual- and mixed-reality devices \u2014 with all the bells and whistles of NVIDIA technologies, including ray tracing and NVIDIA DLSS.\n\nIn addition, NVIDIA will launch the first GeForce RTX-powered data center in India this year, making gaming more accessible around the world. This follows the recent launch of GeForce NOW in Colombia and Chile \u2014 operated by GeForce NOW Alliance partner Digevo \u2014 as well as Thailand coming soon \u2014 to be operated by GeForce NOW Alliance partner Brothers Picture.\n\nGame On\n\nAAA content from celebrated publishers is coming to the cloud. Avowed from Obsidian Entertainment, known for iconic titles such as Fallout: New Vegas, will join GeForce NOW. The cloud gaming platform will also bring DOOM: The Dark Ages from id Software \u2014 the legendary studio behind the DOOM franchise. These titles will be available at launch on PC this year.\n\nAvowed, a first-person fantasy role-playing game, will join the cloud when it launches on PC on Tuesday, Feb. 18. Take on the role of an Aedyr Empire envoy tasked with investigating a mysterious plague. Freely combine weapons and magic \u2014 harness dual-wield wands, pair a sword with a pistol or opt for a more traditional sword-and-shield approach. In-game companions \u2014 which join the players\u2019 parties \u2014 have unique abilities and storylines that can be influenced by gamers\u2019 choices.\n\nDOOM: The Dark Ages is the single-player, action first-person shooter prequel to the critically acclaimed DOOM (2016) and DOOM Eternal. Play as the DOOM Slayer, the legendary demon-killing warrior fighting endlessly against Hell. Experience the epic cinematic origin story of the DOOM Slayer\u2019s rage in 2025.\n\nShiny New Games\n\nLook for the following games available to stream in the cloud this week:\n\nRoad 96 (New release on Xbox, available on PC Game Pass, Jan. 7)\n\n(New release on Xbox, available on PC Game Pass, Jan. 7) Builders of Egypt (New release on Steam, Jan. 8)\n\n(New release on Steam, Jan. 8) DREDGE (Epic Games Store)\n\n(Epic Games Store) Drova \u2013 Forsaken Kin (Steam)\n\n(Steam) Kingdom Come: Deliverance (Xbox, available on Microsoft Store)\n\n(Xbox, available on Microsoft Store) Marvel Rivals ( Steam , coming to the cloud after the launch of Season 1)\n\nWhat are you planning to play this weekend? Let us know on X or in the comments below.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia\u2019s Little Desktop AI Box with Big Unified GPU/CPU Memory",
            "link": "https://www.hpcwire.com/2025/01/09/nvidias-little-desktop-ai-box-with-big-unified-gpu-cpu-memory/",
            "snippet": "Nvidia announced a new $3000 desktop computer developed in collaboration with MediaTek, which is powered by a new cut-down Arm-based Grace CPU and Blackwell...",
            "score": 0.8229506015777588,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Mega Omniverse Blueprint: Nvidia\u2019s AI-Driven Digital Twins",
            "link": "https://technologymagazine.com/articles/mega-omniverse-blueprint-nvidias-ai-driven-digital-twins",
            "snippet": "Nvidia's Mega Omniverse Blueprint revolutionises automation, enabling manufacturers to develop, test and optimise AI robot fleets in virtual environments.",
            "score": 0.7999330163002014,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Netherlands strikes deal with Nvidia for AI supercomputing hub",
            "link": "https://thenextweb.com/news/netherlands-strikes-deal-with-nvidia-for-ai-supercomputing-hub",
            "snippet": "The Dutch government has reached a deal with Nvidia to supply hardware and expertise for the construction of a potential AI facility.",
            "score": 0.8104091882705688,
            "sentiment": null,
            "probability": null,
            "content": "The Dutch government has reached a deal with Nvidia to supply hardware and expertise for the construction of an AI facility in the Netherlands.\n\nThe facility will centre around an AI supercomputer aimed at accelerating research and development, as the country pushes to further digitalise its economy, according to a statement made Thursday.\n\nThe Dutch Minister for Economic Affairs Dirk Beljaarts is currently in Silicon Valley, where he is meeting with executives at Nvidia. The world\u2019s second most valuable company, Nvidia is a world leader in graphics processing units (GPUs) for artificial intelligence applications.\n\n\u201cBefore a shovel can go into the ground, we need to be sure that the required knowledge and hardware is available,\u201d said Minister Beljaarts. \u201cToday the Netherlands has taken an important step together with Nvidia. This brings the construction of a Dutch AI facility a lot closer.\u201d\n\nThe \ud83d\udc9c of EU tech The latest rumblings from the EU tech scene, a story from our wise ol' founder Boris, and some questionable AI art. It's free, every week, in your inbox. Sign up now!\n\nThe ministry didn\u2019t provide a timeline or specific details for the AI facility. Nvidia declined our request for comment.\n\nThe Netherlands, which ranked in the top 10 most digitised countries globally last year, is pushing to position itself as a leader in AI. In January last year, the government ringfenced over \u20ac200mn to boost local investment in the technology.\n\n\u201cAsia and the US have taken the lead [in AI] and Europe will have to catch up,\u201d said the former economic affairs minister Micky Adriaansens at the time.\n\nKey priorities of the government\u2019s strategy include fostering AI talent, building infrastructure, ensuring safe applications, and enabling cooperation through entities like the Dutch AI Coalition. Last year, the Benelux nation also embarked on building its own \u201csafer\u201d large language model (LLM), called GPT-NL.\n\nTo execute its plans, the Netherlands is going to need more computing power to train AI machines. Aside from Nvidia, Beljaarts is also meeting with chipmaker AMD. The Minister is also in California to \u201cstrengthen relations\u201d between Dutch and American technology companies more broadly, the government said.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia, Google, OpenAI Turn To 'Synthetic Data' Factories To Train AI Models",
            "link": "https://www.investors.com/news/technology/nvidia-stock-tech-giants-use-synthetic-data-train-ai-models/",
            "snippet": "Nvidia, Google, OpenAI Turn To 'Synthetic Data' Factories To Train AI Models ... Nvidia (NVDA), Alphabet's Google (GOOGL) and hot startup OpenAI are turning to \"...",
            "score": 0.9300963878631592,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA), Alphabet's Google (GOOGL) and hot startup OpenAI are turning to \"synthetic data\" factories amid demand for massive amounts of data needed to train deep learning artificial intelligence algorithms. At this week's Consumer Electronics Show, the chipmaker's chief executive touted synthetic data's capabilities, which could be a plus for Nvidia stock.\n\nThe move toward synthetic data comes amid reports that AI companies are running out of real world data to train powerful AI models.\n\n\"Synthetic data offers a vital solution for addressing scarce or sensitive data requirements. This trend is accelerating as major AI companies exhaust available internet data for training,\" said data scientist Ben Lorica in a 2025 outlook report.\n\n\"Teams can already leverage foundation models to generate synthetic data for specific use cases, while larger organizations may combine synthetic data with their proprietary datasets,\" added Lorica, who edits the Gradient Flow AI newsletter. \"Look for improved synthetic data generation tools emerging from major AI labs, making this technology more accessible to practitioners.\"\n\nAt CES 2025, Nvidia CEO Jensen Huang touted artificial intelligence's future role in automotive and robotics applications. Synthetic data will help, he says.\n\nNvidia Stock: 'Data Factory'\n\n\"Nvidia is collecting and organizing traditional data and using them to create synthetic data,\" said economist Ed Yardeni in a report on CES developments. \"Both traditional and synthetic data will be used to train AI agents and robots in Nvidia's data factory.\"\n\nYardeni added: \"(The chipmaker) developed the Nvidia Cosmos by having it watch 20 million hours of video about nature, humans, and anything to do with the physical world. Based on those real scenarios, it can also create synthetic data to create even more scenarios. It can then use its real and synthetic data to train robots that need to navigate in the world, whether working in a warehouse or driving an autonomous vehicle.\"\n\nGoogle's cloud computing unit also is making a big push into synthetic data for enterprise applications. Further, OpenAI's newest foundation models capable of enhanced reasoning skills use synthetic data generation techniques.\n\nHeading into 2025, one big debate is whether AI models have started to plateau amid difficulty associated with sourcing high-quality, human-made training data. Also, tech behemoths like Google and Meta Platforms (META) own in-house proprietary data from YouTube, Maps, Instagram and Facebook that can be used to build bigger models.\n\nMeanwhile, Nvidia stock has gained 4% in 2025. Nvidia stock jumped 171% in 2024 after surging 239% in 2023.\n\nFollow Reinhardt Krause on Twitter @reinhardtk_tech for updates on artificial intelligence, cybersecurity and cloud computing.\n\nYOU MAY ALSO LIKE:\n\nLearn The Best Trading Stock Rules From IBD's Investor's Corner\n\nWant To Trade Options? Try Out These Strategies\n\nMonitor IBD's \"Breaking Out Today\" List For Companies Hitting New Buy Points\n\nIBD Digital: Unlock IBD's Premium Stock Lists, Tools And Analysis Today",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Hyundai Motor Group Partners with NVIDIA to Accelerate Development of AI Solutions for Future Mobility",
            "link": "https://www.prnewswire.com/news-releases/hyundai-motor-group-partners-with-nvidia-to-accelerate-development-of-ai-solutions-for-future-mobility-302347065.html",
            "snippet": "PRNewswire/ -- Hyundai Motor Group (the Group) today announced a strategic partnership with NVIDIA to accelerate the development of advanced AI technologies...",
            "score": 0.8895885348320007,
            "sentiment": null,
            "probability": null,
            "content": "Hyundai Motor Group is driving AI-powered innovation with the goal of applying intelligence across its core mobility solutions\n\nThe companies will collaborate in diverse areas, including adopting accelerated computing, generative AI, and industrial digitalization technologies, across the Hyundai Motor Group's business value chain\n\nLAS VEGAS and SEOUL, South Korea, Jan. 9, 2025 /PRNewswire/ -- Hyundai Motor Group (the Group) today announced a strategic partnership with NVIDIA to accelerate the development of advanced AI technologies to drive the future of mobility.\n\n(from left) Heung-Soo Kim, Executive Vice President and Head of Global Strategy Office at Hyundai Motor Group and Rishi Dhall, Vice President of Automotive at NVIDIA View PDF\n\nIn the AI era, Hyundai Motor Group is driving innovation through strategic AI integration, positioning itself at the forefront of smart mobility solutions. The Group operates a variety of AI initiatives and through this partnership aims to further enhance the application of intelligence to its core mobility products, such as software-defined vehicles and robotics, and across its business operations.\n\n\"Hyundai Motor Group is exploring innovative approaches with AI technologies in various fields such as robotics, autonomous driving, and smart factory,\" said Heung-Soo Kim, Executive Vice President and Head of Global Strategy Office at Hyundai Motor Group. \"This partnership is set to accelerate our progress, positioning the Group as a frontrunner in driving AI-empowered mobility innovation.\"\n\nAs part of the agreement, Hyundai Motor Group will harness NVIDIA accelerated computing and AI Enterprise software to help manage the massive amounts of data required to safely develop and train its AI models for various applications.\n\nThe Group will also utilize the NVIDIA Omniverse platform to develop physical AI and digital twin applications to simulate its factories, helping improve manufacturing efficiencies and quality, and streamline costs. In addition, the Group will use the NVIDIA Isaac robot development platform to develop and safely deploy AI robots.\n\nBoth parties will also work closely to create virtual simulation environments for safe and reliable autonomous driving technology and robotics systems.\n\n\"Accelerated computing, generative AI, and Omniverse are unlocking a new era of mobility,\" said Rishi Dhall, Vice President of Automotive at NVIDIA. \"This partnership will drive the creation of safer, more intelligent vehicles, supercharge manufacturing with greater efficiency and quality, and deploy cutting-edge robotics to help build a smarter, more connected digital workplace.\"\n\nStarting with these initiatives, the partnership aims to drive the development of groundbreaking innovations going forward, with more to be announced at a later date.\n\nAbout Hyundai Motor Group\n\nHyundai Motor Group is a global enterprise that has created a value chain based on mobility, steel, and construction, as well as logistics, finance, IT, and service. With about 250,000 employees worldwide, the Group's mobility brands include Hyundai, Kia, and Genesis. Armed with creative thinking, cooperative communication and the will to take on any challenges, we strive to create a better future for all.\n\nMore information about Hyundai Motor Group can be found at:\n\nhttp://www.hyundaimotorgroup.com or Newsroom: Media Hub by Hyundai, Kia Global Media Center (kianewscenter.com), Genesis Media Center.\n\nSOURCE Hyundai Motor Group",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Nvidia's $3,000 mini AI supercomputer draws scorn from Raja Koduri and Tiny Corp \u2014 AI server startup",
            "link": "https://hardforum.com/threads/nvidias-3-000-mini-ai-supercomputer-draws-scorn-from-raja-koduri-and-tiny-corp-ai-server-startup.2039093/",
            "snippet": "Koduri estimated that the FP16 performance of the upcoming GeForce RTX 5070 and even the $250 Intel Arc B580 \u201cseems close\u201d to what a Project Digits machine...",
            "score": 0.9249491691589355,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Jensen Huang says Nvidia\u2019s AI chips are outpacing Moore\u2019s Law",
            "link": "https://www.datacenterdynamics.com/en/news/jensen-huang-says-nvidias-ai-chips-are-outpacing-moores-law/",
            "snippet": "Jensen Huang says Nvidia's AI chips are outpacing Moore's Law ... Nvidia CEO Jensen Huang has said improvements to the chip giant's hardware are outpacing Moore's...",
            "score": 0.8784354329109192,
            "sentiment": null,
            "probability": null,
            "content": "In an interview with TechCrunch after his keynote speech at the Consumer Electronics Show (CES) in Las Vegas, Huang said \u201cOur systems are progressing way faster than Moore\u2019s Law.\u201d\n\nHe added: \u201cWe can build the architecture, the chip, the system, the libraries, and the algorithms all at the same time,\u201d he told the news outlet. \u201cIf you do that, then you can move faster than Moore\u2019s Law because you can innovate across the entire stack.\u201d\n\nMoore\u2019s Law is the theory that the number of transistors on computer chips would double every two years. The term was coined by Intel co-founder Gordon Moore in 1965.\n\nThe death of Moore\u2019s Law has been a much-debated topic in recent years, with chipmakers already pushing the limits of how small they can continue to make semiconductors. Huang himself declared Moore\u2019s Law dead in 2022. He said: \u201cThe ability for Moore\u2019s Law to deliver twice the performance at the same cost, or at the same performance, half the cost, every year and a half, is over.\u201d\n\nWhilst he seemingly did not repeat this claim to TechCrunch, the implication from Huang appears to be that we have moved beyond Moore\u2019s Law, saying that where it had helped to drive down computing costs in the past, \u201cthe same thing is going to happen with inference where we drive up the performance, and as a result, the cost of inference is going to be less.\u201d\n\nHuang, who coined Huang\u2019s Law in 2018, said that when it comes to running AI inference workloads, Nvidia\u2019s latest data center superchip is more than 30x faster than its predecessor.\n\nHuang\u2019s Law refers to the Nvidia CEO's theory that advancements in GPUs are growing much faster than that of traditional CPUs, an observation he made at Nvidia\u2019s 2018 GTC conference where he said the company\u2019s GPUs were \"25 times faster than five years ago\u201d and 20 times faster than comparable CPUs. Under Moore\u2019s Law, the GPUs would have only had an expected 10x performance increase.\n\nIn his interview with TechCrunch, he doubled down on his claim from November 2024 that the AI industry is experiencing \u201chyper Moore\u2019s Law.\u201d According to Huang, Nvidia\u2019s AI chips have seen a 1,000x improvement when compared to those being produced by the company a decade ago \u2013 although no metric for this claim was given.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Netherlands holds supply talks with Nvidia, AMD on AI-facility",
            "link": "https://www.reuters.com/technology/artificial-intelligence/netherlands-secures-nvidias-supply-possible-ai-facility-2025-01-09/",
            "snippet": "The Dutch government said on Thursday it is in discussions with U.S. chip firms Nvidia and AMD about suppling hardware and technological knowledge for a...",
            "score": 0.5905538201332092,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "9": {
            "title": "2 Key Reasons I'm Predicting Nvidia Stock Will Reach $200 in 2025",
            "link": "https://www.fool.com/investing/2025/01/09/2-key-reasons-predicting-nvidia-stock-200-in-2025/",
            "snippet": "Nvidia stock is trading at $144.47 as of this writing, but there are two reasons I think it could soar to $200 (or more) during 2025. If I'm right, it will...",
            "score": 0.6134912371635437,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) is a leading supplier of networking hardware and chips for gaming, computing, robotics, and especially data centers, which is where most artificial intelligence (AI) development takes place.\n\nWhen Nvidia stock went public in 1999, the company had a market capitalization of around $500 million. It has grown to a staggering $3.5 trillion since then, with more than $3 trillion of that value created in the last two years alone, thanks to surging demand for its AI data center chips.\n\nNvidia stock is trading at $144.47 as of this writing, but there are two reasons I think it could soar to $200 (or more) during 2025. If I'm right, it will value the company at almost $4.9 trillion, and translate into a 38% return for investors.\n\nThe first reason: A rapid ramp-up in Blackwell sales\n\nGraphics processing units (GPUs) are more effective in AI workloads than traditional central processing units (CPUs). They are specifically designed for parallel processing, meaning they can handle several tasks at once without losing performance. This is key because AI development is extremely data intensive.\n\nNvidia's flagship H100 GPU was built on its Hopper architecture, and it helped the company win 98% of the entire market for AI data center chips in 2023. It was superseded by the H200 GPU, which started shipping in mid-2024, and offered almost twice the performance. But in 2024, Nvidia revealed an entirely new architecture called Blackwell, which promises an even bigger leap in performance.\n\nThe Blackwell-based GB200 NVL72 GPU system can perform AI inference a whopping 30 times faster than the equivalent H100 system. That performance is unleashed by Nvidia's fifth generation of NVLink networking technology, which allows GPUs to communicate with each other faster than ever. The GB200 NVL72 system is also 25 times more energy-efficient than the equivalent H100 infrastructure, which can save data center operators substantial amounts of money on electricity costs.\n\nNvidia shipped 13,000 sample GB200 GPUs to customers during its fiscal 2025 third quarter (ended Oct. 31, 2024). Demand is significantly higher than supply right now, so sales are likely to ramp up very quickly.\n\nMorgan Stanley (which rates Nvidia stock as a top pick for 2025) forecasted that Nvidia would ship around 450,000 GB200 GPUs in the final three months of calendar 2024, followed by up to 800,000 in the first three months of 2025. Other analysts think Blackwell will scale a little more slowly, but they remain very optimistic about its potential this year.\n\nBlackwell revenue might even surpass Hopper revenue by April, which really highlights how fast Nvidia's business can transform.\n\nNvidia's fiscal year 2025 will wrap up at the end of this month. It's on track to deliver a record $128.6 billion in total revenue, representing 112% growth compared to fiscal 2024. If recent quarters are anything to go by, around 88% of the company's total revenue for the year will come from the data center segment, led by GPU sales.\n\nThat's a big shift from just three years ago in fiscal 2022, when the data center segment made up just 39% of its total business.\n\nThe second reason: Nvidia stock trades at an attractive valuation\n\nNvidia is operating with very high profit margins right now because GPU demand far exceeds supply, which gives the company pricing power. That's why its earnings per share more than tripled (year over year) over the last four quarters, to $2.62.\n\nUsing that figure, Nvidia stock currently trades at a price-to-earnings (P/E) ratio of 56.8, which is actually a discount to its 10-year average of 58.9. In other words, despite surging more than 800% over the last two years alone, Nvidia stock might still be cheap.\n\nThe picture is even more attractive when looking to the future. Wall Street's consensus forecast (provided by Yahoo) suggests that Nvidia could generate $4.43 in earnings per share during fiscal 2026 (which starts next month), placing the stock at a forward P/E ratio of just 32.6:\n\nThat implies that Nvidia stock would have to soar by 80% over the next 12 months just to trade in line with its 10-year average P/E ratio, which translates into a price of $260!\n\nHowever, midway through this year, Wall Street will start looking ahead to Nvidia's potential results for fiscal 2027. If its incredible run of growth appears likely to stall, investors might hesitate to send the stock as high as $260.\n\nIt's simply too early to know what will happen at this stage, especially because competition will almost certainly ramp up in the coming year.\n\nFor now, I think $200 is a realistic target for Nvidia stock during calendar 2025. It implies a P/E ratio of 45.5 at the end of the year, assuming Wall Street's earnings forecast proves accurate. It's a big enough discount to the stock's 10-year average P/E that investors will probably still find it attractively valued.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-08": {
        "0": {
            "title": "Unveiling a New Era of Local AI With NVIDIA NIM Microservices and AI Blueprints",
            "link": "https://blogs.nvidia.com/blog/rtx-ai-garage-ces-pc-nim-blueprints/",
            "snippet": "NIM microservices and AI Blueprints empower enthusiasts and developers to build, iterate and deliver AI-powered experiences to the PC faster than ever.",
            "score": 0.794221818447113,
            "sentiment": null,
            "probability": null,
            "content": "New NIM microservices and AI Blueprints unlock generative AI on RTX AI PCs and workstation \u2014 plus, more announcements from CES recapped in this first installment of the RTX AI Garage series.\n\nOver the past year, generative AI has transformed the way people live, work and play, enhancing everything from writing and content creation to gaming, learning and productivity. PC enthusiasts and developers are leading the charge in pushing the boundaries of this groundbreaking technology.\n\nCountless times, industry-defining technological breakthroughs have been invented in one place \u2014 a garage. This week marks the start of the RTX AI Garage series, which will offer routine content for developers and enthusiasts looking to learn more about NVIDIA NIM microservices and AI Blueprints, and how to build AI agents, creative workflow, digital human, productivity apps and more on AI PCs. Welcome to the RTX AI Garage.\n\nThis first installment spotlights announcements made earlier this week at CES, including new AI foundation models available on NVIDIA RTX AI PCs that take digital humans, content creation, productivity and development to the next level.\n\nThese models \u2014 offered as NVIDIA NIM microservices \u2014 are powered by new GeForce RTX 50 Series GPUs. Built on the NVIDIA Blackwell architecture, RTX 50 Series GPUs deliver up to 3,352 trillion AI operations per second of performance, 32GB of VRAM and feature FP4 compute, doubling AI inference performance and enabling generative AI to run locally with a smaller memory footprint.\n\nNVIDIA also introduced NVIDIA AI Blueprints \u2014 ready-to-use, preconfigured workflows, built on NIM microservices, for applications like digital humans and content creation.\n\nNIM microservices and AI Blueprints empower enthusiasts and developers to build, iterate and deliver AI-powered experiences to the PC faster than ever. The result is a new wave of compelling, practical capabilities for PC users.\n\nFast-Track AI With NVIDIA NIM\n\nThere are two key challenges to bringing AI advancements to PCs. First, the pace of AI research is breakneck, with new models appearing daily on platforms like Hugging Face, which now hosts over a million models. As a result, breakthroughs quickly become outdated.\n\nSecond, adapting these models for PC use is a complex, resource-intensive process. Optimizing them for PC hardware, integrating them with AI software and connecting them to applications requires significant engineering effort.\n\nNVIDIA NIM helps address these challenges by offering prepackaged, state-of-the-art AI models optimized for PCs. These NIM microservices span model domains, can be installed with a single click, feature application programming interfaces (APIs) for easy integration, and harness NVIDIA AI software and RTX GPUs for accelerated performance.\n\nAt CES, NVIDIA announced a pipeline of NIM microservices for RTX AI PCs, supporting use cases spanning large language models (LLMs), vision-language models, image generation, speech, retrieval-augmented generation (RAG), PDF extraction and computer vision.\n\nThe new Llama Nemotron family of open models provide high accuracy on a wide range of agentic tasks. The Llama Nemotron Nano model, which will be offered as a NIM microservice for RTX AI PCs and workstations, excels at agentic AI tasks like instruction following, function calling, chat, coding and math.\n\nSoon, developers will be able to quickly download and run these microservices on Windows 11 PCs using Windows Subsystem for Linux (WSL).\n\nTo demonstrate how enthusiasts and developers can use NIM to build AI agents and assistants, NVIDIA previewed Project R2X, a vision-enabled PC avatar that can put information at a user\u2019s fingertips, assist with desktop apps and video conference calls, read and summarize documents, and more. Sign up for Project R2X updates.\n\nBy using NIM microservices, AI enthusiasts can skip the complexities of model curation, optimization and backend integration and focus on creating and innovating with cutting-edge AI models.\n\nWhat\u2019s in an API?\n\nAn API is the way in which an application communicates with a software library. An API defines a set of \u201ccalls\u201d that the application can make to the library and what the application can expect in return. Traditional AI APIs require a lot of setup and configuration, making AI capabilities harder to use and hampering innovation.\n\nNIM microservices expose easy-to-use, intuitive APIs that an application can simply send requests to and get a response. In addition, they\u2019re designed around the input and output media for different model types. For example, LLMs take text as input and produce text as output, image generators convert text to image, speech recognizers turn speech to text and so on.\n\nThe microservices are designed to integrate seamlessly with leading AI development and agent frameworks such as AI Toolkit for VSCode, AnythingLLM, ComfyUI, Flowise AI, LangChain, Langflow and LM Studio. Developers can easily download and deploy them from build.nvidia.com.\n\nBy bringing these APIs to RTX, NVIDIA NIM will accelerate AI innovation on PCs.\n\nEnthusiasts are expected to be able to experience a range of NIM microservices using an upcoming release of the NVIDIA ChatRTX tech demo.\n\nA Blueprint for Innovation\n\nBy using state-of-the-art models, prepackaged and optimized for PCs, developers and enthusiasts can quickly create AI-powered projects. Taking things a step further, they can combine multiple AI models and other functionality to build complex applications like digital humans, podcast generators and application assistants.\n\nNVIDIA AI Blueprints, built on NIM microservices, are reference implementations for complex AI workflows. They help developers connect several components, including libraries, software development kits and AI models, together in a single application.\n\nAI Blueprints include everything that a developer needs to build, run, customize and extend the reference workflow, which includes the reference application and source code, sample data, and documentation for customization and orchestration of the different components.\n\nAt CES, NVIDIA announced two AI Blueprints for RTX: one for PDF to podcast, which lets users generate a podcast from any PDF, and another for 3D-guided generative AI, which is based on FLUX.1 [dev] and expected be offered as a NIM microservice, offers artists greater control over text-based image generation.\n\nWith AI Blueprints, developers can quickly go from AI experimentation to AI development for cutting-edge workflows on RTX PCs and workstations.\n\nBuilt for Generative AI\n\nThe new GeForce RTX 50 Series GPUs are purpose-built to tackle complex generative AI challenges, featuring fifth-generation Tensor Cores with FP4 support, faster G7 memory and an AI-management processor for efficient multitasking between AI and creative workflows.\n\nThe GeForce RTX 50 Series adds FP4 support to help bring better performance and more models to PCs. FP4 is a lower quantization method, similar to file compression, that decreases model sizes. Compared with FP16 \u2014 the default method that most models feature \u2014 FP4 uses less than half of the memory, and 50 Series GPUs provide over 2x performance compared with the previous generation. This can be done with virtually no loss in quality with advanced quantization methods offered by NVIDIA TensorRT Model Optimizer.\n\nFor example, Black Forest Labs\u2019 FLUX.1 [dev] model at FP16 requires over 23GB of VRAM, meaning it can only be supported by the GeForce RTX 4090 and professional GPUs. With FP4, FLUX.1 [dev] requires less than 10GB, so it can run locally on more GeForce RTX GPUs.\n\nWith a GeForce RTX 4090 with FP16, the FLUX.1 [dev] model can generate images in 15 seconds with 30 steps. With a GeForce RTX 5090 with FP4, images can be generated in just over five seconds.\n\nGet Started With the New AI APIs for PCs\n\nNVIDIA NIM microservices and AI Blueprints are expected to be available starting next month, with initial hardware support for GeForce RTX 50 Series, GeForce RTX 4090 and 4080, and NVIDIA RTX 6000 and 5000 professional GPUs. Additional GPUs will be supported in the future.\n\nNIM-ready RTX AI PCs are expected to be available from Acer, ASUS, Dell, GIGABYTE, HP, Lenovo, MSI, Razer and Samsung, and from local system builders Corsair, Falcon Northwest, LDLC, Maingear, Mifcon, Origin PC, PCS and Scan.\n\nGeForce RTX 50 Series GPUs and laptops deliver game-changing performance, power transformative AI experiences, and enable creators to complete workflows in record time. Rewatch NVIDIA CEO Jensen Huang\u2019s keynote to learn more about NVIDIA\u2019s AI news unveiled at CES.\n\nSee notice regarding software product information.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia vs. AMD: Putting the GPU giants side-by-side",
            "link": "https://mashable.com/article/amd-vs-nvidia-graphics-cards-ces-2025",
            "snippet": "On price alone, AMD has a leg up on Nvidia. AMD offers more affordable graphics card options, focusing on the budget and midrange options.",
            "score": 0.8915905356407166,
            "sentiment": null,
            "probability": null,
            "content": "Table of Contents\n\nTable of Contents Table of Contents AMD vs. Nvidia: Price AMD vs. Nvidia: Performance AMD vs. Nvidia: Beyond gaming AMD vs. Nvidia: Which is better?\n\nThe most anticipated keynote of CES 2025 came from Nvidia, where the brand announced its highly anticipated GeForce RTX 50 series. However, Nvidia isn't alone in the GPU market. AMD, with its Radeon series of graphics cards, gives Nvidia a run for its money. While AMD didn't join Nvidia in making a GPU announcement at CES, instead focusing on its Ryzen CPUs, the two giants dominate the graphics field.\n\nWhether you're browsing gaming laptops and PCs, or are hoping to build your own, you need to get familiar with GPUs, specifically the ones from Nvidia and AMD. As the leaders in the market, you don't need to look further than these two, but which one is better? Well, the answer isn't that simple.\n\nHere's what you need to know about AMD and Nvidia when putting the brands head to head.\n\nAMD vs. Nvidia: Price\n\nOn price alone, AMD has a leg up on Nvidia. AMD offers more affordable graphics card options, focusing on the budget and midrange options. AMD's flagship GPUs, the AMD Radeon RX 70 series, start as low as $269.99, with the upper range peaking at $999.\n\nMeanwhile, Nvidia's existing GeForce 40 series starts at $399.99 with the GeForce RTX 4060 TI, while its most advanced unit, the GeForce RTX 4090, starts at $1,599. However, the 40 series is old news at CES 2025, as the Nvidia 50 series, coming Jan. 30 will range in price from $549 to $1,999.\n\nSo if you're shopping on price alone, AMD is going to be the most affordable option. However, it's important to note that AMD has reportedly stopped producing some of its Radeon RX series to make way for the RNDA 4 GPUs. Based on AMDs historical prices, we still expect the latest GPUs to fall below Nvidia's prices.\n\nAMD vs. Nvidia: Performance\n\nThe competition gets tougher when considering the performance of AMD and Nvidia GPUs. The Nvidia GeForce RTX 4090 is regarded as the best GPU on the market, beating out most of AMD's units. Not only is the 4090 exceptional for 4K gaming, it's the best option if you plan on creating games or working with AI. The 4090 is soon to be replaced by the 5090, which is built with and for AI, enhancing graphics for superior clarity and realistic dimensions.\n\nMashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Loading... Sign Me Up By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy Thanks for signing up!\n\nGenerally, Nvidia is going to offer better ray tracing, the mechanic that simulates lights, creating those realistic displays that the brand is known for. However, when you're not looking at the high-end GPUs from each brand, AMD can compete when you get into midrange and budget options. For example, the midrange AMD Radeon 7700 XT can easily compete with the Nvidia GeForce 4060.\n\nIt should be noted that our favorite gaming laptop, the Alienware m16, is loaded with the Nvidia GeForce RTX 4070. Plus, Alienware is doubling down on Nvidia's new 50 series, as its Area-51 line will come loaded with the new graphics cards.\n\nAMD vs. Nvidia: Beyond gaming\n\nMost of the time, we're discussing GPUs in terms of gaming and how the processors render games. However, GPUs are crucial outside of this realm. AMD keeps its priorities focused on gaming, updating its technology to compete in the growing market of 4K gaming. Its GPUs are generally more user-friendly, not requiring its own software to be exclusively used on the cards \u2014 you can even use AMD software on Nvidia graphic cards.\n\nHowever, if you're building a computer for more than just gaming, planning on doing some heavy-duty video editing, or working with AI, Nvidia is going to be the better choice as its GPUs are already incorporating AI into its devices. The Nvidia GeForce 50 series comes with the Nvidia Studio, a program to assist in video editing, 3D rendering, and graphic design, maximizing your graphics beyond just gaming.\n\nAMD vs. Nvidia: Which is better?\n\nThere really isn't a definitive answer on which GPU maker is superior. The answer is far more nuanced and depends on user needs \u2014 and budget.\n\nIf you're just getting started, playing around with building your first PC, AMD is the most accessible GPU. Budget-wise, it can't be beat, and its midrange options remain friendly on your wallet while managing to compete with Nvidia's performance. However, it should be noted that AMD GPUs seem to be taking a backseat with the brand. The Radeon series is currently out of stock, and we're waiting to hear more on the brand's new AMD line-up.\n\nBut for the more experienced users, investing in Nvidia's GPUs is a worthwhile endeavor. If you plan on working on massive tasks from video game creation to AI, the Nvidia graphics cards are built to handle these jobs, and while you're going to pay nearly $2,000 for a GPU, you are getting the best the market has to offer.\n\nFor the latest tech, Nvidia's GeForce 50 series is coming on Jan. 30, and while it's currently unavailable for preorder, you can shop the 40 series now.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "NVIDIA Quad Slot Beast GPU (FE-style)",
            "link": "https://hardforum.com/threads/nvidia-quad-slot-beast-gpu-fe-style.2039066/",
            "snippet": "4 slot NVIDIA FE design https://youtu.be/lyliMCnrANI?si=Gr1HRE_tZowOUYYs.",
            "score": 0.8954328298568726,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "NVIDIA Unveils Project DIGITS Personal AI Supercomputer",
            "link": "https://www.hpcwire.com/off-the-wire/nvidia-unveils-project-digits-personal-ai-supercomputer/",
            "snippet": "NVIDIA Unveils Project DIGITS Personal AI Supercomputer ... Jan. 8, 2025 \u2014 NVIDIA has unveiled NVIDIA Project DIGITS, a personal AI supercomputer that provides AI...",
            "score": 0.8460872173309326,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "The Ultimate Guide to Investing in Nvidia for Maximum Returns",
            "link": "https://www.fool.com/investing/2025/01/08/the-ultimate-guide-to-investing-in-nvidia-for-maxi/",
            "snippet": "Nvidia's stock has skyrocketed over the past 10 years. It still looks reasonably valued relative to its long-term growth potential.",
            "score": 0.9137943983078003,
            "sentiment": null,
            "probability": null,
            "content": "The AI chipmaker's stock might seem too hot to handle, unless you follow these rules.\n\nNvidia (NVDA 5.27%) has been one of the greatest growth stocks in recent history. Over the past decade, the chipmaker's share price (adjusted for splits) soared by 28,610%. That would have turned a modest $1,000 investment into about $287,100.\n\nThat dazzling growth was initially driven by brisk sales of Nvidia's graphics processing units (GPUs) for video games, as well as to support professional visualization software and cryptocurrency mining. But over the past few years, sales of high-end data center GPUs for processing artificial intelligence (AI) tasks have eclipsed its sales of chips for other uses and become the company's primary growth engine.\n\nFrom fiscal 2014 to fiscal 2024 (which ended in January 2024), Nvidia's revenue grew at a compound annual growth rate (CAGR) of 31% as its net income rose at a CAGR of 52%. From fiscal 2024 to fiscal 2027, analysts expect its revenue and net income to grow at CAGRs of 57% and 65%, respectively, as the AI market continues to expand.\n\nBased on those rosy expectations, Nvidia's stock still looks reasonably valued at 34 times forward earnings. But before you start a new position in this chipmaker, you should review these strategies which could help you maximize your long-term returns.\n\nUnderstand Nvidia's strengths and weaknesses\n\nThe bull thesis for Nvidia is easy to understand, but investors should recognize the longer-term challenges. Nvidia generated 88% of its revenue from its data center chips in its latest reported quarter, which makes it a pure bet on the secular growth of the AI market. If that expansion slows down, Nvidia's sales growth could abruptly stall out.\n\nNvidia could also face more intense competition in that space. It accounted for 98% of all data center GPU shipments worldwide in 2023, according to TechInsights, but Advanced Micro Devices could gradually gain more ground with its lower-cost Instinct GPUs. Many of Nvidia's top hyperscale data center customers -- including Microsoft, Amazon, and Alphabet -- are also developing their own AI accelerator chips.\n\nTrade restrictions for AI chips could also throttle its growth. Nvidia's sales of data center GPUs to China have already been curbed by export bans, and it could face antitrust charges as it continues to monopolize the high-end data center GPU market.\n\nUse dollar-cost averaging to offset the volatility\n\nNvidia's stock has already been volatile, and the challenges ahead -- which could include some steep and protracted drawdowns -- could shake out a lot of investors. To offset how such volatility might impact you over the long term, follow a strategy of dollar-cost averaging, which is the practice of buying set dollar amounts of a stock at regular intervals regardless of its trading price.\n\nFor example, you might commit to investing $1,000 in Nvidia on the first trading day of every month. If its shares are trading lower, that sum will buy more shares. If they're trading higher, it will buy fewer shares. Over a stretch of multiple years, those incremental purchases will dilute your overall risk and even out your long-term returns. It's a strategy that generally works well with more volatile investments.\n\nLock up your shares in an IRA\n\nMany investors sold shares of Nvidia over the past decade, and looking back, many would likely feel they did so prematurely. Though it doubtless seemed prudent to them to take profits on an investment that had massively rallied, patiently sticking with Nvidia and doing nothing would have been the smarter move.\n\nAs Warren Buffett's mentor Benjamin Graham once said, the \"investor's chief problem -- and even his worst enemy -- is likely to be himself.\" So to prevent yourself from becoming your own worst enemy and selling your shares of Nvidia too early, you should lock them up in a traditional or Roth IRA. If you're under the age of 50, you can contribute up to $7,000 per year into both types of IRAs, but you can't withdraw those funds without incurring penalties and taxes before the age of 59 1/2.\n\nWhile you can still buy and sell your shares of Nvidia within an IRA, the inability to withdraw those funds until you're much older might prevent you from impulsively taking profits. So by contributing $7,000 per year to an IRA, using those funds to purchase shares of Nvidia on an annual basis, and not selling those shares until you can withdraw those funds without any penalties should keep you on a disciplined path of dollar-cost averaging while maximizing your long-term returns.\n\nDon't just set it and forget it\n\nInvestors should buy and hold Nvidia for the long term if they expect the company to maintain its lead in the booming AI hardware market. But they shouldn't blindly stick with the stock and ignore its earnings reports and evolving market challenges. If the bull thesis for this chip giant is eventually disrupted, investors should still be ready to sell the stock to lock in their profits.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA CEO\u2019s Predictions Rain on Quantum Stock Parade",
            "link": "https://thequantuminsider.com/2025/01/08/nvidia-ceos-predictions-rain-on-quantum-stock-parade/",
            "snippet": "Jensen Huang's prediction that \"really useful\" quantum computers are likely 20 years away was linked to a drop in quantum computing stocks.",
            "score": 0.8481961488723755,
            "sentiment": null,
            "probability": null,
            "content": "Insider Brief\n\nNVIDIA CEO Jensen Huang\u2019s prediction that \u201cvery useful\u201d quantum computers are likely 20 years away has been linked to a sharp drop in quantum computing stocks, with companies like IonQ and Rigetti experiencing significant after-hours losses.\n\nHuang\u2019s remarks align with NVIDIA\u2019s strategy of focusing on a hybrid quantum-classical era and reflect the company\u2019s collaboration between quantum processors and classical systems to tackle complex computational tasks.\n\nWhile the statement might seem conservative, it is not too far off current industry roadmaps, which foresee practical quantum advancements within the next decade and fault-tolerant quantum supremacy further down the line.\n\nImage: Jensen Huang\u2026 not holding a very useful quantum computer. (NVIDIA)\n\nFinancial media are blaming NVIDIA CEO Jensen Huang\u2019s predictions about quantum computing\u2019s eventual maturity for a pullback among many quantum stocks yesterday.\n\nIn a question-and-answer session with analysts, Huang said that \u201cvery useful\u201d quantum computing was a few decades away.\n\n\u201cIf you kind of said 15 years for very useful quantum computers, that would probably be on the early side. If you said 30, it\u2019s probably on the late side,\u201d Huang told the analysts, Investor\u2019s Business Daily (IBD) reported. \u201cIf you picked 20, I think a whole bunch of us would believe it.\u201d\n\nAs an update \u2014 Huang said the same thing in 2022 at BW Businessworld.\n\nAs a result, the financial news service reported that quantum computing stocks \u2014 including IonQ (IONQ), Rigetti Computing (RGTI), D-Wave Quantum (QBTS) and Quantum Computing (QUBT) plunged after Nvidia (NVDA) \u2014 plunged. IonQ and Quantum Computing Inc. dropped more than 16% after hours while D-Wave Quantum Inc. and Rigetti Computing Inc. fell more than 14%, according to IBD.\n\nThe news may have come as a shock to investors because NVIDIA seemed to be a champion of quantum computing, recently signing a number of key deals with quantum players. NVIDIA announced it is working with Google Quantum AI to accelerate the design of its devices in November, for example, while using the recent SC24 to publicize a long list of partnerships with quantum companies, like Anyon Technologies, IonQ and a range of others.\n\nHowever, parsing Huang\u2019s words a bit, the statement does not seem to reflect a new stance on quantum and, in fact, aligns tightly with NVIDIA\u2019s overall quantum strategy. The predictions \u2014 again depending on the exact meaning of Huang\u2019s phrasing and its alignment with definitions among the quantum industry\u2013 seem to also jibe with most quantum roadmaps already in the books, although admittedly more on the conservative side.\n\nVery Useful or Just Useful?\n\nFirst, there\u2019s a need to parse \u201cvery useful\u201d part (particularly that \u201cvery\u201d part) \u2014 which might suggest a standalone, universal fault tolerant machine \u2014 of Huang\u2019s prediction. Most current roadmaps see that universal machine as the ultimate destination in a process of iterations leading to devices that deliver more and more practical uses.\n\nWhile the industry has yet to agree on official terminology, the process usually starts with scientific advantage \u2014 a quantum or hybrid system that can perform a scientific task more efficiently than a purely classical counterpart; then, practical quantum advantage \u2014 a quantum or hybrid system that can perform a task more efficiently than a purely classical counterpart; then quantum advantage \u2014 a quantum system that can perform a task much more efficiently than a classical computer or supercomputer. Eventually, there\u2019s quantum supremacy, which would be like the result delivered by Google\u2019s Willow chip, but in a practical, real-world task rather than the recent random circuit sampling (RCS), which is a benchmarking test.\n\nIf so \u2014 and it\u2019s important to reiterate that every entity probably defines these terms and their manifestations a bit differently \u2014 Huang\u2019s predictions are close to most roadmaps that see the beginning advantage about five years out with those other levels hitting in successive intervals after that \u2014 so ten to fifteen years for quantum supremacy is not as outlandish as it might seem.\n\nThe Hybrid Future\n\nIt\u2019s also important to consider that NVIDIA\u2019s quantum strategy is built around a lengthy \u201chybrid quantum\u201d era when quantum computers and classical computers work in collaboration based on tasks each is better at handling. One might add that the quantum strategy is existentially built around that concept because NVIDIA\u201ds business is built around AI \u2014 and, therefore, by extension \u2014 around supercomputing.\n\nLast year, Huang outlined the case for NVIDIA\u2019s role in this hybrid quantum relationship at GTC 2024, Motley Fool reports.\n\nHuang told the crowd: \u201cWe should be a great partner for the quantum computing industry. How else are you going to drive a quantum computer? To have the world\u2019s fastest computer sitting next to it? And how are you going to simulate a quantum computer, emulate a quantum computer? What is the programming model for a quantum computer? You can\u2019t just program a quantum computer all by itself. You need to have classical computing sitting next to it. And so the quantum would be kind of a quantum accelerator. \u2026 Who should go do that? Well, we\u2019ve done that.\u201d\n\nNumerous quantum experts, including Dr. Bob Sutor, of the Futurum Group, see that the future will always be hybrid based on the nature of quantum computing\u2019s ability to excel at certain tasks while being theoretically useless at other ones.\n\nIn any case, for NVIDIA, the longer the hybrid quantum era, the better. In fact, a standalone quantum device in a few years might put NVIDIA out of business in quick order.\n\nUltimately, then, Huang\u2019s predictions don\u2019t seem to be knock on quantum computing in general \u2014 which Huang reportedly \u201cloves\u201d \u2014 nor does his statement appear to reflect any change in NVIDIA\u2019s strategy to support quantum companies in this hybrid quantum vision.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "6": {
            "title": "Nvidia CEO Teases CPU Ambitions for PCs",
            "link": "https://www.pcmag.com/news/nvidia-ceo-teases-cpu-ambitions-for-pcs",
            "snippet": "CEO Jensen Huang said Nvidia has more in store for the desktop CPU space. \"You know, obviously we have plans,\u201d he said, according to Reuters.",
            "score": 0.8581078052520752,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "7": {
            "title": "Kion Group teams with Accenture and Nvidia to design intelligent warehouses",
            "link": "https://www.dcvelocity.com/technology/supply-chain-it/simulation-vr-digital-twin/kion-group-teams-with-accenture-and-nvidia-on-intelligent-warehouses",
            "snippet": "Kion said it will use Nvidia's technology to provide digital twins of warehouses that allows facility operators to design the most efficient and safe warehouse...",
            "score": 0.6685369610786438,
            "sentiment": null,
            "probability": null,
            "content": "German lift truck giant Kion Group will work with the consulting firm Accenture to optimize supply chain operations using advanced AI and simulation technologies provided by microchip powerhouse Nvidia, the companies said Tuesday.\n\nThe three companies say the deal will allow clients to both define ideal set-ups for new warehouses and to continuously enhance existing facilities with Mega, an Nvidia Omniverse blueprint for large-scale industrial digital twins. The strategy includes a digital twin powered by physical AI \u2013 AI models that embody principles and qualities of the physical world \u2013 to improve the performance of intelligent warehouses that operate with automated forklifts, smart cameras and automation and robotics solutions.\n\nThe partners\u2019 approach will take advantage of digital twins to plan warehouses and train robots, they said. \u201cFuture warehouses will function like massive autonomous robots, orchestrating fleets of robots within them,\u201d Jensen Huang, founder and CEO of Nvidia, said in a release. \u201cBy integrating Omniverse and Mega into their solutions, Kion and Accenture can dramatically accelerate the development of industrial AI and autonomy for the world\u2019s distribution and logistics ecosystem.\u201d\n\nKion said it will use Nvidia\u2019s technology to provide digital twins of warehouses that allows facility operators to design the most efficient and safe warehouse configuration without interrupting operations for testing. That includes optimizing the number of robots, workers, and automation equipment. The digital twin provides a testing ground for all aspects of warehouse operations, including facility layouts, the behavior of robot fleets, and the optimal number of workers and intelligent vehicles, the company said.\n\nIn that approach, the digital twin doesn\u2019t stop at simulating and testing configurations, but it also trains the warehouse robots to handle changing conditions such as demand, inventory fluctuation, and layout changes. Integrated with Kion\u2019s warehouse management software (WMS), the digital twin assigns tasks like moving goods from buffer zones to storage locations to virtual robots. And powered by advanced AI, the virtual robots plan, execute, and refine these tasks in a continuous loop, simulating and ultimately optimizing real-world operations with infinite scenarios, Kion said.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Nvidia CEO unveils new tools at CES, setting stage for advanced robotics",
            "link": "https://www.semafor.com/article/01/08/2025/nvidia-ceo-unveils-new-tools-at-ces-setting-stage-for-smarter-robotics",
            "snippet": "Jensen Huang announced a new, open-source tool called Cosmos that can generate lifelike training data for robotics at the Las Vegas CES conference.",
            "score": 0.6159504055976868,
            "sentiment": null,
            "probability": null,
            "content": "The Consumer Electronics Show, which for years has been rather sleepy in terms of big announcements, got a gift from Nvidia CEO Jensen Huang earlier this week when he announced the company\u2019s new gaming GPU and other AI-related products.\n\nIt was Huang\u2019s comments on robotics that were the most revealing. He announced a new, open-source tool called Cosmos that can generate lifelike training data for robotics. He said it will reduce the cost of developing self-driving car technology, which he predicted will be the first multitrillion-dollar robotics market. Nvidia is partnering with Toyota to provide an autonomous driving operating system.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Agentech Joins NVIDIA Inception Program to Accelerate AI-Powered Claims Automation with Digital Agents",
            "link": "https://coverager.com/agentech-joins-nvidia-inception-program-to-accelerate-ai-powered-claims-automation-with-digital-agents/",
            "snippet": "Agentech, a leader in AI-driven claims automation, is thrilled to announce its participation in the NVIDIA Inception Program.",
            "score": 0.6458144783973694,
            "sentiment": null,
            "probability": null,
            "content": "Agentech Selected by NVIDIA for Inception Program, Showcasing Leadership in Claims Automation and Access to Advanced AI Resources Within the Insurance Industry.\n\nJanuary 8, 2025, Tulsa, OK \u2014 Agentech, a leader in AI-driven claims automation, is thrilled to announce its participation in the NVIDIA Inception Program, an exclusive accelerator that supports startups innovating in artificial intelligence, data science, and high-performance computing. This partnership underscores Agentech\u2019s commitment to transforming insurance claims workflows through advanced AI capabilities.\n\nThis milestone marks a significant step forward in Agentech\u2019s mission to provide a robust catalog of Agentic agents, adding to their digital workforce of hundreds of agents to support the desk adjuster, claims handler, and field adjuster by automating manual tasks within the claims process. Agentech builds, trains, and deploys AI Agents to tackle the manual tasks that slow down the claims process.\n\nDriving Innovation in Claims Automation\n\nAs a member of the NVIDIA Inception Program, Agentech will gain access to a suite of world-class resources, including:\n\nDeveloper Courses: Advanced training to sharpen technical expertise and innovation.\n\nCloud Computing Credits: Scalable, high-performance computing power to enhance AI model development and deployment.\n\nDiscounts on Software and Hardware: Cutting-edge tools to optimize performance and efficiency.\n\nEngineering Courses and Resources: Specialized learning opportunities to deepen technical capabilities.\n\nThese resources will empower Agentech\u2019s team (and their client partners) to accelerate the development of its AI-driven solutions, which are designed to automate time-consuming claims tasks and improve accuracy, efficiency, and customer satisfaction across the insurance industry.\n\n\u201cAcceptance into the NVIDIA Inception Program is a tremendous opportunity for Agentech,\u201d said Spencer James, founding CTO of Agentech. \u201cThis partnership not only validates the innovative work we\u2019re doing but also equips us with the tools and expertise we need to scale our technology and deliver even greater value to our customers.\u201d These resources will enable Agentech to enhance its Agentic AI Agents\u2014an ensemble of digital assistants designed to automate repetitive claims tasks, streamline operations, and empower adjusters to focus on complex decision-making.\n\nReal-World Impact for Insurance Clients\n\n\u201cThe NVIDIA Inception Program fosters innovation and collaboration among startups leading advancements in AI,\u201d said Alex Pezold, CEO and co-founder of Agentech. \u201cWe\u2019re excited to partner with NVIDIA as we continue bringing transformative AI solutions to the insurance industry, enhancing efficiency and delivering impactful results.\u201d\n\nAgentech\u2019s solutions, powered by NVIDIA\u2019s state-of-the-art GPU technology, are already transforming claims workflows by:\n\nAutomating Document Review and Compliance: Reducing cycle times while ensuring adherence to regulatory guidelines.\n\nEnhancing Fraud Detection: Identifying anomalies in claims data to minimize financial losses.\n\nOptimizing Claims Resolution: Accelerating assessments for faster payouts and improved customer satisfaction.\n\nScalability During Peak Periods: Handling high claims volumes with robust performance, particularly during catastrophic events.\n\nA Look Ahead\n\nAs the insurance industry faces increasing pressure to modernize and streamline operations, Agentech\u2019s inclusion in the NVIDIA Inception Program marks a pivotal step in advancing intelligent automation. By combining Agentech\u2019s expertise in claims automation with NVIDIA\u2019s groundbreaking AI infrastructure, this partnership paves the way for a future where insurance carriers operate more efficiently and effectively than ever before.\n\nAbout the NVIDIA Inception Program\n\nThe NVIDIA Inception Program is renowned for fostering innovation among startups at the forefront of AI and data science. The program supports startups driving advancements in AI and data science by providing critical resources, technical expertise, and go-to-market support to accelerate innovation and scalability.\n\nAbout Agentech\n\nAgentech is an AI-driven SaaS company providing AI Agents to revolutionize claims processing by automating a wide range of administrative tasks within the workflow, enhancing adjuster productivity and accuracy. While their digital workforce doesn\u2019t make decisions or adjudicate claims, it supports desk adjusters and claims handlers by completing the mundane, time-consuming tasks and presenting salient claim information at key decision points to the user, allowing them to focus on complex decision-making and exceptional customer service. By leveraging AI and machine learning, Agentech\u2019s Agentic AI solutions streamline claims tasks, reduce manual effort, and enhance operational efficiency for insurers, TPAs, IA Firms, and Third-Party Providers worldwide.\n\nTo learn more, visit www.agentech.com.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-07": {
        "0": {
            "title": "Nvidia\u2019s New AI Innovations at CES 2025: Explained",
            "link": "https://technologymagazine.com/articles/nvidias-new-ai-innovations-at-ces-2025-explained",
            "snippet": "Nvidia unveils groundbreaking AI innovations at CES 2025 that aim to progress the gaming, content creation, automotive and manufacturing industries.",
            "score": 0.9359541535377502,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Why Enterprises Need AI Query Engines to Fuel Agentic AI",
            "link": "https://blogs.nvidia.com/blog/ai-query-engines-agentic-ai/",
            "snippet": "An AI query engine is a sophisticated system that efficiently processes large amounts of data, extracts and stores knowledge, and performs semantic search on...",
            "score": 0.9332062602043152,
            "sentiment": null,
            "probability": null,
            "content": "Data is the fuel of AI applications, but the magnitude and scale of enterprise data often make it too expensive and time-consuming to use effectively.\n\nAccording to IDC\u2019s Global DataSphere1, enterprises will generate 317 zettabytes of data annually by 2028 \u2014 including the creation of 29 zettabytes of unique data \u2014 of which 78% will be unstructured data and 44% of that will be audio and video. Because of the extremely high volume and various data types, most generative AI applications use a fraction of the total amount of data being stored and generated.\n\nFor enterprises to thrive in the AI era, they must find a way to make use of all of their data. This isn\u2019t possible using traditional computing and data processing techniques. Instead, enterprises need an AI query engine.\n\nWhat Is an AI Query Engine?\n\nSimply, an AI query engine is a system that connects AI applications, or AI agents, to data. It\u2019s a critical component of agentic AI, as it serves as a bridge between an organization\u2019s knowledge base and AI-powered applications, enabling more accurate, context-aware responses.\n\nAI agents form the basis of an AI query engine, where they can gather information and do work to assist human employees. An AI agent will gather information from many data sources, plan, reason and take action. AI agents can communicate with users, or they can work in the background, where human feedback and interaction will always be available.\n\nIn practice, an AI query engine is a sophisticated system that efficiently processes large amounts of data, extracts and stores knowledge, and performs semantic search on that knowledge, which can be quickly retrieved and used by AI.\n\nAI Query Engines Unlock Intelligence in Unstructured Data\n\nAn enterprise\u2019s AI query engine will have access to knowledge stored in many different formats, but being able to extract intelligence from unstructured data is one of the most significant advancements it enables.\n\nTo generate insights, traditional query engines rely on structured queries and data sources, such as relational databases. Users must formulate precise queries using languages like SQL, and results are limited to predefined data formats.\n\nIn contrast, AI query engines can process structured, semi-structured and unstructured data. Common unstructured data formats are PDFs, log files, images and video, and are stored on object stores, file servers and parallel file systems. AI agents communicate with users and with each other using natural language. This enables them to interpret user intent, even when it\u2019s ambiguous, by accessing diverse data sources. These agents can deliver results in a conversational format, so that users can interpret results.\n\nThis capability makes it possible to derive more insights and intelligence from any type of data \u2014 not just data that fits neatly into rows and columns.\n\nFor example, companies like DataStax and NetApp are building AI data platforms that enable their customers to have an AI query engine for their next-generation applications.\n\nKey Features of AI Query Engines\n\nAI query engines possess several crucial capabilities:\n\nDiverse data handling: AI query engines can access and process various data types, including structured, semi-structured and unstructured data from multiple sources, including text, PDF, image, video and specialty data types.\n\nAI query engines can access and process various data types, including structured, semi-structured and unstructured data from multiple sources, including text, PDF, image, video and specialty data types. Scalability : AI query engines can efficiently handle petabyte-scale data, making all enterprise knowledge available to AI applications quickly.\n\n: AI query engines can efficiently handle petabyte-scale data, making all enterprise knowledge available to AI applications quickly. Accurate retrieval : AI query engines provide high-accuracy, high-performance embedding, vector search and reranking of knowledge from multiple sources.\n\n: AI query engines provide high-accuracy, high-performance embedding, vector search and reranking of knowledge from multiple sources. Continuous learning: AI query engines can store and incorporate feedback from AI-powered applications, creating an AI data flywheel in which the feedback is used to refine models and increase the effectiveness of the applications over time.\n\nRetrieval-augmented generation is a component of AI query engines. RAG uses the power of generative AI models to act as a natural language interface to data, allowing models to access and incorporate relevant information from large datasets during the response generation process.\n\nUsing RAG, any business or other organization can turn its technical information, policy manuals, videos and other data into useful knowledge bases. An AI query engine can then rely on these sources to support such areas as customer relations, employee training and developer productivity.\n\nAdditional information-retrieval techniques and ways to store knowledge are in research and development, so the capabilities of an AI query engine are expected to rapidly evolve.\n\nThe Impact of AI Query Engines\n\nUsing AI query engines, enterprises can fully harness the power of AI agents to connect their workforces to vast amounts of enterprise knowledge, improve the accuracy and relevance of AI-generated responses, process and utilize previously untapped data sources, and create data-driven AI flywheels that continuously improve their AI applications.\n\nSome examples include an AI virtual assistant that provides personalized, 24/7 customer service experiences, an AI agent for searching and summarizing video, an AI agent for analyzing software vulnerabilities or an AI research assistant.\n\nBridging the gap between raw data and AI-powered applications, AI query engines will grow to play a crucial role in helping organizations extract value from their data.\n\nNVIDIA Blueprints can help enterprises get started connecting AI to their data. Learn more about NVIDIA Blueprints and try them in the NVIDIA API catalog.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Innoviz, in Collaboration with NVIDIA, Showcasing Cutting-Edge Perception Software with NVIDIA DRIVE AGX Orin at CES 2025, Unveiling New Software-Driven Features",
            "link": "https://www.prnewswire.com/news-releases/innoviz-in-collaboration-with-nvidia-showcasing-cutting-edge-perception-software-with-nvidia-drive-agx-orin-at-ces-2025-unveiling-new-software-driven-features-302344181.html",
            "snippet": "Innoviz will demonstrate how it runs seamlessly with the NVIDIA DRIVE AGX Orin platform, offering a range of software-driven features designed to accelerate...",
            "score": 0.631721019744873,
            "sentiment": null,
            "probability": null,
            "content": "To view Innoviz's cutting-edge perception software, please click here.\n\nThe collaboration with NVIDIA represents a major leap forward in the industry's shift toward high-performance, AI-enabled perception systems. Innoviz's perception software, optimized to run with the NVIDIA DRIVE Orin platform, allows for real-time processing and advanced understanding of the vehicle's environment, enabling exceptional object detection, classification, and tracking capabilities. This solution is poised to improve safety and performance across autonomous driving and advanced driver-assistance systems (ADAS).\n\n\"At Innoviz, we are relentlessly focused on pushing the boundaries of autonomous driving technology to ensure safer and more reliable systems for the future of mobility,\" said Omer Keilaf, CEO of Innoviz Technologies. \"By leveraging the NVIDIA DRIVE Orin platform, we will be able to offer a powerful combination of cutting-edge LiDAR sensors and perception software, enabling automakers to scale from L2+ ADAS all the way to fully autonomous vehicles\".\n\nAt CES 2025, Innoviz will showcase its advanced technology in a live demonstration, illustrating the power of Innoviz's robust software and hardware and the potential of the seamless integration with NVIDIA DRIVE OGX Orin. As autonomous vehicles and smart transportation continue to evolve, we believe that Innoviz's solution represents a crucial step forward in enabling fully autonomous systems that are safe, efficient, and reliable.\n\nTo schedule a demo of Innoviz's cutting-edge LiDAR technology, email [email protected]\n\nAbout Innoviz\n\nInnoviz is a global leader in LiDAR technology, serving as a Tier 1 supplier to the world's leading automotive manufacturers and working towards a future with safe autonomous vehicles on the world's roads. Innoviz's LiDAR and perception software \"see\" better than a human driver and reduce the possibility of error, meeting the automotive industry's strictest expectations for performance and safety. Operating across the U.S., Europe, and Asia, Innoviz has been selected by internationally recognized premium car brands for use in consumer vehicles as well as by other commercial and industrial leaders for a wide range of use cases. For more information, visit innoviz.tech.com.\n\nJoin the discussion: Facebook , LinkedIn , YouTube , Twitter\n\nMedia Contact\n\n[email protected]\n\nInvestor Contact\n\n[email protected]\n\nForward Looking Statements\n\nThis announcement contains certain forward-looking statements within the meaning of the federal securities laws, including statements regarding the services offered by Innoviz, the anticipated technological capability of Innoviz's products, the markets in which Innoviz operates, future series production nominations, and Innoviz's projected future results. These forward-looking statements generally are identified by the words \"believe,\" \"project,\" \"expect,\" \"anticipate,\" \"estimate,\" \"intend,\" \"strategy,\" \"future,\" \"opportunity,\" \"plan,\" \"may,\" \"should,\" \"will,\" \"would,\" \"will be,\" \"will continue,\" \"will likely result,\" and similar expressions. Forward-looking statements are predictions, projections and other statements about future events that are based on current expectations and assumptions and, as a result, are subject to risks and uncertainties. Many factors could cause actual future events to differ materially from the forward-looking statements in this announcement including but not limited to, the ability to implement business plans, forecasts, and other expectations, the ability to convert design wins into definitive orders and the magnitude of such orders, the ability to identify and realize additional opportunities, potential changes and developments in the highly competitive LiDAR technology and related industries, and our expectations regarding the impact of the evolving conflict in Israel to our ongoing operations. The foregoing list is not exhaustive. You should carefully consider such risk and the other risks and uncertainties described in Innoviz's annual report on Form 20-F filed with the SEC on March 12, 2024 and other documents filed by Innoviz from time to time with the SEC. These filings identify and address other important risks and uncertainties that could cause actual events and results to differ materially from those contained in the forward-looking statements. Forward-looking statements speak only as of the date they are made. Readers are cautioned not to put undue reliance on forward-looking statements, and Innoviz assumes no obligation and does not intend to update or revise these forward-looking statements, whether as a result of new information, future events, or otherwise. Innoviz gives no assurance that it will achieve its expectations.\n\nVideo: https://www.youtube.com/watch?v=NGyrXBfuM2A\n\nLogo: https://mma.prnewswire.com/media/1496323/Innoviz_Technologies_Logo.jpg\n\nSOURCE Innoviz Technologies",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "New NVIDIA AI Models empower \u2013 and redefine \u2013 SAP, ServiceNow as agentic leaders",
            "link": "https://erp.today/new-nvidia-ai-models-define-sap-servicenow-as-agentic-leaders/",
            "snippet": "CES 2025 saw NVIDIA launch a series of new language models to advance Agentic AI, with SAP and ServiceNow expected to be among the first users.",
            "score": 0.8026691675186157,
            "sentiment": null,
            "probability": null,
            "content": "CES 2025 saw NVIDIA launch a series of new language models to advance Agentic AI, with SAP and ServiceNow expected to be among the first to use the new models from the GPU leader.\n\nUnveiled in Las Vegas, and built with NVIDIA\u2019s Llama foundation models, the Llama Nemotron family of open large language models (LLMs) is designed to help developers create and deploy AI agents across a range of applications, including customer support, fraud detection, and product supply chain and inventory management optimization.\n\nThe models use AI dev platform NVIDIA NeMo for distilling, pruning and alignment. Using these techniques, the Nemotron models are described as small enough to run on a variety of computing platforms while providing both high accuracy and increased model throughput.\n\nExplore related questions Ask ERP Today what is this?\n\nInterestingly, in the accompanying press release, ERP giant SAP and ITSM leader ServiceNow are described \u2013 perhaps even rebranded \u2013 as \u201cleading AI agent platform providers\u201d, with the two enterprise tech leaders expected to be among the first to use the new NVIDIA models for their software offerings. Alongside SAP, most ERP vendors have been trumpeting Agentic AI ware for a while now. Smart AI agents were positioned in 2024 as the next step in harnessing AI-powered capabilities for innovation within ERP systems, covering the gamut from question answering, protecting sensitive information with AI-powered data classification, integration process design and build, process description detailing and more.\n\n\u201cAI agents that collaborate to solve complex tasks across multiple lines of the business will unlock a whole new level of enterprise productivity beyond today\u2019s generative AI scenarios,\u201d said Philipp Herzig, chief AI officer at SAP. \u201cThrough SAP\u2019s [AI copilot] Joule, hundreds of millions of enterprise users will interact with these agents to accomplish their goals faster than ever before. NVIDIA\u2019s new open Llama Nemotron model family will foster the development of multiple specialized AI agents to transform business processes.\u201d\n\n\u201cAI agents make it possible for organizations to achieve more with less effort, setting new standards for business transformation,\u201d said Jeremy Barnes, vice president of platform AI at ServiceNow. \u201cThe improved performance and accuracy of NVIDIA\u2019s open Llama Nemotron models can help build advanced AI agent services that solve complex problems across functions, in any industry.\u201d\n\nNVIDIA\u2019s Llama Nemotron model families come powered by new NVIDIA Cosmos Nemotron vision language models (VLMs) and NVIDIA NIM microservices for video search and summarization. With these tools, developers can build agents that analyze and respond to images and video from autonomous machines, hospitals, stores, warehouses and more.\n\nThe Llama Nemotron model family will be available as downloadable models and as NVIDIA NIM microservices that can be easily deployed on clouds, data centers, PCs and workstations. They offer enterprises industry-leading performance with reliable, secure and seamless integration into their agentic AI application workflows.\n\nNew NVIDIA language models: What ERP Insiders need to know\n\nCES 2025 marked a pivotal moment for enterprise AI, as NVIDIA launched its Llama Nemotron family of open large language models (LLMs). These cutting-edge models, designed to power intelligent AI agents, are poised to transform ERP systems and business processes across industries. With SAP and ServiceNow leading the charge, ERP professionals have much to gain from this breakthrough.\n\nKey Takeaways\n\n1. NVIDIA\u2019s Llama Nemotron Models Enable Advanced Agentic AI Applications\n\nThe Nemotron models leverage NVIDIA\u2019s NeMo platform for distillation and pruning, making them highly accurate and efficient while remaining versatile enough to run on various computing platforms. By enabling applications in areas such as customer support, fraud detection, and supply chain optimization, these models promise to redefine the role of AI agents in enterprise workflows, from process design to sensitive data protection.\n\n2. ERP and ITSM Leaders Position AI Agents as a New Productivity Paradigm\n\nSAP and ServiceNow are poised to embrace NVIDIA\u2019s Nemotron models to enhance their Agentic AI platforms. SAP\u2019s AI copilot, Joule, aims to empower hundreds of millions of users to achieve faster outcomes, while ServiceNow envisions smarter, more capable AI agents delivering cross-functional solutions. For ERP professionals, this signals an industry-wide shift towards integrating AI agents into core business functions to maximize efficiency and innovation.\n\n3. Broader AI Capabilities Expand Beyond Text to Visual Data Analysis\n\nNVIDIA\u2019s Llama Nemotron family includes capabilities for visual language models (VLMs) and microservices such as video search and summarization. These tools allow developers to build agents that can interpret and respond to visual data from warehouses, stores, and autonomous machines, broadening the scope of ERP solutions to include real-time video and image analysis.\n\nNVIDIA\u2019s innovations underscore a new era in enterprise AI, where LLMs and Agentic AI converge to revolutionize ERP workflows. For ERP professionals, the opportunity to integrate these technologies means staying ahead in a rapidly evolving landscape of AI-powered business transformation.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Arm-powered NVIDIA Project DIGITS Puts High-Performance AI in the Hands of Millions of Developers",
            "link": "https://newsroom.arm.com/blog/arm-nvidia-project-digits-high-performance-ai",
            "snippet": "NVIDIA Project DIGITS is powered by the NVIDIA GB10 Grace Blackwell Superchip, bringing together the NVIDIA Grace CPU and NVIDIA Blackwell GPU with the latest-...",
            "score": 0.8869765996932983,
            "sentiment": null,
            "probability": null,
            "content": "The Project DIGITS Linux-based system featuring Arm-powered CPU cores makes it possible for every AI developer to have a high performance AI system on their desk.\n\nOne of the most exciting trends we see today is the rapid expansion and availability of AI-based applications and features across a variety of edge devices. As AI continues to grow and advance, it is crucial that AI researchers, data scientists, developers and students have access to high performance compute that can be used to develop or run the latest models, whether they are language, vision or multi-modal. With the pace of AI innovation moving faster than ever, we need to enable access to this performance beyond the cloud at the edge, bringing new capabilities directly to developers.\n\nPutting game-changing AI performance at every developers\u2019 fingertips\n\nA big step towards the vision of developing and deploying AI everywhere is NVIDIA Project DIGITS, a personal AI supercomputer announced during NVIDIA founder and CEO Jensen Huang\u2019s keynote at the Consumer Electronics Show (CES) 2025 today (Monday 6th January 2025). The Project DIGITS Linux-based system featuring Arm-powered CPU cores makes it possible for every AI developer to have a high performance AI system on their desk.\n\nNVIDIA Project DIGITS is powered by the NVIDIA GB10 Grace Blackwell Superchip, bringing together the NVIDIA Grace CPU and NVIDIA Blackwell GPU with the latest-generation CUDA cores and fifth-generation Tensor Cores connected via NVLink\u00ae-C2C chip-to-chip interconnect and 128GB of unified memory. The NVIDIA Grace CPU features our leading-edge, highest performance Arm Cortex-X and Cortex-A technology, with 10 Arm Cortex-X925 and 10 Cortex-A725 CPU cores. The NVIDIA GB10 delivers up to one petaflop\u00b9 (1000 TFLOPs) of AI computing performance at FP4 precision , enabling developers to prototype, fine-tune and run inferencing with large AI models and work in conjunction with the cloud or data center.\n\nThe value of the Arm compute platform\n\nLeveraging the ubiquitous Arm compute platform allows new AI models and applications to run more efficiently and faster at the edge. In consumer technology markets, our CPU technologies are found at the heart of today\u2019s edge devices and designed to target the most performant devices entering the market, whether that\u2019s the latest Arm CPUs such as those used in Project DIGITS, or as part of Arm Compute Subsystems (CSS) for Client. All of these technologies are optimized for maximum performance throughout and peak efficiency across real-world applications and workloads.\n\nThe Arm compute platform also offers the flexibility to use different computational engines for different AI use cases. In NVIDIA Project DIGITS, the Arm-based NVIDIA Grace CPU and NVIDIA Blackwell GPU serve complementary roles, enabling developers to use these components for a variety of workloads. This heterogeneous computing approach is essential to achieving maximum AI performance, while managing memory utilization and power consumption.\n\n\u201cOur collaboration with Arm on the GB10 Superchip will fuel the next generation of innovation in AI, combining NVIDIA\u2019s AI expertise with Arm\u2019s scalable compute platform to deliver exceptional performance and efficiency,\u201d said Ashish Karandikar, VP of SoC Products at NVIDIA. \u201cNow, with the introduction of Project DIGITS, every AI developer and researcher can have a powerful supercomputer at their fingertips.\u201d\n\nUnlocking software innovation\n\nFor developers, it is critical to have a fully integrated hardware and software AI platform. Project DIGITS uses the open-source Linux operating system , and users can access an extensive library of NVIDIA AI software, including software development tools, libraries, frameworks and AI models available in the NVIDIA NGC catalog and the NVIDIA Developer portal to accelerate their generative AI workflows.\n\nArm\u2019s presence in datacenters with NVIDIA Grace Hopper and Grace Blackwell provides a consistent platform architecture across both datacenter and edge environments, allowing developers to seamlessly use the same set of tools for AI application development. Moreover, Arm has been supporting and driving critical work in open-source developer communities to enable the software needed to deploy AI everywhere. As a result, over 20 million software developers worldwide are building their applications on the Arm compute platform, enabling a growing open-source community that is innovating at a rapid scale.\n\nThe ideal platform for high performance AI compute\n\nArm is the world\u2019s leading, most pervasive compute platform for AI now and in the future, making it an ideal platform for the GB10 Superchip used in Project DIGITS, a powerful PC desktop platform that can run large AI models of up to 200B parameters, which has not been possible until now. This influence across the AI ecosystem delivers flexible, performant and power-efficient AI capabilities to millions of developers worldwide. Working with NVIDIA and our leading software ecosystem, we cannot wait to see the next generation of highly innovative AI applications deployed.\n\n\u00b9 This is a petaflop based on FP4, as referenced here.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Yeah, Let\u2019s Hope That NVIDIA\u2019s New \u201cAI Boss\u201d Tech Doesn\u2019t Catch On, Because It Sounds Miserable",
            "link": "https://www.vice.com/en/article/yeah-lets-hope-that-nvidias-new-ai-boss-tech-doesnt-catch-on-because-it-sounds-miserable/",
            "snippet": "So, NVIDIA is planning on creating AI boss technology that players can experience in games. Please don't distribute this. I beg you.",
            "score": 0.8147392868995667,
            "sentiment": null,
            "probability": null,
            "content": "Do you know my least favorite thing about Raids? Learning patterns, finding out exactly what I should do to help my team kill a boss, and becoming a better player for it. You know what I do want to see in my future Raids, though? A boss that learns how to overcome these patterns, making every encounter with it extremely miserable and turning what could be a 20-minute run into an hour or more because we keep getting wiped. No, those words are definitely not dripping with sarcasm right now. NVIDIA\u2019s new AI Boss tech could do just that, however, and I hate it.\n\nPlay video\n\nAh Yes, Manmade NVIDIA Horror Beyond Our Comprehension Sounds Very Fun\n\nAlright, so, picture this. You and your friends are looking to settle down after a busy day at work. Sitting down, you log into your favorite MMO or RPG \u2014 or whatever you\u2019re currently playing. You rush through a dungeon, fighting the same enemies you have many times before. After arriving on the doorstep of the final creature, you realize something feels a little\u2026 different than it did before. That\u2019s because this boss is now powered by NVIDIA ACE, AI technology that helps the boss learn player patterns and changes how it reacts.\n\nVideos by VICE\n\nOn paper, this sounds like a kind of cool idea. But after the game has learned what players do and how they do it, it seems like it would become the biggest mess possible. Look at Chessbots. These AI anomalies turn a game of wit and strategy into a nearly unwinnable mess. And if there is one thing we know about gamers? They like to win. Playing against a boss, especially one they\u2019ve fought before, feels like a treat.\n\nPeople don\u2019t go into dungeons in Final Fantasy XIV to face off against Good King Moogle Mog with an advanced AI mindset. They go in because they want to punch the shit out of a giant Moogle while the most Danny Elfman/Tim Burton-ass music plays in the background. It sounds like a solution to a problem people don\u2019t have. Maybe something like this could be cool in a single-player game. But, online gaming?\n\nDon\u2019t let this get anywhere near our favorite games, please, and thank you. Maybe I\u2019m glad I went to AMD over NVIDIA now, for real.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "\u201cThe next frontier of AI is physical AI.\u201d: Nvidia CEO",
            "link": "https://brand-innovators.com/the-next-frontier-of-ai-is-physical-ai-nvidia-ceo/",
            "snippet": "\u201cThe next frontier of AI is physical AI.\u201d: Nvidia CEO ... Nvidia CEO Jensen Huang has a vision for the future and it involves self-driving cars, AI agents and...",
            "score": 0.9365707039833069,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia CEO Jensen Huang has a vision for the future and it involves self-driving cars, AI agents and intelligent robots.\n\nDuring a keynote on the opening night of CES, Huang said that there are huge economic opportunities to develop \u201cphysical AI.\u201d\n\n\u201cThe next frontier of AI is physical AI,\u201d said Huang in a video demonstration he showed on stage at the Michelob Ultra Arena.\n\nSo far AI has been based on large language models but Huang asserts that the next innovation is in the real world. \u201cThe chat GPT moment for General robotics is just around the corner,\u201d he said.\n\nThe challenge is that most models today have a difficult time with understanding physical dynamics like gravity, friction and inertia. So intel has created Cosmos, \u201ca world Foundation model that it was created to understand the physical world.\u201d\n\nDuring the presentation, Huang announced \u201cMega,\u201d an Omniverse Blueprint for developing, testing and optimizing physical AI and robot fleets at scale in real-world facilities.\n\nThink warehouses with hundreds of autonomous mobile robots, robotic arm manipulators and humanoid robots working alongside people. \u201cRobots execute tasks by perceiving and reasoning about their Omniverse digital twin environment planning their next motion and acting the robot brains can see the resulting state through sensor simulations and decide their next action,\u201d explained Huang.\n\nNvidia also announced their next generation processor for cars called Thor, which has 20x the processing power of the last generation. With the new tech, the company revealed a new partnership with Toyota, Aurora and Continental to power self-driving cars and semi-trucks.\n\nToyota, Aurora and Continental join the growing list of NVIDIA partners rolling out automated and autonomous vehicle fleets.\n\n\u201cA trillion miles that are driven around the world each year that\u2019s all going to be either highly autonomous or fully autonomous coming up,\u201d he said. \u201cI predict that this will likely be the first multi-trillion dollar robotics industry.\u201d\n\nWith partners like Volvo already integrating Nvidia\u2019s self-driving tools, the category is already worth $4 billion to Nvidia, and will likely reach $5 billion this year, according to Huang.\n\nHuang also noted that agentic AI is behind Nemotron, a new suite of AI agent tools that companies can use for automated customer service. Unlike most chatbots, which rely on generative AI and point customers to contextual content, agentic AI can autonomously make decisions, take proactive actions and adapt to changing situations in real time.\n\n\u201cThese AI agents are essentially digital workforce that are working alongside your employees um working Al doing things for you on your behalf and so the way that you would bring these specialized agents into your company is to on board them just like you onboard an employee.,\u201d he explained.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Seeks To Turbocharge AI PC Development With GeForce RTX 50 GPUs",
            "link": "https://www.crn.com/news/components-peripherals/2025/nvidia-seeks-to-turbocharge-ai-pc-development-with-geforce-rtx-50-gpus",
            "snippet": "Nvidia has revealed its much-anticipated GeForce RTX 50 GPUs for desktops and laptops, relying on the same Blackwell architecture at the center of its new AI...",
            "score": 0.6381604075431824,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia Seeks To Turbocharge AI PC Development With GeForce RTX 50 GPUs\n\nWhile Nvidia positioned the Blackwell-based GeForce RTX 50 GPUs as a significant upgrade for PC gamers at CES 2025, the AI computing giant is also hoping to lure AI developers and content creators with a bevy of new hardware and software capabilities.\n\nNvidia has revealed its much-anticipated GeForce RTX 50 GPUs for desktops and laptops, relying on the same Blackwell architecture at the center of its new AI data center chips to bring forth PC advancements in graphics, content creation and productivity.\n\nAt its CES 2025 keynote Monday, Nvidia called the GeForce RTX 50 series the \u201cmost powerful\u201d consumer GPUs \u201cever created,\u201d positioning them as a significant upgrade for gamers by claiming they will offer up two times faster graphics performance than the previous generation when using the new AI-powered DLSS 4 image upscaling feature.\n\n[Related: Opinion: Why Nvidia, MediaTek May Enter The PC CPU Market Soon]\n\nBut the AI computing giant is also hoping to lure AI developers and content creators with a bevy of new hardware and software capabilities. For instance, the GPU family\u2019s new fifth-generation Tensor Cores pack a \u201cmassive amount of AI processing horsepower\u201d to run AI models two times faster using less graphics memory when taking advantage of the newly supported 4-bit floating point (FP4) format, according to Nvidia.\n\nOn the desktop side, the flagship GPU, the 32-GB GeForce RTX 5090, will cost $1,999 when it becomes available along with the $999, 16-GB GeForce RTX 5080 on Jan. 30. The $749, 16-GB GeForce RTX 5070 Ti and $549, 12-GB GeForce RTX 5070 will become available in February. The GPUs will be available from Nvidia and add-in board partners as well as in desktops from system builders such as Falcon Northwest and Maingear.\n\nLaptops equipped with RTX 5090, RTX 5080 and RTX 5070 Ti GPUs will debut in March while laptops with the RTX 5070 will launch in the following month from several OEMs, including Acer, Asus, Dell Technologies, HP Inc., Lenovo and MSI.\n\nNvidia Vows \u2018Pipeline\u2019 Of Microservices For AI PC Development\n\nTo foster development of AI PC applications on the new RTX 50 series and other recent generations of GeForce GPUs, Nvidia said it plans to release a \u201cpipeline\u201d of Nvidia NIM microservices and Nvidia AI Blueprints that use first- and third-party models to enable use cases ranging from PDF extraction, computer vision and speech to image generation, large language models and embedded models for retrieval-augmented generation.\n\nThese models include Nvidia\u2019s newly released Llama Nemotron family of models, which are versions of Meta\u2019s Llama models that have been optimized to aid with the development of agentic AI use cases ranging from instruction following and chat to coding and math.\n\nTo demonstrate how NIMs can be used to build AI agents and assistants, Nvidia plans to release a vison-enabled PC avatar called Project R2X that can read and summarize documents, fetch information and \u201cassist with desktop apps and video conference calls.\u201d It will also be able to connect with cloud AI services such as OpenAI\u2019s GPT4o.\n\nNvidia Highlights Content Creation Enhancements\n\nWhen it comes to content creation, Nvidia said the new RTX 50 GPUs come with new hardware features to boost video editing and 3-D rendering workloads on top of new software capabilities for image generation as well as voice and video communication.\n\nFor video editing, Nvidia said the RTX 50 series comes with new video encoders and decoders that provide a \u201cgenerational leap\u201d in capabilities with support for the 4:2:2 pro-grade color format, the multi-view extension of HEVC (high-efficiency video coding) for 3-D and virtual reality video as well as the new AV1 Ultra High Quality mode.\n\nFor 3-D rendering, Nvidia said the GPUs come with fourth-generation RT Cores, which enable applications to run 40 percent faster. One way the chips speed up 3-D rendering is through its fourth-generation AI-based image upscaling technology, DLSS 4, which introduces Multi Frame Generation to increase frame rates.\n\nFor image generation, Nvidia highlighted how the new FP4 support in RTX 50 GPUs will enable models used for such purposes to take up significantly less VRAM compared to the default 16-bit floating point (FP16) format.\n\nAs an example, the company said the FLUX.1 [dev] model from Black Forest Labs will only need less than 10 GB of memory with FP4, which means it can run on each of the four new RTX 50 GPUs since they range from 32-12 GB in VRAM. By contrast, the FLUX.1 [dev] model running with FP16 would require more than 23 GB of memory, which restricts it to the RTX 4090 and professional GPUs from the last generation.\n\nNvidia said it plans to offer a NIM microservice based on FLUX.1 [dev], which will be made available in an Nvidia AI Blueprint for 3-D guided image generation next month.\n\nFor voice and video communication, Nvidia said it plans to add two new features to the Nvidia Broadcast app for AI-enhanced video and voice effects. The first feature, Studio Voice, will make a user\u2019s microphone sound like a high-quality microphone while the second, Virtual Key Light, can relight a \u201csubject\u2019s face to deliver even coverage.\u201d These will initially require an RTX 4080 or higher when they become available in February.\n\nHow The RTX 50 Series Compares To The 40 Series\n\nThe flagship GPU in the RTX 50 series, the RTX 5090, is made up of 92 billion transistors, up 21 percent from the 76 billion transistors of its predecessor, the RTX 4090, which debuted in 2022 using Nvidia\u2019s previous-generation Ada Lovelace architecture.\n\nAcross the RTX 50 series, the GPUs come with new fourth-gen RT Cores and fifth-gen Tensor Cores as well as a streaming multiprocessor that \u201chas been updated with more processing throughput and a tighter integration with the Tensor Cores in order to optimize the performance of neural shaders,\u201d according to Nvidia.\n\nThe RTX 5090 comes with 32 GB of GDDR7 memory and 21,760 CUDA cores, up from the 24 GB of GDDR6X memory and 16,385 CUDA cores of the RTX 4090. The GPU\u2019s boost and base clock frequencies are 2.41GHz and 2.01GHz, respectively, which are lower than the 2.52GHz and 2.23GHz clock speeds of the RTX 4090.\n\nAs for performance, the RTX 5090\u2019s Tensor Cores are capable of hitting 3,352 trillion operations per second (TOPS) in AI computing performance while the RT Cores can achieve 318 trillion floating-point operations per second (TFLOPS). These figures are 2.5 times and 60 percent faster, respectively, than the 1,321 AI TOPs achieved by the RTX 4090\u2019s Tensor Cores and the 191 TFLOPS of the GPU\u2019s RT Cores.\n\nNvidia did not disclose Shader Core performance numbers for the RTX 50 series after providing such information for the previous generation.\n\nThe total graphics power required by the RTX 5090 is 575 watts, up 27 percent from the 450 watts needed for the RTX 4090. The lowest-end GPU, the RTX 5070, requires 250 watts, which is 25 percent higher than the 200 watts needed for the RTX 4070.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia CEO Jensen Huang Says 2025 Is the Year of AI Agents",
            "link": "https://www.barrons.com/articles/nvidia-stock-ceo-ai-agents-8c20ddfb",
            "snippet": "Nvidia CEO Jensen Huang predicted that 2025 will be the year when artificial-intelligence agents, software that can take directions and do multistep tasks,...",
            "score": 0.9231510162353516,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Uber Extends Rebound With Nvidia Self-Driving Partnership",
            "link": "https://www.investors.com/news/technology/uber-stock-nvidia-autonomous-driving-partnership-robotaxi/",
            "snippet": "Uber stock gained after the ride-hailing giant and Nvidia partnered to help develop AI-powered autonomous driving technology.",
            "score": 0.9158114790916443,
            "sentiment": null,
            "probability": null,
            "content": "Uber Technologies (UBER) is collaborating with Nvidia (NVDA) to help develop AI-powered autonomous driving technology, the companies said late Monday. Uber stock was higher in early trading Tuesday.\n\nNvidia Chief Executive Jensen Huang focused part of his keynote Monday night for the CES technology trade show on \"physical AI,\" referring to the chipmakers ambition to power autonomous vehicles and robots. That includes the launch of Nvidia Cosmos, a computing platform for accelerating physical AI development. Uber will be among the first adopters of the AI models. The ride-hailing company offered more details in a news release following the event.\n\nThe millions of daily trips on Uber provide a \"vast and rich source of data\" that Uber and Nvidia will pair with the NVIDIA Cosmos platform and Nvidia's DGX operating system to help AV developers improve their technology, according to the news release.\n\n\"Generative AI will power the future of mobility, requiring both rich data and very powerful compute,\" Uber Chief Executive Dara Khosrowshahi said in the news release. \"By working with NVIDIA, we are confident that we can help supercharge the timeline for safe and scalable autonomous driving solutions for the industry.\"\n\nUber's news release said it will provide more details later this year.\n\nUber Stock\n\nOn the stock market today, Uber stock was up 2.9% at 68.26 in recent premarket trading. Shares are approaching the stock's 50-day moving average. Uber stock gained 2.7% Monday after the company announced it would accelerate a portion of its $7 billion stock buyback authorization.\n\nUber's Autonomous Vehicle Strategy\n\nThe Nvidia plan fits within a broader partnership strategy for autonomous vehicles from Uber. Uber sold off its autonomous vehicle division late in 2020 and has instead pitched itself as a partner to AV developers.\n\nThat includes a partnership with Waymo, Google's self-driving car division. Uber already offers Waymo vehicles in Phoenix and will expand that arrangement to Austin and Atlanta starting next year. The ride-hailing giant also recently added robotaxis to its app in Abu Dhabi through a partnership with Chinese mobility company WeRide (WRD).\n\nInvestors, however, appear uneasy overall about what AVs will mean for Uber's business. Uber stock slid when Waymo recently announced it would expand to Miami using its own app, rather than Uber's. And updates on Tesla's (TSLA) progress toward a robotaxi service over the six months have often pushed Uber stock lower.\n\nUber slumped to close the year, losing 25% from the stock's high near 87 from early October.\n\nUber Stock 2025 Rally\n\nYet Uber is starting 2025 strong, rallying 10% so far this month to retake stock's 21-day moving average. Further gains Tuesday could have Uber test support for its longer-term 50-day moving average.\n\nOverall, Uber has an IBD Composite Rating of 69 out of a best-possible 99, according to IBD Stock Checkup. IBD's Composite Rating combines five separate proprietary ratings into one easy-to-use rating. The best growth stocks have a Composite Rating of 90 or better.\n\nYOU MAY ALSO LIKE:\n\nUber Stock Starts 2025 Strong As Analysts Debate Robotaxi Risk\n\nIBD Live: Learn And Analyze Growth Stocks With The Pros\n\nGet Timely Buy & Sell Alerts With IBD Leaderboard\n\nTop Growth Stocks To Buy And Watch",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-06": {
        "0": {
            "title": "NVIDIA Puts Grace Blackwell on Every Desk and at Every AI Developer\u2019s Fingertips",
            "link": "https://nvidianews.nvidia.com/news/nvidia-puts-grace-blackwell-on-every-desk-and-at-every-ai-developers-fingertips",
            "snippet": "CES\u2014NVIDIA today unveiled NVIDIA\u00ae Project DIGITS, a personal AI supercomputer that provides AI researchers, data scientists and students worldwide with...",
            "score": 0.9440596103668213,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA Project DIGITS With New GB10 Superchip Debuts as World\u2019s Smallest AI Supercomputer Capable of Running 200B-Parameter Models\n\nCES\u2014NVIDIA today unveiled NVIDIA\u00ae Project DIGITS, a personal AI supercomputer that provides AI researchers, data scientists and students worldwide with access to the power of the NVIDIA Grace Blackwell platform.\n\nProject DIGITS features the new NVIDIA GB10 Grace Blackwell Superchip, offering a petaflop of AI computing performance for prototyping, fine-tuning and running large AI models.\n\nWith Project DIGITS, users can develop and run inference on models using their own desktop system, then seamlessly deploy the models on accelerated cloud or data center infrastructure.\n\n\u201cAI will be mainstream in every application for every industry. With Project DIGITS, the Grace Blackwell Superchip comes to millions of developers,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cPlacing an AI supercomputer on the desks of every data scientist, AI researcher and student empowers them to engage and shape the age of AI.\u201d\n\nGB10 Superchip Provides a Petaflop of Power-Efficient AI Performance\n\nThe GB10 Superchip is a system-on-a-chip (SoC) based on the NVIDIA Grace Blackwell architecture and delivers up to 1 petaflop of AI performance at FP4 precision.\n\nGB10 features an NVIDIA Blackwell GPU with latest-generation CUDA\u00ae cores and fifth-generation Tensor Cores, connected via NVLink\u00ae-C2C chip-to-chip interconnect to a high-performance NVIDIA Grace\u2122 CPU, which includes 20 power-efficient cores built with the Arm architecture. MediaTek, a market leader in Arm-based SoC designs, collaborated on the design of GB10, contributing to its best-in-class power efficiency, performance and connectivity.\n\nThe GB10 Superchip enables Project DIGITS to deliver powerful performance using only a standard electrical outlet. Each Project DIGITS features 128GB of unified, coherent memory and up to 4TB of NVMe storage. With the supercomputer, developers can run up to 200-billion-parameter large language models to supercharge AI innovation. In addition, using NVIDIA ConnectX\u00ae networking, two Project DIGITS AI supercomputers can be linked to run up to 405-billion-parameter models.\n\nGrace Blackwell AI Supercomputing Within Reach\n\nWith the Grace Blackwell architecture, enterprises and researchers can prototype, fine-tune and test models on local Project DIGITS systems running Linux-based NVIDIA DGX OS, and then deploy them seamlessly on NVIDIA DGX Cloud\u2122, accelerated cloud instances or data center infrastructure.\n\nThis allows developers to prototype AI on Project DIGITS and then scale on cloud or data center infrastructure, using the same Grace Blackwell architecture and the NVIDIA AI Enterprise software platform.\n\nProject DIGITS users can access an extensive library of NVIDIA AI software for experimentation and prototyping, including software development kits, orchestration tools, frameworks and models available in the NVIDIA NGC catalog and on the NVIDIA Developer portal. Developers can fine-tune models with the NVIDIA NeMo\u2122 framework, accelerate data science with NVIDIA RAPIDS\u2122 libraries and run common frameworks such as PyTorch, Python and Jupyter notebooks.\n\nTo build agentic AI applications, users can also harness NVIDIA Blueprints and NVIDIA NIM\u2122 microservices, which are available for research, development and testing via the NVIDIA Developer Program. When AI applications are ready to move from experimentation to production environments, the NVIDIA AI Enterprise license provides enterprise-grade security, support and product releases of NVIDIA AI software.\n\nAvailability\n\nProject DIGITS will be available in May from NVIDIA and top partners, starting at $3,000. Sign up for notifications today.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "NVIDIA DLSS 4 Introduces Multi Frame Generation & Enhancements For All DLSS Technologies",
            "link": "https://www.nvidia.com/en-us/geforce/news/dlss4-multi-frame-generation-ai-innovations/",
            "snippet": "DLSS Multi Frame Generation generates up to three additional frames per traditionally rendered frame, working in unison with the complete suite of DLSS...",
            "score": 0.7159450054168701,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA DLSS is a suite of neural rendering technologies powered by GeForce RTX Tensor Cores that boosts frame rates while delivering crisp, high-quality images that rival native resolution rendering, as part of over 700 RTX games and apps.\n\nAt CES 2025, we\u2019re advancing DLSS once again with the introduction of DLSS 4, featuring Multi Frame Generation for GeForce RTX 50 Series graphics cards and laptops; 75 games and apps will have support for Multi Frame Generation when they\u2019re released.\n\nDLSS Multi Frame Generation generates up to three additional frames per traditionally rendered frame, working in unison with the complete suite of DLSS technologies to multiply frame rates by up to 8X over traditional brute-force rendering. This massive performance improvement on GeForce RTX 5090 graphics cards unlocks stunning 4K 240 FPS fully ray-traced gaming.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "NVIDIA Blackwell GeForce RTX 50 Series Opens New World of AI Computer Graphics",
            "link": "https://nvidianews.nvidia.com/news/nvidia-blackwell-geforce-rtx-50-series-opens-new-world-of-ai-computer-graphics",
            "snippet": "Next generation of GeForce RTX GPUs deliver stunning visual realism and 2x performance increase, made possible by AI, Neural Shaders and DLSS.",
            "score": 0.8483315110206604,
            "sentiment": null,
            "probability": null,
            "content": "Next Generation of GeForce RTX GPUs Deliver Stunning Visual Realism and 2x Performance Increase, Made Possible by AI, Neural Shaders and DLSS 4\n\nCES\u2014NVIDIA today unveiled the most advanced consumer GPUs for gamers, creators and developers \u2014 the GeForce RTX\u2122 50 Series Desktop and Laptop GPUs.\n\nPowered by the NVIDIA Blackwell architecture, fifth-generation Tensor Cores and fourth-generation RT Cores, the GeForce RTX 50 Series delivers breakthroughs in AI-driven rendering, including neural shaders, digital human technologies, geometry and lighting.\n\n\u201cBlackwell, the engine of AI, has arrived for PC gamers, developers and creatives,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cFusing AI-driven neural rendering and ray tracing, Blackwell is the most significant computer graphics innovation since we introduced programmable shading 25 years ago.\u201d\n\nThe GeForce RTX 5090 GPU \u2014 the fastest GeForce RTX GPU to date \u2014 features 92 billion transistors, providing over 3,352 trillion AI operations per second (TOPS) of computing power. Blackwell architecture innovations and DLSS 4 mean the GeForce RTX 5090 GPU outperforms the GeForce RTX 4090 GPU by up to 2x.\n\nGeForce Blackwell comes to laptops with all the features of desktop models, bringing a considerable upgrade to portable computing, including extraordinary graphics capabilities and remarkable efficiency. The Blackwell generation of NVIDIA Max-Q technology extends battery life by up to 40%, and includes thin and light laptops that maintain their sleek design without sacrificing power or performance.\n\nNVIDIA DLSS 4 Boosts Performance by Up to 8x\n\nDLSS 4 debuts Multi Frame Generation to boost frame rates by using AI to generate up to three frames per rendered frame. It works in unison with the suite of DLSS technologies to increase performance by up to 8x over traditional rendering, while maintaining responsiveness with NVIDIA Reflex technology.\n\nDLSS 4 also introduces the graphics industry\u2019s first real-time application of the transformer model architecture. Transformer-based DLSS Ray Reconstruction and Super Resolution models use 2x more parameters and 4x more compute to provide greater stability, reduced ghosting, higher details and enhanced anti-aliasing in game scenes. DLSS 4 will be supported on GeForce RTX 50 Series GPUs in over 75 games and applications the day of launch.\n\nNVIDIA Reflex 2 introduces Frame Warp, an innovative technique to reduce latency in games by updating a rendered frame based on the latest mouse input just before it is sent to the display. Reflex 2 can reduce latency by up to 75%. This gives gamers a competitive edge in multiplayer games and makes single-player titles more responsive.\n\nBlackwell Brings AI to Shaders\n\nTwenty-five years ago, NVIDIA introduced GeForce 3 and programmable shaders, which set the stage for two decades of graphics innovation, from pixel shading to compute shading to real-time ray tracing. Alongside GeForce RTX 50 Series GPUs, NVIDIA is introducing RTX Neural Shaders , which brings small AI networks into programmable shaders, unlocking film-quality materials, lighting and more in real-time games.\n\nRendering game characters is one of the most challenging tasks in real-time graphics, as people are prone to notice the smallest errors or artifacts in digital humans. RTX Neural Faces takes a simple rasterized face and 3D pose data as input, and uses generative AI to render a temporally stable, high-quality digital face in real time.\n\nRTX Neural Faces is complemented by new RTX technologies for ray-traced hair and skin . Along with the new RTX Mega Geometry , which enables up to 100x more ray-traced triangles in a scene, these advancements are poised to deliver a massive leap in realism for game characters and environments.\n\nThe power of neural rendering, DLSS 4 and the new DLSS transformer model is showcased on GeForce RTX 50 Series GPUs with Zorah, a groundbreaking new technology demo from NVIDIA.\n\nAutonomous Game Characters\n\nGeForce RTX 50 Series GPUs bring industry-leading AI TOPS to power autonomous game characters in parallel with game rendering.\n\nNVIDIA is introducing a suite of new NVIDIA ACE technologies that enable game characters to perceive, plan and act like human players. ACE-powered autonomous characters are being integrated into KRAFTON\u2019s PUBG: BATTLEGROUNDS and InZOI , the publisher\u2019s upcoming life simulation game, as well as Wemade Next\u2019s MIR5 .\n\nIn PUBG, companions powered by NVIDIA ACE plan and execute strategic actions, dynamically working with human players to ensure survival. InZOI features Smart Zoi characters that autonomously adjust behaviors based on life goals and in-game events. In MIR5, large language model (LLM)-driven raid bosses adapt tactics based on player behavior, creating more dynamic, challenging encounters.\n\nAI Foundation Models for RTX AI PCs\n\nShowcasing how RTX enthusiasts and developers can use NVIDIA NIM microservices to build AI agents and assistants, NVIDIA will release a pipeline of NIM microservices and AI Blueprints for RTX AI PCs from top model developers such as Black Forest Labs, Meta, Mistral and Stability AI.\n\nUse cases span LLMs, vision language models, image generation, speech, embedding models for retrieval-augmented generation, PDF extraction and computer vision. The NIM microservices include all the necessary components for running AI on PCs and are optimized for deployment across all NVIDIA GPUs.\n\nTo demonstrate how enthusiasts and developers can use NIM to build AI agents and assistants, NVIDIA today previewed Project R2X, a vision-enabled PC avatar that can put information at a user\u2019s fingertips, assist with desktop apps and video conference calls, read and summarize documents, and more.\n\nAI-Powered Tools for Creators\n\nThe GeForce RTX 50 Series GPUs supercharge creative workflows . RTX 50 Series GPUs are the first consumer GPUs to support FP4 precision, boosting AI image generation performance for models such as FLUX by 2x and enabling generative AI models to run locally in a smaller memory footprint, compared with previous-generation hardware.\n\nThe NVIDIA Broadcast app gains two AI-powered beta features for livestreamers: Studio Voice, which upgrades microphone audio, and Virtual Key Light, which relights faces for polished streams. Streamlabs is introducing the Intelligent Streaming Assistant , powered by NVIDIA ACE and Inworld AI, which acts as a cohost, producer and technical assistant to enhance livestreams.\n\nAvailability\n\nFor desktop users, the GeForce RTX 5090 GPU with 3,352 AI TOPS and the GeForce RTX 5080 GPU with 1,801 AI TOPS will be available on Jan. 30 at $1,999 and $999, respectively.\n\nThe GeForce RTX 5070 Ti GPU with 1,406 AI TOPS and GeForce RTX 5070 GPU with 988 AI TOPS will be available starting in February at $749 and $549, respectively.\n\nThe NVIDIA Founders Editions of the GeForce RTX 5090, RTX 5080 and RTX 5070 GPUs will be available directly from nvidia.com and select retailers worldwide.\n\nStock-clocked and factory-overclocked models will be available from top add-in card providers such as ASUS, Colorful, Gainward, GALAX, GIGABYTE, INNO3D, KFA2, MSI, Palit, PNY and ZOTAC, and in desktops from system builders including Falcon Northwest, Infiniarc, MAINGEAR, Mifcom, ORIGIN PC, PC Specialist and Scan Computers.\n\nLaptops with GeForce RTX 5090, RTX 5080 and RTX 5070 Ti Laptop GPUs will be available starting in March, and RTX 5070 Laptop GPUs will be available starting in April from the world\u2019s top manufacturers, including Acer, ASUS, Dell, GIGABYTE, HP, Lenovo, MECHREVO, MSI and Razer.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "New GeForce RTX 50 Series Graphics Cards & Laptops Powered By NVIDIA Blackwell Bring Game-Changing AI and Neural Rendering Capabilities To Gamers and Creators",
            "link": "https://www.nvidia.com/en-us/geforce/news/rtx-50-series-graphics-cards-gpu-laptop-announcements/",
            "snippet": "GeForce RTX 50 Series GPUs and laptops deliver game-changing performance, power transformative AI experiences, and enable creators to complete workflows in...",
            "score": 0.7083280682563782,
            "sentiment": null,
            "probability": null,
            "content": "Neural rendering is the next era of computer graphics. By integrating neural networks into the rendering process, we can take dramatic leaps forward in performance, image quality, and interactivity to deliver new levels of immersion.\n\nThe very first example of neural rendering was DLSS. We used lower resolution rendered frames as an input to a neural network, which was trained to output a full resolution frame. DLSS has since evolved to the point where it can generate entire frames and understand the composition of a scene including shadows, reflections, and occlusion to generate images that are better than native rendering. With the invention of DLSS 4 with Multi Frame Generation, working in unison with the complete suite of DLSS technologies, we can multiply frame rates by up to 8X over traditional brute-force rendering and provide image quality that is better than native rendering.\n\nBut DLSS is just the beginning.\n\nWe've integrated neural networks inside of programmable shaders to create neural shaders. RTX Neural Shaders will drive the next decade of graphics innovations. They can be used to compress textures by up to 7X, saving massive amounts of graphics memory. And be used to create cinematic-quality textures, and even more advanced lighting effects in games.\n\nRTX Neural Faces offers an innovative, new approach to improve face quality using generative AI. Instead of traditional rendering, Neural Faces takes a simple rasterized face plus 3D pose data as input and uses a real-time generative AI model to infer a more natural face.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "PC Gaming in the Cloud Goes Everywhere With New Devices and AAA Games on GeForce NOW",
            "link": "https://blogs.nvidia.com/blog/geforce-now-ces-2025/",
            "snippet": "GeForce NOW announces support for Steam Deck, Apple Vision Pro, Meta Quest and Pico mixed-reality devices; plus 'DOOM: The Dark Ages' and 'Avowed' are...",
            "score": 0.8414914011955261,
            "sentiment": null,
            "probability": null,
            "content": "Steam Deck, Apple Vision Pro spatial computers, Meta Quest 3 and 3S and Pico mixed-reality devices get expanded GeForce NOW support \u2014 plus, \u2018DOOM: The Dark Ages,\u2019 and \u2018Avowed,\u2019 join the cloud as GeForce RTX-powered data centers launch in India, all announced at CES.\n\nGeForce NOW turns any device into a GeForce RTX gaming PC, and is bringing cloud gaming and AAA titles to more devices and regions.\n\nAnnounced today at the CES trade show, gamers will soon be able to play titles from their Steam library at GeForce RTX quality with the launch of a native GeForce NOW app for the Steam Deck. NVIDIA is working to bring cloud gaming to the popular PC gaming handheld device later this year.\n\nIn collaboration with Apple, Meta and ByteDance, NVIDIA is expanding GeForce NOW cloud gaming to Apple Vision Pro spatial computers, Meta Quest 3 and 3S and Pico virtual- and mixed-reality devices \u2014 with all the bells and whistles of NVIDIA technologies, including ray tracing and NVIDIA DLSS.\n\nIn addition, NVIDIA is launching the first GeForce RTX-powered data center in India, making gaming more accessible around the world.\n\nPlus, GeForce NOW\u2019s extensive library of over 2,100 supported titles is expanding with highly anticipated AAA titles. DOOM: The Dark Ages and Avowed will join the cloud when they launch on PC this year.\n\nRTX on Deck\n\nThe Steam Deck\u2019s portability paired with GeForce NOW opens up new possibilities for high-fidelity gaming everywhere. The native GeForce NOW app will offer up to 4K resolution and 60 frames per second with high dynamic range on Valve\u2019s innovative Steam Deck handheld when connected to a TV, streaming from GeForce RTX-powered gaming rigs in the cloud.\n\nLast year, GeForce NOW rolled out a beta installation method that was eagerly welcomed by the gaming community. Later this year, members will be able to download the native GeForce NOW app and install it on Steam Deck.\n\nSteam Deck gamers can gain access to all the same benefits as GeForce RTX 4080 GPU owners with a GeForce NOW Ultimate membership, including NVIDIA DLSS 3 technology for the highest frame rates and NVIDIA Reflex for ultra-low latency. Because GeForce NOW streams from an RTX gaming rig in the cloud, the Steam Deck uses less processing power, which extends battery life compared with playing locally.\n\nThe streaming experience with GeForce NOW looks stunning, whichever way Steam Deck users want to play \u2014 whether that\u2019s in handheld mode for HDR-quality graphics, connected to a monitor for up to 1440p 120 fps HDR or hooked up to a TV for big-screen streaming at up to 4K 60 HDR. GeForce NOW members can take advantage of RTX ON with the Steam Deck for photorealistic gameplay on supported titles, as well as HDR10 and SDR10 when connected to a compatible display for richer, more accurate color gradients.\n\nGet ready for major upgrades to streaming on the go when the GeForce NOW app launches on the Steam Deck later this year.\n\nStream Beyond Reality\n\nGet immersed in a new dimension of big-screen gaming as GeForce NOW brings AAA titles to life on Apple Vision Pro spatial computers, Meta Quest 3 and 3S and Pico virtual- and mixed-reality headsets. Later this month, these supported devices will give members access to an extensive library of games to stream through GeForce NOW by opening the browser to play.geforcenow.com when the newest app update, version 2.0.70, starts rolling out later this month.\n\nMembers can transform the space around them into a personal gaming theater with GeForce NOW. The streaming experience on these devices will support gamepad-compatible titles for members to play their favorite PC games on a massive virtual screen.\n\nFor an even more enhanced visual experience, GeForce NOW Ultimate and Performance members using these devices can tap into RTX and DLSS technologies in supported games. Members will be able to step into a world where games come to life on a grand scale, powered by GeForce NOW technologies.\n\nLand of a Thousand Lights \u2026 and Games\n\nNVIDIA is broadening cloud gaming in India and Latin America. The first GeForce RTX 4080-powered data center will launch in India in the first half of this year. This follows the launch of GeForce NOW in Japan last year, as well as in Colombia and Chile, to be operated by GeForce NOW Alliance partner Digevo.\n\nGeForce RTX-powered gaming in the rapidly growing Indian gaming market will provide the ability to stream AAA games without the latest hardware. Gamers in the region can look forward to the launch of Ultimate memberships, along with all the new games and technological advancements announced at CES.\n\nSend in the Games\n\nAAA content from celebrated publishers is coming to the cloud. Avowed from Obsidian Entertainment, known for iconic titles such as Fallout: New Vegas, will join GeForce NOW. The cloud gaming platform will also bring DOOM: The Dark Ages from id Software, the legendary studio behind the DOOM franchise. All will be available at launch on PC this year.\n\nAvowed, a first-person fantasy role-playing game, will join the cloud when it launches on PC on Tuesday, Feb. 18. Welcome to the Living Lands, an island full of mysteries and secrets, danger and adventure, choices and consequences and untamed wilderness. Take on the role of an Aedyr Empire envoy tasked with investigating a mysterious plague. Freely combine weapons and magic \u2014 harness dual-wield wands, pair a sword with a pistol or opt for a more traditional sword-and-shield approach. In-game companions \u2014 which join the players\u2019 parties \u2014 have unique abilities and storylines that can be influenced by gamers\u2019 choices.\n\nDOOM: The Dark Ages is the single-player, action first-person shooter prequel to the critically acclaimed DOOM (2016) and DOOM Eternal. Play as the DOOM Slayer, the legendary demon-killing warrior fighting endlessly against Hell. Experience the epic cinematic origin story of the DOOM Slayer\u2019s rage this year.\n\nGet ready to play these titles and more at high performance when they join GeForce NOW at launch. Ultimate members will be able to stream at up to 4K resolution and 120 fps with support for NVIDIA DLSS and Reflex technology, and experience the action even on low-powered devices. Keep an eye out on GFN Thursdays for the latest on their release dates in the cloud.\n\nGeForce NOW is making popular devices cloud-gaming-ready while consistently delivering quality titles from top publishers to bring another ultimate year of gaming to members across the globe.\n\nSee notice regarding software product information.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "NVIDIA Launches AI Foundation Models for RTX AI PCs",
            "link": "https://nvidianews.nvidia.com/news/nvidia-launches-ai-foundation-models-for-rtx-ai-pcs",
            "snippet": "NVIDIA Launches AI Foundation Models for RTX AI PCs. NVIDIA NIM Microservices and AI Blueprints Help Developers and Enthusiasts Build AI Agents and Creative...",
            "score": 0.8936068415641785,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA NIM Microservices and AI Blueprints Help Developers and Enthusiasts Build AI Agents and Creative Workflows on PC\n\nCES\u2014NVIDIA today announced foundation models running locally on NVIDIA RTX\u2122 AI PCs that supercharge digital humans, content creation, productivity and development.\n\nThese models \u2014 offered as NVIDIA NIM\u2122 microservices \u2014 are accelerated by new GeForce RTX\u2122 50 Series GPUs, which feature up to 3,352 trillion operations per second of AI performance and 32GB of VRAM. Built on the NVIDIA Blackwell architecture, RTX 50 Series are the first consumer GPUs to add support for FP4 compute, boosting AI inference performance by 2x and enabling generative AI models to run locally in a smaller memory footprint, compared with previous-generation hardware.\n\nGeForce\u2122 has long been a vital platform for AI developers. The first GPU-accelerated deep learning network, AlexNet, was trained on the GeForce GTX\u2122 580 in 2012 \u2014 and last year, over 30% of published AI research papers cited the use of GeForce RTX.\n\nNow, with generative AI and RTX AI PCs, anyone can be a developer. A new wave of low-code and no-code tools, such as AnythingLLM, ComfyUI, Langflow and LM Studio, enable enthusiasts to use AI models in complex workflows via simple graphical user interfaces.\n\nNIM microservices connected to these GUIs will make it effortless to access and deploy the latest generative AI models. NVIDIA AI Blueprints, built on NIM microservices, provide easy-to-use, preconfigured reference workflows for digital humans, content creation and more.\n\nTo meet the growing demand from AI developers and enthusiasts, every top PC manufacturer and system builder is launching NIM-ready RTX AI PCs with GeForce RTX 50 Series GPUs.\n\n\u201cAI is advancing at light speed, from perception AI to generative AI and now agentic AI,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cNIM microservices and AI Blueprints give PC developers and enthusiasts the building blocks to explore the magic of AI.\u201d\n\nMaking AI NIMble\n\nFoundation models \u2014 neural networks trained on immense amounts of raw data \u2014 are the building blocks for generative AI.\n\nNVIDIA will release a pipeline of NIM microservices for RTX AI PCs from top model developers such as Black Forest Labs, Meta, Mistral and Stability AI. Use cases span large language models (LLMs), vision language models, image generation, speech, embedding models for retrieval-augmented generation (RAG), PDF extraction and computer vision.\n\n\u201cGeForce RTX 50 Series GPUs with FP4 compute will unlock a massive range of models that can run on PC, which were previously limited to large data centers,\u201d said Robin Rombach, CEO of Black Forest Labs. \u201cMaking FLUX an NVIDIA NIM microservice increases the rate at which AI can be deployed and experienced by more users, while delivering incredible performance.\u201d\n\nNVIDIA today also announced the Llama Nemotron family of open models that provide high accuracy on a wide range of agentic tasks. The Llama Nemotron Nano model will be offered as a NIM microservice for RTX AI PCs and workstations, and excels at agentic AI tasks like instruction following, function calling, chat, coding and math.\n\nNIM microservices include the key components for running AI on PCs and are optimized for deployment across NVIDIA GPUs \u2014 whether in RTX PCs and workstations or in the cloud.\n\nDevelopers and enthusiasts will be able to quickly download, set up and run these NIM microservices on Windows 11 PCs with Windows Subsystem for Linux (WSL).\n\n\u201cAI is driving Windows 11 PC innovation at a rapid rate, and Windows Subsystem for Linux (WSL) offers a great cross-platform environment for AI development on Windows 11 alongside Windows Copilot Runtime,\u201d said Pavan Davuluri, corporate vice president of Windows at Microsoft. \u201cNVIDIA NIM microservices, optimized for Windows PCs, give developers and enthusiasts ready-to-integrate AI models for their Windows apps, further accelerating deployment of AI capabilities to Windows users.\u201d\n\nThe NIM microservices, running on RTX AI PCs, will be compatible with top AI development and agent frameworks, including AI Toolkit for VSCode, AnythingLLM, ComfyUI, CrewAI, Flowise AI, LangChain, Langflow and LM Studio. Developers can connect applications and workflows built on these frameworks to AI models running NIM microservices through industry-standard endpoints, enabling them to use the latest technology with a unified interface across the cloud, data centers, workstations and PCs.\n\nEnthusiasts will also be able to experience a range of NIM microservices using an upcoming release of the NVIDIA ChatRTX tech demo.\n\nPutting a Face on Agentic AI\n\nTo demonstrate how enthusiasts and developers can use NIM to build AI agents and assistants, NVIDIA today previewed Project R2X, a vision-enabled PC avatar that can put information at a user\u2019s fingertips, assist with desktop apps and video conference calls, read and summarize documents, and more.\n\nThe avatar is rendered using NVIDIA RTX Neural Faces, a new generative AI algorithm that augments traditional rasterization with entirely generated pixels. The face is then animated by a new diffusion-based NVIDIA Audio2Face\u2122-3D model that improves lip and tongue movement. R2X can be connected to cloud AI services such as OpenAI\u2019s GPT4o and xAI\u2019s Grok, and NIM microservices and AI Blueprints, such as PDF retrievers or alternative LLMs, via developer frameworks such as CrewAI, Flowise AI and Langflow. Sign up for Project R2X updates.\n\nAI Blueprints Coming to PC\n\nNIM microservices are also available to PC users through AI Blueprints \u2014 reference AI workflows that can run locally on RTX PCs. With these blueprints, developers can create podcasts from PDF documents, generate stunning images guided by 3D scenes and more.\n\nThe blueprint for PDF to podcast extracts text, images and tables from a PDF to create a podcast script that can be edited by users. It can also generate a full audio recording from the script using voices available in the blueprint or based on a user\u2019s voice sample. In addition, users can have a real-time conversation with the AI podcast host to learn more about specific topics.\n\nThe blueprint uses NIM microservices like Mistral-Nemo-12B-Instruct for language, NVIDIA Riva for text-to-speech and automatic speech recognition, and the NeMo Retriever collection of microservices for PDF extraction.\n\nThe AI Blueprint for 3D-guided generative AI gives artists finer control over image generation. While AI can generate amazing images from simple text prompts, controlling image composition using only words can be challenging. With this blueprint, creators can use simple 3D objects laid out in a 3D renderer like Blender to guide AI image generation. The artist can create 3D assets by hand or generate them using AI, place them in the scene and set the 3D viewport camera. Then, a prepackaged workflow powered by the FLUX NIM microservice will use the current composition to generate high-quality images that match the 3D scene.\n\nNVIDIA NIM microservices and AI Blueprints will be available starting in February with initial hardware support for GeForce RTX 50 Series, GeForce RTX 4090 and 4080, and NVIDIA RTX 6000 and 5000 professional GPUs. Additional GPUs will be supported in the future.\n\nNIM-ready RTX AI PCs will be available from Acer, ASUS, Dell, GIGABYTE, HP, Lenovo, MSI, Razer and Samsung, and from local system builders Corsair, Falcon Northwest, LDLC, Maingear, Mifcon, Origin PC, PCS and Scan.\n\nLearn more about how NIM microservices, AI Blueprints and NIM-ready RTX AI PCs are accelerating generative AI by joining NVIDIA at CES.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "NVIDIA Launches Cosmos World Foundation Model Platform to Accelerate Physical AI Development",
            "link": "https://nvidianews.nvidia.com/news/nvidia-launches-cosmos-world-foundation-model-platform-to-accelerate-physical-ai-development",
            "snippet": "NVIDIA today announced NVIDIA Cosmos\u2122, a platform comprising state-of-the-art generative world foundation models, advanced tokenizers, guardrails and an...",
            "score": 0.6394513249397278,
            "sentiment": null,
            "probability": null,
            "content": "New State-of-the-Art Models, Video Tokenizers and an Accelerated Data Processing Pipeline, Optimized for NVIDIA Data Center GPUs, Are Purpose-Built for Developing Robots and Autonomous Vehicles\n\nFirst Wave of Open Models Available Now to Developer Community\n\nGlobal Physical AI Leaders 1X, Agile Robots, Agility, Figure AI, Foretellix, Uber, Waabi and XPENG Among First to Adopt\n\nCES\u2014NVIDIA today announced NVIDIA Cosmos \u2122, a platform comprising state-of-the-art generative world foundation models , advanced tokenizers, guardrails and an accelerated video processing pipeline built to advance the development of physical AI systems such as autonomous vehicles (AVs) and robots .\n\nPhysical AI models are costly to develop, and require vast amounts of real-world data and testing. Cosmos world foundation models, or WFMs, offer developers an easy way to generate massive amounts of photoreal, physics-based synthetic data to train and evaluate their existing models. Developers can also build custom models by fine-tuning Cosmos WFMs.\n\nCosmos models will be available under an open model license to accelerate the work of the robotics and AV community. Developers can preview the first models on the NVIDIA API catalog , or download the family of models and fine-tuning framework from the NVIDIA NGC\u2122 catalog or Hugging Face .\n\nLeading robotics and automotive companies, including 1X, Agile Robots, Agility, Figure AI, Foretellix, Fourier, Galbot , Hillbot , IntBot, Neura Robotics , Skild AI, Virtual Incision, Waabi and XPENG, along with ridesharing giant Uber, are among the first to adopt Cosmos.\n\n\u201cThe ChatGPT moment for robotics is coming. Like large language models, world foundation models are fundamental to advancing robot and AV development, yet not all developers have the expertise and resources to train their own,\u201d said Jensen Huang, founder and CEO of NVIDIA. \u201cWe created Cosmos to democratize physical AI and put general robotics in reach of every developer.\u201d\n\nOpen World Foundation Models to Accelerate the Next Wave of AI\n\nNVIDIA Cosmos\u2019 suite of open models means developers can customize the WFMs with datasets, such as video recordings of AV trips or robots navigating a warehouse, according to the needs of their target application.\n\nCosmos WFMs are purpose-built for physical AI research and development, and can generate physics-based videos from a combination of inputs, like text, image and video, as well as robot sensor or motion data. The models are built for physically based interactions, object permanence, and high-quality generation of simulated industrial environments \u2014 like warehouses or factories \u2014 and of driving environments, including various road conditions.\n\nIn his opening keynote at CES , NVIDIA founder and CEO Jensen Huang showcased ways physical AI developers can use Cosmos models, including for:\n\nVideo search and understanding, enabling developers to easily find specific training scenarios, like snowy road conditions or warehouse congestion, from video data.\n\nenabling developers to easily find specific training scenarios, like snowy road conditions or warehouse congestion, from video data. Physics-based photoreal synthetic data generation, using Cosmos models to generate photoreal videos from controlled 3D scenarios developed in the NVIDIA Omniverse \u2122 platform.\n\nusing Cosmos models to generate photoreal videos from controlled 3D scenarios developed in the \u2122 platform. Physical AI model development and evaluation, whether building a custom model on the foundation models, improving the models using Cosmos for reinforcement learning or testing how they perform given a specific simulated scenario.\n\nwhether building a custom model on the foundation models, improving the models using Cosmos for reinforcement learning or testing how they perform given a specific simulated scenario. Foresight and \u201cmultiverse\u201d simulation, using Cosmos and Omniverse to generate every possible future outcome an AI model could take to help it select the best and most accurate path.\n\nAdvanced World Model Development Tools\n\nBuilding physical AI models requires petabytes of video data and tens of thousands of compute hours to process, curate and label that data. To help save enormous costs in data curation, training and model customization, Cosmos features:\n\nAn NVIDIA AI and CUDA\u00ae-accelerated data processing pipeline, powered by NVIDIA NeMo\u2122 Curator , that enables developers to process, curate and label 20 million hours of videos in 14 days using the NVIDIA Blackwell platform, instead of over three years using a CPU-only pipeline.\n\npowered by , that enables developers to process, curate and label 20 million hours of videos in 14 days using the NVIDIA Blackwell platform, instead of over three years using a CPU-only pipeline. NVIDIA Cosmos Tokenizer , a state-of-the-art visual tokenizer for converting images and videos into tokens. It delivers 8x more total compression and 12x faster processing than today\u2019s leading tokenizers.\n\n, a state-of-the-art visual tokenizer for converting images and videos into tokens. It delivers 8x more total compression and 12x faster processing than today\u2019s leading tokenizers. The NVIDIA NeMo framework for highly efficient model training, customization and optimization.\n\nWorld\u2019s Largest Physical AI Industries Adopt Cosmos\n\nPioneers across the physical AI industry are already adopting Cosmos technologies.\n\n1X, an AI and humanoid robot company, launched the 1X World Model Challenge dataset using Cosmos Tokenizer. XPENG will use Cosmos to accelerate the development of its humanoid robot. And Hillbot and Skild AI are using Cosmos to fast-track the development of their general-purpose robots.\n\n\u201cData scarcity and variability are key challenges to successful learning in robot environments,\u201d said Pras Velagapudi, chief technology officer at Agility. \u201cCosmos\u2019 text-, image- and video-to-world capabilities allow us to generate and augment photorealistic scenarios for a variety of tasks that we can use to train models without needing as much expensive, real-world data capture.\u201d\n\nTransportation leaders are also using Cosmos to build physical AI for AVs:\n\nWaabi, a company pioneering generative AI for the physical world starting with autonomous vehicles, is evaluating Cosmos in the context of data curation for AV software development and simulation.\n\nWayve, which is developing AI foundation models for autonomous driving, is evaluating Cosmos as a tool to search for edge and corner case driving scenarios used for safety and validation.\n\nAV toolchain provider Foretellix will use Cosmos, alongside NVIDIA Omniverse Sensor RTX APIs , to evaluate and generate high-fidelity testing scenarios and training data at scale.\n\n, to evaluate and generate high-fidelity testing scenarios and training data at scale. Global ridesharing giant Uber is partnering with NVIDIA to accelerate autonomous mobility. Rich driving datasets from Uber, combined with the features of the Cosmos platform and NVIDIA DGX Cloud\u2122, can help AV partners build stronger AI models even more efficiently.\n\n\u201cGenerative AI will power the future of mobility, requiring both rich data and very powerful compute,\u201d said Dara Khosrowshahi, CEO of Uber. \u201cBy working with NVIDIA, we are confident that we can help supercharge the timeline for safe and scalable autonomous driving solutions for the industry.\u201d\n\nDeveloping Open, Safe and Responsible AI\n\nNVIDIA Cosmos was developed in line with NVIDIA\u2019s trustworthy AI principles, which prioritize privacy, safety, security, transparency and reducing unwanted bias.\n\nTrustworthy AI is essential for fostering innovation within the developer community and maintaining user trust. NVIDIA is committed to safe and trustworthy AI, in line with the White House\u2019s voluntary AI commitments and other global AI safety initiatives.\n\nThe open Cosmos platform includes guardrails designed to mitigate harmful text and images, and features a tool to enhance text prompts for accuracy. Videos generated with Cosmos autoregressive and diffusion models on the NVIDIA API catalog include invisible watermarks to identify AI-generated content, helping reduce the chances of misinformation and misattribution.\n\nNVIDIA encourages developers to adopt trustworthy AI practices and further enhance guardrail and watermarking solutions for their applications.\n\nAvailability\n\nCosmos WFMs are now available under NVIDIA\u2019s open model license on Hugging Face and the NVIDIA NGC catalog. Cosmos models will soon be available as fully optimized NVIDIA NIM microservices.\n\nDevelopers can access NVIDIA NeMo Curator for accelerated video processing and customize their own world models with NVIDIA NeMo . NVIDIA DGX Cloud offers a fast and easy way to deploy these models, with enterprise support available through the NVIDIA AI Enterprise software platform.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NVIDIA Redefines Game AI With ACE Autonomous Game Characters | GeForce News",
            "link": "https://www.nvidia.com/en-us/geforce/news/nvidia-ace-autonomous-ai-companions-pubg-naraka-bladepoint/",
            "snippet": "PUBG: BATTLEGROUNDS, inZOI, MIR5 & NARAKA: BLADEPOINT MOBILE PC VERSION are the first games to incorporate autonomous companions, enemies, and game systems...",
            "score": 0.8781346678733826,
            "sentiment": null,
            "probability": null,
            "content": "The term 'AI' has been used in games for decades. These non-playable characters (NPCs) traditionally follow strict rules designed to mimic intelligence, adhere to a guided story, and provide a scripted interaction with the player. However, with the rise of intelligent language models, game AI is primed for a truly intelligent overhaul.\n\nAt CES 2025, NVIDIA is redefining game AI with the introduction of NVIDIA ACE autonomous game characters.\n\nFirst introduced in 2023, NVIDIA ACE is a suite of RTX-accelerated digital human technologies that bring game characters to life with generative AI.\n\nNVIDIA is now expanding ACE from conversational NPCs to autonomous game characters that use AI to perceive, plan, and act like human players. Powered by generative AI, ACE will enable living, dynamic game worlds with companions that comprehend and support player goals, and enemies that adapt dynamically to player tactics.\n\nEnabling these autonomous characters are new ACE small language models (SLMs), capable of planning at human-like frequencies required for realistic decision making, and multi-modal SLMs for vision and audio that allow AI characters to hear audio cues and perceive their environment.\n\nNVIDIA is partnering with leading game developers to incorporate ACE autonomous game characters into their titles. Interact with human-like AI players and companions in PUBG: BATTLEGROUNDS, inZOI, and NARAKA: BLADEPOINT MOBILE PC VERSION. Fight against ever-learning AI-powered bosses that adapt to your playstyle in MIR5. And experience new gameplay mechanics made possible with AI-powered NPCs in AI People, Dead Meat, and ZooPunk.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "DLSS Multi Frame Generation & New RTX Technologies Coming To Black State, DOOM: The Dark Ages, Dune: Awakening, and More. 75 Games and Apps At Launch & More On The Way",
            "link": "https://www.nvidia.com/en-us/geforce/news/dlss4-multi-frame-generation-ray-tracing-rtx-games/",
            "snippet": "We're announcing that 75 games and apps will support DLSS 4's new DLSS Multi Frame Generation technology at launch of our GeForce RTX 50 Series GPUs.",
            "score": 0.7513594627380371,
            "sentiment": null,
            "probability": null,
            "content": "Each week, games and apps integrate NVIDIA DLSS, ray-traced effects, and AI, ensuring GeForce RTX owners receive the definitive experience. Such is the pace of RTX adoption, there are now over 700 #RTXON games and apps.\n\nTo begin 2025 with a bang, we\u2019re announcing that 75 games and apps will support DLSS 4\u2019s new DLSS Multi Frame Generation technology at launch of our GeForce RTX 50 Series GPUs.\n\nIncluded on the list are today\u2019s most played and highest rated games, including Alan Wake 2, Cyberpunk 2077, Diablo IV, Dragon Age\u2122: The Veilguard, God of War Ragnar\u00f6k, Indiana Jones and the Great Circle\u2122, NARAKA: BLADEPOINT, SILENT HILL 2, S.T.A.L.K.E.R. 2: Heart of Chornobyl, and Star Wars Outlaws\u2122.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "NVIDIA Expands Omniverse With Generative Physical AI",
            "link": "https://nvidianews.nvidia.com/news/nvidia-expands-omniverse-with-generative-physical-ai",
            "snippet": "NVIDIA today announced generative AI models and blueprints that expand NVIDIA Omniverse\u2122 integration further into physical AI applications such as robotics,...",
            "score": 0.6375573873519897,
            "sentiment": null,
            "probability": null,
            "content": "\u2022 New Models, Including Cosmos World Foundation Models, and Omniverse Mega Factory and Robotic Digital Twin Blueprint Lay the Foundation for Industrial AI\n\n\u2022 Leading Developers Accenture, Altair, Ansys, Cadence, Microsoft and Siemens Among First to Adopt Platform Libraries\n\nCES\u2014NVIDIA today announced generative AI models and blueprints that expand NVIDIA Omniverse \u2122 integration further into physical AI applications such as robotics, autonomous vehicles and vision AI. Global leaders in software development and professional services are using Omniverse to develop new products and services that will accelerate the next era of industrial AI.\n\nAccenture, Altair, Ansys, Cadence , Foretellix, Microsoft and Neural Concept are among the first to integrate Omniverse into their next-generation software products and professional services. Siemens, a leader in industrial automation, announced today at the CES trade show the availability of Teamcenter Digital Reality Viewer \u2014 the first Siemens Xcelerator application powered by NVIDIA Omniverse libraries.\n\n\u201cPhysical AI will revolutionize the $50 trillion manufacturing and logistics industries. Everything that moves \u2014 from cars and trucks to factories and warehouses \u2014 will be robotic and embodied by AI,\u201d said Jensen Huang, founder and CEO at NVIDIA. \u201cNVIDIA\u2019s Omniverse digital twin operating system and Cosmos physical AI serve as the foundational libraries for digitalizing the world\u2019s physical industries.\u201d\n\nNew Models and Frameworks Accelerate World Building for Physical AI\n\nCreating 3D worlds for physical AI simulation requires three steps: world building, labeling the world with physical attributes and making it photoreal.\n\nNVIDIA offers generative AI models that accelerate each step. The USD Code and USD Search NVIDIA NIM\u2122 microservices are now generally available, letting developers use text prompts to generate or search for OpenUSD assets. A new NVIDIA Edify SimReady generative AI model unveiled today can automatically label existing 3D assets with attributes like physics or materials, enabling developers to process 1,000 3D objects in minutes instead of over 40 hours manually.\n\nNVIDIA Omniverse, paired with new NVIDIA Cosmos \u2122 world foundation models , creates a synthetic data multiplication engine \u2014 letting developers easily generate massive amounts of controllable, photoreal synthetic data. Developers can compose 3D scenarios in Omniverse and render images or videos as outputs. These can then be used with text prompts to condition Cosmos models to generate countless synthetic virtual environments for physical AI training.\n\nNVIDIA Omniverse Blueprints Speed Up Industrial, Robotic Workflows\n\nDuring the CES keynote , NVIDIA also announced four new blueprints that make it easier for developers to build Universal Scene Description (OpenUSD)-based Omniverse digital twins for physical AI. The blueprints include:\n\nNew free Learn OpenUSD courses are also now available to help developers build OpenUSD-based worlds faster than ever.\n\nMarket Leaders Supercharge Industrial AI Using NVIDIA Omniverse\n\nGlobal leaders in software development and professional services are using Omniverse to develop new products and services that are poised to accelerate the next era of industrial AI.\n\nBuilding on its adoption of Omniverse libraries in its Reality Digital Twin data center digital twin platform, Cadence, a leader in electronic systems design, announced further integration of Omniverse into Allegro, its leading electronic computer-aided design application used by the world\u2019s largest semiconductor companies.\n\nAltair, a leader in computational intelligence, is adopting the Omniverse blueprint for real-time CAE digital twins for interactive computational fluid dynamics (CFD). Ansys is adopting Omniverse into Ansys Fluent, a leading CAE application. And Neural Concept is integrating Omniverse libraries into its next-generation software products, enabling real-time CFD and enhancing engineering workflows.\n\nAccenture, a leading global professional services company, is using Mega to help German supply chain solutions leader KION by building next-generation autonomous warehouses and robotic fleets for their network of global warehousing and distribution customers.\n\nAV toolchain provider Foretellix, a leader in data-driven autonomy development, is using the AV simulation blueprint to enable full 3D sensor simulation for optimized AV testing and validation. Research organization MITRE is also deploying the blueprint, in collaboration with the University of Michigan\u2019s Mcity testing facility, to create an industry-wide AV validation platform.\n\nKatana Studio is using the Omniverse spatial streaming workflow to create custom car configurators for Nissan and Volkswagen, allowing them to design and review car models in an immersive experience while improving the customer decision-making process.",
            "description": null,
            "finbert_sentiment": "positive"
        }
    },
    "2025-01-05": {
        "0": {
            "title": "How Big A Threat Are Custom AI Chips To Nvidia Stock?",
            "link": "https://www.nasdaq.com/articles/how-big-threat-are-custom-ai-chips-nvidia-stock",
            "snippet": "Broadcom's sales from its custom AI chips and networking processors surged by 220% to $12.2 billion in 2024, up from $3.8 billion in revenue.",
            "score": 0.7046056985855103,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock (NASDAQ:NVDA) had a stellar 2024, rising by almost 3x to about $135 per share. Business has boomed led by the surging demand for graphics processing units (GPUs) which have emerged as the backbone of the generative artificial intelligence era. That said, over the last month or so investors appear to have their eyes on another sector of the semiconductor market, namely ASICs (application-specific integrated circuits) that could play a bigger role in AI computing.\n\nThis comes after two major ASIC players, Broadcom and Marvell Technology, reported a surge in demand for their ASICs from large cloud customers in the most recent quarters. Do Custom AI Chips Make Marvell Stock A Buy? For instance, Broadcom\u2019s sales from its custom AI chips and networking processors surged by 220% to $12.2 billion in 2024, up from $3.8 billion in revenue that the company generated from AI silicon in FY\u201923. To be sure, Nvidia\u2019s sales are head and shoulders above, estimated to come in at about $129 billion this fiscal year, but its growth rates are slowing. So could ASICs threaten Nvidia\u2019s AI dominance as the market matures? Separately, if you want upside with a smoother ride than an individual stock, consider the High Quality portfolio, which has outperformed the S&P, and clocked >91% returns since inception.\n\nASICS vs GPUs\n\nASICs have been around for more than five decades. However, they are seeing renewed interest in the AI era. While GPUs from companies such as Nvidia are pretty versatile and can be programmed for AI as well as other tasks, ASICs are custom-designed semiconductors built to perform specific tasks giving them certain advantages versus general processors. By focusing on targeted functionalities these chips offer several advantages versus GPUs for AI. For instance, these specialized chips could be more cost-effective than GPUs, which are designed for a wider range of applications.\n\nASICs also consume less electricity and this makes them ideal for data centers aiming to reduce electricity costs \u2013 a key cost of operating large AI systems. ASICs can also achieve higher performance for dedicated tasks than general-purpose GPUs from Nvidia or AMD as they are purpose-built. These chips could be well suited for large cloud computing providers given that they operate at a scale that can justify the design and development costs of ASICs. For instance, Broadcom, a company that is viewed as the biggest beneficiary of a potential pivot toward ASICs, recently said that three of its hyperscaler customers intend to build clusters of 1 million custom chips across a single network.\n\nChange In The AI Space Benefits ASICs\n\nCompanies have devoted immense resources to building AI models over the last two years or so. Now training these massive models is more of a one-time affair that requires considerable computing power and Nvidia has been the biggest beneficiary of this, as its GPUs are regarded as the fastest and most efficient for these tasks. However, the AI landscape may be shifting. Incremental performance gains are expected to diminish as models grow larger in terms of several parameters. Separately, the availability of high-quality data for training models is likely to become a bottleneck as much of the Internet\u2019s high-quality data is already run through large language models. Considering this, the significantly front-loaded AI training phase could wind down. The underlying economics of the end market for GPU chips and the broader AI ecosystem are weak, and most of Nvidia\u2019s customers likely aren\u2019t generating meaningful returns on their investments just yet. See How Nvidia Stock Could Fall To $65.\n\nAs shareholders eventually seek better returns, we could see more customers looking toward ASICs to reduce upfront costs as well as operating costs. The longer-term focus of AI will be on inference, where trained models are used in real-world applications. This phase is less computationally intensive and could open the door for alternative AI processors that are less powerful. ASIC chips customized for inference could also be a top choice for these tasks. There is some historical precedent to this as well. For example, in the the crypto industry, Bitcoin miners initially used GPUs for mining, but some large players have since switched to ASICs as they scaled up given their better hash rates (effectively how many guesses the system makes per second to solve the crypto puzzle) and overall cost-effectiveness.\n\nWhat This Means For Nvidia\n\nWhile NVDA stock has seen strong growth over recent years, the Trefis High Quality (HQ) Portfolio, with a collection of 30 stocks, has provided better returns with less risk versus the benchmark S&P 500 index over the last four years; less of a roller-coaster ride as evident in HQ Portfolio performance metrics. So what lies ahead for Nvidia stock?\n\nTo be sure, we don\u2019t expect Nvidia\u2019s business to be supplanted by these new waves of processors, given the company\u2019s head start in the AI market and its deeply entrenched CUDA software stack and development tools which results in high switching costs for customers. Moreover, if the market sees a strong shift, Nvidia too could potentially expand its presence in the space. However, Nvidia\u2019s premium valuation may not fully account for the potential risks and slowing growth. We value Nvidia stock at about $93 per share, about 32% below the current market price. See our analysis of Nvidia valuation: Expensive or Cheap.\n\nReturns Jan 2025\n\nMTD [1] Since start\n\nof 2024 [1] 2017-25\n\nTotal [2] NVDA Return 0% 178% 5007% S&P 500 Return 0% 23% 163% Trefis Reinforced Value Portfolio 0% 16% 748%\n\n[1] Returns as of 1/1/2025\n\n[2] Cumulative total returns since the end of 2016\n\nInvest with Trefis Market-Beating Portfolios\n\nSee all Trefis Price Estimates\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "S&P 500 would have underperformed European stocks, if not for Nvidia",
            "link": "https://seekingalpha.com/news/4392542-sp-500-would-have-underperformed-european-stocks-if-not-for-nvidia",
            "snippet": "If we pull Nvidia (NVDA) out of the S&P 500, the index's total returns since 2022 underperform the eurozone's stock benchmark, the MSCI EMU Index.",
            "score": 0.7522402405738831,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "2": {
            "title": "Is NVIDIA Corporation (NASDAQ:NVDA) the Hottest Large-Cap Stock Right Now?",
            "link": "https://finance.yahoo.com/news/nvidia-corporation-nasdaq-nvda-hottest-151623587.html",
            "snippet": "We recently compiled a list of the 10 Hottest Large-Cap Stocks Right Now. In this article, we are going to take a look at where NVIDIA Corporation...",
            "score": 0.9155541062355042,
            "sentiment": null,
            "probability": null,
            "content": "We recently compiled a list of the 10 Hottest Large-Cap Stocks Right Now. In this article, we are going to take a look at where NVIDIA Corporation (NASDAQ:NVDA) stands against the other large-cap stocks.\n\nThis article will analyze several prominent large-cap stocks that are currently exerting significant influence on market dynamics. These stocks are currently considered \"hot\" because their stock prices are relatively more volatile, and they draw the attention of a large pool of investors. These stocks are the most talked about, with high trading volumes, large price actions and overall hot atmosphere surrounding them currently.\n\nWhat to watch when it comes to Large-Cap Stocks?\n\nLarge-Cap Stocks are usually household names, stocks which even the non-investing population has heard of. They are considered safer investments than small-cap stocks, so they will naturally bring a larger volume when it comes to trading.\n\nPrice change over the past week is the first parameter we will analyze when talking about the hottest Large-Cap Stocks. Another parameter which we will analyze is the volume of shares traded over the course of the past trading week. Even with Large-Caps, when investors and traders see large changes in volume, they could get spooked or could see an opportunity to jump in and aboard the train.\n\nThe first days of the new year, as well as the last days of the year gone, are usually very volatile. There are a lot of speculation and tax-loss harvesting going on, which affects the broader market dynamics. On the other hand, investors who took profit in 2024 are looking for new investments to start their new investment year strong. The New Year's Day holiday also affects the trading continuity, further deepening the volatility. To see which firms kicked this year off in the red, you can check out the following article.\n\nThe Large-Caps listed here are all market titans, with market caps over $200 billion dollars.\n\nIs Nvidia Corp. (NVDA) Among Firms Under Tech, Energy, Aviation that Led Friday Rally?\n\nA close-up of a colorful high-end graphics card being plugged in to a gaming computer.\n\nNVIDIA Corporation (NASDAQ:NVDA)\n\nReturn: 4.27%\n\nNVIDIA Corporation (NASDAQ:NVDA) had a great week in the market. It was the strongest stock of the \"Magnificent 7\", closing 4.27% in green.\n\nSince the AI sector doesn't show signs of stopping in 2025, investors are positioning their portfolios hoping that NVDA can have another great year as it did last year, when it returned 171%. The demand is still strong, pushing market leaders such as NVDA to new heights.\n\nNVDA is still years ahead of its competition when it comes to the products and services it provides, so this competitive advantage could still fuel further growth in 2025. Analysts are expecting further earnings-per-share growth rates over the course of this year, so NVDA, even after a nearly 200% surge in 2024, is still considered \"hot\" and the leader of the Magnificent 7.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "NVIDIA takes a step toward AR Glasses with a new patent",
            "link": "https://www.androidheadlines.com/2025/01/nvidia-ar-glasses-new-patent.html",
            "snippet": "NVIDIA has filed a patent for smart glasses featuring AR technology, which could offer a compact design that looks like regular glasses.",
            "score": 0.5002194046974182,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA has filed a patent for augmented reality (AR) glasses. These glasses may look like regular glasses but with some special features on the inside. It also appears that the company is looking to make these glasses lightweight and energy-efficient while offering high-quality visuals.\n\nThe patent titled \u201cBacklight-Free Augmented Reality Digital Holography\u201d can be found under the number 20250004275A1 on the United States Patent and Trademark Office (USPTO) website. Thanks to SETI Park, an independent patent researcher, for sharing its details on X.\n\nThis patent reveals some interesting information\n\nAccording to the patent, the new glasses will not darken the entire lens like traditional sunglasses. Instead, NVIDIA\u2019s design will darken specific portions of the lens to showcase AR content. This might help save power and enhance the overall user experience.\n\nThe patent mentions that the glasses will use a spatial light modulator. This device controls light to make AR visuals clearer. NVIDIA could also use a neural network that will improve the performance of the light modulator so the visuals can look more realistic.\n\nIt should be noted that just because NVIDIA has filed a patent, it doesn\u2019t mean the glasses are under production right now. Sometimes, companies file patents for ideas but don\u2019t begin working on them immediately. Even if NVIDIA moves forward with this design, it could take several years before the glasses are available for sale. However, this patent indicates that the company may be planning to become a major player in the AR glasses market.\n\nNVIDIA may join the race to develop AR glasses\n\nSome big tech companies are racing to make AR glasses, and it seems NVIDIA is also not far in this exciting race. Meta launched the Orion glasses last year and termed them \u201cthe most advanced glasses the world has ever seen.\u201d\n\nSamsung is reportedly preparing to showcase its AR glasses at the Galaxy Unpacked event, which could take place on January 22. Apple is also reportedly working on AR glasses, though reports say it will likely take 3-5 years before these glasses are ready.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "New Nvidia GeForce RTX 5090 leak suggests huge memory boost",
            "link": "https://mashable.com/article/nvidia-geforce-rtx-5090-leak-details-ces-2025",
            "snippet": "The 5090 will come with 32GB of GDDR7 memory, twice the capacity of the rumored 5080 graphics card. In layman's terms, that means it should be really fast.",
            "score": 0.8074693083763123,
            "sentiment": null,
            "probability": null,
            "content": "Everyone who follows graphics cards is pretty sure Nvidia will announce the GeForce RTX 50-series at CES 2025. One new leak makes that seem almost certain.\n\nChinese manufacturer Inno3D appears to have leaked its iChill X3 version of the rumored RTX 5090 GPU, courtesy of VideoCardz. The leak, which amounts to some product art that presumably exists for a future online shop webpage, doesn't tell us much. However, the packaging seems to confirm that the 5090 will come with 32GB of GDDR7 memory, twice the capacity of the rumored 5080 graphics card.\n\nMashable Light Speed Want more out-of-this world tech, space and science stories? Sign up for Mashable's weekly Light Speed newsletter. Loading... Sign Me Up By clicking Sign Me Up, you confirm you are 16+ and agree to our Terms of Use and Privacy Policy Thanks for signing up!\n\nIn layman's terms, that means it should be really fast and capable of doing a lot, whether it's in the realm of gaming, bitcoin farming, or artificial intelligence applications. It will also likely be expensive, as these things tend to be. Unfortunately, this leak doesn't tell us much else, such as pricing or a specific release date.\n\nBut we likely won't have to wait much longer to find out. Nvidia is hosting the big keynote address to kick off CES on Monday.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Nvidia\u2019s RTX 5090 leaks with 32GB of GDDR7 memory",
            "link": "https://www.theverge.com/2025/1/5/24336321/nvidia-rtx-5090-32gb-vram-gddr7-memory-ces-leak-rumor",
            "snippet": "Nvidia's RTX 5090 has leaked just a day before its CES keynote. The Inno3D packaging suggests the RTX 5090 will ship with 32GB of GDDR7 memory.",
            "score": 0.9078214764595032,
            "sentiment": null,
            "probability": null,
            "content": "is a senior editor and author of Notepad , who has been covering all things Microsoft, PC, and tech for over 20 years.\n\nNvidia\u2019s RTX 5090 has leaked today in the form of a marketing image of the unannounced next-gen GPU. VideoCardz has obtained a box shot of the RTX 5090, which suggests that the rumors of 32GB of GDDR7 memory are true.\n\nWhile the packaging of the unannounced Inno3D RTX 5090 iChill X3 doesn\u2019t reveal more specs about Nvidia\u2019s flagship next-gen GPU, it does suggest that this particular model will ship with a 3.5-slot cooler.\n\nInno3D\u2019s RTX 5090 packaging. Image: VideoCardz\n\nThe RTX 5090 is expected to have double the VRAM of the RTX 5080, which is rumored to include 16GB of GDDR7 memory. It\u2019s also rumored to include 21,760 CUDA cores, nearly 1.8TB/s of memory bandwidth, and a TDP of 575 watts \u2014 125 watts more than the RTX 4090.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Apple embraces Nvidia GPUs to accelerate LLM inference via its open source ReDrafter tech",
            "link": "https://www.techradar.com/pro/apple-embraces-nvidia-gpus-to-accelerate-llm-inference-via-its-open-source-tech-redrafter",
            "snippet": "Nvidia says collaboration opens up exciting possibilities for future LLM workloads.",
            "score": 0.8170072436332703,
            "sentiment": null,
            "probability": null,
            "content": "ReDrafter delivers 2.7x more tokens per second compared to traditional auto-regression\n\nReDrafter could reduce latency for users while using fewer GPUs\n\nApple hasn't said when ReDrafter will be deployed on rival AI GPUs from AMD and Intel\n\nApple has announced a collaboration with Nvidia to accelerate large language model inference using its open source technology, Recurrent Drafter (or ReDrafter for short).\n\nThe partnership aims to address the computational challenges of auto-regressive token generation, which is crucial for improving efficiency and reducing latency in real-time LLM applications.\n\nReDrafter, introduced by Apple in November 2024, takes a speculative decoding approach by combining a recurrent neural network (RNN) draft model with beam search and dynamic tree attention. Apple\u2019s benchmarks show that this method generates 2.7x more tokens per second compared to traditional auto-regression.\n\nCould it extend beyond Nvidia?\n\nThrough its integration into Nvidia\u2019s TensorRT-LLM framework, ReDrafter extends its impact by enabling faster LLM inference on Nvidia GPUs widely used in production environments.\n\nTo accommodate ReDrafter\u2019s algorithms, Nvidia introduced new operators and tweaked existing ones within TensorRT-LLM, making the tech available for any developers looking to optimize performance for large-scale models.\n\nIn addition to the speed improvements, Apple says ReDrafter has the potential to reduce user latency while requiring fewer GPUs. This efficiency not only lowers computational costs but also lessens power consumption, a vital factor for organizations managing large-scale AI deployments.\n\nWhile the focus of this collaboration remains on Nvidia\u2019s infrastructure for now, it\u2019s possible that similar performance benefits could be extended to rival GPUs from AMD or Intel at some point in the future.\n\nAre you a pro? Subscribe to our newsletter Sign up to the TechRadar Pro newsletter to get all the top news, opinion, features and guidance your business needs to succeed! Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nBreakthroughs like this can help improve machine learning efficiency. As Nvidia says, \"This collaboration has made TensorRT-LLM more powerful and more flexible, enabling the LLM community to innovate more sophisticated models and easily deploy them with TensorRT-LLM to achieve unparalleled performance on Nvidia GPUs. These new features open exciting possibilities, and we eagerly anticipate the next generation of advanced models from the community that leverage TensorRT-LLM capabilities, driving further improvements in LLM workloads.\u201d\n\nYou can read more about the collaboration with Apple on the Nvidia Developer Technical Blog.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "7": {
            "title": "NVIDIA's next-gen GeForce RTX 5090 confirmed with 32GB GDDR7, new retail packaging spotted",
            "link": "https://www.tweaktown.com/news/102396/nvidias-next-gen-geforce-rtx-5090-confirmed-with-32gb-gddr7-new-retail-packaging-spotted/index.html",
            "snippet": "NVIDIA's next-gen GeForce RTX 5090 confirmed with 32GB GDDR7, new retail packaging spotted \u00b7 Inno3D's upcoming flagship GeForce RTX 5090 iChill X3 graphics card...",
            "score": 0.6257175207138062,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "I\u2019m a Nintendo Switch 2 skeptic but this one rumored feature might change my mind",
            "link": "https://www.tomsguide.com/gaming/nintendo/im-a-nintendo-switch-2-skeptic-but-this-one-rumored-feature-might-change-my-mind",
            "snippet": "Nintendo Switch 2 rumors are abundant, but this one rumored feature could be a game-changer for skeptics.",
            "score": 0.8926858305931091,
            "sentiment": null,
            "probability": null,
            "content": "The Nintendo Switch 2 and its potentially imminent announcement have dominated gaming discourse for well over a year. While certain folks are naturally excited about Nintendo\u2019s next system, I wasn\u2019t one of them. Don\u2019t get me wrong, the NES and SNES rank among my favorite gaming consoles, and I respect what the company has done for gaming as a whole. However, I\u2019m not the biggest fan of Nintendo's first-party games or the hardware those titles run on.\n\nI bought the original Nintendo Switch at launch and barely touched it. I played almost exclusively in docked mode since I found the actual handheld uncomfortable to hold. And while games I loved like Super Mario Odyssey and Metroid Dread looked decent enough on my old 1080p TV, every game looked atrocious when I upgraded to the LG CX OLED 4K. The Marseille MClassic graphics enhancer peripheral helped make the games look better, but the experience wasn\u2019t great.\n\nGiven that, I had no true interest in the Nintendo Switch 2. However, after the console\u2019s motherboard allegedly leaked on Reddit, I\u2019m having a change of heart. Though the Switch\u2019s reported specs won\u2019t allow it to play games with the same graphical fidelity and performance as they would on a PS5, it could have a game-changer feature that might make me consider buying Nintendo\u2019s system.\n\nThat DLSS magic\n\n(Image credit: Nvidia)\n\nBased on images of the alleged Nintendo Switch 2 motherboard, the Switch 2 will utilize an Nvidia SoC (system on a chip). The chip in question is built on Samsung Foundry\u2019s 8nm process \u2014 which is the same as Nvidia Ampere. This chip focuses on improved ray tracing cores, faster memory and AI performance. With an Ampere chip and a reported 12GB of RAM, the Switch 2 could employ Nintendo\u2019s take on DLSS or Nvidia\u2019s upscaling technology.\n\nRecently, Nintendo filed a patent detailing a trained neural network that can upscale a 540p image to 1080p or a 720p image to 4K. This would be the equivalent of DLSS 2.0 Performance mode and Ultra Performance mode, respectively. Based on the patent, it appears that upscaling features are off by default when the handheld is running on battery and activated when connected to power. The patent suggests you can enable this tech when the Switch 2 is undocked, though that\u2019ll certainly drain battery life.\n\nWe can\u2019t say this will happen right away. After all, companies file patents all the time to make sure competitors don\u2019t beat them to the punch. However, putting the pieces together, it\u2019s not unreasonable to believe the Switch 2 could feature upscaling tech that would help its games look sharp on 4K TV when the system is docked. This would certainly get skeptical folks like me to at least consider getting the Switch 2. Sure, graphics aren\u2019t everything, but they\u2019re still important.\n\nNintendo confirmed Switch 2 will be backward compatible. If the system employs some form of AI-powered upscaling and frame-boosting, it makes me wonder if older Switch games will automatically get a graphical boost. Yes, The Legend of Zelda: Breath of the Wild and its sequel have a unique art style, but it\u2019s hard to appreciate that due to the low resolution and spotty performance. If original Switch games get a graphical and performance boost on Switch 2, it\u2019d be hard for me to dismiss the console.\n\nOutlook\n\nThe Nintendo Switch 2 hasn\u2019t been officially released, so we have to take everything we\u2019ve heard about it with a healthy dose of skepticism. However, if Nintendo\u2019s upcoming system does indeed feature a form of DLSS technology, it should be more appealing to folks who aren\u2019t die-hard Nintendo fans.\n\nHopefully, Nintendo can deliver a machine that looks good on modern TVs and monitors. If so, sign me up!",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Rumor: The First Specs Of The Nvidia RTX 5090 Have Leaked",
            "link": "https://gamerant.com/rumor-the-first-specs-of-the-nvidia-rtx-5090-have-leaked/",
            "snippet": "Some leaked details about the Nvidia GeForce RTX 5090 stated that the upcoming graphics card would have up to 32GB of GDDR7 video memory and a staggering power...",
            "score": 0.8895429372787476,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-04": {
        "0": {
            "title": "Cerence AI expands collaboration with Nvidia for CaLLM family of language models",
            "link": "https://finance.yahoo.com/news/cerence-ai-expands-collaboration-nvidia-131112424.html",
            "snippet": "Cerence (CRNC) announced an expanded collaboration with NVIDIA (NVDA) to advance the capabilities of its CaLLM family of language models, including its...",
            "score": 0.9259772896766663,
            "sentiment": null,
            "probability": null,
            "content": "https://www.tipranks.com/news/the-fly/snowflake-price-target-raised-to-215-from-200-at-needham\n\nCerence (CRNC) announced an expanded collaboration with NVIDIA (NVDA) to advance the capabilities of its CaLLM family of language models, including its cloud-based Cerence Automotive Large Language Model and its CaLLM Edge embedded small language model. Through this collaboration, CaLLM is powered by NVIDIA AI Enterprise, an end-to-end, cloud-native software platform, and some aspects of CaLLM Edge are powered by NVIDIA DRIVE AGX Orin. Integrating agentic frameworks with in-car conversations in both cloud and embedded forms requires a comprehensive, cross-disciplinary effort combining hardware, software, and UX domain expertise. Working alongside NVIDIA hardware and software engineers, Cerence AI enhanced its ability to meet production timelines and productize generative AI innovation for automotive. Specifically, Cerence AI has accelerated the development and deployment of CaLLM by leveraging the NVIDIA AI Enterprise software platform, including NVIDIA TensorRT-LLM and NVIDIA NeMo, an end-to-end framework to build, customize, and deploy generative AI applications into production. As a result, Cerence AI has optimized and customized its CaLLM family of models to: Deliver faster in-vehicle assistant performance on NVIDIA accelerated computing and SoCs; Develop an automotive-optimized implementation of NVIDIA NeMo Guardrails, helping ensure Cerence-powered systems can navigate the nuances of in-car interaction; Implement and optimize an agentic architecture on CaLLM Edge via NVIDIA DRIVE AGX Orin, helping advance the next generation of in-vehicle user experiences\n\nStay Ahead of the Market:\n\nDiscover outperforming stocks and invest smarter with Top Smart Score Stocks\n\nFilter, analyze, and streamline your search for investment opportunities using Tipranks' Stock Screener\n\nPublished first on TheFly \u2013 the ultimate source for real-time, market-moving breaking financial news. Try Now>>\n\nSee Insiders\u2019 Hot Stocks on TipRanks >>\n\nRead More on CRNC:",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "Where Will Nvidia Stock Be in 3 Years?",
            "link": "https://www.fool.com/investing/2025/01/04/where-will-nvidia-stock-be-in-3-years/",
            "snippet": "A 68% CAGR over the next three years would bring Nvidia's top line to almost $610 billion by the end of fiscal 2028. While that figure may seem extremely...",
            "score": 0.9359924793243408,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA 5.27%) stock has made investors significantly richer in the past three years, turning an investment of $1,000 into more than $4,500 as of this writing. This is thanks to the 354% jump in the company's shares during this period, on account of its dominant position in the lucrative market for artificial intelligence (AI) chips.\n\nIt is worth noting that shares of the chipmaker have handsomely outperformed the Nasdaq Composite's (NASDAQINDEX: ^IXIC) gains of 23% in the past three years. Nvidia's outstanding stock market returns have been powered by the terrific growth in its revenue and earnings, as customers and governments have been lining up to get their hands on its AI chips to train and deploy AI models.\n\nNow that we are at the beginning of 2025, it would be a good time to take a closer look at Nvidia's prospects for the next three years and see if this high-flying AI stock can continue delivering more upside to investors in the future as well.\n\nNvidia's massive addressable opportunity suggests that it isn't done growing yet\n\nThe size of Nvidia's business has grown immensely over the past three years. The company is on track to finish fiscal 2025 (which will end this month) with revenue of $128.6 billion (calculated by adding its fiscal Q4 revenue forecast of $37.5 billion to the $91.1 billion revenue it has generated in the first nine months of the year).\n\nFor comparison, Nvidia ended fiscal year 2022 (which coincided with the majority of 2021) with $26.9 billion in revenue. So, the chipmaker's revenue is on track to increase at a compound annual growth rate (CAGR) of 68% during this three-year period. Investors may be wondering if Nvidia is capable of replicating such stunning revenue growth over the next three years as well.\n\nA 68% CAGR over the next three years would bring Nvidia's top line to almost $610 billion by the end of fiscal 2028. While that figure may seem extremely ambitious at first, investors should note that Nvidia has a huge addressable market opportunity that could indeed allow it to get closer to that figure. For instance, the company sees a $1 trillion revenue opportunity in the data center market alone.\n\nCEO Jensen Huang points out that \"every single data center will have GPUs\" in the future to enable accelerated computing, which is done through specialized hardware such as graphics cards that Nvidia sells to perform more work in less time. As a result, accelerated computing enables users to get more done while consuming less energy, which is why it is expected to play a central role in keeping data center power consumption in check in the long run.\n\nMoreover, the construction of new data center capacity is going to be another long-term tailwind for Nvidia. McKinsey, for example, estimates that global data center capacity could jump at an annual rate of 19% to 22% through 2030 to support the booming demand for generative AI. All this indicates that Nvidia's data center business still has a lot of room for growth, considering that the company is on track to end fiscal 2025 with just under $100 billion in revenue from this segment.\n\nThe $1 trillion opportunity suggested by Nvidia indicates that it has scratched just 10% of the opportunity on offer in this space. Moreover, the company is the leading player in the data center GPU market, with a market share of more than 85%, indicating that the growing adoption of accelerated computing could significantly lift Nvidia's data center revenue over the next three years.\n\nThe good part is that Nvidia's growth opportunity isn't limited to just data centers. The company's GPUs are also being used for other purposes as well, such as creating digital twins for industrial applications, powering gaming and AI personal computers (PCs), and in automotive and robotics. Nvidia reported robust growth in these three segments last quarter, generating combined revenue of $4.2 billion. That was a 20% jump over the prior-year period.\n\nThese segments should continue to be tailwinds for Nvidia. The gaming GPU market, for instance, is expected to add $49 billion in revenue between 2023 and 2028, growing at a CAGR of 21% during this period, as per TechNavio. Nvidia is the top player in gaming GPUs, with a market share of 90%, according to Jon Peddie Research. This puts the company in a nice position to make the most of the incremental growth opportunity in this space.\n\nMeanwhile, the digital twin market is expected to generate $110 billion in revenue in 2028, as compared to $10 billion in 2023. Nvidia's GPUs support the growth of this market, as they are used for creating virtual models of factories and also for automating workflows in factories to enable higher operating efficiency. Multiple companies such as Foxconn, Reliance, Toyota, and others are using digital twins in their business operations by deploying Nvidia's GPUs.\n\nAll this indicates that Nvidia's multiple growth drivers could indeed help it sustain its impressive growth over the next three years.\n\nHow much upside can investors expect?\n\nWe have seen that Nvidia is indeed capable of sustaining its outstanding revenue growth rate over the next three years. However, to estimate the stock's potential upside, we are going to rely on consensus estimates from YCharts. As the chart shows us, Nvidia's earnings are expected to grow from $2.95 per share in fiscal 2025 to $5.59 in fiscal 2027. That points toward an annual earnings growth rate of 37% for the next two years.\n\nIf we take a conservative view and estimate that Nvidia's earnings grow even 30% in fiscal 2028, its bottom line could hit $7.27 per share. If we multiply the projected earnings after three years with the Nasdaq-100's earnings multiple of 33 (using the index as a proxy for tech stocks), its stock price could hit $240.\n\nThat points toward 79% gains from current levels over the next three years. Given that Nvidia is trading at 32 times forward earnings right now, investors are getting a good deal on this AI stock, indicating that they can still consider buying it, as it seems to have room for more upside going forward.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "This Little-Known AI Stock Just Scored A Major Deal With Nvidia",
            "link": "https://www.barchart.com/story/news/30294693/this-little-known-ai-stock-just-scored-a-major-deal-with-nvidia",
            "snippet": "This Little-Known AI Stock Just Scored a Major Deal with Nvidia ... Little-known company Cerence AI (CRNC) is a big mover on Friday, with shares up nearly 110% in...",
            "score": 0.8504000306129456,
            "sentiment": null,
            "probability": null,
            "content": "Switch the Market flag\n\nOpen the menu and switch the\n\nMarket flag for targeted data from your country of choice.\n\nfor targeted data from your country of choice.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Are Nvidia Stock Investors Expecting Too Much From Its Upcoming Keynote?",
            "link": "https://www.nasdaq.com/articles/are-nvidia-stock-investors-expecting-too-much-its-upcoming-keynote",
            "snippet": "Don't miss this second chance at a potentially lucrative opportunity \u00b7 Nvidia: if you invested $1,000 when we doubled down in 2009, you'd have $374,613!*",
            "score": 0.6267668008804321,
            "sentiment": null,
            "probability": null,
            "content": "In today's video, I discuss Nvidia (NASDAQ: NVDA) and recent updates affecting the company. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the after-market prices of Jan. 2, 2025. The video was published on Jan. 2, 2025.\n\nWhere to invest $1,000 right now? Our analyst team just revealed what they believe are the 10 best stocks to buy right now. See the 10 stocks \u00bb\n\nDon\u2019t miss this second chance at a potentially lucrative opportunity\n\nEver feel like you missed the boat in buying the most successful stocks? Then you\u2019ll want to hear this.\n\nOn rare occasions, our expert team of analysts issues a \u201cDouble Down\u201d stock recommendation for companies that they think are about to pop. If you\u2019re worried you\u2019ve already missed your chance to invest, now is the best time to buy before it\u2019s too late. And the numbers speak for themselves:\n\nNvidia: if you invested $1,000 when we doubled down in 2009, you\u2019d have $374,613 !*\n\nif you invested $1,000 when we doubled down in 2009, !* Apple: if you invested $1,000 when we doubled down in 2008, you\u2019d have $46,088 !*\n\nif you invested $1,000 when we doubled down in 2008, !* Netflix: if you invested $1,000 when we doubled down in 2004, you\u2019d have $475,143!*\n\nRight now, we\u2019re issuing \u201cDouble Down\u201d alerts for three incredible companies, and there may not be another chance like this anytime soon.\n\nSee 3 \u201cDouble Down\u201d stocks \u00bb\n\n*Stock Advisor returns as of December 30, 2024\n\nJose Najarro has positions in Nvidia. The Motley Fool has positions in and recommends Nvidia. The Motley Fool has a disclosure policy. Jose Najarro is an affiliate of The Motley Fool and may be compensated for promoting its services. If you choose to subscribe through their link they will earn some extra money that supports their channel. Their opinions remain their own and are unaffected by The Motley Fool.\n\nThe views and opinions expressed herein are the views and opinions of the author and do not necessarily reflect those of Nasdaq, Inc.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "NVIDIA Giving Away CEO-Signed GeForce GTX 1080 Ti GPU, The Best \u201cTi\u201d of All",
            "link": "https://wccftech.com/nvidia-geforce-gtx-1080-ti-giveaway/",
            "snippet": "NVIDIA GeForce on X has announced that it is giving away the GeForce GTX 1080 Ti, signed by the NVIDIA CEO. This is the third in a row and will be followed by...",
            "score": 0.4997439384460449,
            "sentiment": null,
            "probability": null,
            "content": "Yet another giveaway has started and this time, it's none other than one of the best high-end GPUs of its time, the NVIDIA GeForce GTX 1080 Ti.\n\nNVIDIA Announces its Third GPU Giveaway, The Winner will Receive NVIDIA's CEO-Signed GeForce GTX 1080 Ti\n\nIt's time for the third giveaway. NVIDIA GeForce on X has announced that it is giving away the GeForce GTX 1080 Ti, signed by the NVIDIA CEO. This is the third in a row and will be followed by two more. One will be tomorrow and the other on the 5th.\n\nWin this GeForce GTX 1080 Ti - \"The Ultimate GeForce\" Signed by NVIDIA CEO Jensen Huang - NVIDIA GeForce (X)\n\nNVIDIA's first GPU giveaway was the GeForce 256, where it said that it would give away \"Iconic\" GPUs for five days straight just before CES.\n\nWe expected the NVIDIA GeForce GTX 1080 Ti to be part of this giveaway chain as it is one of the most successful high-end GPUs NVIDIA has ever offered. To this day, NVIDIA hasn't been able to deliver such a massive performance leap over the previous generation while keeping the price at just $700.\n\nWith the following generations, the flagship GPU price increased dramatically. The GTX 1080 Ti was pretty fast and even in 2025, it can play almost all games at 1080p ultra settings at 60+ fps. Being a GTX 1080 Ti owner since 2017, I have never regretted my decision to purchase this card. So fans around the world can get their hands on this exclusive card, which comes signed by Jensen.\n\nThe GeForce GTX 1080 Ti uses the GP102 die and is based on the Pascal architecture. It features 3584 Cuda Cores and 11 GB of GDDR5X memory on a 352-bit bus. It's one of the rare cards that features such an unusual memory capacity, but thankfully, 11 GB is quite sufficient for modern titles even today.\n\nThree GPU giveaways have been announced, and we just reported about the second giveaway just yesterday. That giveaway includes the GeForce 8800 Ultra, signed by Jensen Huang. Let's see what GPUs NVIDIA will follow up on in the next two days. Meanwhile, you can enter the GTX 1080 Ti competition by commenting #GeForceGreats on NVIDIA's thread on X.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Hon Hai teams up with Nvidia to develop humanoid robots",
            "link": "https://focustaiwan.tw/business/202501040004",
            "snippet": "Taiwan-based manufacturing giant Hon Hai Precision Industry Co. is partnering with American artificial intelligence chip designer Nvidia Corp. to develop...",
            "score": 0.6953875422477722,
            "sentiment": null,
            "probability": null,
            "content": "To activate the text-to-speech service, please first agree to the privacy policy below.\n\nTaipei, Jan. 4 (CNA) Taiwan-based manufacturing giant Hon Hai Precision Industry Co. is partnering with American artificial intelligence chip designer Nvidia Corp. to develop humanoid robots in the current artificial intelligence boom.\n\nSpeaking with reporters on the sidelines of the Kaohsiung Smart City annual meeting held on Friday, Hon Hai Chairman Young Liu (\u5289\u63da\u5049) said his company is working with Nvidia by adopting the American chip designer's software technologies and hardware platforms to develop human-like robots in the southern Taiwanese city.\n\nAs iPhone assembler Hon Hai has been intensifying efforts in recent years to diversify its product portfolio, Liu said the company has set sights on Kaohsiung to launch humanoid robot services.\n\nHon Hai has already introduced non-humanoid robots as part of its production lines, using Nvidia's technologies. Local media cited Liu as saying the company is planning to introduce humanoid robots to other services including the healthcare industry.\n\nHon Hai, also known as Foxconn on the global markets, and Nvidia have become business partners in recent years. The Taiwanese company rolls out AI servers powered by the U.S. firm's chips.\n\nNvidia CEO Jensen Huang (\u9ec3\u4ec1\u52f3) has touted robots will become the next wave of AI development, predicting that upcoming innovations will be dominated by humanoid robots and self-driving cars.\n\nNvidia provides software and hardware for robot manufacturing as \"everything is going to be robotic,\" Huang said.\n\nWhile Hon Hai is working with Kaohsiung in smart city development, Liu said the company is also planning to cooperate with the cities of Taipei and Keelung in this field.\n\nHe said there is great potential for cooperation in the development of apps used in smartphones.\n\nIn the future, Liu said, Hon Hai will integrate resources to allow software developers to gain access to its efforts in smart city development to establish an environment of sovereign AI, which refers to a nation's capabilities to produce artificial intelligence using its own infrastructure, data, workforce and business networks.\n\nMeanwhile, Liu said he forecast Hon Hai's consolidated sales will top NT$7 trillion (US$213 billion) in 2025 on the back of strong global demand for AI applications.\n\nIn the first 11 months of last year, Hon Hai reported consolidated sales of NT$6.21 trillion, up 8.87 percent from a year earlier, and more than the NT$6.16 trillion in revenue posted for 2023 as a whole.\n\nAI servers accounted for more than 40 percent of Hon Hai's server revenue in the first three quarters of 2024, with its cloud and networking division making up more than 30 percent of the company's total revenue.\n\nDue to robust global demand, AI servers are forecast to make up 50 percent of Hon Hai's server sales in 2025 and shipments of AI servers are likely to grow from quarter to quarter to become the most important driver of the company's sales growth, according to Hon Hai.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Here's how to watch NVIDIA's huge CES 2025 press conference live: GeForce RTX 50 unveil event",
            "link": "https://www.tweaktown.com/news/102388/heres-how-to-watch-nvidias-huge-ces-2025-press-conference-live-geforce-rtx-50-unveil-event/index.html",
            "snippet": "NVIDIA CEO Jensen Huang will take the stage at CES 2025 next Monday on January 6 at 6:30pm PT, the huge GeForce RTX 50 unveil, lots of AI, and more.",
            "score": 0.9430813789367676,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia, AMD, among chip companies likely to make news at CES",
            "link": "https://seekingalpha.com/news/4392243-nvidia-amd-among-chip-companies-likely-to-make-news-at-ces",
            "snippet": "The Consumer Electronics Show kicks off in earnest next week, with Nvidia (NVDA) Chief Executive Officer Jensen Huang giving the keynote address on Monday.",
            "score": 0.8552188873291016,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia to Hold CES Keynote 6th January @ 6:30PM PT",
            "link": "https://xboxera.com/2025/01/04/nvidia-to-hold-ces-keynote-6th-january-630pm-pt/",
            "snippet": "Nvidia CEO Jensen Huang will be leading the CES event and will be covering the company's latest hardware and software advancements.",
            "score": 0.9480367302894592,
            "sentiment": null,
            "probability": null,
            "content": "Today, Nvidia announced over Twitter that they\u2019ll be holding a keynote the day before CES begins, on the 6th of January at 6:30PM Pacific Time. Nvidia CEO Jensen Huang will be leading the CES event and will be covering the company\u2019s latest hardware and software advancements.\n\nJoin us next Monday on January 6 at 6:30pm PT as our CEO Jensen Huang takes the stage for the #CES2025 keynote: https://t.co/woWnawG9wf\n\n\n\nDiscover how our latest technologies are driving advancements in #AI, gaming, virtual worlds, autonomous vehicles, and more. @CES pic.twitter.com/HiA5GCFrkG \u2014 NVIDIA (@nvidia) January 3, 2025\n\nBesides work on autonomous cars and Large Language Models, the company is also expected to announce the latest series of Geforce RTX 50 video cards. Rumours have been making the rounds, with pictures of MSI\u2019s RTX 5080 popping up online and giving us an idea of potential specifications and just how large this kit is going to be. We\u2019ve also gotten near-final TDP specs per two leakers within the technology sphere, with the RTX 5090 sitting at 575W and the 5080 sitting at around 360W.\n\nNvidia\u2019s proprietary upscaler DLSS 4 is also expected to be unveiled at this keynote. But all shall be revealed next Monday\u2014it\u2019s the quiet before the CES storm. Follow this link to add the coming Nvidia livestream to your calendar.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Researchers from NVIDIA, CMU and the University of Washington Released \u2018FlashInfer\u2019: A Kernel Library that Provides State-of-the-Art Kernel Implementations for LLM Inference and Serving",
            "link": "https://www.marktechpost.com/2025/01/04/researchers-from-nvidia-cmu-and-the-university-of-washington-released-flashinfer-a-kernel-library-that-provides-state-of-the-art-kernel-implementations-for-llm-inference-and-serving/",
            "snippet": "Researchers from NVIDIA, CMU and the University of Washington Released 'FlashInfer': A Kernel Library that Provides State-of-the-Art Kernel Implementations...",
            "score": 0.6948310732841492,
            "sentiment": null,
            "probability": null,
            "content": "Reddit Vote Flip Share 0 Shares\n\nLarge Language Models (LLMs) have become an integral part of modern AI applications, powering tools like chatbots and code generators. However, the increased reliance on these models has revealed critical inefficiencies in inference processes. Attention mechanisms, such as FlashAttention and SparseAttention, often struggle with diverse workloads, dynamic input patterns, and GPU resource limitations. These challenges, coupled with high latency and memory bottlenecks, underscore the need for a more efficient and flexible solution to support scalable and responsive LLM inference.\n\nResearchers from the University of Washington, NVIDIA, Perplexity AI, and Carnegie Mellon University have developed FlashInfer, an AI library and kernel generator tailored for LLM inference. FlashInfer provides high-performance GPU kernel implementations for various attention mechanisms, including FlashAttention, SparseAttention, PageAttention, and sampling. Its design prioritizes flexibility and efficiency, addressing key challenges in LLM inference serving.\n\nFlashInfer incorporates a block-sparse format to handle heterogeneous KV-cache storage efficiently and employs dynamic, load-balanced scheduling to optimize GPU usage. With integration into popular LLM serving frameworks like SGLang, vLLM, and MLC-Engine, FlashInfer offers a practical and adaptable approach to improving inference performance.\n\nTechnical Features and Benefits\n\nFlashInfer introduces several technical innovations:\n\nComprehensive Attention Kernels: FlashInfer supports a range of attention mechanisms, including prefill, decode, and append attention, ensuring compatibility with various KV-cache formats. This adaptability enhances performance for both single-request and batch-serving scenarios. Optimized Shared-Prefix Decoding: Through grouped-query attention (GQA) and fused-RoPE (Rotary Position Embedding) attention, FlashInfer achieves significant speedups, such as a 31x improvement over vLLM\u2019s Page Attention implementation for long prompt decoding. Dynamic Load-Balanced Scheduling: FlashInfer\u2019s scheduler dynamically adapts to input changes, reducing idle GPU time and ensuring efficient utilization. Its compatibility with CUDA Graphs further enhances its applicability in production environments. Customizable JIT Compilation: FlashInfer allows users to define and compile custom attention variants into high-performance kernels. This feature accommodates specialized use cases, such as sliding window attention or RoPE transformations.\n\nPerformance Insights\n\nFlashInfer demonstrates notable performance improvements across various benchmarks:\n\nLatency Reduction : The library reduces inter-token latency by 29-69% compared to existing solutions like Triton. These gains are particularly evident in scenarios involving long-context inference and parallel generation.\n\n: The library reduces inter-token latency by 29-69% compared to existing solutions like Triton. These gains are particularly evident in scenarios involving long-context inference and parallel generation. Throughput Improvements : On NVIDIA H100 GPUs, FlashInfer achieves a 13-17% speedup for parallel generation tasks, highlighting its effectiveness for high-demand applications.\n\n: On NVIDIA H100 GPUs, FlashInfer achieves a 13-17% speedup for parallel generation tasks, highlighting its effectiveness for high-demand applications. Enhanced GPU Utilization: FlashInfer\u2019s dynamic scheduler and optimized kernels improve bandwidth and FLOP utilization, particularly in scenarios with skewed or uniform sequence lengths.\n\nFlashInfer also excels in parallel decoding tasks, with composable formats enabling significant reductions in Time-To-First-Token (TTFT). For instance, tests on the Llama 3.1 model (70B parameters) show up to a 22.86% decrease in TTFT under specific configurations.\n\nConclusion\n\nFlashInfer offers a practical and efficient solution to the challenges of LLM inference, providing significant improvements in performance and resource utilization. Its flexible design and integration capabilities make it a valuable tool for advancing LLM-serving frameworks. By addressing key inefficiencies and offering robust technical solutions, FlashInfer paves the way for more accessible and scalable AI applications. As an open-source project, it invites further collaboration and innovation from the research community, ensuring continuous improvement and adaptation to emerging challenges in AI infrastructure.\n\nCheck out the Paper and GitHub Page. All credit for this research goes to the researchers of this project. Also, don\u2019t forget to follow us on Twitter and join our Telegram Channel and LinkedIn Group. Don\u2019t Forget to join our 60k+ ML SubReddit.\n\n\ud83d\udea8 FREE UPCOMING AI WEBINAR (JAN 15, 2025): Boost LLM Accuracy with Synthetic Data and Evaluation Intelligence \u2013 Join this webinar to gain actionable insights into boosting LLM model performance and accuracy while safeguarding data privacy.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-03": {
        "0": {
            "title": "Nvidia stock jumps, leads 'Magnificent 7' gains to start 2025 after end-of-year tech sell-off",
            "link": "https://finance.yahoo.com/news/nvidia-stock-jumps-leads-magnificent-7-gains-to-start-2025-after-end-of-year-tech-sell-off-154139053.html",
            "snippet": "Nvidia stock led the so-called \u201cMagnificent 7\u201d tech stocks higher to start the year after a group-wide selloff in the last days of 2024.",
            "score": 0.898110568523407,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock (NVDA) led gains among the \u201cMagnificent Seven\u201d tech stocks to start the new year after a group-wide sell-off in the last days of 2024. Shares of the AI chipmaker rose 4.5% Friday after gaining roughly 3% the prior day.\n\nThat upswing followed a 4% dip between Christmas Eve and New Year\u2019s Eve as megacap tech stocks dropped across the board in the absence of a \u201cSanta Claus\" rally, where the stock market typically enjoys a surge between Dec. 24 and Jan. 2. Tesla (TSLA) stock plunged nearly 13% over that time frame, while Amazon (AMZN) and Microsoft (MSFT) dropped more than 4%. Meanwhile, Meta (META) and Google (GOOG) fell just under 4%, and Apple (AAPL) dropped 3%.\n\nEven with its December decline, Nvidia shares still ended 2024 up more than 150%. Wall Street analysts have remained bullish on the stock, estimating shares will rise to roughly $173 over the next year from their current level of $138, according to Yahoo Finance data.\n\nBank of America\u2019s Vivek Arya told Brian Sozzi on the Opening Bid podcast Thursday that broader market forces and company-specific issues drove the sell-off in Nvidia stock late last year.\n\n\"What we have seen in the market is a rotation of money from semiconductors to software,\" Arya said, noting that the latter was less exposed to US trade restrictions on goods to and from China. He added that for Nvidia, \"the last two quarters have not been clean, really, because they're going through growing pains from one generation of product that was Hopper to the new generation of product.\"\n\nArya was referring to concerns surrounding delays in shipments of the company's new Blackwell artificial intelligence chips.\n\n\"In our view, these are short-term issues,\" he said. That could create a buying opportunity: While its Magnificent Seven peers are trading at two times their average earnings growth estimates for 2025, Arya noted that Nvidia shares are trading at less than one times the consensus earnings growth for the company this upcoming year.\n\nArya has a Buy rating on Nvidia stock and sees shares rising to $190 over the next 12 months.\n\nShares of Nvidia and its Magnificent Seven peers have benefited from investors' massive bets on artificial intelligence, as well as earnings growth. As Yahoo Finance's Josh Schafer reported, through roughly three-quarters of reports, the combination of Apple, Alphabet, Microsoft, Amazon, Meta, Tesla, and Nvidia grew earnings year over year by 33% in 2024, compared to just 4.2% growth for the other 493 S&P 500 companies, per FactSet data.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "1": {
            "title": "Opinion: Why Nvidia, MediaTek May Enter The PC CPU Market Soon",
            "link": "https://www.crn.com/news/components-peripherals/2025/opinion-why-nvidia-mediatek-may-enter-the-pc-cpu-market-soon",
            "snippet": "The PC market for CPUs is set to get crowded with AI computing giant Nvidia and Taiwanese chip designer MediaTek reportedly set to enter the segment this year.",
            "score": 0.9130819439888,
            "sentiment": null,
            "probability": null,
            "content": "Opinion: Why Nvidia, MediaTek May Enter The PC CPU Market Soon\n\nWhile the fast-growing AI PC category represents a new way to stay relevant for traditional CPU players Intel and AMD, it also represents an opportunity for Qualcomm, Nvidia and MediaTek\u2014with help from Microsoft\u2014to chip away at the x86 CPU duopoly.\n\nThe PC market for CPUs is set to get crowded with AI computing giant Nvidia and Taiwanese chip designer MediaTek reportedly set to enter the segment this year.\n\nWhile several reports have pointed to PC CPU development activities by the two companies over the past couple of years, executives at Intel and Arm have openly acknowledged that Intel, AMD and Qualcomm are set to face more competition soon.\n\n[Related: Qualcomm Ramps Up Global Channel Hiring To Fight Intel, AMD In PC Market]\n\n\u201cEverybody is really excited about the PC market, so we have more competitors than we\u2019ve ever had. You will see more competitors enter the marketplace in 2025, and we\u2019re going to have to be on our toes and making sure that we\u2019re winning,\u201d said Michelle Johnston Holthaus, one of Intel\u2019s interim co-CEOs, at an investor event last month.\n\nThose comments came months after a similar suggestion was made last year by the CEO of Arm, which licenses the Arm instruction set architecture and Arm-based chip designs to third parties such as Qualcomm and Apple to develop their own CPUs.\n\nThe CEO, Rene Haas, told Reuters in an interview that he expects Arm-based CPUs to pose a significant challenge to x86-based CPUs from Intel and AMD over the next few years, thanks to Qualcomm and other companies entering the space. (This projection is apparently notwithstanding Arm\u2019s legal dispute with Qualcomm, which ended in a mixed verdict in December, leading the company to seek a new trial).\n\n\u201cArm\u2019s market share in Windows\u2014I think, truly, in the next five years, it could be better than 50 percent,\u201d Haas said in the June 2024 interview.\n\nWhat Nvidia And MediaTek Could Be Planning\n\nMultiple reports have suggested that new Arm-based chips for PCs could be on the way from Nvidia and MediaTek. Both companies currently design Arm-based CPUs for other segments, with Nvidia focused on servers and edge computers while MediaTek is focused on Chromebooks, smartphones and smart devices.\n\nA June 2024 report from Reuters, for instance, said MediaTek plans to introduce a line of Arm-based processors for Windows PCs near the end of this year.\n\nThe Reuters report said the MediaTek chips are designed for Microsoft\u2019s Copilot+ PC program, which kicked off last June with Qualcomm\u2019s Snapdragon X chips and then expanded to new x86 chips from Intel and AMD for testing in December.\n\nSeparately, MediaTek is working with Nvidia on another line of Arm-based chips for Windows PCs, according to Reuters. This product line, also rumored for late 2025, is expected to combine a MediaTek CPU with an Nvidia GPU on a system-on-chip, according to other reports.\n\nNvidia is also planning a solo line of Arm-based chips using its own CPU and GPU designs for Windows PCs, DigiTimes in Taiwan reported last October. The company may potentially introduce the chips in September of this year.\n\nWhy MediaTek And Nvidia May Be Making PC Chips\n\nMediaTek and Nvidia are reportedly planning to enter the PC CPU market during what vendors hope could be the PC\u2019s biggest inflection point in more than a decade.\n\nI\u2019m talking, of course, about the AI PC, which a wide range of vendors, including Microsoft, AMD, Qualcomm, HP Inc., Dell Technologies and Lenovo, have been clamoring about as the next big thing in personal computing for more than a year.\n\nThese companies are hoping that the explosion in demand for AI applications, which have largely been cloud-based up until this point, will translate into a need for AI applications that that take advantage of a PC\u2019s CPU, GPU and NPU to improve responsiveness, privacy and personalization while reducing costs and unlocking new experiences.\n\nDespite these lofty promises, vendors have yet to make a definitive case for the existence of the AI PC, as I wrote in a previous CRN opinion piece, and this is largely due to the slow rollout of new features and applications from the likes of Microsoft, Apple and ISVs.\n\nMicrosoft has also been forced to pump the brakes on its flagship Recall feature for Copilot+ PCs due to security and privacy concerns. While the feature was originally set for release last June, it only became available in October to members of the Windows Insider Community for testing after Microsoft implemented new safeguards.\n\nBut while there has been a shortage of widely available software that takes advantage of AI PCs, that is likely to change this year with Microsoft, Apple and ISVs primed to release more applications and features that make a more compelling case for the nascent category.\n\nWith this expectation, research firms IDC and Gartner believe that AI PCs will grow fast and represent a substantial portion, if not a majority, of personal computers that get shipped by vendors over the next few years. IDC, for example, has forecasted that AI PC shipments will more than triple between 2024 and 2027 to represent 60 percent of total PC shipments across the world by the end of that period.\n\nWhile this represents a new way to stay relevant for traditional CPU players Intel and AMD, it also represents an opportunity for Qualcomm, Nvidia and MediaTek to gain a foothold in the market and chip away at the x86 CPU duopoly.\n\nThe move to release a CPU for PCs would echo Nvidia\u2019s data center strategy, in which it has sought to own a larger share of computations by introducing its own Arm-based CPU that is tuned to optimize system performance in conjunction with its GPUs.\n\nIt\u2019s worth noting that Nvidia has already done significant groundwork in enabling and accelerating the development of AI PC applications that take advantage of its RTX GPU. For instance, the company announced last June the Nvidia RTX AI Toolkit, which allows developers to customize, optimize and deploy generative AI models on RTX AI PCs.\n\nMeanwhile, MediaTek\u2019s entry into the PC CPU market would represent a lateral move from the Arm-based CPUs it designs for Chromebooks.\n\nWhy Microsoft Plays A Big Role In New CPU Contenders\n\nThe other force that is likely driving Nvidia and MediaTek to release their own PC CPUs is none other than Microsoft, which has spent the past several years developing and hardening Windows support for Arm processors.\n\nMicrosoft has shown that it is very interested in fighting against the popularity of Apple\u2019s Mac computers\u2014which have been using custom, Arm-based chips since 2020\u2014by partnering with whichever chip company it can to push the boundaries for performance, efficiency and new experiences on Windows.\n\nWhen it came to the June launch of its Copilot+ PC program, Qualcomm was in the right place at the right time to make all the difference. Meanwhile, Copilot+ features have yet to become generally available for PCs powered by the latest x86 chips from Intel and AMD.\n\nIn other words, we\u2019re far away from the days of the \u201cWintel\u201d Windows-Intel alliance or even the idea of Windows being synonymous with the x86 architecture. Microsoft is now approaching chip architecture with a more open mind.\n\nWith Microsoft\u2019s partnership with Qualcomm serving as part of a larger effort to make Arm viable for Windows PCs, the CPU plans by Nvidia and MediaTek could further the tech giant\u2019s agenda by giving it more options for chips that enable game-changing experiences.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Cerence Teams with NVIDIA to Supercharge AI-Powered Vehicle Assistants Using CaLLM Technology",
            "link": "https://www.stocktitan.net/news/CRNC/cerence-ai-expands-collaboration-with-nvidia-to-advance-its-ca-llm-rwadz64m0cxk.html",
            "snippet": "This partnership enables Cerence to deliver enhanced driver experiences with advanced performance, reduced latency, improved privacy, and robust security...",
            "score": 0.6711468696594238,
            "sentiment": null,
            "probability": null,
            "content": "Cerence AI Expands Collaboration with NVIDIA to Advance its CaLLM Family of Language Models\n\n01/03/2025 - 08:00 AM\n\nCompany will leverage the NVIDIA AI Enterprise software platform, including the NVIDIA TensorRT-LLM open-source library and the NVIDIA NeMo framework, to optimize performance\n\nBURLINGTON, Mass., Jan. 03, 2025 (GLOBE NEWSWIRE) -- Cerence Inc. (NASDAQ: CRNC) (\u201cCerence AI\u201d), a global industry leader in voice AI, today announced an expanded collaboration with NVIDIA to advance the capabilities of its CaLLM\u2122 family of language models, including its cloud-based Cerence Automotive Large Language Model (CaLLM) and its CaLLM Edge embedded small language model. Through this collaboration, CaLLM is powered by NVIDIA AI Enterprise , an end-to-end, cloud-native software platform, and some aspects of CaLLM Edge are powered by NVIDIA DRIVE AGX Orin .\n\nIntegrating agentic frameworks with in-car conversations in both cloud and embedded forms requires a comprehensive, cross-disciplinary effort combining hardware, software, and UX domain expertise. Working alongside NVIDIA hardware and software engineers, Cerence AI enhanced its ability to meet production timelines and productize generative AI innovation for automotive. Specifically, Cerence AI has accelerated the development and deployment of CaLLM by leveraging the NVIDIA AI Enterprise software platform, including NVIDIA TensorRT-LLM and NVIDIA NeMo , an end-to-end framework to build, customize, and deploy generative AI applications into production. As a result, Cerence AI has optimized and customized its CaLLM family of models to:\n\nDeliver faster in-vehicle assistant performance on NVIDIA accelerated computing and SoCs\n\nDevelop an automotive-optimized implementation of NVIDIA NeMo Guardrails , helping ensure Cerence-powered systems can navigate the nuances of in-car interaction\n\n, helping ensure Cerence-powered systems can navigate the nuances of in-car interaction Implement and optimize an agentic architecture on CaLLM Edge via NVIDIA DRIVE AGX Orin, helping advance the next generation of in-vehicle user experiences\n\n\n\nOverall, this expanded collaboration with NVIDIA equips Cerence AI with scalable, reliable tools and resources to develop next-generation user experiences in partnership with its automaker customers. This, in turn, facilitates enriched driver experiences intended to deliver advanced performance, reduced latency, enhanced privacy and security, and robust protection against malicious or unwanted interactions.\n\n\u201cBy optimizing the performance of our CaLLM family of language models, we are delivering cost savings and improved performance to our automaker customers, who are running quickly to deploy generative AI-powered solutions to their drivers,\u201d said Nils Schanz, Executive Vice President, Product & Technology, Cerence AI. \u201cAs we advance our next-gen platform, with CaLLM as its foundation, these advanced capabilities will deliver faster, more reliable interaction to drivers, enhancing their safety, enjoyment and productivity on the road.\u201d\n\n\u201cLarge language models are offering vast, new user experiences, but complexities in size and deployment can make it difficult for developers to get AI-powered solutions into the hands of end users,\u201d said Rishi Dhall, Vice President of Automotive, NVIDIA. \u201cThrough this expanded collaboration, Cerence AI is deploying advanced NVIDIA AI and accelerated computing technologies to optimize its LLM development and deployment.\u201d\n\nTo learn more about Cerence AI, visit www.cerence.ai, and follow the company on LinkedIn.\n\nAbout Cerence Inc.\n\nCerence Inc. (NASDAQ: CRNC) is a global industry leader in creating intuitive, seamless, AI-powered experiences across automotive and transportation. Leveraging decades of innovation and expertise in voice, generative AI, and large language models, Cerence powers integrated experiences that create safer, more connected, and more enjoyable journeys for drivers and passengers alike. With more than 500 million cars shipped with Cerence technology, the company partners with leading automakers, transportation OEMs, and technology companies to advance the next generation of user experiences. Cerence is headquartered in Burlington, Massachusetts, with operations globally and a worldwide team dedicated to pushing the boundaries of AI innovation. For more information, visit www.cerence.ai.\n\nKate Hickman | Tel: 339-215-4583 | Email: kate.hickman@cerence.com",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "AI Dominance to Continue in 2025 With Nvidia Leading the Way",
            "link": "https://www.etftrends.com/leveraged-inverse-channel/ai-dominance-continue-2025-nvidia-leading-way/",
            "snippet": "AI was a constant investing theme in 2024. Its dominance in 2025 could go unabated, with Nvidia being one of the top names leading the way.",
            "score": 0.756110429763794,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "4": {
            "title": "Nvidia RTX 50 potentially teased in new video \u2014 this could be the next-gen GPU",
            "link": "https://www.tomsguide.com/computing/nvidia-rtx-50-potentially-teased-in-new-video-this-could-be-the-next-gen-gpu",
            "snippet": "Nvidia hasn't officially announced the RTX 50-series but rumors and reports suggest the company could unveil its next-gen GPUs soon. As we reported, alleged...",
            "score": 0.8970712423324585,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia might have inadvertently given us our first glimpse of an RTX 50-series GPU. During a video advertising the company\u2019s upcoming LAN 50 event, we see a gaming PC cloaked in shadows. As VideoCardz spotted, brightening the image reveals an RTX GPU featuring a never-before-seen design,\n\nThe design in question might not be final, as VideoCardz notes. The GPU could look like this when it hits retail, or it could have an entirely different appearance. It\u2019s unclear if this is the Nvidia GeForce RTX 5080 we previously reported about or another RTX 50-series GPU like the RTX 5070. VideoCardz believes this GPU\u2019s size suggests it\u2019s not the rumored RTX 5090.\n\n(Image credit: Nvidia (via VideoCardz.com))\n\nSo what stands out about this potential RTX 50-series GPU? VideoCardz says that this component strays from the push/pull air design and instead has two front fans. The GPU has an LED strip and its connector has an angle adaptor or cable, which VideoCardz says is \u201cunusual.\u201d It\u2019s hard to discern any further details from even the brightened-up image.\n\nNvidia hasn\u2019t officially announced the RTX 50-series but rumors and reports suggest the company could unveil its next-gen GPUs soon. As we reported, alleged leaked images of the Nvidia RTX 5080 were posted on the Chiphell Forums (via Wccftech ). The images reveal MSI\u2019s Nvidia GeForce RTX 5080 Gaming Trio OC Edition. Before that, a massive leak revealed Nvidia RTX 5090, 5080 and 5070 GPUs ahead of CES 2025.\n\nGeForce LAN 50 Missions and Rewards - YouTube Watch On\n\nOutlook\n\nNothing is definitive, but it seems like a safe bet Nvidia will drop its next-gen GPUs sooner rather than later. This is not only because of all the alleged leaks and rumors but also because it has been over two years since the launch of the Nvidia GeForce RTX 40 series. Hopefully, the company will keep its GPU prices somewhat reasonable.\n\nWith Nvidia CEO Jensen Huang set to deliver a CES keynote on January 6th at 6:30 pm PT and the Nvidia LAN 50 event running from January 4 to the 6th, we\u2019re sure to hear something substantial during CES 2025. Stay tuned for more news as it develops!\n\nMore from Tom's Guide",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Hiding in the shadows, this Nvidia teaser video might have given us our first look at a Founders Edition RTX 50-series GPU",
            "link": "https://www.pcgamer.com/hardware/graphics-cards/hiding-in-the-shadows-this-nvidia-teaser-video-might-have-given-us-our-first-look-at-a-founders-edition-rtx-50-series-gpu/",
            "snippet": "A newly-released teaser video and some image-editing techniques, our first look at what might be a Founders Edition RTX 50-series card may have been spotted.",
            "score": 0.8809109926223755,
            "sentiment": null,
            "probability": null,
            "content": "With CES 2025 mere days away, the rumour mill surrounding Nvidia's next generation GPUs is now spinning at an astonishing rate. The internet is clamouring for any potential info on RTX 50-series graphics cards, and thanks to a newly-released teaser video and some image-editing techniques, our first look at what might be a Founders Edition RTX 50-series card may have been spotted.\n\nThe GeForce LAN 50 event will run alongside Nvidia CEO Jen-Hsun Huang's keynote, in which we're expecting to see the new GPUs announced. A video has been released detailing giveaways and rewards at the event, including a shadowy shot of a gaming PC. The eagle-eyed folks over at Videocardz adjusted the brightness, and whaddaya know, it looks like there's a previously unseen GPU sitting in the PCIe slot.\n\n(Image credit: Nvidia)\n\nThe card in question has the GeForce RTX text emblazoned on the side, along with a vertically oriented power connector and what looks to be an LED strip mounted somewhere inside the cooler.\n\nIt certainly doesn't look like any Nvidia card we're currently aware of, so the thinking is that this may well be an RTX 50-series card hidden in an image from Nvidia itself.\n\nIt's a bit of a tiddler, though. That suggests that if this is indeed an RTX 50-series card, it's likely something lower down the range than something like the RTX 5090 or RTX 5080\u2014as they're unlikely to be smaller than the current RTX 4090 and RTX 4080 GPUs, both of which are pretty chonky.\n\nSo, the RTX 5070 then? Possibly. It's also entirely possible that this is a placeholder, rendered to look like a generic RTX card sitting in a generic chassis. That being said, the rumours currently indicate that we'll see an RTX 5070 card launched alongside its more powerful siblings.\n\nThere's some odd details going on with the cooler here as well. Looking at the underside of the card, it looks like a twin fan design rather than a single fan on either side in a push-pull design like the current 40-series FE cards. That'd be a change from the norm, along with an LED light strip that may be running along one of the edges of the fan shrouds.\n\nThe biggest gaming news, reviews and hardware deals Keep up to date with the most important stories and the best deals, as picked by the PC Gamer team. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nIt's this glowing doohickey that makes the card stand out in the darkness of the overall image in the first place, and may well be a design note we see repeated on the RTX 50-series as a whole.\n\nCES 2025 (Image credit: Future) Catch up with CES 2025: We're on the ground in sunny Las Vegas covering all the latest announcements from some of the biggest names in tech, including Nvidia, AMD, Intel, Asus, Razer, MSI and more.\n\nAnd a vertically oriented power connector? Why not. The power cable here appears to run across the underside of the card into oblivion, but it doesn't seem unreasonable to think that Nvidia might change up the connector orientation on the new cards.\n\nOr perhaps it wont. Again, there's the potential for this to be nothing but a render, but there are enough oddly specific details here to suggest that we might be looking at a proper RTX 50-series card in the flesh, so to speak.\n\nLike the rest of the world, we'll just have to wait for the official Nvidia announcement to find out if this image is accurate. Think of this particular tease as a tantalising glimpse into a future that may well be, yet may not come to pass. It's more fun that way, isn't it?",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Nvidia Stock Rally: AI-Powered Surge & 2025 Expectations",
            "link": "https://www.industryleadersmagazine.com/nvidia-stock-rally-ai-powered-surge-2025-expectations/",
            "snippet": "Nvidia Stock Rally: AI-Powered Surge & 2025 Expectations. Nvidia's AI chip dominance and Blackwell's success are driving its stock rally, with 2025 poised for...",
            "score": 0.8933009505271912,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia stock surged 3% by noon on January 2, marking a gain on this year\u2019s first trading day. On January 6, CEO Jensen Huang will deliver the goals and objectives of the company at CES 2025 in Las Vegas, which is keeping up the expectations of investors. Tesla is the second favorite stock for real investors but the recently released fourth-quarter report delivered disappointing sales, which has paved the way for Nvidia to be an all-time favorite stock.\n\nHowever, Nvidia\u2019s Blackwell chips are enough to keep them on top of investors\u2019 favorite stock. There is a high demand for these GPU chips in the market, at least enough to keep Nvidia busy for 12 months. Blackwell chips are one of the most powerful technologies arrived in the market of artificial intelligence and robotics.\n\nNvidia\u2019s Surge: AI-Powered Rally!\n\nMarket analysts are expecting to see a surge in Nvidia stocks after the CES event on January 6. Blackwell chips will be the superhero of 2025 and Nvidia\u2019s stock prices are expected to get as high as 20% by the first half of the year. However, GPU chips are not the only boon for the chipmaker company.\n\nNvidia\u2019s market value skyrocketed from $1.2 trillion at the end of 2023 to $3.28 trillion by the end of 2024, driven by strong demand for its AI chips, according to Reuters. Meanwhile, Apple maintained its position as the top company by market value, nearing $4 trillion, with investors excited about its AI advancements. Following close behind were Microsoft at $3.1 trillion, Alphabet at around $2.3 trillion, and Amazon, also at about $2.3 trillion.\n\nWill Nvidia Enter the Robotics Era?\n\nIn 2025, Nvidia is planning to enter the new arena of Robotics as its market is expected to be as big as $12 trillion as per Ark Invest report. If there is even 1% accuracy in the reports, it would easily surpass the revenue of Big Tech\u2019s investment in AI. Nvidia\u2019s expertise in software, like creating real-time 3D environments for robot training, allows the company to build a powerful ecosystem that could easily dominate the market.\n\nNvidia\u2019s Cash Flow Strategy: Smooth Sailing Ahead?\n\nNvidia has received orders for Blackwell chips for the next 12 months and there will be a lot of cash to manage. Analysts believe that 2025 is going to be Blackwell\u2019s chip year and it will strengthen the strong cash pile of Nvidia. The one thing that company don\u2019t have to worry about is paying healthy dividends.\n\nThe chipmaker has enough cash to pay out dividends without having any impact on future acquisitions. Nvidia has all the needed hype before CES 2025, which is visible to the investors.\n\nWas 2024 Nvidia\u2019s Big Break?\n\nIn 2024, Nvidia strengthened its hold on the AI market with new hardware and tools. The launch of the Blackwell B100 and B200 GPUs enhanced generative AI and solidified Nvidia\u2019s leadership in high-performance computing. Additionally, the $249 Jetson Orin Nano Super Developer Kit helped smaller players enter AI development. CEO Jensen Huang shared that AI growth is just beginning, and the industry is set to expand for years to come.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NVIDIA Unveils Hymba 1.5B: a Hybrid Approach to Efficient NLP Models",
            "link": "https://www.infoq.com/news/2025/01/nvidia-hymba/",
            "snippet": "NVIDIA researchers have unveiled Hymba 1.5B, an open-source language model that combines transformer and state-space model (SSM) architectures to achieve...",
            "score": 0.507097601890564,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA researchers have unveiled Hymba 1.5B, an open-source language model that combines transformer and state-space model (SSM) architectures to achieve unprecedented efficiency and performance. Designed with NVIDIA\u2019s optimized training pipeline, Hymba addresses the computational and memory limitations of traditional transformers while enhancing the recall capabilities of SSMs.\n\nTraditional transformer-based language models excel at long-term recall and parallelization but face substantial challenges with their quadratic computational complexity and large memory demands. On the other hand, SSMs like Mamba and Mamba-2 offer constant complexity and hardware optimization but underperform in memory recall tasks. Hymba resolves these trade-offs by combining the strengths of both architectures.\n\nThe hybrid-head module in Hymba fuses attention heads for high-resolution recall with SSM heads for efficient context summarization, enabling both components to work in parallel rather than sequentially. This design reduces computation and memory requirements without sacrificing performance.\n\nThe Hymba 1.5B architecture focuses on improving efficiency while maintaining accuracy by introducing several innovative mechanisms:\n\nAttention Overhead Reduction: over 50% of attention computation is replaced by SSM processing, maintaining task accuracy while reducing costs.\n\nLocal Attention Dominance: global attention is minimized, as local attention paired with SSMs sufficiently summarizes global information.\n\nKV Cache Optimization: Hymba introduces cross-layer KV cache sharing, which reduces cache redundancy across layers and significantly reduces memory usage\u2014up to 10 times less than comparable transformer models.\n\nMeta-Tokens: a set of 128 learnable embeddings prepended to prompts acts as memory initializers.\n\nSource: NVIDIA Blog\n\n\n\nThe role of learnable meta-tokens has been discussed. Daniel Svonava, a machine learning specialist at Superlinked, posed a question:\n\nCan you explain how learnable meta tokens improve the focus of the attention mechanism compared to traditional methods?\n\nMarek Bar\u00e1k, a data scientist, explained:\n\nAttention has this issue where it puts too much focus on the first token in the sentence. This has very little semantic reason since the first token does not really hold a lot of information. With meta tokens, you get a more balanced softmax distribution over tokens.\n\nHymba 1.5B has proven itself as a top performer in head-to-head comparisons with leading models under 2 billion parameters, including Llama 3.2 1B, OpenELM 1B, and Qwen 2.5 1.5B. Across benchmarks such as MMLU, ARC-C, Hellaswag, and SQuAD-C, Hymba outperformed its competitors.\n\n\n\nSource: https://arxiv.org/pdf/2411.13676\n\nNVIDIA optimized Hymba\u2019s training pipeline to balance task performance and efficiency. The pretraining strategy involved a two-stage process: early training on a diverse, unfiltered dataset, followed by fine-tuning on high-quality data. Instruction fine-tuning enhanced the model's capabilities through stages like supervised fine-tuning (SFT) and reinforcement learning via direct preference optimization (DPO).\n\nHymba 1.5B is available as an open-source release on Hugging Face and GitHub, enabling researchers and developers to test its capabilities in real-world applications.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Nvidia gives away five classic GPUs signed by CEO Jensen Huang",
            "link": "https://www.tomshardware.com/pc-components/gpus/nvidia-gives-away-five-classic-gpus-signed-by-ceo-jensen-huang-personally-the-first-two-are-the-geforce-256-and-geforce-8800-ultra",
            "snippet": "The trillion-dollar GPU giant has posted the first two GPU giveaways on X, featuring the GeForce 256 and GeForce 8800 Ultra.",
            "score": 0.8493714332580566,
            "sentiment": null,
            "probability": null,
            "content": "Ahead of CES 2025, Nvidia is giving away five classic GPUs signed by CEO Jensen Huang. The trillion-dollar GPU giant has posted the first two GPU giveaways on X, featuring the GeForce 256 and GeForce 8800 Ultra.\n\nThe giveaway is under the \"GeforceGreats\" hashtag that Nvidia has set up for all of its CES 2025-related content. Both GPUs have been installed in a black display box featuring Jensen's signature in gold handwriting at the bottom right. To win, comment on each X post.\n\nNvidia strategically chooses these two specific GPUs for its first and second (out of five) classic GPUs to giveaway. The GeForce 256 was Nvidia's first-ever GeForce GPU and the first-ever video card to be sold with the GPU moniker (video cards previously were dubbed \"3D\" cards). The GeForce 8800 Ultra was the first CUDA GPU to be introduced worldwide. This technology would become one of Nvidia's most valuable and popular technologies the company ever created, shaping the way GPUs are used even today.\n\n5 days to CES.5 classic cards up for grabs.ALL signed by NVIDIA CEO Jensen Huang \ud83d\udc40Up first: GeForce 256, the world's 1st GPUWant it? Comment #GeForceGreats for a chance to win... pic.twitter.com/Hu0z0nAIlvJanuary 1, 2025\n\nThe Geforce 256 was released in 1999, at the height of the dot-com boom and the beginning of modern 3D graphics rendering functionality in computers. The GPU was equipped with a whopping four (yes, just four) pixel shaders, four TMUs, and four ROPs. It had 32MB of memory and operated on a 64-bit memory bus with 1.144 GB/s of memory bandwidth. The GPU had one VGA output and supported DirectX 7.0 and OpenGL 1.2.\n\nThe GeForce 256 represented a mindset shift in how video cards were perceived. Previously, all video cards were dubbed \"3D\" cards because they were designed specifically for 3D rendering and were often less complex than CPUs. However, the GeForce 256 was a different beast because it had 23 million transistors in the same range as the best CPUs during that time, such as AMD's Athlon chips and Intel's Pentium III CPUs. Several years later, this would be a precursor to CUDA when GPUs started to compete directly with CPUs, performing the same calculations as CPUs.\n\nThe GeForce 256's huge transistor count at the time would be complemented by support from new graphics technology. One of these technologies was support for the new Transform and Lighting Engine at the time. This technology enabled the GeForce 256 to convert a 3D scene with all of its objects from \"world space\" into \"screen space,\" essentially converting assets from the 3D engine into a viewable image on the screen.\n\n4 days to CES.Win this GeForce 8800 Ultra - the 1st CUDA GPUSigned by NVIDIA CEO Jensen Huang \ud83d\udd8b\ufe0fComment #GeForceGreats for a chance to WIN! pic.twitter.com/xKNXmq5NmQJanuary 2, 2025\n\nBefore the 256's arrival, this workload was very demanding on the CPU. \"3D\" video cards often wait for the CPU to finish this task. With this functionality baked into the GeForce 256, the GPU could perform this task by improving performance and enabling developers to push the envelope of 3D rendering further than ever before.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nThe GeForce 8800 Ultra, launched in 2007, features 128 shader cores, 32 TMUs, 24 ROPs, 16 SMs, 96 KB of L2 cache, a core clock of 612 MHz, a shader clock of 1,512 MHz, and a 384-bit memory bus with 768 MB of GDDR3 memory and 104 GB/s of memory bandwidth. It was the flagship GPU to run on the G80 Tesla GPU die, the first core and first GPU architecture to support CUDA, and it supported DirectX 11.1 and the Shader Model 4.0 standard.\n\n\ufeffCUDA is one of the most important technologies Nvidia has ever produced. This key technology changed how GPUs could be used forever by enabling them to run general-purpose calculations, which the same CPUs can run. First-generation CUDA supported C code and required a dedicated CUDA driver to function.\n\nCUDA has become the predominant method of performing general-purpose computing tasks that greatly benefit from parallelization in enterprise applications. CUDA started the general-purpose GPU era, where GPUs were no longer used purely for graphics acceleration but as general multi-purpose processing units.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia Stock Vs. Custom AI Chips: Should Investors Worry?",
            "link": "https://www.forbes.com/sites/greatspeculations/2025/01/03/nvidia-stock-vs-custom-ai-chips-should-investors-worry/",
            "snippet": "ASICs can also achieve higher performance for dedicated tasks than general-purpose GPUs from Nvidia or AMD as they are purpose-built. These chips could be well...",
            "score": 0.7756195664405823,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-02": {
        "0": {
            "title": "How AI Is Helping Us Do Better\u2014for the Planet and for Each Other",
            "link": "https://blogs.nvidia.com/blog/ai-social-impact-2024/",
            "snippet": "From treating cancer to revolutionizing recycling, AI can help solve some of humanity's most pressing and complex problems.",
            "score": 0.5522494316101074,
            "sentiment": null,
            "probability": null,
            "content": "From treating cancer to revolutionizing recycling, AI can help solve some of humanity's most pressing and complex problems.\n\nArtificial intelligence and accelerated computing are being used to help solve the world\u2019s greatest challenges.\n\nNVIDIA has reinvented the computing stack \u2014 spanning GPUs, CPUs, DPUs, networking and software. Our platform drives the AI revolution, powering hundreds of millions of devices in every cloud and fueling 75% of the world\u2019s TOP500 supercomputers.\n\nPut in the hands of entrepreneurs and enterprises, developers and scientists, that platform becomes a system for invention, and a force for good across industries and geographies.\n\nHere are five examples of how these technologies are being put to work from the past year:\n\nSupporting Surgeons\n\nIllinois-based startup SimBioSys has created TumorSight Viz, a technology that converts MRI images into 3D models of breast tissue. This helps surgeons better treat breast cancers by providing detailed visualizations of tumors and surrounding tissue.\n\nSaving Lives and Energy\n\nResearchers at the Wellcome Sanger Institute, a key player in the Human Genome Project, analyze tens of thousands of cancer genomes annually, providing insights into cancer formation and treatment effectiveness. NVIDIA accelerated computing and software drastically reduce the institute\u2019s analysis runtime and energy consumption per genome.\n\nCleaning Up Our Waters\n\nClearbot, developed by University of Hong Kong grads, is an AI-driven sea-cleaning boat that autonomously collects trash from the water. Enabled by the NVIDIA Jetson platform, Clearbot is making a splash in Hong Kong and India, helping keep tourist regions clean.\n\nGreening Recycling Plants\n\nGreyparrot, a UK-based startup, has developed the Greyparrot Analyzer, an AI-powered device that offers \u201cwaste intelligence\u201d to recycling plants. Using embedded cameras and machine learning, the analyzer identifies and differentiates materials on conveyor belts, significantly improving recycling efficiency.\n\nDriving Technological Advancement in Africa\n\nA new AI innovation hub has launched in Tunisia, part of NVIDIA\u2019s efforts to train 100,000 developers across Africa. Built in collaboration with the NVIDIA Deep Learning Institute, the hub offers training, technologies and business networks to drive AI adoption across the continent.\n\nAll of these initiatives \u2014 whether equipping surgeons with new tools or making recycling plants greener \u2014 rely on the ingenuity of human beings across the globe, humans increasingly supercharged by AI.\n\nFind more examples of how AI is helping people from across industries and the globe to make a difference and drive positive social impact.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "GPU Stack and AI innovation: Nvidia's strategic vision",
            "link": "https://siliconangle.com/2025/01/02/ai-innovation-enabled-nvidia-success-thecube/",
            "snippet": "Jensen Huang has propelled Nvidia to the forefront of AI innovation, shaping the future of technology with groundbreaking advancements and strategic...",
            "score": 0.8502208590507507,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia's market value gets $2 trillion boost in 2024 on AI rally",
            "link": "https://www.reuters.com/technology/artificial-intelligence/global-markets-marketcap-pix-2025-01-02/",
            "snippet": "Nvidia emerged as the biggest global gainer in market capitalization for 2024, driven by surging interest in artificial intelligence and the robust demand...",
            "score": 0.9302965998649597,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia invested $1bn in AI companies in 2024",
            "link": "https://www.datacenterdynamics.com/en/news/nvidia-invested-1bn-in-ai-companies-in-2024/",
            "snippet": "Nvidia invested in 50 startup funding rounds and corporate deals during 2024, a 15 percent increase on the $872 million the company spent across 39 funding...",
            "score": 0.8513815402984619,
            "sentiment": null,
            "probability": null,
            "content": "According to a report from the FT citing corporate filings and Dealroom research, Nvidia invested in 50 startup funding rounds and corporate deals during 2024, a 15 percent increase on the $872 million the company spent across 39 funding rounds in 2023.\n\nHaving analyzed the data, the FT said that Nvidia had primarily invested in AI companies with large compute infrastructure needs, including organizations that had bought Nvidia chips to power their workloads.\n\nHowever, while Nvidia participated in more funding rounds than Amazon and Microsoft, Google outpaced all three companies during the same 12-month period, participating in approximately 120 VC rounds.\n\nIn late December 2024, Nvidia was announced as an investor in xAI\u2019s $6bn Series C funding round, which also saw participation from AMD, Blackrock, Morgan Stanley, and Sequoia Capital, amongst others. Founded by Elon Musk, the generative AI startup launched its supercomputer last year in Memphis with up to 100,000 Nvidia H100 GPUs. The new funds are expected to primarily be spent on acquiring another 100,000 Nvidia GPUs.\n\nOther companies Nvidia has invested in include OpenAI, Cohere, Mistral, Perplexity, and CoreWeave.\n\nAs the AI bubble continues to inflate, regulators across the globe have raised concerns about the dominance of some companies in the industry.\n\nIn August, the US Department of Justice (DOJ) launched two separate antitrust probes into the GPU giant, evaluating whether the company has abused its market dominance and forced companies to buy additional products to receive GPUs while penalizing those that buy rival chips.\n\nThe DOJ is also reportedly looking into the company's $700 million acquisition of Run:ai in April 2024 and its 2022 purchase of software firm Bright Computing.\n\nMeanwhile, Nvidia\u2019s French offices were raided in 2023 and it\u2019s believed the French Autorit\u00e9 de la concurrence (Competition Authority) is considering raising anti-competition charges against the company. The UK and EU are looking into AI competition risks more broadly.\n\nIn its report, the FT cited former chair of the US Federal Trade Commission, Bill Kovacic, who said it was important for regulators to investigate if a \u201cdominant enterprise making these big investments\u201d would lead to \u201cexclusivity\u201d through the purchase of company stakes.\n\nNvidia rejected the notion that any funding from the company was linked to infrastructure use requirements. In a statement to DCD, a spokesperson for the company said: \" Nvidia is working to grow our ecosystem, support great companies, and enhance our platform for everyone. We compete and win on merit, independent of any investments we make. Every company should be free to make independent technological choices that best suit their needs and strategies.\"\n\nNvidia reached a $3 trillion market cap in June 2024 and twice overtook Apple as the world\u2019s most valuable company. In November 2024, the company posted total revenues of $35.1bn, up 94 percent year-over-year.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia Earns 2024\u2019s Biggest Gain in Market Cap Amid AI Boom",
            "link": "https://www.pymnts.com/artificial-intelligence-2/2025/nvidia-earns-2024s-biggest-gain-in-market-cap-amid-ai-boom/",
            "snippet": "The AI boom drove chipmaker Nvidia to become 2024's biggest global gainer in market capitalization from $1.2 trillion to $3.28 trillion.",
            "score": 0.9234136343002319,
            "sentiment": null,
            "probability": null,
            "content": "The artificial intelligence (AI) boom drove chipmaker Nvidia to become 2024\u2019s biggest global gainer in terms of market capitalization and the world\u2019s second-most valuable listed company.\n\nNvidia had a market value of $1.2 trillion at the end of 2023 and $3.28 trillion at the end of 2024, Reuters reported Thursday (Jan. 2), attributing the surge in value to demand for the company\u2019s AI chips.\n\nApple continued to be the world leader in market value, according to the report. Apple neared a $4 trillion valuation as investors anticipated the company benefiting from its AI enhancements.\n\nRounding out the top five companies in market value in 2024 were Microsoft at $3.1 trillion, Alphabet at about $2.3 trillion, and Amazon, also at about $2.3 trillion, per the report.\n\nThe AI boom has fueled explosive growth in the tech industry, with companies from chip manufacturers to security startups reporting soaring revenues and attracting significant investment, PYMNTS reported in October.\n\nIn December, HSBC Innovation Banking said venture capital in the United States has moved to AI companies at an \u201cunprecedented\u201d rate, with the scale of capital invested in these companies by U.S. venture investors approaching that allocated to the rest of the venture market.\n\n\u201cVenture capital has always gravitated toward transformative industries, but the level of consolidation we\u2019re seeing within one category is unprecedented,\u201d HSBC U.S. Innovation Banking Head Dave Sabow said in a Dec. 16 press release.\n\nAs for Nvidia, the company advanced its grip on the AI market in 2024 with new hardware and tools. Nvidia\u2019s moves during the year included the launch of its Blackwell B100 and B200 GPUs that boosted generative AI capabilities and cemented the company\u2019s lead in high-performance computing; the introduction of the $249 Jetson Orin Nano Super Developer Kit that opened AI development to smaller players; and the formation of a growing number of partnerships across key sectors.\n\nNvidia CEO Jensen Huang said in November that the trends seen among AI services are \u201creally just beginning.\u201d\n\n\u201cWe expect this to happen, this growth, this modernization and the creation of a new industry to go on for several years,\u201d Huang said.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "5": {
            "title": "Nvidia's market value gets $2 trillion boost to more than double in 2024 amid AI rally",
            "link": "https://finance.yahoo.com/news/nvidias-market-value-gets-2-091406933.html",
            "snippet": "(Reuters) - Nvidia (NVDA) emerged as the biggest global gainer in market capitalization for 2024, driven by surging interest in artificial intelligence and...",
            "score": 0.9408747553825378,
            "sentiment": null,
            "probability": null,
            "content": "(Reuters) - Nvidia (NVDA) emerged as the biggest global gainer in market capitalization for 2024, driven by surging interest in artificial intelligence and the robust demand for its AI-centric chips across various industries.\n\nThe chipmaker's market value increased by over $2 trillion last year, reaching $3.28 trillion at the close of 2024, making it the second-most valuable listed company in the world. Its market value was $1.2 trillion at the end of 2023.\n\nMeanwhile, Apple (AAPL) continued to lead global companies in market value, nearing a historic $4 trillion valuation. This surge was fuelled by investor enthusiasm for the company's anticipated AI enhancements, aimed at revitalizing sluggish iPhone sales.\n\nAt the end of 2024, Microsoft (MSFT) ranked third with a market value of $3.1 trillion, followed by Alphabet Inc (GOOG, GOOGL) and Amazon (AMZN), each valued at approximately $2.3 trillion.\n\nThese tech companies significantly boosted their respective global indexes in 2024, with the S&P 500 (^GSPC) index surging 23.3% and the Nasdaq (^IXIC) climbing 28.6%.\n\nVietnam's Prime Minister Pham Minh Chinh (R) and Nvidia CEO and founder Jensen Huang (C) drink beers at an eatery in old quarters of Hanoi on December 5, 2024. (Hoang Tuan / AFP via Getty Images) \u00b7 HOANG TUAN via Getty Images\n\nDespite the shares' higher valuations, looming U.S.-China tariff tensions, and potentially slower U.S. interest rate cuts, analysts remain optimistic about the sustained strong performance by tech firms in 2025.\n\nDaniel Ives of Wedbush predicts a 25% gain in tech stocks in 2025, attributing potential growth to a less regulatory environment under Donald Trump, forthcoming strong AI initiatives, and a stable foundation for Big Tech and Tesla in 2025 and beyond.\n\n\"We believe tech stocks will be robust in 2025 on the shoulders of the AI Revolution and $2 trillion+ of incremental AI cap-ex over the next 3 years,\" he said.\n\n(Reporting By Patturaja Murugaboopathy and Gaurav Dogra in Bengaluru; Editing by Mrigank Dhaniwala)",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "6": {
            "title": "Run.ai software will be made open source in wake of Nvidia acquisition",
            "link": "https://www.itpro.com/infrastructure/run-ai-software-will-be-made-open-source-in-wake-of-nvidia-acquisition",
            "snippet": "At the moment, Run:ai's technology currently only works with Nvidia GPUs \u2014 but the acquisition is set to change that with Run:ai pledging to open source its...",
            "score": 0.934741735458374,
            "sentiment": null,
            "probability": null,
            "content": "Run:ai has confirmed its acquisition by Nvidia has been successful, adding that the chip giant plans to make its AI optimization tools open source so it can work across a wider variety of systems beyond Nvidia's own GPUs.\n\nFounded in 2018, Run:ai develops software to help support AI infrastructure, building a platform on Kubernetes, the orchestration layer for much of the hardware that powers AI and the cloud.\n\nNvidia offered a reported $700 million for the startup in April, but the deal has faced regulatory scrutiny. Weeks after European regulators approved the deal, a blog post by Run:ai's founders has confirmed that the Nvidia acquisition is now complete.\n\n\"AI and accelerated computing are transforming the world at an unprecedented pace, and we believe this is just the beginning,\" said CTO Ronen Dar and CEO Omri Geller, the company's co-founders, in a blog post .\n\n\"GPUs and AI infrastructure will remain at the forefront of driving these transformative innovations and joining Nvidia provides us an extraordinary opportunity to carry forward a joint mission of helping humanity solve the world\u2019s greatest challenges.\n\nRun:ai says its software manages and optimizes the infrastructure that powers AI \u2014 whether in the cloud, on-premise, or a hybrid design \u2014 with the aim of boosting development, making best use of resources, and helping companies innovate in AI.\n\nIt allows AI developers to manage shared compute from a centralized interface, including pooling GPUs and applying them to different tasks, as well as managing users and their access.\n\nGet the ITPro. daily newsletter Sign up today and you will receive a free copy of our Focus Report 2025 - the leading guidance on AI, cybersecurity and other IT challenges as per 700+ senior executives Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\n\"We will continue to help our customers to get the most out of their AI Infrastructure and offer the ecosystem maximum flexibility, efficiency and utilization for GPU systems, wherever they are: On-Prem, in the cloud through native solutions, or on Nvidia DGX Cloud, co-engineered with leading CSPs,\" they added.\n\nOpen source for open AI\n\nAt the time the deal was announced , Nvidia said it would continue to offer Run:ai's products under the same existing business model for the \"immediate future\".\n\nAt the moment, Run:ai's technology currently only works with Nvidia GPUs \u2014 but the acquisition is set to change that with Run:ai pledging to open source its software.\n\n\"While Run:ai currently supports only Nvidia GPUs, open sourcing the software will enable it to extend its availability to the entire AI ecosystem,\" the co-founders added.\n\nRELATED WHITEPAPER (Image credit: Snyk) 7 steps to establish successful security champions program\n\n\"True to our open-platform philosophy, as part of Nvidia, we will keep empowering AI teams with the freedom to choose the tools, platforms, and frameworks that best suit their needs,\" they wrote.\n\n\"We will continue to strengthen our partnerships and work alongside the ecosystem to deliver a wide variety of AI solutions and platform choices.\"\n\nRegulatory challenges\n\nThe deal was first announced in April, with Nvidia offering $700 million for the Israeli startup.\n\nThe European Commission examined the deal, citing antitrust concerns, but approved the acquisition in December on the grounds that Run:ai has \"negligible\" revenue so fails to meet necessary thresholds to take action.\n\nThe EC had been concerned that the deal would further cement Nvidia's dominance in the AI market. At present, Nvidia controls about 80% of the market for AI GPUs.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia Carried the S&P 500 to New Highs in 2024",
            "link": "https://www.statista.com/chart/32015/contributors-to-the-sp500-return/",
            "snippet": "This chart shows the largest positive and negative contributors to the total return of the S&P 500 in 2024.",
            "score": 0.905609667301178,
            "sentiment": null,
            "probability": null,
            "content": "HTML code to embed chart\n\nCan I integrate infographics into my blog or website?\n\nYes, Statista allows the easy integration of many infographics on other websites. Simply copy the HTML code that is shown for the relevant statistic in order to integrate it. Our standard is 660 pixels, but you can customize how the statistic is displayed to suit your site by setting the width and the display size. Please note that the code must be integrated into the HTML code (not only the text) for WordPress pages and other CMS sites.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "8": {
            "title": "Nvidia\u2019s $1B AI Investment Overshadowed by Potential $1B Fine in China",
            "link": "https://www.ccn.com/news/technology/nvidias-1b-ai-investment-overshadowed-by-potential-1b-fine-in-china/",
            "snippet": "Nvidia invested $1 billion into AI companies in 2024, solidifying itself as a key backer of start-ups looking to capitalize on the AI boom.",
            "score": 0.9530882239341736,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "9": {
            "title": "Why Nvidia\u2019s AI Chip Lead Is Sustainable",
            "link": "https://www.investors.com/news/technology/nvidia-stock-nvda-ai-chip-lead-sustainable/",
            "snippet": "Nvidia's lead in the artificial intelligence market is sustainable for years to come, a Wall Street investment bank says. Nvidia stock rose.",
            "score": 0.9229838848114014,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia's (NVDA) lead in the artificial intelligence market is sustainable for years to come, a Wall Street investment bank says. Nvidia stock rose on Thursday.\n\nIn a report Wednesday, Loop Capital Markets analysts Ananda Baruah and John Donovan said Nvidia's \"longevity potential\" is underappreciated.\n\nThat assessment comes as some analysts fret about the competitive threat from custom chips designed by hyperscale cloud service providers working with fabless chipmakers Broadcom (AVGO) and Marvell Technology (MRVL).\n\nMeanwhile, Nvidia executives believe they are sitting on top of a more than $2 trillion revenue opportunity in accelerated computing, generative AI and inference and that \"they can hold a material leadership position in each,\" Loop Capital said.\n\nBaruah and Donovan compared Nvidia's impact on computing to musical group Nirvana, which had a \"sea-change impact\" on the music industry and sustainable success.\n\nThe analysts rate Nvidia stock as buy with a price target of 175. On the stock market today, Nvidia stock rose 3% to close at 138.31.\n\nKey to Nvidia's AI leadership is its expertise in the total technology stack. That includes semiconductors, systems and software, Loop Capital said. That stack along with its installed base of users create a competitive moat, the firm said.\n\nNvidia Moving Into AI Inferencing\n\n\"We believe it's underappreciated the degree to which Nvidia has already created critical 'up the stack' capabilities that could be positioning them for moat-like characteristics moving forward in both training and inference,\" Baruah and Donovan wrote.\n\nThe first wave of the generative AI computing trend was training and the next wave is inference. AI training is the process of teaching an AI model to recognize patterns and make predictions, while AI inference is when the model uses that training to make predictions on new data.\n\n\"Nvidia may already have a lock on inferencing and the game has just started,\" the analysts said. \"Nvidia is already positioned as the 'Gen AI Inference Platform Of Choice' \u2026 and it isn't yet widely recognized.\"\n\nThat position is evident in the 5 million developers in its AI ecosystem, thousands of developed applications, and a presence with every relevant cloud provider and hardware supplier, Baruah and Donovan said.\n\nNvidia Stock Is On Two IBD Lists\n\nElsewhere on Wall Street, BofA Securities analyst Vivek Arya reiterated his buy rating and price target of 190 on Nvidia stock. He called Nvidia a \"top sector pick.\"\n\nIn a client note Wednesday, Arya said he expects Nvidia to have a positive showing at the CES 2025 trade show next week.\n\nNvidia stock is on two IBD lists: Leaderboard and Tech Leaders.\n\nFollow Patrick Seitz on X, formerly Twitter, at @IBD_PSeitz for more stories on consumer technology, software and semiconductor stocks.\n\nYOU MAY ALSO LIKE:\n\nThese 10 Stocks Called 'Tech Winners For The AI Revolution'\n\nNetflix Sports Push Could Lead To Subscription Price Increases\n\nSee Stocks On The List Of Leaders Near A Buy Point\n\nFind Winning Stocks With MarketSurge Pattern Recognition & Custom Screens\n\nJoin IBD Live For Stock Ideas Each Morning Before The Open",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2025-01-01": {
        "0": {
            "title": "Where I Plan To Buy Nvidia Stock Next",
            "link": "https://io-fund.com/artificial-intelligence/semiconductors/where-i-plan-to-buy-nvidia-stock-next",
            "snippet": "Nvidia appears to be setting up for the next swing higher. As long as any further weakness holds over $116, this move should target between $165 - $173, with...",
            "score": 0.9396640062332153,
            "sentiment": null,
            "probability": null,
            "content": "This article was originally published on Forbes on Dec 23, 2024,05:03pm EST\n\nBlackwell is the word for Nvidia as the AI leader heads into 2025, with multiple configurations and a mid-year upgrade (B300/GB300) for its new powerful GPU set to ramp significantly over the next few quarters. As recapped to the I/O Fund\u2019s premium members after its Q3 earnings report, the I/O Fund is tracking multiple supply chain signals indicating Blackwell sales will likely far exceed the GPU sales we saw in 2023 and 2024 combined \u2013 to the tune of bringing Nvidia to $200 billion in data center revenue.\n\nAnalysts are already increasing their forecasts for Blackwell shipments for Q4 and for Q1, with forecasts for 250,000 to 300,000 shipments in Q4 nearly tripling to 750,000 to 800,000 in Q1. This compares to previous views seeing Q4 shipments of 150,000 to 200,000 ramping to 550,000 in Q1. This suggests Blackwell revenue estimates for Q1 are already moving 40-60% higher, potentially driving positive revenue revisions throughout the year as it becomes Nvidia\u2019s primary GPU product.\n\nNvidia has tailwinds in 2025 from increased pricing power with Blackwell, output and shipment estimates already rising before the ramp begins, AI capex still quickly growing, and GPU clusters starting in the 100K range where Hopper maxed out, even as competition from AMD, Broadcom and others begins to increase.\n\nNvidia also has the benefit from the end of its fiscal year early next year, with the Street soon looking to 2026 numbers \u2013 which very well could be too low given the signals Blackwell is already giving. At the moment, Nvidia is trading at just 30x 2026\u2019s estimated earnings of $4.43, its cheapest bottom line valuation since shares were $95 in May 2024 \u2013 and Blackwell still holds the potential to drive quarterly revenue beats the same way Hopper has and with margins returning to Hopper\u2019s highs.\n\nThe bigger picture for Nvidia moving forward is that Blackwell holds the potential to dwarf Hopper, and the I/O plans on keeping its members informed on what it sees ahead for Nvidia with frequent updates for members. With that in mind, here\u2019s what the I/O Fund sees as 2024 ends and 2025 begins.\n\nSign up for I/O Fund's free newsletter with gains of up to 2600% because of Nvidia's epic run - Click here\n\nNvidia Technicals: A Swing Higher In the Cards\n\nNvidia appears to be setting up for the next swing higher. As long as any further weakness holds over $116, this move should target between $165 - $173, with the potential to reach as high as $193.\n\nIf this swing gets confirmed, it would likely be the final 5th wave in the historic uptrend that started in October of 2022. This does not mean that the technicals do not support significantly higher prices, it only means that Nvidia will first have to deal with a notable correction in both price and time before it sees those levels.\n\nThe pattern off the October 2022 low developed as a classic 5 wave pattern. In early 2024 price went vertical. This was accompanied with max volume and peak momentum. This is the standard pattern seen in 3rd waves, and it tends to be the most powerful part of a 5 wave pattern. From the perspective of sentiment, this is the part of the trend where everyone realizes at once the direction of the trend. Shorts cover at the same time as the crowd buys, creating that standard pattern in 3rd waves.\n\nThis would mean that the correction in June of 2024 was the 4th wave, and that this is likely in the final 5th wave higher. The sentiment pattern in 5th waves to new highs in price, but on lower momentum and lower volume, which is what is happening now.\n\nNvidia appears to be setting up for the next swing higher. Source: I/O Fund\n\nZooming into the 4th wave correction that started in June of 2024 offers a better idea of the two potential paths that I am currently tracking.\n\nZooming into the 4th wave correction that started in June of 2024 gives a better idea of the two potential paths that I am currently tracking. Source: I/O Fund\n\nBlue \u2013 The final 5th wave is playing out as an ending diagonal pattern, which is common for 5th waves. This type of pattern is a 5 wave pattern in itself that is characterized with large swings in both directions. Our target zone for the bottom on this 4th wave is $126 - $116. If Nvidia can push over $140.75, then then odds favor this scenario.\n\nRed \u2013 Nvidia is in a much more complex 4th wave. If this is playing out, NVDA would see the $116 level break, which opens the door to a potential low at $101, $90, or $78.\n\nOne final point worth mentioning is how the broad semiconductor sector is performing in relation to the S&P 500. Semiconductors tend to be much more sensitive to the consumer, and economy than most sectors. For this reason, in periods of economic expansion, semiconductors tend to lead, outperforming the broad market.\n\nHowever, when this sector starts to move against the broad market, it tends to be a warning that volatility is ahead. In fact, every time that the semiconductor sector has made a lower high while the broad market made a higher high \u2013 i.e., semiconductors do not confirm the move higher \u2013 this preceded some period of volatility since the 2021 top.\n\nWhen semiconductors start to move against the broad market, it tends to be a warning that volatility is ahead. Source: I/O Fund\n\nThis pattern can be seen going back to 2000 and consistently warned of weakness. As of now, this is one of the largest and longest periods of divergence between the semiconductor sector and the broad market on record.\n\nIf the broader semiconductor sector stays below its July 2024 high, I would consider this a warning. This does not mean that I do not see potential upside, it only means that any long positions the I/O Fund takes will have strict targets at which we take gains and stops to protect us in case the market turns against us.\n\nEvery Thursday at 4:30 pm Eastern, the I/O Fund team holds a webinar for premium members to discuss how to navigate the broad market, as well as various stock entries and exits. We offer trade alerts plus an automated hedging signal. The I/O Fund team is one of the only audited portfolios available to individual investors. Learn more here.\n\nConclusion\n\nMake no mistake, Nvidia is the best stock of the decade and it\u2019s only four years in. The I/O Fund has an aggressive buy plan at key levels should the stock pull back, and we have a backup plan should the stock overcome the peer pressure we are seeing from the semiconductor industry and meaningfully breakout.\n\nNvidia has been our largest position for the last 4 years. The I/O Fund sent out nine buy alerts to our readers to buy this position below $20 in 2021 \u2013 2022. The I/O Fund believes the future is bright for Nvidia, and believe the potential next swing is worth playing. However, with all the warning signs, any new long position will have strict risk controls until these warnings reset.\n\nThe I/O Fund is also closely analyzing the supply chain to identify overlooked beneficiaries of the AI infrastructure buildout, sharing this information as well as potential buy and sell plans and real time trade alerts with premium members. The I/O Fund recently entered two separate beneficiaries for gains of 23% and 17% since November. Learn more here.\n\nI/O Fund Portfolio Manager Knox Ridley and I/O Fund Equity Analyst Damien Robbins contributed to this report.\n\nRecommended Reading:\n\nThis Is Not Broadcom\u2019s \u2018Nvidia Moment\u2019 Yet\n\nSemiconductor Stocks Exposed To China With Tariffs Incoming\n\nShopify Stock Is A Black Friday Beneficiary That Faces Key Test In Q4\n\nNvidia\u2019s Stock Has 70% Potential Upside For 2025",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "ByteDance\u2019s $7B Nvidia Gambit \u2014 Chips Reportedly Stored Offshore to Sidestep US Export Rules",
            "link": "https://www.ccn.com/news/technology/bytedances-nvidia-gambit-chips-offshore-sidestep-us-export-rules/",
            "snippet": "ByteDance to spend $7 billion on Nvidia chips despite US restrictions, circumventing export controls through strategic data center locations.",
            "score": 0.5798967480659485,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "2": {
            "title": "Nvidia, Google and the Hardware Revolution Behind AI\u2019s Rise in 2024",
            "link": "https://www.pymnts.com/artificial-intelligence-2/2025/nvidia-google-and-the-hardware-revolution-behind-ais-rise-in-2024/",
            "snippet": "Nvidia, Google and the Hardware Revolution Behind AI's Rise in 2024 ... Silicon Valley's artificial intelligence (AI) race transformed the tech industry in 2024,...",
            "score": 0.8953856229782104,
            "sentiment": null,
            "probability": null,
            "content": "Silicon Valley\u2019s artificial intelligence (AI) race transformed the tech industry in 2024, driving Apple toward a $4 trillion valuation and rewiring how tech\u2019s biggest players operate.\n\nApple\u2019s stock jumped 16% in late 2024 as investors bet on AI-enhanced iPhones. The company\u2019s plans for smarter Siri features and AI-powered photo editing tools sparked interest in the potential of AI for mobile devices. For Apple, which had moved cautiously on AI, this marked a dramatic shift from its traditionally measured approach to new technology. The surge reflected Wall Street\u2019s growing conviction that AI would drive the next wave of consumer electronics spending.\n\nThe Battle for AI Hardware\n\nMeta pursued a different vision, announcing AI-powered upgrades to its Ray-Ban smart glasses. CEO Mark Zuckerberg bet these wearables could replace smartphones for daily tasks. After missing the smartphone revolution years ago, Meta sees AI-enabled glasses as its shot at defining computing\u2019s next chapter. The move signaled Meta\u2019s largest hardware investment since its Quest virtual reality headsets, with augmented reality features aimed at blending digital information into users\u2019 daily lives.\n\nGoogle parent Alphabet showcased its technical prowess with Gemini 2.0, its most advanced AI model yet. The company pushed beyond software with its Trillium AI chip and Willow quantum processor. These advances sent Alphabet\u2019s stock to record highs as investors recognized its unique position in bridging AI and quantum computing. Unlike rivals focused solely on consumer applications, Google\u2019s breakthroughs promised to reshape everything from cloud computing to autonomous systems.\n\nNvidia advanced its grip on the AI market in 2024 with new hardware and tools reshaping the industry. Its Blackwell B100 and B200 GPUs boosted generative AI capabilities, cementing the company\u2019s lead in high-performance computing. Meanwhile, the $249 Jetson Orin Nano Super Developer Kit opened AI development to smaller players, reflecting a shift toward broader accessibility. These moves, paired with growing partnerships in healthcare and automotive, signaled Nvidia\u2019s expanding influence in how AI is deployed across key sectors.\n\nAI Moves From Lab to Market\n\nThe AI wave reached beyond Silicon Valley. Defense firms Palantir Technologies and Anduril Industries brought AI to government agencies, modernizing military logistics and infrastructure security. These partnerships highlighted AI\u2019s growing role in national strategy as Silicon Valley\u2019s expertise became crucial for solving public sector challenges. The collaboration between tech firms and defense agencies showcased AI\u2019s potential for addressing complex organizational and security challenges.\n\nThe transformation spread through corporate America. Companies rebuilt R&D budgets around AI, with many directing much of their research spending to AI projects. Chipmakers retooled production lines for AI-optimized designs, creating new competitive dynamics in the semiconductor industry.\n\nMarkets rewarded clear AI strategies while punishing companies that were slow to adapt. From healthcare to financial services, AI capabilities have become key determinants of market value. Companies without clear AI integration plans faced mounting pressure to catch up or risk obsolescence. This dynamic played out across industries, forcing traditional businesses to rethink their approach to technology and innovation.\n\nThe year saw AI capabilities expand beyond simple automation to tackle complex tasks. In healthcare, AI systems began assisting with diagnosis and treatment planning. Financial institutions deployed AI for risk assessment and fraud detection. Manufacturing companies use AI to optimize supply chains and predict equipment maintenance needs.\n\nBy year\u2019s end, tech\u2019s landscape had fundamentally shifted. AI wasn\u2019t just another feature or stock catalyst \u2014 it had become the lens through which companies viewed their futures. The question shifted from whether AI would transform business to how companies would harness its potential. With billions invested and entire corporate strategies rebuilt around AI, the technology\u2019s central role in reshaping American business appeared secure.\n\nIn 2025, the industry faces new challenges in converting AI advances into sustainable growth. But one thing remains clear: AI has moved beyond hype to become the foundation of modern technology companies. Those who master it will likely shape the next decade of innovation as the technology continues evolving from experimental features to core business drivers.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "Nvidia Stock Could Spell Trouble for Some Semiconductor Stocks After Its Jan. 6 Keynote",
            "link": "https://www.fool.com/investing/2025/01/02/nvidia-stock-could-spell-trouble-for-some-semicond/",
            "snippet": "Nvidia ... Could Nvidia release a consumer CPU this year? In today's video, I discuss Nvidia (NVDA 2.63%) and a few semiconductor stocks that could be impacted...",
            "score": 0.9670577645301819,
            "sentiment": null,
            "probability": null,
            "content": "Could Nvidia release a consumer CPU this year?\n\nIn today's video, I discuss Nvidia (NVDA 5.27%) and a few semiconductor stocks that could be impacted after its CES 2025 keynote. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the after-market prices of Dec. 31, 2024. The video was published on Jan. 1, 2025.",
            "description": null,
            "finbert_sentiment": "negative"
        },
        "4": {
            "title": "Monitor These Nvidia Stock Price Levels After Two Years of Massive Gains",
            "link": "https://www.investopedia.com/monitor-these-nvidia-stock-price-levels-after-two-years-of-massive-gains-8768049",
            "snippet": "Below, we take a closer look at Nvidia's chart and use technical analysis to identify key price levels to watch out for in early January.",
            "score": 0.883568286895752,
            "sentiment": null,
            "probability": null,
            "content": "Key Takeaways Shares in chipmaker Nvidia will be in the spotlight ahead of a presentation by CEO Jensen Huang scheduled for next Monday at the CES consumer electronics trade show in Las Vegas.\n\nNvidia shares gained 170% in 2024, after surging 240% the year before, amid booming demand for its AI chips.\n\nSince hitting a record high in late November, Nvidia shares have traded within a descending channel, with the price tagging the pattern\u2019s upper and lower trendlines on several occasions since that time.\n\nInvestors should watch key support levels on Nvidia's chart around $130 and $115, while also monitoring important resistance levels near $140 and $150.\n\nShares in artificial intelligence (AI) chipmaker Nvidia (NVDA) will be in the spotlight ahead of a presentation by CEO Jensen Huang scheduled for next Monday at the CES consumer electronics trade show in Las Vegas.\n\nInvestors will be watching for updates from Huang on sales projections for the company\u2019s Blackwell chips and details about Rubin, Blackwell's successor, which Nvidia plans to release in 2026. Several leading Wall Street firms have named the AI behemoth as their 2025 \u201ctop pick,\u201d pointing out that strong demand for its Blackwell platform positions the company for another year of explosive growth.\n\nNvidia shares closed out 2024 with a gain of 170% amid surging demand for its silicon as big tech customers\u2014including Microsoft (MSFT), Meta (META) and Alphabet\u2019s (GOOGL) Google\u2014beefed up their AI datacenter and cloud computing infrastructure. Last year's big gain came on the heels of a 240% increase in 2023.\n\nBelow, we take a closer look at Nvidia\u2019s chart and use technical analysis to identify key price levels to watch out for in early January.\n\nDescending Channel Takes Shape\n\nAfter setting their record high in late November, Nvidia shares have traded within a descending channel, with the price tagging the pattern\u2019s upper and lower trendlines on several occasions since that time.\n\nMore recently, the stock ran into selling pressure near the channel\u2019s top trendline and 50-day moving average (MA), though the move occurred on light end-of-year share turnover.\n\nThe relative strength index (RSI) signals slightly bearish price momentum in the stock to kick off 2025 with the indicator falling below 50.\n\nLet\u2019s look at key support and resistance levels on Nvidia\u2019s chart that investors may be eyeing as the first quarter gets underway.\n\nKey Support Levels to Watch\n\nThe first level to watch sits around $130, a location on the chart where the shares may encounter support near a trendline that connects the prominent August swing high with the December swing low.\n\nA decisive close below this important technical level could see the shares break down beneath the descending channel\u2019s lower trendline and revisit lower support around $115. This location, currently just below the rising 200-day MA, would likely attract buying interest near a horizontal line that links a range of comparable price points between May and October last year.\n\nImportant Resistance Levels to Monitor\n\nUpon a move higher from current levels, investors should initially monitor the $140 area. The shares may run into resistance in this region near the descending channel's upper trendline, which also closely aligns with the stock\u2019s June 2024 peak.\n\nBuying above this level could see the shares rally up to around $150. Investors who have bought the recent retracement may seek to lock in profits in this area near a series of price action situated just below the stock\u2019s record high.\n\nThe comments, opinions, and analyses expressed on Investopedia are for informational purposes only. Read our warranty and liability disclaimer for more info.\n\nAs of the date this article was written, the author does not own any of the above securities.\n\n\n\nCORRECTION\u2014Jan. 7, 2025: This article has been updated to reflect the official name of the Consumer Technology Association\u2019s annual consumer electronics trade show is CES.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia Stock Should Win Again in 2025. Here\u2019s Why.",
            "link": "https://www.barrons.com/articles/nvidia-stock-price-2025-chips-outlook-8369e596",
            "snippet": "Wall Street analysts expect Nvidia to grow revenue by 112%, to $129 billion, for its fiscal year ending in January 2025.",
            "score": 0.7453896999359131,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Best Graphics Cards for Gaming in 2025",
            "link": "https://www.tomshardware.com/reviews/best-gpus,4380.html",
            "snippet": "We've provided options for every budget and mindset below. Whether you're after the fastest graphics card, the best value, or the best card at a given price,...",
            "score": 0.6707175374031067,
            "sentiment": null,
            "probability": null,
            "content": "The best graphics cards are the beating heart of any gaming PC, and everything else comes second. Without a powerful GPU pushing pixels, even the fastest of the best CPUs for gaming won't manage much. No one graphics card will be right for everyone, so we've provided options for every budget and mindset below. Whether you're after the fastest graphics card, the best value, or the best card at a given price, we've got you covered.\n\n\n\nWhere our GPU benchmarks hierarchy ranks all of the cards based purely on performance, our list of the best graphics cards looks at the whole package. Current GPU pricing, performance, features, efficiency, and availability are all important, though the weighting becomes more subjective. Factoring in all of those aspects, these are the best graphics cards that are currently available.\n\nWe haven't had a 'new' GPU launch since February's RX 7900 GRE, joining the four GPUs that came out in January: RTX 4080 Super, RTX 4070 Ti Super, RTX 4070 Super, and RX 7600 XT. All of these are in our performance charts, and some make our overall picks, replacing former entries that are now officially discontinued (even if retail inventory remains).\n\nWe anticipate the Nvidia Blackwell and RTX 50-series, AMD RDNA 4, and Intel Battlemage GPUs will arrive in the next couple of months. Most likely all will see an early 2025 release, though there's still a (slim) chance that a few new GPUs could slip out before the end of 2024.\n\n\n\nPrices are mostly sitting at MSRP and below for the least expensive offering on each GPU. That applies to everything except the highest tier Nvidia GPU, which has been trending upward lately: The RTX 4090 now starts at $1,900, which is $300 more than the original launch price. Unless you really need such a GPU, you should just wait for the RTX 5090 at this point. Yes, it will probably cost $1,999 or more, but then if you're already looking at $1,900, what's another $100 for a potential 50% boost in performance?\n\n\n\nThat last bit of advice applies to all the high-end cards: If you haven't already upgraded, waiting until the next-gen GPUs arrive makes a lot of sense.\n\nBest graphics cards for gaming, at a glance\n\nNote: We're showing current online prices alongside the official launch MSRPs in the above table, with the GPUs sorted by performance. Retail prices can fluctuate quite a bit over the course of a month; the table lists the best we could find at the time of writing. Also note that the \"average power draw\" column is the average power across all of our testing for each GPU \u2014 so the 4090 in particular uses far less power when it's CPU limited at 1080p, and that skews the overall average down.\n\n\n\nThe above list shows all the current generation graphics cards, grouped by vendor and sorted by reverse product name \u2014 so faster and more expensive cards are at the top of the chart. There are 22 current generation parts, and most of them are viable choices for the right situation. Previous generation cards are mostly not worth considering these days, as they're either discontinued or in the process of being phased out. If you want to see how the latest GPUs stack up in comparison to older parts, check our GPU benchmarks hierarchy.\n\n\n\nThe performance ranking incorporates 19 games from our latest test suite, with both rasterization and ray tracing performance included. Note that we are not including upscaling results in the table, which would generally skew things more in favor of Nvidia GPUs, depending on the selection of games.\n\n\n\nWhile performance can be an important criteria for a lot of gamers, it's not the only metric that matters. Our subjective rankings below factor in price, power, and features colored by our own opinions. Others may offer a slightly different take, but all of the cards on this list are worthy of your consideration.\n\n1. Nvidia RTX 4070 Super\n\nWhy you can trust Tom's Hardware Our expert reviewers spend hours testing and comparing products and services so you can choose the best for you. Find out more about how we test.\n\n(Image credit: Tom's Hardware)\n\n1. Nvidia GeForce RTX 4070 Super The best overall GPU right now Our expert review: Specifications GPU: AD104 GPU Cores: 7168 Boost Clock: 2,475 MHz Video RAM: 12GB GDDR6X 21 Gbps TGP: 200 watts Today's Best Deals View at Amazon View at Amazon Check Walmart Reasons to buy + Good overall performance + Excellent efficiency + DLSS, AI, AV1, and ray tracing + Improved value over non-Super 4070 Reasons to avoid - 12GB is the minimum for a $400+ GPU - Generational price hike - Frame Generation marketing - Ugly 16-pin adapters\n\nNvidia refreshed its 40-series lineup at the start of 2024 with the new Super models. Of the three, the RTX 4070 Super will likely interest be of interest to the most people. It inherits the same $599 MSRP as the non-Super 4070 (which has dropped to $549 to keep it relevant), with all the latest features of the Nvidia Ada Lovelace architecture. It's slightly better than a linear boost in performance relative to price, which is as good as you can hope for these days.\n\n\n\nThere appear to be plenty of RTX 4070 Super base-MSRP models available at retail. We like the stealthy black aesthetic of the Founders Edition, and it runs reasonably cool and quiet, but third-party cards with superior cooling are also available \u2014 sometimes at lower prices than the reference card.\n\n\n\nThe 4070 Super bumps core counts by over 20% compared to the vanilla 4070, and in our testing we've found that the general lack of changes to the memory subsystem doesn't impact performance as much as you might expect. It's still 16% faster overall (at 1440p), even with the same VRAM capacity and bandwidth \u2014 though helped by the 33% increase in L2 cache size.\n\n\n\nCompared to the previous generation Ampere GPUs, even with less raw bandwidth, the 4070 Super generally matches or beats the RTX 3080 Ti, and delivers clearly superior performance than the RTX 3080. What's truly impressive is that it can do all that while cutting power use by over 100W.\n\n\n\nFurther Reading:\n\nNvidia GeForce RTX 4070 Super review\n\n2. AMD RX 7900 GRE\n\nAMD Radeon RX 7900 GRE \u2014 Sapphire Pulse (Image credit: Tom's Hardware)\n\n2. AMD Radeon RX 7900 GRE AMD's best overall GPU right now Our expert review: Specifications GPU: RDNA 3 Navi 31 GPU Cores: 5120 Boost Clock: 2,245 MHz Video RAM: 16GB GDDR6 18 Gbps TBP: 260 watts Today's Best Deals Check Amazon Check Walmart Reasons to buy + Plenty of VRAM and 256-bit interface + Great for 1440p and 1080p + Strong in rasterization testing + AV1 and DP2.1 support Reasons to avoid - Still lacking in DXR and AI performance - Not as efficient as Nvidia\n\nThe Radeon RX 7900 GRE displaces the RX 7800 XT as our top AMD pick. It uses the AMD RDNA 3 architecture, but instead of the Navi 32 GPU in the 7800 XT, it sticks with the larger Navi 31 GCD and offers 33% more compute units. AMD balances that by reducing the GPU and GDDR6 clocks, though it's possible overclocking can return some of those losses \u2014 now that AMD has fixed the 7900 GRE overclocking 'bug'. But the 7800 XT remains a close second, and that remains a decent alternative.\n\n\n\nBoth the 7900 GRE and 7800 XT offer a good blend of performance and price, but the 7900 GRE ends up about 10% faster for 10% more money. Linear performance scaling on a high-end GPU makes the more expensive card the best overall pick of Team Red's current lineup. That's assuming you're like us and tend to want a great 1440p gaming experience while spending as little as possible. There are certainly faster GPUs, but the 7900 XT costs 27% more while delivering about 20% more performance, so diminishing returns kick in beyond this point.\n\n\n\nEfficiency has been a bit hit or miss with RDNA 3, but the 7900 GRE ends up as AMD's most efficient GPU right now, thanks at least in part to the reduced clocks. More processing clusters running at lower clocks is a good way to improve efficiency. It's also faster than the previous generation 6950 XT while using over 60W less power. In addition, you get AV1 encoding support and DP2.1 video output, plus improved compute and AI capabilities \u2014 it's over three times the image throughput of the 6950 XT in Stable Diffusion, for example.\n\n\n\nAMD continues to offer more bang of the buck in rasterization games than Nvidia, though it also falls behind in ray tracing \u2014 sometimes far behind in games like Alan Wake 2 that support full path tracing. Is that something you want to try? It doesn't radically change the gameplay, though it can look better overall. If you're curious about fully path traced games, including future RTX Remix mods, Nvidia remains the best option. Otherwise, AMD's rasterization performance on the 7900 GRE ends up tying the 4070 Super at 1440p, for $50 less money.\n\n\n\nWe're also at the point where buying a new GPU that costs over $500 means we really want to see 16GB of memory. Yes, Nvidia fails in that regard, though in practice it seems as though Nvidia roughly matches AMD's 16GB cards with 12GB offerings. There are exceptions of course, but the RX 7900 GRE still checks all the right boxes for a high-end graphics card that's still within reach of most gamers.\n\n\n\nFurther reading:\n\nAMD Radeon RX 7900 GRE review\n\nAMD Radeon RX 7800 XT review\n\n\n\n3. Nvidia RTX 4070\n\n(Image credit: Tom's Hardware)\n\nNvidia's RTX 4070 didn't blow us away with extreme performance or value, but it's generally equal to the previous generation RTX 3080, comes with the latest Ada Lovelace architecture and features, and now costs about $150 less. With the launch of the RTX 4070 Super (see above), it also got a further price cut and the lowest cost cards start at around $530.\n\n\n\nWe recently looked at the RTX 4070 versus RX 7900 GRE, and ultimately gave the nod to the Nvidia card. That's based more on the Nvidia ecosystem, including technologies like DLSS and AI features, as well as the very efficient architecture. If you're only interested in performance, the RX 7900 GRE makes a compelling case.\n\n\n\nNvidia rarely goes after the true value market segment, but with the price adjustments brought about with the recent 40-series Super cards, things are at least reasonable. The RTX 4070 can still deliver on the promise of ray tracing and DLSS upscaling, it only uses 200W of power (often less), and in raw performance it outpaces AMD's RX 7800 XT \u2014 slightly slower in rasterization, faster in ray tracing, plus it has DLSS support.\n\n\n\nNvidia is always keen to point out how much faster the RTX 40-series is, once you enable DLSS 3 Frame Generation. As we've said elsewhere, these generated frames aren't the same as \"real\" frames and increase input latency. It's not that DLSS 3 is bad, but we prefer to compare non-enhanced performance, and in terms of feel we'd say DLSS 3 improves the experience over the baseline by perhaps 10\u201320 percent, not the 50\u2013100 percent you'll see in Nvidia's performance charts.\n\n\n\nThe choice between the RTX 4070 and the above RTX 4070 Super really comes down whether you're willing to pay a bit more for proportionately higher performance, or if you'd rather save money. The two offer nearly the same value proposition otherwise, with the 4070 Super costing 13% more while offering 16% higher performance on average.\n\n\n\nFurther Reading:\n\nNvidia GeForce RTX 4070 review\n\n4. Nvidia RTX 4090\n\n(Image credit: Tom's Hardware)\n\n4. Nvidia GeForce RTX 4090 The fastest GPU \u2014 great for creators, AI, and professionals Our expert review: Average Amazon review: \u2606 \u2606 \u2606 \u2606 \u2606 Specifications GPU: Ada AD102 GPU Cores: 16384 Boost Clock: 2,520 MHz Video RAM: 24GB GDDR6X 21 Gbps TGP: 450 watts Today's Best Deals View at Amazon View at Amazon Check Walmart Reasons to buy + The fastest GPU, period + Excellent 4K and maybe even 8K gaming + Powerful ray tracing hardware + DLSS and DLSS 3 + 24GB is great for content creation workloads Reasons to avoid - Extreme price and power requirements - Needs a fast CPU and large PSU - Frame Generation is a bit gimmicky\n\nFor some, the best graphics card is the fastest card, pricing be damned. Nvidia's GeForce RTX 4090 caters to precisely this category of user. It was also the debut of Nvidia's Ada Lovelace architecture and represents the most potent card Nvidia has to offer, likely until later this year when the next generation Blackwell GPUs are set to arrive.\n\n\n\nNote also that pricing of the RTX 4090 has become quite extreme, with many cards now selling above $2,000. That's due to a combination of factors, including China RTX 4090 export restrictions and the rise of AI and deep learning \u2014 we've heard quite a few AI companies are leveraging RTX 4090 cards rather than paying three times as much for the often-slower RTX 6000 Ada Generation. If you don't already have a 4090, you're probably best off giving it a pass now.\n\n\n\nThe RTX 4090 creates a larger gap between itself and the next closest Nvidia GPU. Across our suite of gaming benchmarks, it's 35% faster overall than the RTX 4080 at 4K, and 32% faster than the RTX 4080 Super. It's also 47% faster than AMD's top performing RX 7900 XTX \u2014 though it also costs nearly twice as much online right now.\n\n\n\nLet's be clear about something: You really need a high refresh rate 4K monitor to get the most out of the RTX 4090. At 1440p its advantage over a 4080 Super shrinks to 22%, and it's only 13% at 1080p \u2014 and that includes demanding DXR games. The lead over the RX 7900 XTX also falls to only 27% at 1080p. Not only do you need a high resolution, high refresh rate monitor, but you'll also want the fastest CPU possible to get the most out of the 4090.\n\n\n\nIt's not just gaming performance, though. In professional content creation workloads like Blender, Octane, and V-Ray, the RTX 4090 is up to 42% faster than the RTX 4080 Super. And with Blender, it's over three times faster than the RX 7900 XTX. Don't even get us started on artificial intelligence tasks. In Stable Diffusion testing, the RTX 4090 is also around triple the performance of the 7900 XTX for 512x512 and 768x768 images.\n\n\n\nThere are numerous other AI workloads that currently only run on Nvidia GPUs. In short, Nvidia knows a thing or two about content creation applications. The only potential problem is that it uses drivers to lock improved performance in some apps (like some of those in SPECviewperf) to its true professional cards, i.e. the RTX 6000 Ada Generation.\n\n\n\nAMD's RDNA 3 response to Ada Lovelace might be a better value, at least if you're only looking at rasterization games, but for raw performance the RTX 4090 reigns as the current champion. Just keep in mind that you may also need a CPU and power supply upgrade to get the most out of the 4090.\n\n\n\nRead: Nvidia GeForce RTX 4090 review\n\n5. AMD RX 7900 XTX\n\nAMD Radeon RX 7900 XTX (Image credit: Tom's Hardware)\n\nAMD's Radeon RX 7900 XTX ranks as the fastest current graphics card from AMD, and lands near the top of the charts \u2014 with a generational price bump to match. Officially priced at $999, the least expensive models now start at around $900, and supply has basically caught up to demand. There's good reason for the demand, as the 7900 XTX comes packing AMD's latest RDNA 3 architecture.\n\n\n\nThat gives the 7900 XTX a lot more potential compute, and you get 33% more memory and bandwidth as well versus the prior generation. Compared to the RX 6950 XT, on average the new GPU is 44% faster at 4K, though that shrinks to 34% at 1440p and just 27% at 1080p. It also delivers that performance boost without dramatically increasing power use or graphics card size.\n\n\n\nAMD remains a potent solution for anyone that doesn't care as much about ray tracing \u2014 and when you see the massive hit to performance for often relatively mild gains in image fidelity, we can understand why many feel that way. Still, the number of games with RT support continues to grow, and most of those also support Nvidia's DLSS technology, something AMD hasn't fully countered even if FSR2/FSR3 can at times come close. If you want the best DXR/RT experience right now, Nvidia still wins hands down.\n\n\n\nAMD's GPUs can also be used for professional and content creation tasks, but here things get a bit hit and miss. Certain apps in the SPECviewperf suite run great on AMD hardware, others come up short. However, if you want to do AI or deep learning research, there's no question Nvidia's cards are a far better pick. For this generation, the RX 7900 XTX is AMD's fastest option, and it definitely packs a punch. If you're willing to step down to the 7900 XT, that's also worth considering (see below), as it tends to be priced quite a bit lower.\n\n\n\nFurther Reading:\n\nAMD Radeon RX 7900 XTX review\n\n6. Nvidia RTX 4060\n\nAsus GeForce RTX 4060 Dual (Image credit: Tom's Hardware)\n\n6. Nvidia GeForce RTX 4060 The best budget Nvidia GPU Our expert review: Specifications GPU: Ada AD107 GPU Cores: 3072 Boost Clock: 2,460 MHz Video RAM: 8GB GDDR6 17 Gbps TGP: 115 watts Today's Best Deals View at Best Buy View at Amazon View at Walmart Reasons to buy + Great 1080p performance + Very efficient and quiet + Faster and cheaper than RTX 3060 Reasons to avoid - Only 8GB of VRAM - 128-bit memory interface - It seems more like an RTX 4050\n\nWith the launch of the RTX 4060, Nvidia appears to have gone as low as it plans for this generation of desktop graphics cards based on the Ada Lovelace architecture. We're not ruling out an eventual desktop RTX 4050, but we're also skeptical such a card would warrant consideration, considering how pared down the 4060 is already.\n\n\n\nThere are certainly drawbacks with this level of GPU. Nvidia opted to cut down the memory interface to just 128 bits, which in turn limits the memory capacity options. Nvidia could do a 16GB card if it really wanted, but 8GB is the standard configuration and we don't expect anything else \u2014 outside of its professional GPUs, only the 4060 Ti 16GB has the doubled VRAM option, and we weren't particularly impressed by that card. The 4060-class cards also have an x8 PCIe interface, which shouldn't matter too much, though it might reduce performance if you're on an older platform that only supports PCIe 3.0.\n\n\n\nNvidia also cut down the number of GPU cores on the RTX 4060 compared to its RTX 3060 ancestor. The 3060 had 28 SMs (Streaming Multiprocessors, with 128 CUDA cores each) while the 4060 only has 24 SMs. Higher clocks make up for the deficit, but the RTX 4070 and above all have at least the same number of SMs as their predecessors.\n\n\n\nThe good news is that, as promised, performance is at least faster than the previous generation RTX 3060, by about 20% at 1080p and 1440p. There are edge cases in some games (meaning, 4K at max settings) where the 12GB on the 3060 can pull ahead, but performance is already well below the acceptable level at that point. As an example, Borderlands 3 ran at 26.5 fps on the 4060 versus 28.9 fps on the 3060 at 4K Badass settings; neither is a great experience, even though the 3060 is technically faster.\n\n\n\nThere are other benefits with the 4060 as well. You get all the latest Ada features, including DLSS 3 framegen support (yawn). Also, the power draw is just 115W for the reference model, and typically won't exceed 125W on overclocked cards (like the Asus Dual OC that we used for testing). Most RTX 4060 cards also don't bother with the questionable 16-pin power connector and adapter shenanigans.\n\n\n\nAMD's closest alternative is the previous generation RX 6700 XT. You get the usual results: higher rasterization performance from AMD, worse ray tracing performance, and higher power requirements \u2014 about 100W more in this case. The new RX 7600 XT ostensibly competes with the 4060 as well, but it also costs more and we still think the 6700 XT (or 6750 XT) are a better pick than the new 7600 XT.\n\n\n\nAn alternative view is that this is an upgraded RTX 3050, with the same 115W TGP and 60% better performance. Too bad it costs $50 extra, though the 3050 was mostly priced at $300 and above until later in its life cycle.\n\n\n\nRead: Nvidia GeForce RTX 4060 review\n\n7. AMD RX 7900 XT\n\nAMD Radeon RX 7900 XT reference card (Image credit: Tom's Hardware)\n\nWith prices now heading up on many previous generation cards, AMD's RX 7900 XT has become more attractive with time. It generally beats the RTX 4070 Ti in rasterization performance but trails by quite a bit in ray tracing games \u2014 with both cards now starting at around $699. That brings some good competition from AMD, with all the RDNA 3 architectural updates.\n\n\n\nAMD also doesn't skimp on VRAM, providing you with 20GB. That's 67% more than the competing 4070 Ti. However, you won't get DLSS support, and FSR2 works on Nvidia as well as AMD, so it's not really an advantage (plus DLSS still looks better). Some refuse to use upscaling of any form, however, so the importance of DLSS and FSR2 can be debated.\n\n\n\nSomething else to consider is that while it's possible to run AI workloads on AMD's GPUs, performance can be substantially slower. That's because the \"AI Accelerators\" in RDNA 3 share the same execution pipelines as the GPU shaders, and FP16 or INT8 throughput is only double the FP32 rate. That's enough for AI inference, but it only matches a modest GPU like the RTX 3060 in pure AI number crunching. Most AI projects are also heavily invested in Nvidia's ecosystem, which makes them easier to get running.\n\n\n\nAMD made a lot of noise about its new innovative GPU chiplet architecture, and it could certainly prove to be a game changer... in future iterations. For now, GPU chiplets are more about saving cost than improving performance. Consider that the die sizes of AD104 and Navi 31's GPC are similar, but AMD also has to add five MCDs and you can see why it was supposed to be the more expensive card. And yet, performance still slightly favors Nvidia's 4070 Ti overall \u2014 and that's before accounting for DLSS and DLSS 3.\n\n\n\nWhile the RX 7900 XT is good and offers a clear performance upgrade over the prior generation, you may also want to consider the above RX 7800 XT. It costs $200 less (a 29% decrease) and performance is only about 20% lower. Unfortunately, overall generational pricing has increased, with new product names to obfuscate that fact. If the 7900 XTX replaced the 6900 XT, then logically this represents a 6800 XT replacement, but that's not the way AMD decided to name things.\n\n\n\nRead: AMD Radeon RX 7900 XT review\n\n8. Intel Arc A750\n\nIntel Arc A750 Limited Edition, now officially discontinued (Image credit: Tom's Hardware)\n\nTesting the Intel Arc A750 was a bit like dealing with Dr. Jekyll and Mr. Hyde. At times, performance looked excellent, sometimes surpassing the GeForce RTX 3060. Other times, Arc came up far short of expectations, trailing the RTX 3050. The drivers continued to improve, however, and with prices now starting at around $200, this represents an excellent value \u2014 just note that some of the cost savings will ultimately show up in your electrical bill, as it's not as efficient as the competition.\n\n\n\nThere are some compromises, like the 8GB of VRAM \u2014 the A770 doubles that to 16GB, but also costs around $100 extra. Intel's A750 also has to go up against AMD's RX 7600, which is the primary competition at this price (even if AMD's prices have recently increased). Depending on the game, performance may end up favoring one or the other, though Intel now holds the overall edge by a scant 2% at 1080p ultra. Like Nvidia GPUs, ray tracing games tend to favor Intel, while rasterization games are more in the AMD camp.\n\n\n\nIntel was the first company to deliver hardware accelerated AV1 encoding and decoding support, and QuickSync continues to deliver an excellent blend of encoding performance and quality. There's also XeSS, basically a direct competitor to Nvidia's DLSS, except it uses Arc's Matrix cores when present, and can even fall back into DP4a mode for non-Arc GPUs. But DLSS 2 still comes out on top, and it's in far more games.\n\n\n\nThe Arc A750 isn't a knockout blow, by any stretch, but it's also nice to have a third player in the GPU arena. The A750 competes with the RTX 3060 and leaves us looking forward to Intel's future Arc Battlemage GPUs, which could conceivably arrive later this year. You can also check out the Arc A770 16GB, if you're willing to give Intel a chance, though it's a steep upsell these days.\n\n\n\nFurther Reading:\n\nIntel Arc A750 Limited Edition review\n\nIntel Arc A770 Limited Edition review\n\n9. Nvidia RTX 4080 Super\n\nNvidia GeForce RTX 4080 Super Founders Edition (Image credit: Tom's Hardware)\n\nThe RTX 4080 Super at least partially addresses one of the biggest problems with the original RTX 4080: the price. The 4080 cost $1,199 at launch, a 72% increase in generational pricing compared to the RTX 3080. Now, the 4080 Super refresh drops the price to $999, which represents a 17% drop in generational pricing compared to the (overpriced) RTX 3080 Ti. Of course, the 3080 Ti felt overpriced, but it nearly matched the 3090 in performance, just with half the VRAM. The 4080 Super still trails the 4090 by a sizeable margin of 24% at 4K.\n\n\n\nWith the current price premiums on the RTX 4090, likely caused by AI companies buying up consumer GPUs, we've also seen increased pricing on the RTX 4080 Super \u2014 many cards cost about $200 over MSRP, though at least a couple of MSRP-priced models are available. If you're interested, check out the PNY RTX 4080 Super and Zotac 4080 Super at $999. That's $700 less than the cheapest 4090, if you're keeping track.\n\n\n\nOn paper at least, the 4080 Super is certainly better than the now discontinued vanilla 4080, even if the upgrades are relatively minor. The 4080 Super increases core counts by 5%, GPU clocks by 2%, and VRAM clocks by 3%, which works out to a net 3% increase in performance. Faster and less expensive would be moving in the right direction, if cards were available at MSRP.\n\n\n\nNvidia also goes directly after AMD's top GPU with the 4080 Super, and with the price reduction it becomes a far more compelling choice. Overall, the 4080 Super leads the 7900 XTX by 14\u201316 percent at 4K and 1440p, and even if we only look at rasterization performance, it's basically tied or slightly faster than AMD's best. Paying $60 extra for Nvidia's faster and more efficient card, plus getting the other Nvidia extras, should be a relatively easy decision unless you absolutely refuse to consider buying an Nvidia GPU.\n\n\n\nIt's not all smooth sailing for the 4080 Super, given the current retail availability and prices. Our advice is to only pick up a card if it costs closer to $1,000. Otherwise, there are better options. You could also just wait for the inevitable RTX 5080 replacement that will likely land in October/November, though predicting pricing and performance of such future GPUs surpasses the capabilities of my crystal ball.\n\n\n\nFurther Reading:\n\nNvidia RTX 4080 Super review\n\nNvidia RTX 4080 review\n\n10. AMD RX 7600\n\n(Image credit: Tom's Hardware)\n\nThe Radeon RX 7600 and its Navi 33 GPU have replaced the previous generation Navi 23 parts, mostly as a sideways move with a few extra features. Performance ends up just slightly faster than the RX 6650 XT, while using about 20W less power, for about $20 extra. The extras include superior AI performance (if that matters to you in a budget GPU), AV1 encoding and decoding hardware, and DisplayPort 2.1 UHBR13.5 outputs.\n\n\n\nPrices dropped about $20 when the RX 7600 first appeared, but have crept back up to $259 at present. The previous generation RX 6600-class GPUs have generally kept pace on pricing changes. Specs are similar as well, with the same 8GB GDDR6 and a 128-bit memory interface. Both the 6650 XT and RX 7600 also use pretty much the same 18 Gbps memory (the 6650 XT is clocked at 17.5 Gbps), so bandwidth isn't a big factor.\n\n\n\nAcross our test suite of 15 games, the RX 7600 outperformed the RX 6650 XT by 6% overall at 1080p, and just 3% at 1440p ultra. There really is almost no discernable difference in gaming performance. The RX 7600 does substantially better in AI workloads like Stable Diffusion, however \u2014 not as fast as Nvidia's 4060, but far more competitive than the RX 6650 XT when using the latest projects.\n\n\n\nPrice-wise, there's not really a direct Nvidia competitor. The RTX 4060 costs $30\u2013$50 more, and while the RTX 3050 8GB costs about $40 less, it's also a far less capable GPU. AMD gets an easy win over the anemic 3050, delivering 40% more performance, but we never really liked that card \u2014 and would strongly recommend avoiding the RTX 3050 6GB model. The RTX 4060 meanwhile offers roughly 25% more performance for 12% more money.\n\n\n\nBuy the RX 7600 if you're mostly looking for acceptable performance and efficiency at the lowest price possible, and you don't want to deal with Intel's drivers and don't care about ray tracing. Otherwise, give the Arc A750 and RTX 4060 a thought. The RX 6600 also warrants a look, depending on how low you want to go on pricing \u2014 that card sells for under $200 now, though performance ends up being quite a bit lower than the RX 7600 (22% slower across our test suite).\n\n\n\nThere's also the new RX 7600 XT, which doubles the VRAM to 16GB and gives a minor boost to GPU clocks, with an increase in power use as well. It's about 10% faster than the vanilla 7600 overall, but costs 27% more \u2014 rather like the RTX 4060 Ti 16GB in that regard.\n\n\n\nFurther reading:\n\nAMD Radeon RX 7600 review\n\nAMD Radeon RX 7600 XT review\n\n11. AMD RX 7700 XT\n\nSapphire Radeon RX 7700 XT Pure (Image credit: Tom's Hardware)\n\nAfter the successful RX 6700 XT and RX 6750 XT, we had high hopes for the RX 7700 XT. It only partially delivers, though a recent official price cut definitely helps \u2014 and retail prices can lop another $20\u2013$50 off the cost.\n\n\n\nAMD's latest generation midrange/high-end offering follows the usual path, trimming specs compared to the top models with a smaller Navi 32 GPU that can sell at lower prices. Both the RX 7800 XT (discussed above) and 7700 XT use the same Navi 32 GCD (Graphics Compute Die), but the 7800 XT has four MCDs (Memory Cache Dies) while the 7700 XT cuts that number down to three, as well as having fewer CUs (Compute Units) enabled on the GCD.\n\n\n\nThe result of the cuts is that the RX 7700 XT ends up around 14% slower than the 7800 XT, and currently costs 14% less as well \u2014 linear scaling of price and performance, in other words. We like the 7800 XT and 7900 GRE more, which is why it's higher up on this list, but the 7700 XT still fills a niche. Basically, any of these three AMD GPUs warrants consideration, depending on your budget.\n\n\n\nThe closest competition from Nvidia comes in the form of the RTX 4060 Ti 16GB, which has nearly the same starting retail price these days ($420, give or take). AMD's 7700 XT easily wins in rasterization performance, beating Nvidia's GPU by up to 21%. Nvidia still keeps the ray tracing crown with a 10% lead, but DXR becomes less of a factor as we go down the performance ladder. However, Nvidia also has a sizeable lead in power use, averaging about 80W less power consumption.\n\n\n\nCompared to the previous generation RX 6700 XT, the 7700 XT delivers around 30\u201335 percent more performance overall, and costs about 26% more. That makes it the better choice, assuming your budget will stretch to $420, and it comes with the other upgrades of RDNA 3 \u2014 AV1, DP2.1, improved efficiency, and better AI performance.\n\n\n\nFurther reading:\n\nAMD Radeon RX 7700 XT review\n\nAMD Radeon RX 7800 XT review\n\n12. Nvidia GeForce RTX 4060 Ti\n\nNvidia GeForce RTX 4060 Ti Founders Edition (Image credit: Tom's Hardware)\n\n12. Nvidia GeForce RTX 4060 Ti Mainstream Nvidia Ada for under $400 Our expert review: Specifications GPU: AD106 GPU Cores: 4352 Boost Clock: 2,535 MHz Video RAM: 8GB GDDR6 18 Gbps TGP: 160 watts Today's Best Deals View at Newegg View at Amazon Check Walmart Reasons to buy + Great efficiency + Latest Nvidia architecture + Generally faster than 3060 Ti Reasons to avoid - 8GB and 128-bit bus for $399? - Less VRAM than RTX 3060 - DLSS 3 is no magic bullet\n\nWhat is this, 2016? Nvidia's RTX 4060 Ti is a $399 graphics card, with only 8GB of memory and a 128-bit interface \u2014 or alternatively, the 4060 Ti 16GB doubles the VRAM on the same 128-bit interface, for about $55 extra. We thought we had left 8GB cards in the past after the RTX 3060 gave us 12GB, but Nvidia seems more intent on cost-cutting and market segmentation these days. But the RTX 4060 Ti does technically beat the previous generation RTX 3060 Ti, by 10\u201315 percent overall in our testing.\n\n\n\nThere are plenty of reasons to waffle on this one. The larger L2 cache does mostly overcome the limited bandwidth from the 128-bit interface, but cache hit rates go down as resolution increases, meaning 1440p and especially 4K can be problematic. At least the price is the same as the outgoing RTX 3060 Ti, street prices start at $375 right now, and you do get some new features. But the 8GB still feels incredibly stingy, and the 16GB model mostly helps at 4K ultra, at which point the performance still tends to be poor.\n\n\n\nAs with the RTX 4060 above, note that Nvidia reduced the number of GPU cores on the RTX 4060 Ti compared to its RTX 3060 Ti predecessor. The 3060 Ti had 38 SMs while the 4060 Ti only has 34 SMs. Again, ~45% higher GPU clocks make up for the 11% core count deficit, but the reduction in memory interface width and GPU cores does feel incredibly stingy on Nvidia's part.\n\n\n\nThere's not nearly as much competition in the mainstream price segment as we would expect. AMD's RX 7700 XT (above) is technically up to 10% faster than the 4060 Ti overall, but it costs about 15% more. Alternatively, it's also 8% faster than the 16GB variant for basically the same price, but it's the usual mix of winning in rasterization and losing in ray tracing. AMD's next step down is either the 7600 XT, which trails the 4060 Ti by 25\u201330 percent in performance, or the previous generation RX 6700 XT / 6750 XT, which are still about 15% slower for 10% less money.\n\n\n\nLooking at performance, the 4060 Ti generally manages 1440p ultra at 60 fps in rasterization games, but for ray tracing you'll want to stick with 1080p \u2014 or use DLSS. Frame Generation is heavily used in Nvidia's marketing materials, and it can provide a significant bump to your fps. However, it's more of a frame smoothing technique as it interpolates between two frames and doesn't apply any new user input to the generated frame.\n\n\n\nUltimately, the 4060 Ti \u2014 both 8GB and 16GB variants \u2014 are worth considering, but there are better picks elsewhere in our opinion. There's a reason this ranks down near the bottom of our list: It's definitely serviceable, but there are plenty of compromises if you go this route.\n\n\n\nFurther Reading:\n\nNvidia RTX 4060 Ti review\n\nNvidia RTX 4060 Ti 16GB review\n\n13. Intel Arc A380\n\nIntel Arc A380 by Gunnir (Image credit: Gunnir)\n\nThe Arc A380 perhaps best represents Intel's dedicated GPU journey over the past year. Our initial review found a lot of problems, including game incompatibility, rendering errors, and sometimes downright awful performance. More than a year of driver updates has fixed most of those problems, and if you're looking for a potentially interesting HTPC graphics card, video codec support remains a strong point.\n\n\n\nWe've routinely seen the Arc A380 on sale for $99, and at that point, we can forgive a lot of its faults. We suggest you hold out for another \"sale\" if you want to pick one up, rather than spending more \u2014 depending on the week, the least expensive cards may cost anywhere between $99 and $119. It's very much a budget GPU, with slower performance than the RX 6500 XT, but it has 6GB of VRAM and costs less.\n\n\n\nThe Intel Arc Alchemist architecture was the first to add AV1 encoding and decoding support, and the quality of the encodes ranks right up there with Nvidia Ada, with AMD RDNA 3 trailing by a decent margin. And the A380 is just as fast at encoding as the beefier A770. Note also that we're reporting boost clocks, as in our experience that's where the GPU runs: Across our full test suite, the A380 averaged 2449 MHz on the Gunnir card that comes with a 50 MHz factory overclock. Intel's \"2050 MHz Game Clock\" is very conservative, in other words.\n\n\n\nMake no mistake that you will give up a lot of performance in getting down to this price. The A380 and RX 6500 XT trade blows in performance, but the RX 6600 ends up roughly twice as fast. It doesn't have AV1 support, though, and costs closer to $180\u2013$200. Ray tracing is also mostly a feature checkbox here, with performance in every one of our DXR tests falling below 30 fps, with the exception of Metro Exodus Enhanced that averaged 40 fps at 1080p medium.\n\n\n\nArc A380 performance might not be great, but it's still a step up from integrated graphics solutions. Just don't add an Arc card to an older PC, as it really needs ReBAR (Resizable Base Address Register) support and at least a PCIe 4.0 slot.\n\n\n\nRead: Intel Arc A380 review\n\nHow we test the best graphics cards\n\nDetermining pure graphics card performance is best done by eliminating all other bottlenecks \u2014 as much as possible, at least. Our 2024 graphics card testbed consists of a Core i9-13900K CPU, MSI Z790 MEG Ace DDR5 motherboard, 32GB G.Skill DDR5-6600 CL34 memory, and a Sabrent Rocket 4 Plus-G 4TB SSD, with a be quiet! 80 Plus Titanium PSU and a Cooler Master CPU cooler.\n\n\n\nWe test across the three most common gaming resolutions, 1080p, 1440p, and 4K, using 'medium' and 'ultra' settings at 1080p and 'ultra' at 4K. Where possible, we use 'reference' cards for all of these tests, like Nvidia's Founders Edition models and AMD's reference designs. Most midrange and lower GPUs do not have reference models, however, and in some cases we only have factory overclocked cards for testing. We do our best to select cards that are close to the reference specs in such cases.\n\n\n\nFor each graphics card, we follow the same testing procedure. We run one pass of each benchmark to \"warm up\" the GPU after launching the game, then run at least two passes at each setting/resolution combination. If the two runs are basically identical (within 0.5% or less difference), we use the faster of the two runs. If there's more than a small difference, we run the test at least twice more to determine what \"normal\" performance is supposed to be.\n\n\n\nWe also look at all the data and check for anomalies, so for example RTX 3070 Ti, RTX 3070, and RTX 3060 Ti all generally going to perform within a narrow range \u2014 3070 Ti is about 5% faster than 3070, which is about 5% faster than 3060 Ti. If we see games where there are clear outliers (i.e. performance is more than 10% higher for the cards just mentioned), we'll go back and retest whatever cards are showing the anomaly and figure out what the \"correct\" result should be.\n\n\n\nDue to the length of time required for testing each GPU, updated drivers and game patches inevitably come out that can impact performance. We periodically retest a few sample cards to verify our results are still valid, and if not, we go through and retest the affected game(s) and GPU(s). We may also add games to our test suite over the coming year, if one comes out that is popular and conducive to testing \u2014 see our what makes a good game benchmark for our selection criteria.\n\nBest graphics cards performance results\n\nOur updated test suite of games consists of 19 titles \u2014 we've added Assassin's Creed Mirage, Avatar: Frontiers of Pandora, Diablo IV, and The Last of Us, Part 1. Two of those have DXR enabled (Avatar and Diablo), and three are AMD-promoted (AC Mirage, Avatar, and The Last of Us). We also enabled Quality mode upscaling in Avatar and Diablo, as a different view of things (performance scales similarly on all GPUs thanks to upscaling, though image quality is lower with FSR2 than with XeSS or DLSS).\n\n\n\nThe data in the following charts is from testing conducted during the past several months. Only the fastest cards are tested at 1440p and 4K, but we do our best to test everything at 1080p medium and ultra. For each resolution and setting, the first chart shows the geometric mean (i.e. equal weighting) for all 19 games. The second chart shows performance in the 11 rasterization games, and the third chart focuses in on ray tracing performance in eight games. Then we have the 19 individual game charts, for those who like to see all the data.\n\n\n\nOf the 19 games, only three lack a modern temporal upscaling solution (meaning DLSS, FSR 2/3, or XeSS): Borderlands 3, Far Cry 6, and Total War: Warhammer 3. AMD's FSR has now been out for over two years now, with FSR 2.0 now having surpassed the year mark, Nvidia's DLSS 2 has been around since mid-2019, and Intel's XeSS formally launched in October 2023. 16 of the games in our test suite support DLSS 2 or later, six more support DLSS 3, ten support FSR 2 or later, and five support XeSS.\n\n\n\nWe've run nearly all the benchmarks at native resolution for these tests, except for Avatar and Diablo where we used Quality mode as appropriate. We have a separate article looking at FSR and DLSS, and the bottom line is that DLSS and XeSS improve performance with less compromise to image quality, but FSR2/3 work on any GPU.\n\n\n\nThe charts below contain all the current Nvidia RTX 40-series, AMD RX 7000-series, and Intel Arc A-series graphics cards. Our GPU benchmarks hierarchy contains additional results for those who are interested. The charts are color coded with AMD in red, Nvidia in blue, and Intel in gray to make it easier to see what's going on.\n\n\n\nThe following charts are up to date as of November 11, 2024. Nearly all current generation generation GPUs are included.\n\nBest Graphics Cards \u2014 1080p Medium\n\nImage 1 of 22 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nBest Graphics Cards \u2014 1080p Ultra\n\nImage 1 of 22 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nBest Graphics Cards \u2014 1440p Ultra\n\nImage 1 of 22 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nBest Graphics Cards \u2014 4K Ultra\n\nImage 1 of 22 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nBest Graphics Cards \u2014 Power, Clocks, and Temperatures\n\nMost of our discussion has focused on performance, but for those interested in power and other aspects of the GPUs, here are the appropriate charts.\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nImage 1 of 4 (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware) (Image credit: Tom's Hardware)\n\nChoosing among the best graphics cards\n\nWe've provided a baker's dozen of choices for the best graphics cards, recognizing that there's plenty of potential overlap. The latest generation GPUs consist of Nvidia's Ada Lovelace architecture, which improves on the previous Ampere architecture. AMD's RDNA3 architecture likewise takes over from the previous RDNA2 architecture offerings. Finally, Intel Arc Alchemist GPUs provide some competition in the budget and midrange sectors.\n\n\n\nConveniently, Arc Alchemist, RDNA2/3, and Ada/Ampere all support the same general features (DirectX 12 Ultimate and ray tracing), though Arc and RTX cards also have additional tensor core hardware.\n\n\n\nWe've listed the best graphics cards that are available right now, along with their current online prices, which we track in our GPU prices guide. With many cards now selling below MSRP, plenty of people seem ready to upgrade, and supply also looks good in general (with the exception of the 4080 and 4090 cards). Our advice: Don't pay more today for yesterday's hardware. The RTX 40-series and RX 7000-series offer better features and more bang for the buck, unless you buy a used previous generation card.\n\n\n\nIf your main goal is gaming, you can't forget about the CPU. Getting the best possible gaming GPU won't help you much if your CPU is underpowered and/or out of date. So be sure to check out the Best CPUs for Gaming page, as well as our CPU Benchmark hierarchy to make sure you have the right CPU for the level of gaming you're looking to achieve.\n\n\n\nOur current recommendations reflect the changing GPU market, factoring in all of the above details. The GPUs are ordered using subjective rankings, taking into account performance, price, features, and efficiency, so slower cards may end up higher on our list.\n\nAdditional Shopping Tips\n\nWhen buying a graphics card, consider the following:\n\n\n\n\u2022 Resolution: The more pixels you're pushing, the more performance you need. You don't need a top-of-the-line GPU to game at 1080p.\n\n\u2022 PSU: Make sure that your power supply has enough juice and the right 6-, 8- and/or 16-pin connector(s). For example, Nvidia recommends a 550-watt PSU for the RTX 3060, and you'll need at least an 8-pin connector and possibly a 6-pin PEG connector as well. Newer RTX 40-series GPUs use 16-pin connectors, though all of them also include the necessary 8-pin to 16-pin adapters.\n\n\u2022 Video Memory: A 4GB card is the absolute minimum right now, 6GB models are better, and 8GB or more is strongly recommended. A few games can now use 12GB of VRAM, though they're still the exception rather than the rule.\n\n\u2022 FreeSync or G-Sync? Either variable refresh rate technology will synchronize your GPU's frame rate with your screen's refresh rate. Nvidia supports G-Sync and G-Sync Compatible displays (for recommendations, see our Best Gaming Monitors list), while AMD's FreeSync tech works with Radeon cards.\n\n\u2022 Ray Tracing and Upscaling: The latest graphics cards support ray tracing, which can be used to enhance the visuals. DLSS provides intelligent upscaling and anti-aliasing to boost performance with similar image quality, but it's only on Nvidia RTX cards. AMD's FSR works on virtually any GPU and also provides upscaling and enhancement, but on a different subset of games. New to the party are DLSS 3 with Frame Generation and FSR 3 Frame Generation, along with Intel XeSS, with yet another different subset of supported games \u2014 DLSS 3 also provides DLSS 2 support for non 40-series RTX GPUs.\n\nFinding Discounts on the Best Graphics Cards\n\nWith the GPU shortages mostly over, you might find some particularly tasty deals on occasion. Check out the latest Newegg promo codes, Best Buy promo codes and Micro Center coupon codes.\n\nWant to comment on our best graphics picks for gaming? Let us know what you think in the Tom's Hardware Forums.\n\nMORE: HDMI vs. DisplayPort: Which Is Better For Gaming?\n\nMORE: GPU Benchmarks and Hierarchy",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "NVIDIA GeForce RTX 5080 reportedly launches January 21st",
            "link": "https://videocardz.com/newz/nvidia-geforce-rtx-5080-reportedly-launches-january-21st",
            "snippet": "The GeForce RTX 5080 graphics card will be released on January 21. This marks the first confirmed launch date for NVIDIA's Blackwell GPU aimed at gamers.",
            "score": 0.925397515296936,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Key Tech Developments Shaping 2025: Nvidia, Coreweave, Tesla, Google, and Broadcom",
            "link": "https://www.globaltrademag.com/key-tech-developments-shaping-2025-nvidia-coreweave-tesla-google-and-broadcom/",
            "snippet": "Key Tech Developments Shaping 2025: Nvidia, Coreweave, Tesla, Google, and Broadcom \u00b7 Nvidia's Upcoming CES Presentation. Related Content: \u00b7 Coreweave's IPO and...",
            "score": 0.865368664264679,
            "sentiment": null,
            "probability": null,
            "content": "IndexBox\n\n[shareaholic app=\"share_buttons\" id=\"13106399\"]\n\nThe tech sector is brimming with anticipation as 2025 approaches, with significant developments poised to reshape the landscape. According to a recent report, Nvidia (NVDA) remains a dominant player in the AI arena, closing 2024 with staggering gains nearing 200%. However, the industry is witnessing aggressive moves by competitors, eager to capture a share of this burgeoning market.\n\nRead also: AI Investment Trends Drive Tech Stocks to New Heights\n\nNvidia\u2019s Upcoming CES Presentation\n\nThe Consumer Electronics Show (CES) 2025 will host Nvidia CEO Jensen Huang as he delivers a keynote address on January 6, expected to be a pivotal moment for the company. Nvidia plans to unveil its RTX 5000 series GPUs, promising to revolutionize the gaming sector with enhanced graphics capabilities.\n\nCoreweave\u2019s IPO and Expansion Potential\n\nLooking forward, Coreweave, a prominent player in GPU-focused cloud computing, plans to launch its initial public offering in the second quarter of 2025. Backed by Nvidia and prominent figures from Apple and GitHub, Coreweave seeks a valuation exceeding $35 billion, reflecting the surging demand and expansion within the AI industry.\n\nTesla and Google Face High Stakes in 2025\n\nTesla is geared to introduce its self-driving taxi service, Cybercab, heralding a new era in autonomous transportation. Meanwhile, Google confronts potential upheaval, challenged by the U.S. Department of Justice to sell its Chrome browser due to antitrust concerns, with a verdict expected by August.\n\nBroadcom\u2019s Rise as a Major Competitor\n\nBroadcom (AVGO), now a trillion-dollar entity, emerged as a formidable contender against Nvidia. As attention shifts toward Broadcom\u2019s innovative silicon chips, the firm is well-positioned to capture market share, with widespread bullish sentiment propelling its trajectory into 2025, according to IndexBox data.\n\nSource: IndexBox Market Intelligence Platform",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "DeepSeek sparks AI stock selloff; Nvidia posts record market-cap loss",
            "link": "https://www.hawaiitribune-herald.com/2025/01/28/nation-world-news/deepseek-sparks-ai-stock-selloff-nvidia-posts-record-market-cap-loss/",
            "snippet": "NEW YORK/LONDON/SINGAPORE \u2014 Global investors dumped tech stocks on Monday as they worried that the emergence of a low-cost Chinese artificial intelligence...",
            "score": 0.8759281635284424,
            "sentiment": null,
            "probability": null,
            "content": "NEW YORK/LONDON/SINGAPORE \u2014 Global investors dumped tech stocks on Monday as they worried that the emergence of a low-cost Chinese artificial intelligence model would threaten the dominance of AI leaders like Nvidia, evaporating $593 billion of the chipmaker\u2019s market value, a record one-day loss for any company on Wall Street.\n\nLast week, Chinese startup DeepSeek launched a free AI assistant that it says uses less data at a fraction of the cost of incumbent services. By Monday, the assistant had overtaken U.S. rival ChatGPT in downloads from Apple\u2019s app store.\n\nADVERTISING\n\nThis led the tech-heavy Nasdaq to fall 3.1% on Monday. Nvidia was the Nasdaq\u2019s biggest drag, with its shares tumbling just under 17% and marking a record one-day loss in market capitalization for a Wall Street stock, according to LSEG data.\n\nNvidia\u2019s market-cap loss on Monday was more than double the previous one-day record, set by Nvidia last September.\n\nThe Nasdaq\u2019s next-biggest drag was chipmaker Broadcom Inc, which finished down 17.4%, followed by ChatGPT backer Microsoft, which fell 2.1% and then Google parent Alphabet, which ended down 4.2%.\n\nThe Philadelphia semiconductor index tumbled 9.2%, for its biggest percentage drop since March 2020 and its biggest decliner was Marvell Technology, which tumbled 19.1%.\n\nU.S. equity declines followed a selloff that started in Asia, with Japan\u2019s SoftBank Group finishing down 8.3%, and moved through Europe where ASML fell 7%.\n\n\u201cIf it\u2019s true that DeepSeek is the proverbial \u2018better mousetrap,\u2019 that could disrupt the entire AI narrative that has helped drive the markets over the last two years,\u201d said Brian Jacobsen, chief economist at Annex Wealth Management in Menomonee Falls, Wisconsin.\n\n\u201cIt could mean less demand for chips, less need for a massive build-out of power production to fuel the models, and less need for large-scale data centers.\u201d\n\nThe hype around AI has powered a huge inflow of capital into equities in the last 18 months, inflating valuations and lifting stock markets to new highs.\n\nAs recently as Wednesday, U.S. AI-related stocks had rallied sharply after President Donald Trump announced a private-sector plan for what he said would be a $500 billion investment in AI infrastructure through a joint venture known as Stargate.",
            "description": null,
            "finbert_sentiment": "negative"
        }
    },
    "2024-12-31": {
        "0": {
            "title": "NVIDIA CEO Jenson Huang\u2019s Vision for Agentic AI In 2025: The Rundown",
            "link": "https://www.cxtoday.com/customer-data-platform/nvidia-ceo-jenson-huangs-vision-for-agentic-ai-in-2025-the-rundown/",
            "snippet": "The first wave of AI Agents will take hold in 2025 and see the rise of digital workers capable of understanding tasks, planning, and taking action.",
            "score": 0.9001418352127075,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia finalizes $800m purchase of Israeli AI firm Run:ai",
            "link": "https://www.jns.org/nvidia-finalizes-800m-purchase-of-israeli-ai-firm-runai/",
            "snippet": "The delay proved fortuitous for Run:ai's founders and employees, who will receive $200 million of the acquisition price in Nvidia shares. During the regulatory...",
            "score": 0.8666513562202454,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Nvidia Completes Acquisition of AI Startup Run:ai",
            "link": "https://www.industryleadersmagazine.com/nvidia-completes-acquisition-of-ai-startup-runai/",
            "snippet": "Nvidia has finalized the $700 million acquisition of AI startup Run:ai, with plans to open-source its GPU orchestration software. The European Commission has...",
            "score": 0.9330095052719116,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has completed the acquisition of AI startup Run:ai, a software company that builds hardware to optimize AI. The chipmaker company has not disclosed the official price of the acquisition. However, some sources believe that the deal was finalized at $700 million. In April 2024, Nvidia disclosed its intent to acquire the Israeli AI startup, Run:ai.\n\nRun:ai confirmed that after the merger its software will be open-sourced, which currently works only for Nvidia. This means that Nvidia\u2019s competitors, like AMD and Intel, will be able to adjust it for their hardware. Run:ai said, \u201cWe are eager to build on the achievements we\u2019ve obtained until now, expand our talented team, and grow our product and market reach. Open sourcing the software will enable it to extend its availability to the entire AI ecosystem.\u201d\n\nNvidia Acquires Run:ai\n\nSince April 2024, Nvidia had intentions of acquiring the AI startup, Run:ai and finally the deal was closed on December 30. The deal was reported by the official sites of Run:ai and Nvidia. Run:ai mentioned in its post that the chipmaker company will open-source its software. However, the reason behind open-sourcing software has not been officially addressed by the companies.\n\nRun:ai\u2019s software was only serving Nvidia, by remotely scheduling all the GPU resources for AI in the cloud. Neither company explained why Run:ai will make its platform open source, but it\u2019s not hard to guess. Nvidia, now the top maker of AI chips, has seen its stock value soar to $3.56 trillion, making it the world\u2019s most valuable company. While that\u2019s great for Nvidia, it also makes acquisitions tricky due to antitrust rules.\n\nA Nvidia spokesperson simply said, \u201cWe\u2019re excited to welcome the Run:ai team to Nvidia.\n\nEuropean Commission Greenlights Nvidia Acquisition\n\nThe European Commission approved Nvidia to acquire Run:ai on December 19. The Commission confirmed it approved the deal without conditions and stated that the transaction would not raise any competition issues in the European Economic Area (EEA).\n\nThe case was referred to the Commission by the Italian competition authority in September, which asked the Commission to assess whether the acquisition would impact competition in the EEA, as mentioned in a press release on Thursday. Nvidia designs and provides graphic processing units (GPUs) for data centers, while Run:ai offers software that helps businesses manage their AI computing infrastructure, according to the release.\n\nThe Commission decided that the acquisition wouldn\u2019t cause competition issues because Nvidia can\u2019t block its GPUs from working with other orchestration software. Plus, customers have access to other software options that provide alternatives to Run:ai, according to the release.\n\nNvidia Stock Update\n\nNvidia outperformed the Wall Street expectations with its third-quarter earnings report released last month. The stock rally and high demand for Blackwell chips have made Nvidia one of the most valuable companies in the world. However, last few weeks the stock has been dipping as the US market saw one of its lowest trading days.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "3": {
            "title": "The AI Wave Is More Than Nvidia. Cisco, Broadcom, and More Stocks Will Benefit.",
            "link": "https://www.barrons.com/articles/ai-networking-nvidia-cisco-broadcom-arista-bce88c76",
            "snippet": "The most powerful artificial-intelligence systems need a network to work their wonders. That's a plus for Broadcom, Marvell, Cisco, and Arista.",
            "score": 0.5175275802612305,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia's $65 Billion AI Boom: The Stock That's Too Big to Ignore",
            "link": "https://finance.yahoo.com/news/nvidias-65-billion-ai-boom-180718107.html",
            "snippet": "ByteDance's $7B Deal and Blackwell Chip Innovations Put Nvidia at the Center of the AI Revolution.",
            "score": 0.7132605314254761,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NASDAQ:NVDA) is setting the stage for another massive growth spurt, fueled by insatiable global demand for AI infrastructure. Analysts are calling it the \"Apple of the AI era,\" with Nvidia's cutting-edge chips positioned as the backbone of this revolution. Retail investors seem to agree, pouring $30 billion into the stock this yearmore than any other in 2024. Even with a few bumpsexecution hiccups and looming geopolitical risksthe outlook remains dazzling. The Blackwell AI chip, hailed as the most innovative tech release of 2024, could propel Nvidia's 2025 revenues by $65 billion, with earnings per share expected to double. Simply put, Nvidia isn't just part of the AI storyit is the AI story.\n\nThe AI momentum just got another boost. ByteDance, TikTok's parent company, dropped a jaw-dropping $7 billion on Nvidia's AI chips for 2025, sidestepping U.S. export restrictions by leveraging offshore facilities. This move not only highlights Nvidia's dominance but also underscores the adaptability of global tech players under tightening regulations. ByteDance's aggressive AI push has put it ahead of its Chinese rivals, and this deal ensures Nvidia stays at the core of that growth. With major players like ByteDance in its corner, Nvidia's chips are becoming as essential to the tech world as GPUs are to gaming.\n\nLooking ahead, Nvidia's potential seems limitless. CEO Jensen Huang's upcoming keynote at CES 2025 could be the catalyst that recharges market enthusiasm, especially with analysts projecting another blockbuster year. Nvidia's dominance in the AI space is unrivaled, and while challenges remain, the company's ability to innovate and meet soaring demand keeps it the undisputed leader. For investors eyeing the future of tech, Nvidia isn't just a stock to watchit's a portfolio cornerstone.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Jetson Thor: Nvidia's hammer strike for humanoid robot dominance",
            "link": "https://interestingengineering.com/innovation/nvidia-plans-robotic-domination-2025",
            "snippet": "Jetson Thor is a compact computer meant to position Nvidia as the go-to platform for humanoid robots.",
            "score": 0.9122499227523804,
            "sentiment": null,
            "probability": null,
            "content": "Talla explained that this initiative commenced as a response to two technological breakthroughs. These include the rapid advancement of generative AI and the enhanced capabilities to train robots in increasingly sophisticated simulated settings. \u201cThe shift is a calculated move based on our technological progress and market readiness,\u201d Talla noted.\n\nBy enabling developers to exploit the full potential of generative AI and advanced simulations, Nvidia aims to become a leader in robotics.\n\nJetson Thor is destined for success in next-gen robotics\n\nDuring an interview, Talla stated that the market has reached a pivotal \u201ctipping point\u201d for physical AI and robotics. Nvidia\u2019s strategic approach does not involve direct competition with established manufacturers like Tesla and Figure AI.\n\nInstead, the company plans to provide essential OEM services to a vast network of robot makers. \u201cOur goal is to empower the hundreds of thousands of innovators in the robotics space with the tools they need to create and evolve,\u201d Talla explained at a press conference in Tokyo.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Prediction: Nvidia Will Dominate the AI Chip Market in 2025",
            "link": "https://www.fool.com/investing/2024/12/31/prediction-nvidia-will-dominate-the-ai-chip-market/",
            "snippet": "Nvidia has a massive moat that would be difficult for competitors to take down.",
            "score": 0.5029575228691101,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has a massive moat that would be difficult for competitors to take down.\n\nIn today's video, I discuss Nvidia (NVDA 5.27%), how it dominated the AI market in 2024, and why it can do the same in 2025. To learn more, check out the short video, consider subscribing, and click the special offer link below.\n\n*Stock prices used were the market prices of Dec. 27, 2024. The video was published on Dec. 29, 2024.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia GeForce RTX 5000: Everything You Need To Know",
            "link": "https://www.forbes.com/sites/antonyleather/2024/12/31/nvidia-geforce-rtx-5000-everything-you-need-to-know/",
            "snippet": "Based on this time scale, with the Nvidia GeForce RTX 5000 being announced in January 2025, reviews for the first launch wave that will likely include the RTX...",
            "score": 0.9402427673339844,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "NVIDIA GeForce RTX 5080 & RTX 5070 Ti 16 GB GPUs Featured In iBuyPower\u2019s Next-Gen Gaming PCs",
            "link": "https://wccftech.com/nvidia-geforce-rtx-5080-rtx-5070-ti-16-gb-gpus-ibuypowers-next-gen-gaming-pcs/",
            "snippet": "iBuyPower is listing at least four pre-built gaming PCs with NVIDIA's GeForce RTX 50 GPUs, mainly the RTX 5080 and the RTX 5070 Ti.",
            "score": 0.9263419508934021,
            "sentiment": null,
            "probability": null,
            "content": "NVIDIA's upcoming GeForce RTX 5080 & RTX 5070 Ti GPUs with 16 GB memory have been listed by PC manufacturer, iBuyPower.\n\niBuyPower Preps Its Next-Gen Pre-Built PC Lineup With NVIDIA GeForce RTX 5080 & RTX 5070 Ti GPUs\n\nWhile the unveiling of the NVIDIA GeForce RTX 50 \"Blackwell\" Gaming GPUs is still a few days away, we have seen several retail and online listings for the graphics cards. These GPUs will launch later in January, but it looks like everyone is excited to have them listed on the official web pages. So let's see what these pre-builts offer for the gaming community.\n\n2 of 9\n\niBuyPower is listing at least four pre-built gaming PCs with NVIDIA's GeForce RTX 50 GPUs, mainly the RTX 5080 and the RTX 5070 Ti. It's interesting to see that the 5070 Ti has been listed instead of the RTX 5070. The PCs come in both Intel and AMD options, supporting the latest chips such as the Arrow Lake \"Core Ultra 200S\" or the Zen 5 \"Ryzen 9000\" lineup. The PCs are equipped with up to 800W \"Gold\" PSUs and feature 32 GB of DDR5-5200 memory along with AIO liquid cooling.\n\nInterestingly, these models are also listed in \"SUPER\" flavors, but that could just be an error since the previous listings included 4080 and 4070 Ti SUPER models too, so whoever updated the page forgot to remove the \"SUPER\" nomenclature. We don't expect any \"SUPER\" variant for the RTX 50 series launching next year. No prices have been mentioned for either Pre-built.\n\nNVIDIA GeForce RTX 5080 16 GB Graphics Card\n\nThe NVIDIA GeForce RTX 5080 graphics card will be based on the PG144/147-SKU45 PCB and will incorporate the GB203-400-A1 GPU die. This card will utilize the full GB203 GPU die with 84 SMs and 10,752 cores, but that's a big -51% reduction compared to the RTX 5090. For comparison, the RTX 4090 and RTX 4080 had a -40 percent difference in the number of cores, so the overall performance is going to vary by a huge margin.\n\nOther than that, the RTX 5080 is also going to feature half the VRAM configuration with a 16 GB capacity running across a 256-bit bus interface while utilizing GDDR7 modules. The graphics card will feature the fastest GDDR7 memory on the market, offering up to 32 Gbps speeds for 1024 GB/s or 1 TB/s of total bandwidth. The graphics card will feature a 400W TBP. This will be a 25% increase in the power wall, but once again, the real-world figures should be very different.\n\nNVIDIA GeForce RTX 5070 Ti 16 GB Graphics Card\n\nFor the GeForce RTX 5070 Ti, NVIDIA will be leveraging the GB203-300-A1 GPU core on the PG147-SKU60 PCB. This chip is expected to feature 8960 CUDA cores, so a little less than the RTX 5080 but with the same memory specifications of 16 GB VRAM running across a 256-bit wide bus interface. It will feature slower GDDR7 speeds of 28 Gbps but will still end up offering almost 1 TB/s bandwidth (896 GB/s to be precise). The card is expected to feature a TBP of around 300-350W.\n\nNVIDIA's CES 2025 unveiling of its next-gen gaming GPU lineup is a few days away, so stay tuned for a jam-packed presentation of the GeForce RTX 50 \"Blackwell\" Gaming lineup.\n\nNVIDIA GeForce RTX 50 GPU Specs (Preliminary):\n\nGraphics Card Name NVIDIA GeForce RTX 5090 NVIDIA GeForce RTX 5080 NVIDIA GeForce RTX 5070 Ti NVIDIA GeForce RTX 5070 NVIDIA GeForce RTX 5060 Ti NVIDIA GeForce RTX 5060 NVIDIA GeForce RTX 5050 GPU Name Blackwell GB202-300 Blackwell GB203-400 Blackwell GB203-300-A1 Blackwell GB205-300-A1 Blackwell GB206-300 Blackwell GB206? Blackwell GB207-300 GPU SMs 170 (192 Full) 84 (84 Full) 70 (84 Full) 50 (50 Full) 36 (36 Full) 30 20 (20 Full) GPU Cores 21760 10752 8960 6144 4608 3840 2560 Clock Speeds 2.41 GHz 2.62 GHz 2.45 GHz 2.51 GHz TBD TBD TBD Memory Capacity 32 GB GDDR7 16 GB GDDR7 16 GB GDDR7 12 GB GDDR7 16 GB / 8 GB GDDR7 8 GB GDDR7 8 GB GDDR6 Memory Bus 512-bit 256-bit 256-bit 192-bit 128-bit 128-bit 128-bit Memory Speed 28 Gbps 30 Gbps 28 Gbps 28 Gbps 28 Gbps? 28 Gbps? TBD Bandwidth 1792 GB/s 960 GB/s 896 GB/s 672 GB/s 448 GB/s 448 GB/s TBD Power Interface 1 12V-2x6 (16-Pin) 1 12V-2x6 (16-Pin) 1 12V-2x6 (16-Pin) 1 12VHPWR (16-Pin) 1 12VHPWR (16-Pin) 1 12VHPWR (16-Pin) TBD Launch 30th January, 2025 30th January, 2025 20th February, 2025 5th March, 2025 March - April 2025 April 2025 April 2025 TBP 575W 360W 300W 250W 180W 150W 135W Price $1999 US $999 US $749 US $549 US $449-$399? $299? $249-$199?\n\nNews Source: Momomo_US",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia Acquires Run:ai for $700 Million",
            "link": "https://autogpt.net/nvidia-acquires-runai-for-700-million/",
            "snippet": "Nvidia, unarguably the world's largest tech company has acquired Isreali AI company, Run:ai, for $700 Million.",
            "score": 0.9361850023269653,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia, the tech giant synonymous with great advancements in AI and graphics, has finalized its acquisition of Israeli startup Run:ai. The deal, reportedly valued at $700 million, is a huge stride forward in the AI hardware infrastructure space. But beyond the numbers, what does this mean for the industry, and why should you care?\n\nRun:ai: The Startup That Caught Nvidia\u2019s Eye\n\nFounded in Israel, Run:ai has gained recognition for its innovative approach to managing and optimizing AI hardware infrastructure. In simpler terms, the company builds software that helps organizations maximize the efficiency of their AI systems.\n\nRun:ai\u2019s technology is already tightly integrated with Nvidia\u2019s GPUs, which dominate the AI market. By acquiring Run:ai, Nvidia is poised to bolster its dominance in the field while addressing a growing demand for streamlined AI infrastructure solutions.\n\nOpening the Doors to Competition\n\nOne of the most surprising announcements tied to the acquisition is Run:ai\u2019s decision to open source its software. This means that competitors like AMD and Intel can adapt the technology for their hardware.\n\nWhy does this matter?\n\nLeveling the playing field : By making the software accessible to others, Nvidia demonstrates confidence in its own products while fostering innovation across the industry.\n\n: By making the software accessible to others, Nvidia demonstrates confidence in its own products while fostering innovation across the industry. Broader adoption: Open-sourcing allows more organizations to leverage Run:ai\u2019s capabilities, driving advancements in AI systems globally.\n\nRegulatory Hurdles\n\nThe acquisition wasn\u2019t without its challenges. Nvidia announced its intent to purchase Run:ai in April, but regulatory bodies like the European Commission and the U.S. Department of Justice launched investigations to ensure the deal wouldn\u2019t stifle competition.\n\nWhy the scrutiny? Nvidia\u2019s commanding presence in the AI hardware space raised concerns about potential monopolistic behavior. However, after months of review, the European Commission gave its approval in December, clearing the final obstacle.\n\nNvidia\u2019s Vision for the Future\n\nExpanding the team : Run:ai plans to grow its talented workforce, bringing fresh perspectives to the table.\n\n: Run:ai plans to grow its talented workforce, bringing fresh perspectives to the table. Enhancing product reach : By opening up its software to the entire AI ecosystem, Run:ai aims to create solutions that benefit a wider range of users.\n\n: By opening up its software to the entire AI ecosystem, Run:ai aims to create solutions that benefit a wider range of users. Shaping the future of AI: Nvidia\u2019s resources combined with Run:ai\u2019s expertise could lead to groundbreaking developments in AI infrastructure.\n\nWhat This Means for You:",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    },
    "2024-12-30": {
        "0": {
            "title": "Everything you need to know about Nvidia",
            "link": "https://www.itpro.com/business/business-strategy/everything-you-need-to-know-about-nvidia",
            "snippet": "Nvidia's holistic approach ensures that customers receive not only powerful and innovative technologies but also environmentally conscious solutions. The...",
            "score": 0.9441671371459961,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia is a leader in the technology industry, renowned for its ground-breaking work in graphics processing units (GPUs) and its role in advancing artificial intelligence (AI) and high-performance computing.\n\nHeadquartered in Santa Clara, California, Nvidia's innovations have made a substantial impact across diverse sectors, from gaming and professional visualization to data centers and automotive technology. Its cutting-edge AI platforms and technologies have been instrumental in driving AI adoption across multiple industries, establishing Nvidia as a key force in the ongoing AI revolution.\n\nHistory of the company\n\nQuick Facts about Nvidia (Image credit: Getty Images) Founded: April 5, 1993 Founders: Jensen Huang (pictured), Chris Malachowsky, and Curtis Priem Current CEO: Jensen Huang Headquarters: Santa Clara, California, USA Annual Revenue: $60.9 billion (2023) Number of Employees: Approximately 29,600 Market Capitalisation: Over $3.44 trillion as of 2024\n\nNvidia was established in 1993 with a mission to bring advanced graphics to mainstream computing. Initially focused on 3D graphics for gaming and multimedia, the company made its first major inroad in 1999 with the release of the GeForce 256, recognized as the world\u2019s first GPU. This invention not only transformed gaming graphics but also solidified Nvidia\u2019s place in the industry as a leader in visual computing.\n\nOver the following years, Nvidia expanded its focus from consumer graphics to professional applications, laying the groundwork for future innovations. In 2006, Nvidia introduced the CUDA (Compute Unified Device Architecture) platform, enabling developers to use GPUs for a wide range of computational tasks beyond graphics. This was a game-changer for high-performance computing, scientific research, and engineering, providing the power needed for complex simulations, data processing, and research in fields like genomics and weather forecasting.\n\nAs AI technology began to surge, Nvidia was well-positioned to capitalize on the trend, becoming a vital player in the AI and data center sectors. The company launched its DGX systems, designed specifically for AI research, providing researchers and organizations with the computational power to advance deep learning and machine learning models. Additionally, Nvidia's Omniverse platform has brought new capabilities to industries in need of 3D design and simulation, such as architecture, automotive, and media, offering collaborative, real-time environments for complex virtual worlds and digital twins.\n\nIn recent years, Nvidia's impact has reached unprecedented levels. The company has leveraged its expertise in AI and machine learning to drive innovations across diverse industries, from healthcare and autonomous vehicles to cloud computing.\n\nIn 2024, Nvidia achieved significant milestones, notably surpassing Apple to become the world's most valuable company by market capitalization, reaching approximately $3.44 trillion. This surge was driven by increased demand for its AI chips from tech giants like Meta and Alphabet. Additionally, Nvidia addressed a design flaw in its Blackwell AI chips, collaborating with TSMC to resolve production delays and maintain its leadership in AI hardware.\n\nGet the ITPro. daily newsletter Sign up today and you will receive a free copy of our Focus Report 2025 - the leading guidance on AI, cybersecurity and other IT challenges as per 700+ senior executives Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nWhat does Nvidia sell?\n\n(Image credit: Nvidia)\n\nNvidia's product offerings span a wide range of industries, supporting everything from gaming and professional visualization to AI, data centers, and autonomous vehicles.\n\nNvidia is particularly dominant in the gaming industry; its GeForce GPUs are at the heart of PC gaming systems, delivering high-performance graphics that power immersive experiences. The company is estimated to hold approximately 88% of the GPU market, significantly outpacing its closest rival AMD. The Nvidia Shield series extends the company's reach into home entertainment, offering devices that combine streaming, gaming, and smart home capabilities. For cloud gaming, Nvidia provides GeForce Now, a subscription-based service that enables gamers to play high-quality games on virtually any device via cloud streaming.\n\nFor professionals in media, entertainment, and design, Nvidia offers its Quadro GPUs, known for their precision and reliability in 3D modeling, animation, and high-end visualization tasks.\n\nIn the data center sector, Nvidia has become a powerhouse with its AI-focused GPUs, particularly the A100 Tensor Core GPU, which supports demanding workloads in machine learning and inference. These GPUs are widely adopted by companies looking to scale AI operations and enable deep learning applications, ranging from natural language processing to image recognition.\n\nNvidia has also made significant advancements in the automotive industry, where its Nvidia DRIVE platform provides end-to-end solutions for autonomous vehicle development. DRIVE integrates powerful AI computing with sensor data processing, enabling advanced driver assistance systems (ADAS) and supporting fully autonomous driving features. This platform is central to Nvidia's vision of an AI-driven future for automotive safety and efficiency.\n\nIn addition to hardware, Nvidia offers platforms that streamline AI and machine learning development, such as the Jetson platform, designed for edge AI and robotics. Jetson provides compact, energy-efficient modules that support AI-driven applications at the edge, making it ideal for autonomous machines, drones, and IoT devices. Nvidia\u2019s product ecosystem also includes Omniverse, a collaboration platform for 3D design and simulation, allowing professionals across industries to work together in real time on complex digital projects. Through its comprehensive range of hardware, software, and cloud-based services, Nvidia is positioned as a key technology provider for industries undergoing rapid digital transformation.\n\nNvidia\u2019s mergers and acquisitions\n\nTo bolster its technological capabilities, Nvidia has made several strategic acquisitions.\n\nIn 2020, the company acquired Mellanox Technologies, enhancing its data center offerings.\n\nNvidia acquired DeepMap in 2021 to improve mapping solutions for autonomous driving, and in 2022, it strengthened its AI and high-performance computing capabilities by acquiring Bright Computing.\n\nAlthough Nvidia's proposed acquisition of ARM Holdings was terminated in 2022 due to regulatory challenges, it underscored the company's ambition to expand its influence in the semiconductor industry.\n\nIn 2024, Nvidia continued its strategic expansion through several notable acquisitions aimed at enhancing its capabilities in artificial intelligence (AI) and cloud computing.\n\nIn April 2024, Nvidia announced its agreement to acquire Run, an Israeli startup specializing in AI workload management and orchestration software. This acquisition is intended to optimize AI computing resources for customers, facilitating more efficient deployment and management of AI workloads across various infrastructures.\n\nIn July 2024, Nvidia acquired Brev.dev, a San Francisco-based startup that provides a development platform for building, training, and deploying AI and machine learning models on cloud-based GPU instances. This acquisition aims to simplify access to GPU resources across multiple cloud service providers, thereby enhancing Nvidia's cloud AI offerings.\n\nIn September 2024, Nvidia acquired OctoAI, a Seattle-based startup specializing in tools for building and running generative AI models more efficiently. This acquisition is expected to bolster Nvidia's capabilities in AI model deployment and optimization, further strengthening its position in the AI industry.\n\nKey figures at Nvidia\n\nJensen Huang co-founded Nvidia and serves as its president and CEO. He has been with the company since its beginning and continues to guide its strategic direction and innovation efforts.\n\nColette Kress holds the position of executive vice president and chief financial officer. Her responsibilities include managing Nvidia's financial strategies and overseeing the company's financial health and growth.\n\nTim Teter is the executive vice president, general counsel, and secretary. His role involves overseeing Nvidia's legal and regulatory matters within the tech industry.\n\nJay Puri is the executive vice president of worldwide field operations. His responsibilities include managing global sales and marketing efforts, which contribute to Nvidia's international expansion and market presence.\n\nWhat can customers expect from doing business with Nvidia?\n\n(Image credit: Getty Images)\n\nCustomers engaging with Nvidia can expect access to cutting-edge technology that consistently drives innovation. Nvidia's products, renowned for high performance and reliability, cater to a wide range of applications, from gaming to professional visualisation and AI development.\n\nThe company's GPUs, like the GeForce and Quadro series, are industry leaders in providing superior graphics and computational power, making them indispensable tools for both gamers and professionals who require advanced computing capabilities.\n\nRELATED WHITEPAPER (Image credit: Getty Images/Josep Lago) Discover how AI can help you stay ahead of the competition\n\nIn addition to performance, Nvidia's commitment to sustainability is a cornerstone of its business strategy. The company strives to create efficient computing solutions that minimise environmental impact. For instance, Nvidia\u2019s data center products, such as the A100 Tensor Core GPU, are designed to deliver solid AI training and inference performance while optimising energy consumption. This focus on energy efficiency not only supports the environmental goals of its customers but also reduces operational costs, making Nvidia\u2019s products economically advantageous as well.\n\nNvidia's holistic approach ensures that customers receive not only powerful and innovative technologies but also environmentally conscious solutions. The company's initiatives in sustainable computing, combined with its relentless pursuit of innovation, position Nvidia as a leader in the tech industry. This blend of high performance, reliability, and sustainability makes Nvidia a preferred partner for businesses and individuals looking to harness the latest advancements in technology.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "1": {
            "title": "Nvidia to open-source Run:ai, the software it acquired for $700M to help companies manage GPUs for AI",
            "link": "https://venturebeat.com/ai/nvidia-acquires-software-maker-runai-to-orchestrate-gpu-clouds-for-ai/",
            "snippet": "Nvidia has completed its acquisition of Run:ai, a software company that makes it easier for customers to orchestrate GPU clouds for AI.",
            "score": 0.6830945014953613,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "2": {
            "title": "Research Galore From 2024: Recapping AI Advancements in 3D Simulation, Climate Science and Audio Engineering",
            "link": "https://blogs.nvidia.com/blog/ai-research-2024/",
            "snippet": "The pace of technology innovation has accelerated in the past year, most dramatically in AI. And in 2024, there was no better place to be a part of creating...",
            "score": 0.5554365515708923,
            "sentiment": null,
            "probability": null,
            "content": "The pace of technology innovation has accelerated in the past year, most dramatically in AI. And in 2024, there was no better place to be a part of creating those breakthroughs than NVIDIA Research.\n\nNVIDIA Research is comprised of hundreds of extremely bright people pushing the frontiers of knowledge, not just in AI, but across many areas of technology.\n\nIn the past year, NVIDIA Research laid the groundwork for future improvements in GPU performance with major research discoveries in circuits, memory architecture and sparse arithmetic. The team\u2019s invention of novel graphics techniques continues to raise the bar for real-time rendering. And we developed new methods for improving the efficiency of AI \u2014 requiring less energy, taking fewer GPU cycles and delivering even better results.\n\nBut the most exciting developments of the year have been in generative AI.\n\nWe\u2019re now able to generate, not just images and text, but 3D models, music and sounds. We\u2019re also developing better control over what is generated: to generate realistic humanoid motion and to generate sequences of images with consistent subjects.\n\nThe application of generative AI to science has resulted in high-resolution weather forecasts that are more accurate than conventional numerical weather models. AI models have given us the ability to accurately predict how blood glucose levels respond to different foods. Embodied generative AI is being used to develop autonomous vehicles and robots.\n\nAnd that was just this year. What follows is a deeper dive into some of NVIDIA Research\u2019s greatest generative AI work in 2024. Of course, we continue to develop new models and methods for AI, and expect even more exciting results next year.\n\nConsiStory: AI-Generated Images With Main Character Energy\n\nConsiStory, a collaboration between researchers at NVIDIA and Tel Aviv University, makes it easier to generate multiple images with a consistent main character \u2014 an essential capability for storytelling use cases such as illustrating a comic strip or developing a storyboard.\n\nThe researchers\u2019 approach introduced a technique called subject-driven shared attention, which reduces the time it takes to generate consistent imagery from 13 minutes to around 30 seconds.\n\nRead the ConsiStory paper.\n\nEdify 3D: Generative AI Enters a New Dimension\n\nNVIDIA Edify 3D is a foundation model that enables developers and content creators to quickly generate 3D objects that can be used to prototype ideas and populate virtual worlds.\n\nEdify 3D helps creators quickly ideate, lay out and conceptualize immersive environments with AI-generated assets. Novice and experienced content creators can use text and image prompts to harness the model, which is now part of the NVIDIA Edify multimodal architecture for developing visual generative AI.\n\nRead the Edify 3D paper and watch the video on YouTube.\n\nFugatto: Flexible AI Sound Machine for Music, Voices and More\n\nA team of NVIDIA researchers recently unveiled Fugatto, a foundational generative AI model that can create or transform any mix of music, voices and sounds based on text or audio prompts.\n\nThe model can, for example, create music snippets based on text prompts, add or remove instruments from existing songs, modify the accent or emotion in a voice recording, or generate completely novel sounds. It could be used by music producers, ad agencies, video game developers or creators of language learning tools.\n\nRead the Fugatto paper.\n\nGluFormer: AI Predicts Blood Sugar Levels Four Years Out\n\nResearchers from the Weizmann Institute of Science, Tel Aviv-based startup Pheno.AI and NVIDIA led the development of GluFormer, an AI model that can predict an individual\u2019s future glucose levels and other health metrics based on past glucose monitoring data.\n\nThe researchers showed that, after adding dietary intake data into the model, GluFormer can also predict how a person\u2019s glucose levels will respond to specific foods and dietary changes, enabling precision nutrition. The research team validated GluFormer across 15 other datasets and found it generalizes well to predict health outcomes for other groups, including those with prediabetes, type 1 and type 2 diabetes, gestational diabetes and obesity.\n\nRead the GluFormer paper.\n\nLATTE3D: Enabling Near-Instant Generation, From Text to 3D Shape\n\nAnother 3D generator released by NVIDIA Research this year is LATTE3D, which converts text prompts into 3D representations within a second \u2014 like a speedy, virtual 3D printer. Crafted in a popular format used for standard rendering applications, the generated shapes can be easily served up in virtual environments for developing video games, ad campaigns, design projects or virtual training grounds for robotics.\n\nRead the LATTE3D paper.\n\nMaskedMimic: Reconstructing Realistic Movement for Humanoid Robots\n\nTo advance the development of humanoid robots, NVIDIA researchers introduced MaskedMimic, an AI framework that applies inpainting \u2014 the process of reconstructing complete data from an incomplete, or masked, view \u2014 to descriptions of motion.\n\nGiven partial information, such as a text description of movement, or head and hand position data from a virtual reality headset, MaskedMimic can fill in the blanks to infer full-body motion. It\u2019s become part of NVIDIA Project GR00T, a research initiative to accelerate humanoid robot development.\n\nRead the MaskedMimic paper.\n\nStormCast: Boosting Weather Prediction, Climate Simulation\n\nIn the field of climate science, NVIDIA Research announced StormCast, a generative AI model for emulating atmospheric dynamics. While other machine learning models trained on global data have a spatial resolution of about 30 kilometers and a temporal resolution of six hours, StormCast achieves a 3-kilometer, hourly scale.\n\nThe researchers trained StormCast on approximately three-and-a-half years of NOAA climate data from the central U.S. When applied with precipitation radars, StormCast offers forecasts with lead times of up to six hours that are up to 10% more accurate than the U.S. National Oceanic and Atmospheric Administration\u2019s state-of-the-art 3-kilometer regional weather prediction model.\n\nRead the StormCast paper, written in collaboration with researchers from Lawrence Berkeley National Laboratory and the University of Washington.\n\nNVIDIA Research Sets Records in AI, Autonomous Vehicles, Robotics\n\nThrough 2024, models that originated in NVIDIA Research set records across benchmarks for AI training and inference, route optimization, autonomous driving and more.\n\nNVIDIA cuOpt, an optimization AI microservice used for logistics improvements, has 23 world-record benchmarks. The NVIDIA Blackwell platform demonstrated world-class performance on MLPerf industry benchmarks for AI training and inference.\n\nIn the field of autonomous vehicles, Hydra-MDP, an end-to-end autonomous driving framework by NVIDIA Research, achieved first place on the End-To-End Driving at Scale track of the Autonomous Grand Challenge at CVPR 2024.\n\nIn robotics, FoundationPose, a unified foundation model for 6D object pose estimation and tracking, obtained first place on the BOP leaderboard for model-based pose estimation of unseen objects.\n\nLearn more about NVIDIA Research, which has hundreds of scientists and engineers worldwide. NVIDIA Research teams are focused on topics including AI, computer graphics, computer vision, self-driving cars and robotics.",
            "description": null,
            "finbert_sentiment": "positive"
        },
        "3": {
            "title": "Nvidia closes $700 mln Run:ai acquisition after regulatory hurdles",
            "link": "https://www.reuters.com/markets/deals/nvidia-closes-700-mln-runai-acquisition-after-regulatory-hurdles-2024-12-30/",
            "snippet": "Chipmaker Nvidia has completed its acquisition of Israeli AI firm Run:ai, the startup said on Monday, following antitrust scrutiny over the buyout.",
            "score": 0.5304905772209167,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "4": {
            "title": "Nvidia Completes $700 Million Acquisition of AI Startup Run:ai",
            "link": "https://finance.yahoo.com/news/nvidia-completes-700-million-acquisition-151816718.html",
            "snippet": "Nvidia (NVDA, Financials) completed its $700 million purchase of Israeli AI startup Run:ai, a business focused on optimizing artificial intelligence...",
            "score": 0.8696224093437195,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia (NVDA, Financials) completed its $700 million purchase of Israeli AI startup Run:ai, a business focused on optimizing artificial intelligence workloads. The purchase is intended to improve Nvidia's AI-oriented products and increase its capacity to maximize GPU resource consumption in demanding computing scenarios.\n\nThe deal coincides with Nvidia's ongoing leadership in artificial intelligence technologies becoming more solidified. Run:ai is well-known for its own software, which makes virtualization and orchestration of AI workloads possiblea necessary ability for effective GPU allocation. Nvidia wants to include this technology into its DGX Cloud system so that it may provide business users more options for managing artificial intelligence workload.\n\nEarly this month, regulatory obstacles were removed when the European Commission decided the merger would not hinder competition in the AI and GPU sectors and gave unqualified clearance for it. The U.S. Department of Justice, looking at Nvidia's impact in the artificial intelligence sector, also scrutinized the purchase.\n\nAs Nvidia keeps extending its portfolio of AI-driven products, its acquisition approach fits with its more general goals to control the AI ecosystem. These include hardware, software, and cloud-based artificial intelligence systems. The financial specifics of how Nvidia intends to integrate Run:ai's activities into its larger architecture are still unknown.\n\nThe purchase is seen as a calculated action to maintain Nvidia's competitive advantage as demand for artificial intelligence technology rises in several sectors.\n\nThis article first appeared on GuruFocus.\n\n",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "5": {
            "title": "Nvidia vs. AMD: Which Is the Better AI Chip Stock for 2025?",
            "link": "https://www.fool.com/investing/2024/12/30/nvidia-vs-amd-which-is-better-ai-chip-stock-2025/",
            "snippet": "In 2024, Nvidia's stock has been the clear winner, trading up by about 175% as of this writing. AMD stock, meanwhile, is down by about 15% on the year.",
            "score": 0.8472306132316589,
            "sentiment": null,
            "probability": null,
            "content": "Graphic processing units (GPUs) serve as a key component in the foundation of the world's artificial intelligence (AI) infrastructure build-out. Training AI models and running AI inference demands high-speed processing power, and it creates computational workloads that can best be handled using parallel processing. That positions GPUs -- originally developed to speed up the rendering of video game graphics -- as some of the best hardware options for providing the right type of processing power.\n\nTwo companies dominate the GPU market. Nvidia (NVDA 5.27%) is by far the leader in the space. Rival Advanced Micro Devices (AMD 2.92%), meanwhile, is trying to take Nvidia on and gain a meaningful share of the data center market. In 2024, Nvidia's stock has been the clear winner, trading up by about 175% as of this writing. AMD stock, meanwhile, is down by about 15% on the year.\n\nBut which stock is likely to perform better in 2025?\n\nThe GPU market keeps growing\n\nThe outlook for the GPU market continues to look strong. Most large hyperscalers (companies that operate mega-sized data centers) have already indicated that they plan to increase their AI-related capital expenditures in 2025. Demand for cloud computing is soaring due to AI, and all the major cloud computing companies are working to expand their infrastructure to meet that demand.\n\nMeanwhile, GPU clusters are growing larger and larger as big tech companies and well-funded start-ups such as OpenAI and Elon Musk-owned xAI race to develop more sophisticated AI models.\n\nIt is expected that Meta Platforms will use 160,000 GPUs to train its upcoming Llama 4 model -- 10 times as many as it used for Llama 3. The case is similar with xAi's Grok 3 model, which is expected to require 100,000, up from 20,000 used to train Grok 2. Meanwhile, there is talk of companies deploying clusters with as many as 1 million GPUs in the near future.\n\nNvidia has been the biggest beneficiary of these companies' insatiable demand for GPUs, and its revenue has far outpaced AMD's. A big reason for this is that Nvidia long ago developed its free (but proprietary) CUDA software platform, which allows developers to program the GPUs they buy for tasks other than graphics rendering. As such, CUDA became the software platform on which many developers learned to program GPUs, creating a wide moat for the company. In the years since, Nvidia has added developer tools and AI-specific microlibraries through CUDA X, extending its software lead.\n\nToday, AMD has its own GPU software platform, and it makes GPUs that are just as powerful as Nvidia's, if not more so -- at least, on paper. However, testing by independent research and analysis company SemiAnalysis has concluded that AMD's software is holding back the performance of its GPUs. In its report, it called AMD's out-of-the-box experience \"unusable,\" and said that it needed \"multiple teams of AMD engineers\" to help it fix software bugs. In the end, the paper specs for AMD's latest GPU did not match its real-world performance. By contrast, SemiAnalysis described the out-of-the-box performance of Nvidia's H100 and H200 GPUs as \"amazing.\"\n\nThis helps explain why Nvidia generated data center revenue of $30.8 billion last quarter compared to only $3.5 billion for AMD. Notably, though, both showed similar rates of data center revenue growth: Nvidia's rose by 112% and AMD's rose by 122%. Of course, Nvidia was doubling its revenue from a much larger base, which makes its performance all the more impressive.\n\nOne area in which AMD has been able to carve out a niche for itself is AI inference. SemiAnalysis noted that AMD's customers tend to deploy its GPUs in inference, which presents narrow, well-defined use cases. As such, one way AMD could start to gain market share in the coming years is if more of the GPU market shifts from training to inference. AMD's GPUs are cheaper than Nvidia's, so such a shift could potentially benefit it.\n\nValuation and verdict\n\nFrom a valuation perspective, AMD is a modestly cheaper stock, trading at a forward price-to-earnings (P/E) ratio of 24 compared to nearly 31 for Nvidia. However, Nvidia has been growing its total revenue more rapidly (94% vs. 18% last quarter) as at AMD, GPUs are just one piece of a larger hardware portfolio.\n\nAt this point, I think AI training will remain important over the next several years as companies continue to try to develop more advanced AI models. As such, I much prefer Nvidia's stock as a holding from here and think investors could still profitably add it to their portfolios in 2025.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "6": {
            "title": "Why is Nvidia open sourcing Run:ai, having just acquired it?",
            "link": "https://www.techzine.eu/news/infrastructure/127459/why-is-nvidia-open-sourcing-runai-having-just-acquired-it/",
            "snippet": "After some wrangling, Nvidia received approval to acquire Israeli-based Run:ai. Now the deal is officially done. Nvidia also plans to make Run:ai's software...",
            "score": 0.9294418692588806,
            "sentiment": null,
            "probability": null,
            "content": "Recently, after some wrangling, Nvidia received approval to acquire Israeli-based Run:ai. Now the deal is officially done. Nvidia also plans to make Run:ai\u2019s software open-source. Why is it doing that?\n\nAccording to insiders, Nvidia paid $700 million for Run:ai, software maker targeting GPU cluster orchestration for AI workloads. The combination is an obvious one: via Run:ai Dev, and the Run:ai API, Control Plane and Cluster Engine, AI workloads can be controlled with a fine comb.\n\nThe software currently works only on Nvidia GPUs, as is often the case for enterprise AI software. After all, Nvidia\u2019s AI chips are completely dominant in this area, with only TPUs on Google Cloud and a curiosity cabinet of specialized accelerators providing some variety. The vast majority of the AI ecosystem, however, revolves around CUDA, the architecture behind Nvidia. It just so happens that Run:ai and Nvidia aim to change that.\n\nConditional approval?\n\nThe European Union was the only regulatory body to resist Nvidia\u2019s Run:ai takeover. It\u2019s not clear if anything came of its initial doubts. \u201cConcrete risks to competition\u201d were raised within EU member state Italy, but the Commission ruled over a week ago that there were no major concerns, having further investigated the matter as it normally does.\n\nThe approval in and of itself is unsurprising: optimizing Nvidia GPUs can be done in many ways, meaning Run:ai is one of many. Cloud GPU specialists like CoreWeave receive extensive support from Nvidia to maximize their compute for end customers. This is also regularly done based on Ethernet connectivity, not just on Nvidia\u2019s own InfiniBand standard. It goes to show that Nvidia is perfectly willing to play nice with potential rivals as things stand.\n\nStill, it seems that some skepticism from regulators needed to be answered. Now that Run:ai is officially part of Nvidia, the acquired party\u2019s plan is to open-source its own software. \u201cWhile Run:ai currently supports only NVIDIA GPUs, open sourcing the software will enable it to extend its availability to the entire AI ecosystem.\u201d\n\nTrickier than it looks\n\nThe statement above essentially means that those who want it can tie their own AMD and Intel support to the Run:ai stack, can do so at their leisure. That\u2019s happening more often these days, usually to make AMD\u2019s equivalent of CUDA, ROCm, deployable for large-scale AI workloads. As the only serious alternative to Nvidia, AMD Instinct GPUs come with support with some workarounds.\n\n\u201cThe reality is that people want to write at higher levels of abstraction,\u201d AMD SVP of AI Vamsi Boppana told The Register. Consider PyTorch, which also provides AMD and Intel with AI frameworks. However, this level of support isn\u2019t as deeply ingrained as it seems, with various plugins and other regularly used tooling missing in action if not on an Nvidia chip. Optimizations for Nvidia (and only Nvidia) are still common among popular AI tooling. From his own experience, James Wang, Creative Ventures General Partner, calls past CUDA alternatives a \u201cpain in the ass.\u201d He compares Nvidia\u2019s control over the AI stack to Apple\u2019s dominance over its own ecosystem. Anyone who takes a look at the historical level of support for Android versions of iOS apps in the early years of the smartphone revolution knows how stark that contrast can be. GPU optimization now for AI workloads is now in a similar phase, with Nvidia being as Apple-like as it can dare to be.\n\nThis means that open-sourcing a product like Run:ai poses no problems for Nvidia. It\u2019s a good move not only because it objectively increases choice for developers. Above all, it\u2019s a repeat of what happened with previous AI tooling: the optimization for Nvidia has long since taken place, now it\u2019s up to the alternatives to build a real ecosystem that can compete with it. Run:ai itself, at least, has not seen that need.\n\nAlso read: EU approves Nvidia\u2019s acquisition of Run:ai",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "7": {
            "title": "Nvidia completes acquisition of AI infrastructure startup Run:ai",
            "link": "https://techcrunch.com/2024/12/30/nvidia-completes-acquisition-of-ai-infrastructure-startup-runai/",
            "snippet": "Nvidia has completed its acquisition of Run:ai, an Israeli startup that helps manage and optimize AI hardware infrastructure.",
            "score": 0.9107251167297363,
            "sentiment": null,
            "probability": null,
            "content": "In Brief\n\nNvidia has completed its acquisition of Run:ai, an Israeli startup that helps manage and optimize AI hardware infrastructure.\n\nAs part of the merger, Run:ai said its software, which currently only works with Nvidia products, will be open sourced, meaning Nvidia rivals like AMD and Intel will be able to adapt it for their hardware.\n\n\u201cWe are eager to build on the achievements we\u2019ve obtained until now, expand our talented team, and grow our product and market reach,\u201d Run:ai told Bloomberg in a statement. \u201cOpen sourcing the software will enable it to extend its availability to the entire AI ecosystem.\u201d\n\nNvidia announced its intent to acquire Run:ai in April. At the time, sources told TechCrunch that the price tag was $700 million. But the deal ran into regulatory hurdles. The European Commission and U.S. Department of Justice launched separate investigations into whether Nvidia\u2019s purchase would harm competition.\n\nThe European Commission approved the deal in December.",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "8": {
            "title": "Nvidia completes $700M Run:ai acquisition following antitrust scrutiny",
            "link": "https://siliconangle.com/2024/12/30/nvidia-completes-700m-runai-acquisition-following-antitrust-scrutiny/",
            "snippet": "Nvidia completes $700M Run:ai acquisition following antitrust scrutiny ... Nvidia Corp. has completed its acquisition of Run:ai, a startup with software that...",
            "score": 0.589090883731842,
            "sentiment": null,
            "probability": null,
            "content": "",
            "description": null,
            "finbert_sentiment": "neutral"
        },
        "9": {
            "title": "Nvidia finalizes acquisition of AI software firm Run:ai, takes software open source \u2013 company reportedly cost $700 million",
            "link": "https://www.tomshardware.com/tech-industry/artificial-intelligence/nvidia-finalizes-acquisition-of-ai-software-firm-run-ai-takes-software-open-source-company-reportedly-cost-usd700-million",
            "snippet": "Nvidia's acquisition of Run:ai has finally gone through after the GPU company announced its intention to buy the software firm in April.",
            "score": 0.8865845203399658,
            "sentiment": null,
            "probability": null,
            "content": "Nvidia has finally completed its acquisition of Run:ai, which has been in the works since April when the GPU giant announced it wanted to purchase the AI software company. Run:ai offers GPU orchestration software to organize groups of GPUs more efficiently and has been a close partner with Nvidia since 2020; in fact, the only GPUs Run:ai currently supports are those made by Nvidia. The company was founded in 2018, around the same time Nvidia started dipping its toes into artificial intelligence with technologies like deep learning supersampling. The company says it will open-source its software.\n\nNeither company has confirmed the deal's worth, but Nvidia reportedly spent around $700 million to acquire Run:ai.\n\nGiven that Nvidia has been so crucial for the AI industry with respect to hardware, it\u2019s not surprising that the green giant is also emphasizing software, which represents vertical integration for Nvidia\u2019s business. However, the GPU company already has a pretty good grip on software, thanks to its closed-source CUDA software, which has been around since 2007. CUDA has been a major advantage for Nvidia thanks to its entrenchment in the industry, and open-source competitors like AMD\u2019s ROCm have struggled to find an opening.\n\nThe Run:ai acquisition could have increased the size of Nvidia\u2019s walled garden even further, but Run:ai founders Omri Geller and Ronen Dar say that won\u2019t be happening.\n\n\u201cTrue to our open-platform philosophy, as part of NVIDIA, we will keep empowering AI teams with the freedom to choose the tools, platforms, and frameworks that best suit their needs,\u201d a press release written by the duo reads. \u201cWe will continue to strengthen our partnerships and work alongside the ecosystem to deliver a wide variety of AI solutions and platform choices.\u201d\n\nThe two say they also plan on making Run:ai\u2019s software open source, but no hard details were given with respect to what that would actually look like and when it would happen.\n\nWhy Nvidia would make Run:ai open source only after it completed the acquisition is unclear. It might have been a condition set by Run:ai itself since Geller and Dar cite their \u201copen-platform philosophy\u201d in their joint statement. Open-sourcing the software might also have helped convince regulators to allow the acquisition to go through, as Nvidia faced heavy scrutiny in both the US and the EU.\n\nStay On the Cutting Edge: Get the Tom's Hardware Newsletter Get Tom's Hardware's best news and in-depth reviews, straight to your inbox. Contact me with news and offers from other Future brands Receive email from us on behalf of our trusted partners or sponsors\n\nHowever, there might be a caveat to open-source Run:ai software: It isn\u2019t clear whether Nvidia will offer expanded services with open source Run:ai software.\n\nNo matter how it goes, however, it seems Nvidia has expanded its influence in AI even further. That could pressure AMD and Intel, the two main rivals of Nvidia in the GPU space, to respond in some way, perhaps with their own acquisitions or partnerships.",
            "description": null,
            "finbert_sentiment": "neutral"
        }
    }
}